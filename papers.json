[
    {
        "title": "Gender imbalance in doctoral education: an analysis of the Spanish university system (1977–2021)",
        "implementation_urls": [],
        "doi": "10.1007/s11192-023-04648-y",
        "arxiv": null,
        "abstract": "AbstractDoctoral education is a key feature of university systems, as well as a basic foundation of scientific practice. That period culminates in a dissertation and examination of the candi‑date that has been studied from several points of view. This paper reports the results of an analysis on the evolution and characteristics of gender imbalance of a complete doctoral system for a wide period of time. Data from the database Teseo was used in order to iden‑tify the individuals involved in the process, the scientific fields in which the dissertations where classified, and the institutions in which the examination took place. Results: the Spanish system shows a clear evolution towards gender balance, but also some concern‑ing trends that are worth tracking. Seemingly, STEM disciplines look to be evolving more slowly than other branches of science in several aspects. A leaky pipeline is characterized in this system around the roles of supervisors, candidates, members and chairs of the dis‑sertation committees. Gender assortativity is also studied and described, and its possible effects discussed around the academic relations that surround doctoral examination.Keywords  Gender imbalance · STEM · Dissertations · Teseo · Leaky pipeline · Gender assortativity *\t Rodrigo Sánchez‑Jiménez \t rodsanch@ucm.es1\t Library and Information Science Department, SCImago Group, Universidad Complutense de Madrid, Madrid, Spain2\t Library and Information Science Department (Internet Medialab Research Group), Universidad Complutense de Madrid, Madrid, Spain3\t Sales Engineering EMEA, Neo4j, London, UK4\t Ontology Engineering Group (Artificial Intelligence Department), Universidad Politécnica de Madrid, Madrid, Spain5\t Library and Information Science Department, Universidad Complutense de Madrid, Madrid, Spainhttp://orcid.org/0000-0002-3685-7060http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-023-04648-y&domain=pdf2578\t Scientometrics (2023) 128:2577–25991 3IntroductionGender imbalance and gender bias in science have been studied and described for a long time. Zuckerman and Cole (1975) already described this issue in quantitative terms and advanced the “principle of triple penalty” (cultural inappropriateness/perceived incompe‑tence/direct discrimination). Shiebinger (1987) went over the very abundant literature on the history of women in science and described how at least the number of female scientists was growing faster (low numbers having been associated with that cultural inappropriate‑ness) but the perception of a lesser competence by women (women were systematically employed in less prestigious jobs in the academia) and blatant discrimination (unjustified salary gaps were huge) was ever persistent. Etzkowitz, et al (1992) moved on to put the focus on the de-genderization of science and society, and on the existence of “different gender styles of scientific work”, an idea that has been one way or another behind many studies comparing output, collaboration, and impact of men and women. Bordons et  al. (2003) acknowledge this factor as a warning to interpret their SCI-based results but take it a step further. They also explained the cumulative advantage of achieving high ranks in academia over productivity, which in turn accounts for the gender differences in pro‑ductivity. Several years later, Lariviere et al., (2011) reached a somewhat different conclu‑sion, finding again that gender differences were present in terms of production and funding, although the nature of these differences was complex. The subject is therefore very much open to debate, and the focus on its study has varied significantly over time (Tomassini, ",
        "publication_date": "2023-02-11",
        "authors": "Rodrigo Sánchez Jiménez, Iuliana Botezán, Jesús Barrasa-Rodríguez, Mari Carmen Suárez-Figueroa, Manuel Blázquez Ochando",
        "file_name": "10!1007%s11192-023-04648-y.pdf",
        "file_path": "output/PDFs/10!1007%s11192-023-04648-y.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s11192-023-04648-y.pdf"
    },
    {
        "title": "Social Services Diagnosis by Deep Learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-94649-8_38",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-07-04",
        "authors": "Emilio Serrano, Pedro del Pozo-Jiménez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Using the SPAR Ontology Network to Represent the Scientific Production of a University: A Case Study",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-16187-3_20",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Mariela Tapia-León, Janneth Chicaiza, Paola Espinoza-Arias, Idafen Santana-Pérez, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Caracterización de riesgos urbanos en prensa aplicando minería de texto para el enriquecimiento de datos abiertos",
        "implementation_urls": [],
        "doi": "10.22201/iibi.24488321xe.2022.91.58538",
        "arxiv": null,
        "abstract": "AbstractNews is freely spread and widely available to Internet users much more easily than traditional media. In the news, we can find an infinite number of hidden “minor data,” that can provide valuable information not col-lected in other sources of information. In this context, we have been interested in analyzing and characteriz-ing the urban risks contained in the Uruguayan open newspapers using text mining techniques. This pro-posal makes it possible to create a news corpus based on risk events included in open data. The corpus cov-ers 2003-2019 and is built from the digital open news-papers El Eco Digital, Montevideo Portal, and La Red 21. Various text mining techniques are applied to this corpus using the QDA-MinerLite software and the Python language (concretely, through the Scattertext library) to identify, characterize, and discover insights on these events. The corpus processing results help en-rich the existing open data on risks in Uruguay, incor-porating information on their effects, actors, and asso-ciated interventions.Keywords: Urban Risk; Text Mining; Open Digi-tal Newspapers; Open DataCARACTERIZACIÓN DE RIESGOS URBANOS EN PRENSA APLICANDO MINERÍA...87DOI: http://dx.doi.org/10.22201/iibi.24488321xe.2022.9",
        "publication_date": "2022-05-09",
        "authors": "Luis M. Vilches‐Blázquez, Diana Comesaña",
        "file_name": "10!22201%iibi!24488321xe!2022!91!58538.pdf",
        "file_path": "output/PDFs/10!22201%iibi!24488321xe!2022!91!58538.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards the Definition of a Language-Independent Mapping Template for Knowledge Graph Creation (short paper).",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "ABSTRACTThe use of knowledge graphs is spreading in the scientific commu-nity across different domains, from social sciences to biomedicine.The creation of knowledge graphs usually needs the integrationof multiple heterogeneous data sources in different formats andschemas. One common way to achieve this process is using declara-tive mappings, which establish the relationships between the sourcedata and the ontology, improving relevant aspects such as main-tainability, readability and understandability. Learning how to useand create mappings is not an easy task, hindering the use of thistechnology to anyone outside the area. As a result, this task is usu-ally carried out by experts. To ease the mapping creation, severalmapping editors have been developed, but their success is limited.In this paper, we devise the use of a well-known tool commonlyused in the scientific community, the spreadsheets, to specify themapping rules in a language-independent way. Our aim is to easethe mapping creation and make it more accessible for the commu-nity. We also show a real use case, in which using spreadsheetshelps in the mapping creation process and enables a handy way forediting and visualizing mapping rules.CCS CONCEPTS•Computingmethodologies→Artificial intelligence; Knowl-edge representation and reasoning.KEYWORDSKnowledge graph, spreadsheet, declarative mapping1 INTRODUCTIONThe expansion of the Semantic Web technologies has reached usersacross several domains, such as legal and biomedical. An increasingnumber of knowledge graphs from these areas are being created,restructuring knowledge in a machine-readable way [4]. For theirconstruction it is necessary to integrate different data sources; thenthey allow search optimization and the possibility of applying ma-chine learning techniques to obtain new knowledge, among otherpossibilities. Some examples are DBpedia [1] and Wikidata [18].There are multiple approaches to create knowledge graphs, fromusing ad-hoc tools to declarative mappings. The later defines rulesCopyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).to establish relationships between the global schema and the datasources. Examples of mappings languages are the W3C recommen-dation R2RML [7] and its extension RML [9].The use of declarative mappings for semantic web non-expertsis often complicated. That is one of the reasons why the mappingcreation is usually carried out by knowledge engineers. This posesa barrier for potential users from other domains. To face this issue,several mapping editors have been proposed. They aim at makingthe mapping creation and editing easier and more intuitive [11, 16].Despite these efforts, users prefer to use tools like OpenRefine1,which is non-declarative, thus hindering the reproducibility andmaintainability of the transformations performed.",
        "publication_date": "2019-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho",
        "file_name": "20250514081943.pdf",
        "file_path": "output/PDFs/20250514081943.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short3.pdf"
    },
    {
        "title": "Characterizing water quality datasets through multi-dimensional knowledge graphs: a case study of the Bogota river basin",
        "implementation_urls": [],
        "doi": "10.2166/hydro.2022.070",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-02-23",
        "authors": "Juan D. Rondón Díaz, Luis M. Vilches‐Blázquez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "RML-star : a declarative mapping language for RDF-star generation",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. RDF-star was recently proposed as a convenient represen-tation to annotate statements in RDF with metadata by introducingthe so-called RDF-star triples, bridging the gap between RDF and prop-erty graphs. However, even though there are many solutions to generateRDF graphs, there is no systematic approach so far to generate RDF-star graphs from heterogeneous data sources. In this paper, we proposeRML-star, an extension of the RML mapping language to generate RDF-star. We introduce the extension of the RML ontology and the associatedspecification with representative examples.URL: https://w3id.org/kg-construct/rml-starKeywords: RML · R2RML · RDF-star · Knowledge Graphs.1 IntroductionRDF-star was proposed as a compact representation to annotate statements inRDF with metadata [4]. For instance, the following declares that Bob claims Al-ice was born in 1996: :bob :claims <<:alice :birthYear 1996>>. Followingthe uptake of the proposed solution, a W3C Community Group was formed3 anda W3C Draft Report [5] was recently released with improvements over the orig-inal proposal. By now, several RDF-related programming libraries, e.g., EclipseRDF4J, Apache Jena, RDF.rb, and N3.js, and RDF graph database systems,e.g., Blazegraph, AnzoGraph, Stardog and GraphDB, have adopted RDF-star4.However, no mapping language supports the generation of RDF-star graphsso far. Most data are still heterogeneous, represented in different formats (e.g.,relational databases, CSV, JSON, or XML). One of the most common approachesnowadays to integrate them into RDF graphs is the use of declarative mappingCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.w3.org/community/rdf-dev/4 https://blog.liu.se/olafhartig/https://orcid.org/0000-0001-9521-2185https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-2138-7972https://w3id.org/kg-construct/rml-starhttps://www.w3.org/community/rdf-dev/https://blog.liu.se/olafhartig/Delva et al.Fig. 1: The RML-star extension (Chowlk notation [3]). Orange classes and darkorange object properties show the additions to the RML ontology, light orangeobject properties represent extensions (i.e., change in domain and/or range).languages such as R2RML [1] and RML [2]. R2RML is the W3C Recommenda-tion mapping language to generate RDF graphs from relational databases. RMLis a superset of R2RML that generates RDF graphs from data formats beyondrelational databases, such as CSV, JSON, or XML. Extending a mapping lan-guage to specify how RDF-star datasets can be generated from heterogeneousdata sources can potentially increase the amount of available RDF-star datasetsand, thus, foster the adoption of the RDF-star proposal.In this paper, we propose RML-star, an extension of RML to generate RDF-star graphs from heterogeneous data sources. We introduce a set of new classes",
        "publication_date": "2021-01-01",
        "authors": "Thomas Delva, Julián Arenas-Guerrero, Ana Iglesias-Molina, Óscar Corcho, David Chaves-Fraga, Anastasia Dimou",
        "file_name": "20250514081947.pdf",
        "file_path": "output/PDFs/20250514081947.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper374.pdf"
    },
    {
        "title": "Supervising Attention in an E-Learning System",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-01746-0_46",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-11-04",
        "authors": "Dalila Durães, Javier Bajo, Paulo Nováis",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "publication_date": "2021-07-19",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": "output/PDFs/10!23919%annsim52504!2021!9552126.pdf",
        "pdf_link": null
    },
    {
        "title": "Depicting Vocabulary Summaries with Devos",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3587359",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-28",
        "authors": "Ahmad Alobaid, Jhon Toledo, Óscar Corcho, María Poveda‐Villalón",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Re-Construction Impact on Metadata Representation Models",
        "implementation_urls": [],
        "doi": "10.1145/3587259.3627554",
        "arxiv": null,
        "abstract": "ABSTRACTReification in knowledge graphs has been present since the incep-tion of RDF to allow capturing additional information in triples,usually metadata. The need of adopting or changing a metadatarepresentation in a pre-existing graph to enhance the knowledgecapture and access can lead to inducing complex structural changesin the graph, according the target representation’s schema. In thesesituations, it is necessary to decide whether to construct the knowl-edge graph again from its original sources, or to re-construct itusing the current version of the graph. In this paper we conduct anempirical study to analyze which re-construction approach is moresuitable for switching the representation approach from the createdgraph ensuring that the additional represented knowledge is pre-served. We study four well-known metadata representations, usingmapping languages to construct the graph, and SPARQL CONSTRUCTqueries to re-construct it. With this work we aim to provide insightsabout the impact of re-construction on metadata representationsinteroperability and the implications of different approaches.CCS CONCEPTS• Information systems→ResourceDescription Framework (RDF);Graph-based database models; Data exchange; Data model exten-sions;KEYWORDSKnowledge Graphs, Metadata, SPARQL, Declarative Mappings.ACM Reference Format:Ana Iglesias-Molina, Jhon Toledo, Oscar Corcho, and David Chaves-Fraga.2023. Re-Construction Impact onMetadata RepresentationModels. InKnowl-edge Capture Conference 2023 (K-CAP ’23), December 5–7, 2023, Pensacola, FL,USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3587259.3627554Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.K-CAP ’23, December 5–7, 2023, Pensacola, FL, USA© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0141-2/23/12. . . $15.00https://doi.org/10.1145/3587259.36275541 INTRODUCTIONKnowledge graphs (KG) have gained popularity in recent years forintegrating and publishing knowledge on the web [22]. KGs aremodelled according to schemes that are not immutable, as theyare subject to modifications triggered by changes in the domain ofknowledge or in the consumption needs of downstream tasks. Anexample is the need to incorporate additional knowledge about atriple, which is known as statement reification. Reification is usuallyused to include metadata and provenance to existing triples [15].",
        "publication_date": "2023-11-29",
        "authors": "Ana Iglesias-Molina, Jhon Toledo, Óscar Corcho, David Chaves-Fraga",
        "file_name": "10!1145%3587259!3627554.pdf",
        "file_path": "output/PDFs/10!1145%3587259!3627554.pdf",
        "pdf_link": null
    },
    {
        "title": "Declarative generation of RDF-star graphs from heterogeneous data",
        "implementation_urls": [],
        "doi": "10.3233/sw-243602",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2024-03-20",
        "authors": "Julián Arenas-Guerrero, Ana Iglesias-Molina, David Chaves-Fraga, Daniel Garijo, Óscar Corcho, Anastasia Dimou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "DLT-Based Personal Data Access Control with Key-Redistribution",
        "implementation_urls": [],
        "doi": "10.1109/bcca58897.2023.10338895",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-10-24",
        "authors": "Fadi Barbàra, Mirko Zichichi, Stefano Ferretti, Claudio Schifanella",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Decentralized Health Data Distribution: A DLT-based Architecture for Data Protection",
        "implementation_urls": [],
        "doi": "10.1109/blockchain55522.2022.00023",
        "arxiv": null,
        "abstract": "Abstract—The management, protection and sharing of sensitivedata such as those associated with the health domain are crucialin enabling personal care and contributing to worldwide medicaladvancements. Distributed Ledger Technologies (DLTs) allow fordata protection compliant solutions in untrusted contexts thatguarantee data immutability, protection and transparency whenneeded. This paper proposes an architecture based on DLTs,Smart Contracts and Distributed File Storage (DFS), enablinguser data sovereignty, confidentiality and secure access control.A use case on health data is presented, where we apply acombination of DLT, DFS and an access control mechanism toallow users to distribute their data. Finally, we show an ex-perimental evaluation of the overall architecture to demonstratethe feasibility of implementing practical DLT-based healthcaresolutions. The results are collected through independent tests,available opensource, that verify the system’s response time ineach of its functions and as the load increases. The results arepromising and show that the system is feasible and can scale asthe load increases.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed StorageI. INTRODUCTIONDigital technologies are continuously transforming society.Personal devices are foundational to this transformation, whereindividuals are the primary sources of information generation.Storing data in inaccessible and disconnected data lakes makesthem inaccessible to the public for innovation [1]. Followingthis, the interest in data ownership arises first and foremostfrom the lack of transparency in how data is collected, storedand used by different services and companies. In this regard, alow effort has been spent on easing the data management foran individual to understand and manage the risks associatedwith exploiting his private data.Healthcare could enormously benefit from the ability toshare information, transitioning from centralized to decentral-ized system architectures. Individuals can make a substantialcontribution through personal devices to science, specificallyin personalized medicine [2], [3]. Unfortunately, there arestrong barriers represented by privacy and security for thehealth sector: sharing information without the individual’sexplicit consent constitutes a substantial violation of an in-dividual’s rights. Regulations such as the European Union’sThis work has received funding from Regione Marche with DDPF n. 1189and from the EU’s Horizon 2020 research and innovation programme underthe MSCA ITN grant agreement No 814177 LAST-JD-RIoE.General Data Protection Regulation (GDPR) [4] help promotea pro-individual view. Specifically, these regulations imposemany accountability measures on actors responsible for pro-cessing personal data and assign several rights to individuals.However, these do not always address the lack of transparency",
        "publication_date": "2022-08-01",
        "authors": "Gioele Bigini, Mirko Zichichi, Emanuele Lattanzi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%blockchain55522!2022!00023.pdf",
        "file_path": "output/PDFs/10!1109%blockchain55522!2022!00023.pdf",
        "pdf_link": null
    },
    {
        "title": "EBOCA: Evidences for BiOmedical Concepts Association Ontology",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-17105-5_11",
        "arxiv": "2208.01093",
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Andrea Álvarez-Pérez, Ana Iglesias-Molina, Lucía Prieto Santamaría, María Poveda‐Villalón, Carlos Badenes-Olmedo, Alejandro Rodríguez‐González",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "VocEditor - An Integrated Environment to Visually Edit, Validate and Versioning RDF Vocabularies",
        "implementation_urls": [],
        "doi": "10.1109/icsc50631.2021.9475916",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "André Valdestilhas, Gustavo Publio, Andrea Cimmino Arriaga, Thomas Riechert",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Defying Wikidata: Validation of terminological relations in the web of data",
        "implementation_urls": [],
        "doi": "10.13025/rev8-z474",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-05-16",
        "authors": "Patricia Martín-Chozas, Sina Ahmadi, Elena Montiel-Ponsoda",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "CORAL: A Corpus of Ontological Requirements Annotated with Lexico-Syntactic Patterns",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-21348-0_29",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Alba Fernández-Izquierdo, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "LACLICHEV: Exploring the History of Climate Change in Latin America Within Newspapers Digital Collections",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-85082-1_11",
        "arxiv": "2105.00792",
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Genoveva Vargas‐Solar, José-Luis Zechinelli-Martini, Javier A. Espinosa-Oviedo, Luis M. Vilches‐Blázquez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "SAD Generator: Eating Our Own Dog Food to Generate KGs and Websites for Academic Events",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-32327-1_19",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Pieter Heyvaert, David Chaves-Fraga, Freddy Priyatna, Juan Sequeda, Anastasia Dimou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2009.10263",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Juan A. Cabrera, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda",
        "file_name": "arxiv.2009.10263",
        "file_path": "10.48550/arxiv.2009.10263",
        "pdf_link": null
    },
    {
        "title": "Interpretable machine learning models for predicting and explaining vehicle fuel consumption anomalies",
        "implementation_urls": [],
        "doi": "10.1016/j.engappai.2022.105222",
        "arxiv": "2010.16051",
        "abstract": "AbstractIdentifying anomalies in the fuel consumption of the vehiclesof a fleet is a crucial aspect for optimizing consumption andreduce costs. However, this information alone is insufficient,since fleet operators need to know the causes behind anoma-lous fuel consumption.We combine unsupervised anomaly detection techniques, do-main knowledge and interpretable Machine Learning modelsfor explaining potential causes of abnormal fuel consumptionin terms of feature relevance. The explanations are used forgenerating recommendations about fuel optimization, that areadjusted according to two different user profiles: fleet man-agers and fleet operators.Results are evaluated over real-world data from telematics de-vices connected to diesel and petrol vehicles from differenttypes of industrial fleets. We measure the proposal regardingmodel performance, and using Explainable AI metrics thatcompare the explanations in terms of representativeness, fi-delity, stability, contrastiveness and consistency with aprioribeliefs. The potential fuel reductions that can be achieved isround 35%.Keywords Explainable Artificial Intelligence. Fuel Con-sumption. Anomaly Detection. Explainable Boosting Ma-chine. Feature Relevance. Generalized Additive Models.1. IntroductionCombining Advanced Analytics techniques together withIoT (Internet of Things) data offers many possibilities tofind and extract relevant insights for business decisions. AtTelefónica, for instance, we see how the union of MachineLearning (ML) with IoT data helps to create new use casesfor the Fleet Management Industry. An example of it is theusage of ML for anomaly detection of the fuel consumptionof vehicles. For a fleet manager, it is very useful to be able tofind which vehicles are having an abnormal fuel consump-tion, since it is crucial for optimizing costs.*Corresponding author at Telefónica, 28050 Madrid, Spain. E-mail address: alberto.barbadogonzalez@telefonica.com (A. Bar-bado)Copyright © 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.However, detecting which vehicles have an anomalousfuel consumption alone is not enough. Only providing thatinformation leads to more questions than answers. Why arethe vehicles consuming that extra amount of fuel? Howcould it be reduced?. These questions are not answered by abinary output that indicates which consumption are anoma-lous and which ones are not.This is one of the reasons why Explainable AI (XAI) isrelevant: it enhances that initial information with differenttypes of explanations, helping to answer those questions that",
        "publication_date": "2022-07-30",
        "authors": "Alberto Barbado, Óscar Corcho",
        "file_name": "10!1016%j!engappai!2022!105222.pdf",
        "file_path": "output/PDFs/10!1016%j!engappai!2022!105222.pdf",
        "pdf_link": null
    },
    {
        "title": "MAS: A Corpus of Tweets for Marketing in Spanish",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-98192-5_53",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel, Idafen Santana-Pérez, Alba Fernández-Izquierdo, Alberto Sánchez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Linked Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "KerA: Scalable Data Ingestion for Stream Processing",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00152",
        "arxiv": null,
        "abstract": "Abstract—Big Data applications are increasingly moving frombatch-oriented execution models to stream-based models thatenable them to extract valuable insights close to real-time. Tosupport this model, an essential part of the streaming processingpipeline is data ingestion, i.e., the collection of data from vari-ous sources (sensors, NoSQL stores, filesystems, etc.) and theirdelivery for processing. Data ingestion needs to support highthroughput, low latency and must scale to a large number of bothdata producers and consumers. Since the overall performanceof the whole stream processing pipeline is limited by that ofthe ingestion phase, it is critical to satisfy these performancegoals. However, state-of-art data ingestion systems such as ApacheKafka build on static stream partitioning and offset-based recordaccess, trading performance for design simplicity. In this paperwe propose KerA, a data ingestion framework that alleviate thelimitations of state-of-art thanks to a dynamic partitioning schemeand to lightweight indexing, thereby improving throughput,latency and scalability. Experimental evaluations show that KerAoutperforms Kafka up to 4x for ingestion throughput and up to5x for the overall stream processing throughput. Furthermore,they show that KerA is capable of delivering data fast enough tosaturate the big data engine acting as the consumer.Keywords—Stream processing, dynamic partitioning, ingestion.I. INTRODUCTIONBig Data real-time stream processing typically relies onmessage broker solutions that decouple data sources from ap-plications. This translates into a three-stage pipeline describedin Figure 1. First, in the production phase, event sources (e.g.,smart devices, sensors, etc.) continuously generate streams ofrecords. Second, in the ingestion phase, these records are ac-quired, partitioned and pre-processed to facilitate consumption.Finally, in the processing phase, Big Data engines consume thestream records using a pull-based model.Since users are interested in obtaining results as soon aspossible, there is a need to minimize the end-to-end latencyof the three stage pipeline. This is a non-trivial challengewhen records arrive at a fast rate and create the need tosupport a high throughput at the same time. To this purpose,Big Data engines are typically designed to scale to a largenumber of simultaneous consumers, which enables processingfor millions of records per second [1], [2]. Thus, the weaklink of the three stage pipeline is the ingestion phase: it needsto acquire records with a high throughput from the producers,serve the consumers with a high throughput, scale to a largenumber of producers and consumers, and minimize the writelatency of the producers and, respectively, the read latency ofthe consumers to facilitate low end-to-end latency.Fig. 1. Stream processing pipeline: records are collected at event timeand made available to consumers earliest at ingestion time, after the eventsare acknowledged by producers; processing engines continuously pull these",
        "publication_date": "2018-07-01",
        "authors": "Ovidiu-Cristian Marcu, Alexandru Costan, Gabriel Antoniu, Marı́a S. Pérez, Bogdan Nicolae, Radu Tudoran, Stefano Bortoli",
        "file_name": "10!1109%icdcs!2018!00152.pdf",
        "file_path": "output/PDFs/10!1109%icdcs!2018!00152.pdf",
        "pdf_link": null
    },
    {
        "title": "International Conferences on Software Engineering and Knowledge Engineering",
        "implementation_urls": [],
        "doi": "10.18293/seke2020",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-08-18",
        "authors": "",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Context aware ontology‐based hybrid intelligent framework for vehicle driver categorization",
        "implementation_urls": [],
        "doi": "10.1002/ett.3729",
        "arxiv": null,
        "abstract": "Castro Abstract— In public vehicles, one of the major concerns is driver’s level of expertise for its direct proportionality to safety of passengers. So before a driver is subjected to certain type of vehicle, he should be thoroughly evaluated and categorized with respect to certain parameters instead of only one-time metric of having driving license. These aspects may be driver’s expertise, vigilance, aptitude, experience years, cognition, driving style, formal education, terrain, region, minor violations, major accidents and age group etc. The purpose of this categorization is to ascertain suitability of a driver for certain vehicle type(s) to ensure passengers’ safety. Currently, no driver categorization technique fully comprehends the implicit as well as explicit characteristics of drivers dynamically. In this paper, machine learning based dynamic and adaptive technique named D-CHAIT (Driver Categorization based on Hybrid Artificial Intelligence Techniques) is proposed for driver categorization with an objective focus on driver’s attributes modeled in DriverOntology. A supervised mode of learning has been employed on a labeled dataset, having diverse profiles of drivers with attributes pertinent to drivers’ perspectives of demographics, behaviors, expertise and inclinations. A comparative analysis of D-CHAIT with three other machine learning techniques (Fuzzy Logic, Case Based Reasoning, and Artificial Neural Networks) is also presented. The efficacy of all techniques was empirically measured while categorizing the drivers based on their profiles through metrics of accuracy, precision, recall, f-measure performance and associated costs. These empirical quantifications assert D-CHAIT as a better technique than contemporary ones. The novelty of proposed technique is signified through preprocessing of feature attributes, quality of data, training of machine learning model on more relevant data and adaptivity. Index Terms— Artificial Neural Networks, Case Based Reasoning, Vehicle Driver Categorization, Fuzzy Logic, Machine Learning  I. INTRODUCTIONThe foremost concern of any transportation authority would be assuring the safety of passengers using their facility. Adoption of modern technologies in transportation has not  This work is submitted on 30 Nov 2018.  We hereby acknowledge the funds support under the project PrivSoft sponsored by Higher Education Commission under the grant number:112116-Eg043.  Dr Sohail Sarwar is with the Department of Computing University of Gujrat Pakistan (e-mail: sohail.sarwar@seecs.edu.pk).  Saad Zia is with National University of Computing and Emerging Sciences (NUCES)-FAST, Pakistan (emai: Saad.zia@nu.edu.pk) Dr Zia Ul Qayyum is working with Allama Iqbal Open University Islamabad, UK (email: vc@uog.edu.pk) Dr Muddessar is working with London South Bank University, UK (m.iqbal@lsbu.ac.uk, m.iqbal@essex.ac.uk) . He is also working as Visiting ",
        "publication_date": "2019-08-29",
        "authors": "Sohail Sarwar, Saad Zia, Zia Ul Qayyum, Muddesar Iqbal, Muhammad Safyan, Shahid Mumtaz, Raúl García‐Castro, Kostromitin Konstantin Igorevich",
        "file_name": "10!1002%ett!3729.pdf",
        "file_path": "output/PDFs/10!1002%ett!3729.pdf",
        "pdf_link": null
    },
    {
        "title": "Government plans in the 2016 and 2021 Peruvian presidential elections: A natural language processing analysis of the health chapters",
        "implementation_urls": [],
        "doi": "10.12688/wellcomeopenres.16867.5",
        "arxiv": null,
        "abstract": "AbstractWe describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections ofdiscrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which eachitem of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, inturn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context oftext modeling, the topic probabilities provide an explicit representation of a document. We presentefficient approximate inference techniques based on variational methods and an EM algorithm forempirical Bayes parameter estimation. We report results in document modeling, text classification,and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSImodel.1. IntroductionIn this paper we consider the problem of modeling text corpora and other collections of discretedata. The goal is to find short descriptions of the members of a collection that enable efficientprocessing of large collections while preserving the essential statistical relationships that are usefulfor basic tasks such as classification, novelty detection, summarization, and similarity and relevancejudgments.Significant progress has been made on this problem by researchers in the field of informa-tion retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999). The basic methodology proposed byIR researchers for text corpora—a methodology successfully deployed in modern Internet searchengines—reduces each document in the corpus to a vector of real numbers, each of which repre-sents ratios of counts. In the popular tf-idf scheme (Salton and McGill, 1983), a basic vocabularyof “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of thenumber of occurrences of each word. After suitable normalization, this term frequency count iscompared to an inverse document frequency count, which measures the number of occurrences of ac©2003 David M. Blei, Andrew Y. Ng and Michael I. Jordan.BLEI, NG, AND JORDANword in the entire corpus (generally on a log scale, and again suitably normalized). The end resultis a term-by-document matrix X whose columns contain the tf-idf values for each of the documentsin the corpus. Thus the tf-idf scheme reduces documents of arbitrary length to fixed-length lists ofnumbers.While the tf-idf reduction has some appealing features—notably in its basic identification of setsof words that are discriminative for documents in the collection—the approach also provides a rela-tively small amount of reduction in description length and reveals little in the way of inter- or intra-document statistical structure. To address these shortcomings, IR researchers have proposed severalother dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwesteret al., 1990). LSI uses a singular value decomposition of the X matrix to identify a linear subspacein the space of tf-idf features that captures most of the variance in the collection. This approach canachieve significant compression in large collections. Furthermore, Deerwester et al. argue that thederived features of LSI, which are linear combinations of the original tf-idf features, can capturesome aspects of basic linguistic notions such as synonymy and polysemy.To substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it isuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI torecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generativemodel of text, however, it is not clear why one should adopt the LSI methodology—one can attemptto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.A significant step forward in this regard was made by Hofmann (1999), who presented theprobabilistic LSI (pLSI) model, also known as the aspect model, as an alternative to LSI. The pLSIapproach, which we describe in detail in Section 4.3, models each word in a document as a samplefrom a mixture model, where the mixture components are multinomial random variables that can beviewed as representations of “topics.” Thus each word is generated from a single topic, and different",
        "publication_date": "2022-10-25",
        "authors": "Rodrigo M. Carrillo‐Larco, Manuel Castillo-Cara, Jesús Lovón-Melgarejo",
        "file_name": "10!12688%wellcomeopenres!16867!5.pdf",
        "file_path": "output/PDFs/10!12688%wellcomeopenres!16867!5.pdf",
        "pdf_link": null
    },
    {
        "title": "Supporting Demand-Response strategies with the DELTA ontology",
        "implementation_urls": [],
        "doi": "10.1109/aiccsa53542.2021.9686935",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-11-01",
        "authors": "Alba Fernández-Izquierdo, Andrea Cimmino, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine",
        "implementation_urls": [],
        "doi": "10.3390/app131910778",
        "arxiv": null,
        "abstract": "Abstract: Due to the success of artificial intelligence (AI) applications in the medical field overthe past decade, concerns about the explainability of these systems have increased. The reliabilityrequirements of black-box algorithms for making decisions affecting patients pose a challenge evenbeyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integratingexplainability into these systems. While most traditional AI methods and expert systems are in-herently interpretable, the recent literature has focused primarily on explainability techniques formore complex models such as deep learning. This scoping review critically analyzes the existingliterature regarding the explainability and interpretability of AI methods within the clinical do-main. It offers a comprehensive overview of past and current research trends with the objective ofidentifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) inthe field of medicine. Such constraints encompass the diverse requirements of key stakeholders,including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition,the absence of standardised evaluation criteria, the potential for mistaking explanations for causalrelationships, and the apparent trade-off between model accuracy and interpretability. Furthermore,this review discusses possible research directions aimed at surmounting these challenges. Theseinclude alternative approaches to leveraging medical expertise to enhance interpretability withinclinical settings, such as data fusion techniques and interdisciplinary assessments throughout thedevelopment process, emphasizing the relevance of taking into account the needs of final users todesign trustable explainability methods.Keywords: artificial intelligence; medicine; explainable AI; interpretable AI1. Introduction1.1. AI in Medicine: Opportunities and ChallengesToday’s Artificial Intelligence (AI), with its capability to automate and ease almost anykind of task, frequently appearing to surpass human performance, has become a popularand widespread technology for many applications, especially over the last decade, thanksto advances in deep learning (DL), with clinical healthcare being no exception.Medicine has been one of the most challenging, but also most attention-getting appli-cation fields for AI for the past five decades, with diagnostic decision support, the interpre-tation of medical images and clinical lab tests, drug development, patient management,and others all demonstrating the broad and diverse scope of AI techniques applied tomedical issues.AI methods have promised a range of potential advantages for medical informaticssystems. Automating burdensome tasks can be of great help, alleviating clinicians fromunnecessary efforts and allowing them to focus on more important issues surroundingAppl. Sci. 2023, 13, 10778. https://doi.org/10.3390/app131910778 https://www.mdpi.com/journal/applscihttps://doi.org/10.3390/app131910778https://doi.org/10.3390/app131910778https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0002-1215-3333https://doi.org/10.3390/app131910778https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app131910778?type=check_update&version=1Appl. Sci. 2023, 13, 10778 2 of 23patient care. AI systems can perform these tasks with high precision and efficiency, and also,they can assist the extraction of relevant information from the large quantities of databeing produced by modern medicine [1]. AI systems might be particularly beneficial in",
        "publication_date": "2023-09-28",
        "authors": "Raquel González-Alday, Esteban García-Cuesta, Casimir A. Kulikowski, Víctor Maojo",
        "file_name": "20250514082215.pdf",
        "file_path": "output/PDFs/20250514082215.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/13/19/10778/pdf?version=1695880594"
    },
    {
        "title": "A Context-Aware Indoor Air Quality System for Sudden Infant Death Syndrome Prevention",
        "implementation_urls": [],
        "doi": "10.3390/s18030757",
        "arxiv": null,
        "abstract": "Abstract: Context-aware monitoring systems designed for e-Health solutions and ambient assistedliving (AAL) play an important role in today’s personalized health-care services. The majority of thesesystems are intended for the monitoring of patients’ vital signs by means of bio-sensors. At present,there are very few systems that monitor environmental conditions and air quality in the homes ofusers. A home’s environmental conditions can have a significant influence on the state of the healthof its residents. Monitoring the environment is the key to preventing possible diseases caused byconditions that do not favor health. This paper presents a context-aware system that monitors airquality to prevent a specific health problem at home. The aim of this system is to reduce the incidenceof the Sudden Infant Death Syndrome, which is triggered mainly by environmental factors. In theconducted case study, the system monitored the state of the neonate and the quality of air while it wasasleep. The designed proposal is characterized by its low cost and non-intrusive nature. The resultsare promising.Keywords: context-aware; SIDS; non-intrusive; e-health; pediatric1. IntroductionDue to the recent advances in sensor systems, the Internet-of-Things, and medical devices, it ispossible to provide personalized and continuous health care at home [1]. People with chronic diseasesor elderly people are the ones that most benefit from these home care systems [2]. Thanks to thesedevelopments, patients can be more independent and reduce their visits to the doctor. This contributesgreatly to the quality of their life [3]. The evolution of monitoring systems entails sensors collectinggreater amounts of data; this means that we have to be prepared to process more medical data [4].It is therefore essential to find solutions that will allow for the management of large amountsof data in a fast, efficient, and accurate way [5]. To achieve efficient management, it is essential toapply a context layer to the collected data. Context-aware systems play a notable role in the processingand analyzing of data [6]. The correct contextualization of the collected information is essential forunderstanding it and generating knowledge that can be used in decision making. To contextualize data,it is necessary to have additional information on the context that will make the collected data coherentand reliable [7]. In many cases, additional context information comes from heterogeneous data sources,and it is necessary that they are reliable and accurate context providers. Poorly contextualized datasetscan lead intelligent applications and systems to make the wrong decisions. This is a very seriousissue for medical systems, in which a wrong decision caused by a contextualization error can put thehealth of the patient at risk [8]. Hence, when designing homecare systems, it is important to pay closeattention to the performance of the systems that add and manage contexts in context-aware systems.Sensors 2018, 18, 757; doi:10.3390/s18030757 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18030757http://www.mdpi.com/journal/sensorsSensors 2018, 18, 757 2 of 22Another important issue in today’s homecare systems is environmental factors such as air quality.It has been demonstrated that acting over these factors is a determinant in the prevention of differentdiseases [9]. The problem is that much of current sensor systems employ only biomedical measurementdevices that users must carry with them or with which they must interact in some way (such as a pulsesensor, body temperature meter, or glucometer). Just a few projects implement systems that monitorenvironmental factors in the home.The lack of research in this area creates a necessity for new proposals in the field of air qualitymonitoring systems in homecare. These systems must measure air quality in domestic environmentswith the aim of improving user’s health. Current systems are compatible with traditional health",
        "publication_date": "2018-03-02",
        "authors": "Daniel H. de la Iglesia, Juan F. De Paz, Gabriel Villarrubia González, Alberto López Barriuso, Javier Bajo",
        "file_name": "20250514082217.pdf",
        "file_path": "output/PDFs/20250514082217.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/3/757/pdf?version=1519986905"
    },
    {
        "title": "R4R: Template-based REST API Framework for RDF Knowledge Graphs.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs (KGs) are increasingly being used to makestructured information available on the Web, by means of REST APIsand/or SPARQL endpoints. In many cases, these REST APIs are gen-erated on top of the SPARQL endpoints, using existing technology ap-proaches that are based on proprietary configuration files or ontologies tocreate the APIs. These approaches may impose content-based or struc-tural constraints when composing Web resources. To relax these con-straints we propose R4R, a more flexible solution based on Web stan-dards and REST principles that creates and publishes customizable APIsexposing Web resources from SPARQL queries organized in file systemdirectories. R4R features include individual and nested resources, pagi-nated queries, optional fields, web authentication, query parameters andsorting lists.Resource type: SoftwareLicense: Apache License 2.0DOI: https://doi.org/10.5281/zenodo.3543320Keywords: API · Knowledge Graph · REST · SPARQL1 IntroductionKnowledge graphs (KGs) are drawing increasing attention from both academiaand industry for representing, sharing and using knowledge in applications [7, 3].They may be made available as RDF-based datasets, including a SPARQL end-point (e.g., DBpedia), and/or via REST APIs (e.g., Google Knowledge Graph).In both cases, KGs share many commonalities from the data representation pointof view (both use triples to represent facts), but they are radically different interms of query capabilities: SPARQL provides a more expressive query languagethan what can be normally done with a REST API, but it can be a barrier fornon-expert users. Web APIs usually present data according to REpresentativeState Transfer (REST) architecture principles mapping HTTP verbs (POST,GET, PUT, DELETE) to CRUD operations (Create, Read, Update, Delete).Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0)Fig. 1. Step-by-step processing of a request in R4RHowever, API resources do not necessarily match to KG resources. A publicprocurement-focused API and a technology-focused API may present, in a dif-ferent way, the information retrieved from the same KG about companies. Onemore focused on merit and the other on innovations.In this demo we present R4R, an open source framework that facilitates thepublication of a KG via a REST API over HTTP. Our approach proposes afully customizable definition of resources, both naming and content (and evennesting), through a hierarchical organization of data (Figure 1). It deploys a webservice based on SPARQL queries to retrieve the information and provides tem-plates to compose resources that are organized in folders in a system directory.Finally, we describe a motivating example where R4R is used to enhance KGaccess.2 KG data consumption via Web APISeveral approaches are available to provide Web developers with mechanismsto ease KG data consumption without dealing with the complexity of SemanticWeb standards and technologies, namely SPARQL. Some of these approacheshave been focused on the provision of Web APIs that allow developers to in-teract with KG data. There are tools [1][5] that generate Web APIs from set of",
        "publication_date": "2021-01-01",
        "authors": "Carlos Badenes-Olmedo, Paola Espinoza-Arias, Óscar Corcho",
        "file_name": "20250514082219.pdf",
        "file_path": "output/PDFs/20250514082219.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper339.pdf"
    },
    {
        "title": "Predicting the risk of suffering chronic social exclusion with machine learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-62410-5_16",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-06-20",
        "authors": "Emilio Serrano, Pedro del Pozo-Jiménez, Mari Carmen Suárez-Figueroa, Jacinto González‐Pachón, Javier Bajo, Asunción Gómez‐Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Retraction notice to “Applying case-based reasoning in social computing to transform colors into music” [Eng. Appl. Artif. Intell. 72 (2018) 1–9]",
        "implementation_urls": [],
        "doi": "10.1016/j.engappai.2019.03.007",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-01",
        "authors": "María Navarro-Cáceres, Sara Rodrı́guez, Javier Bajo, Juan M. Corchado",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Relationship recommender system in a business and employment-oriented social network",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2017.12.050",
        "arxiv": null,
        "abstract": "levels of abstractions and openness, which is not the case of most solutions [13] . Moreover, although agent frameworks andplatforms have similarities, there are subtle differences too. Each platform uses different models and syntax and providesdifferent libraries. In our case [41] , we have tried to use standards whose robustness has already been demonstrated andwhich are known within the research community, making the implementation of the system easier and highly reliable. Aservice oriented platform that allows for the implementation of an open MAS is used [41] to take maximum advantage ofthe distribution of resources. To this end, all services are implemented as Web Services. Due to its service orientation, dif-ferent tools modeled with agents that consume Web services can be integrated and operated from the platform, regardlessof their physical location or implementation. Distributed MAS have become very sophisticated in the last years [14,48] , with a rising potential to handle large vol-umes of data and coordinate the operations of many organizations. MAS offer a general computing paradigm for solv-ing difficult computational problems as well as for characterizing complex systems based on concepts such as autonomy,self-aggregation, self-organization and emergent behavior. Due to its characteristics, MAS are especially suitable for solvinglarge-scale and distributed problems such as social network presented in this research. Looking into the current researchesbackground, it is possible to observe that MAS are been widely used to model and solve complex real-world problems intext mining and recommender systems in social networks. New methods have been proposed and applied to different areasincluding social network analysis, gene network analysis or web clustering. Most of the existing methods for mining com-munities are centralized, although in studies such as [24,47] . In this case, a group of autonomous agents work together tomine a network through a proposed self-aggregation and self-organization mechanism. In [27] a new model is presentedfor finding influential agent groups based on group centrality analyses in citation networks. The authors present a modelfor finding influential agent groups in multi-agent software systems considering that the impact of an agent is mainly char-acterized by its citation relations (a group’s impact is determined by both direct and indirect citations). Another exampleis presented in [15] , a novel distributed model for HMAS. The proposed model assumes an interaction network among theagents of a MAS. This decentralized and local-info-based nature of the suggested model makes it an appropriate approachfor large-scale open MAS. The outcomes of this work can be used in a wide range of distributed applications. There is a notable trend in filling the gap between social network analysis and control. This trend was triggered by theadvancements in complex networks theory and MAS, the inclusion of new mathematical models describing dynamics ofsocial groups, and the development of new computational tools for relevant information analysis. Control of collective behavior is one of the most desirable goals in many applications related to social network analysisand mining. This work also intends to go a step further in this regard, by integrating a comprehensive recommender systeminto a social based agent architecture for the extraction and analysis of relevant information. The system was implementedin an agent framework that was subsequently used to extract the behavior of users in a social network. 3. Proposed system This section describes the process that is carried out before a relationship is recommended (or not) to a user. To providea recommendation, the system must detect a tie between two users or between a user and a job offer. According to thesocial tie proposals [31] , which are based on mathematical sociology, ties can be classified into three types: • Strong ties : these ties represent a formal and direct link. On the social network, a strong tie represents an existing anddirect relationship between users or a user and a job offer. • Weak ties : these are the most common ties on social networks. They are created when there is a link between twoentities (user-user or user-job offer) that is not strong, but there are some elements connect users or a user and a joboffer within the system. • Absent ties : ties between objects that do not substantiate any true link, these are not considered by the system. In such way, the system searches for weak ties and evaluates them, recommending suitable relationships to users. Oncea user accepts a recommendation, a weak tie converts into a strong tie. In addition, we should specify that these links are not bidirectional, they are unidirectional. Therefore, the system cansuggest a relationship to only one of the users, since the affinity is calculated in such a way that it adapts to the profile ofeach user individually and automatically. In the job offer RS, particular job offers are recommended to compatible users. The rest of this section addresses the findings and evaluations made in the four categories: (i) VOs of agents; (ii) extrac-tion of relevant information from unstructured texts; (iii) user-user relationships; (iv) user-job offer relationships. 3.1. Virtual organizations By using VO, the system can be structured as a group of agents which are coordinated through the integrated platform",
        "publication_date": "2017-12-26",
        "authors": "Pablo Chamoso, Alberto Rivas, Sara Rodrı́guez, Javier Bajo",
        "file_name": "10!1016%j!ins!2017!12!050.pdf",
        "file_path": "output/PDFs/10!1016%j!ins!2017!12!050.pdf",
        "pdf_link": null
    },
    {
        "title": "Prediction and failure analysis of composite resin restorations in the posterior sector applied in teaching dental students",
        "implementation_urls": [],
        "doi": "10.1007/s12652-020-01804-7",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-02-28",
        "authors": "Ignacio Aliaga, Juan F. De Paz, Vicente Vera, Álvaro Enrique García Barbero, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Conformance Test Cases for the RDF Mapping Language (RML)",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-21395-4_12",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Pieter Heyvaert, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho, Erik Mannens, Ruben Verborgh, Anastasia Dimou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A framework for creating knowledge graphs of scientific software metadata",
        "implementation_urls": [],
        "doi": "10.1162/qss_a_00167",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Aidan Kelley, Daniel Garijo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "SoMEF: A Framework for Capturing Scientific Software Metadata from its Documentation",
        "implementation_urls": [],
        "doi": "10.1109/bigdata47090.2019.9006447",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-12-01",
        "authors": "Allen Mao, Daniel Garijo, Shobeir Fakhraei",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Semantically Enhanced UPnP Control Point for Sharing Multimedia Content",
        "implementation_urls": [],
        "doi": "10.1109/mic.2011.83",
        "arxiv": null,
        "abstract": "Abstracts on Human Factors in Computing Systems (CHI 97), ACM Press, pp. 168–169. 11.\t J.P. Chin, V.A. Diehl, and K.L. Norman, “Development of an Instrument Measuring User Satisfaction of the Human-Computer Interface,” Proc. ACM Conf. Human Factors in Computing Systems (CHI 88), ACM Press, 1988, pp. 213–218.Mariano Rico is a teaching assistant in the computer sci-ence department at the Universidad Autónoma de Madrid and a collaborator in the Ontology Engineering Group at the Universidad Politécnica de Madrid. His research interests include Semantic Web technologies, virtual worlds, user interfaces, and natural language processing. Rico has a PhD in computer science from the Universidad Autónoma de Madrid. Contact him at mariano.rico@uam.es.Oscar Corcho is an associate professor in the artificial intelligence department at the Universidad Politéc-nica de Madrid, where he’s a member of the Ontology Engineering Group. His research activities focus on semantic e-Science and real-world Internet, although he also conducts research in the more general areas of Semantic Web and ontological engineering. Corcho has a PhD in artificial intelligence from the Universi-dad Politécnica de Madrid. Contact him at ocorcho@fi.upm.es.Víctor Méndez is a research fellow at Intelligent Software Components (iSOCO) in Madrid. His research interests include Semantic Web applied to multimedia, opinion mining, reputation, and provenance. Méndez has a BSc in computer engineering from the Carlos III University of Madrid. Contact him at vmendez@isoco.com.Jose Manuel Gomez-Perez is the director of R&D at Intel-ligent Software Components (iSOCO) in Madrid. His research aims to support users in creating, shar-ing, and accessing knowledge and spans knowledge acquisition, provenance analysis, intelligent infor-mation access, and their applications. Gomez-Perez has a PhD in computer science from the Universidad Politécnica de Madrid. Contact him at jmgomez@isoco.com.IC-15-06-Rico.indd   64 10/8/11   5:46 PM",
        "publication_date": "2011-07-06",
        "authors": "Mariano Rico, Óscar Corcho, Víctor Méndez Muñoz, Asunción Gómez‐Pérez",
        "file_name": "10!1109%mic!2011!83.pdf",
        "file_path": "output/PDFs/10!1109%mic!2011!83.pdf",
        "pdf_link": null
    },
    {
        "title": "Knowledge Engineering of PhD Stories: A Preliminary Study",
        "implementation_urls": [],
        "doi": "10.1145/3460210.3493579",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-11-24",
        "authors": "Viet Bach Nguyen, Vojtěch Svátek, Marek Dudáš, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "TermInteract: An Online Tool for Terminologists Aimed at Providing Terminology Quality Metrics",
        "implementation_urls": [],
        "doi": "10.1109/cist49399.2021.9357197",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-06-05",
        "authors": "Pedro Hernandez-Vegas, Lucía Guasp, Mariano Rico",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Toward proactive social inclusion powered by machine learning",
        "implementation_urls": [],
        "doi": "10.1007/s10115-018-1230-x",
        "arxiv": null,
        "abstract": "Abstract The ñght against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a c o m m o n practice. This paper describes a data analysis of over 16 K cases with over 60 predictors from the Spanish region of Castilla y León. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion: around 9 0 % in the most conservative predictions. This prediction models offer a quick rule of thumb that can detect citizens who are in danger of been excluded from the society beyond a temporary situation, allowing social workers to further study these cases. Keywords Social exclusion - Social services - Data analysis - Machine learning - Data mining 1 Introduction S b c W gxcZwjWM is a complex and multidimensional process involving the lack of resources, rights, goods and services, and the inability to participate in the normal relationships and activities, available to most people in a society, whether in economic, social, cultural or political scopes [14]. Social exclusion affects not only the quality of life of individuals, but also the equity and cohesion of society as a whole. El Emilio Serrano emilioserra @ fi.upm.es Mari Carmen Suárez-Figueroa mcsuarez @ ñ.upm. es Jacinto González-Pachón jgpachon@fi.upm.es Asunción Gómez-Pérez asun@fi.upm.es t Ontology Engineering Group, Artificial Intelligence Department, Universidad Politécnica de Madrid, Madrid, Spain http://fi.upm.esmailto:jgpachon@fi.upm.esmailto:asun@fi.upm.esThe economic crisis is undermining the sustainability of social protection systems in the E U [6]: 2 4 % of all the E U population (over 120 million people) are at risk of poverty or social exclusion [6]. The fight against poverty and social exclusion is at the heart of the Europe 2020 strategy for smart, sustainable and inclusive growth. In chronic medical diseases, there is strong evidence supporting that early detection results in less severe outcomes. This paper intends to provide social workers with methods and tools to bring this early detection, which is so beneficial in the medical field, to the challenging problem of chronic social exclusion. Note that although poverty has a significant effect on some dimensions of social exclusion, there are other important causes such as age, ethnicity, disability, gender, and employment status. Therefore, it is considerably more challenging to analyze, detect, treat, and predict social exclusion than poverty. This paper contributes with an (1) analysis of the social services data of Castilla y León (CyL), which is the largest region in Spain and counts with around two and a half million inhabitants. This analysis allows getting insights into why social exclusion can become chronic. Furthermore, a(2) machine learning model capable ofquantifying the risk ofchronic social exclusion is build. Finally, a (3) responsive web application is deployed to allow queries by social workers through a number of devices such as smartphones, tablets, or laptops. A RESTful web service is also provided to integrate the predictive capabilities into other ",
        "publication_date": "2018-06-28",
        "authors": "Emilio Serrano, Mari Carmen Suárez-Figueroa, Jacinto González‐Pachón, Asunción Gómez‐Pérez",
        "file_name": "10!1007%s10115-018-1230-x.pdf",
        "file_path": "output/PDFs/10!1007%s10115-018-1230-x.pdf",
        "pdf_link": null
    },
    {
        "title": "OBA: An Ontology-Based Framework for Creating REST APIs for Knowledge Graphs",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-62466-8_4",
        "arxiv": "2007.09206",
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Maximiliano Osorio",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Predicting incorrect mappings",
        "implementation_urls": [],
        "doi": "10.1145/3167132.3167164",
        "arxiv": null,
        "abstract": "ABSTRACTDBpedia releases consist of more than 70multilingual datasets thatcover data extracted from different language-specific Wikipediainstances. The data extracted from those Wikipedia instances aretransformed into RDF usingmappings created by theDBpedia com-munity. Nevertheless, not all the mappings are correct and con-sistent across all the distinct language-specific DBpedia datasets.As these incorrect mappings are spread in a large number of map-pings, it is not feasible to inspect all such mappings manually toensure their correctness. Thus, the goal of this work is to pro-pose a data-driven method to detect incorrect mappings automati-cally by analyzing the information from both instance data as wellas ontological axioms. We propose a machine learning based ap-proach to building a predictive model which can detect incorrectmappings. We have evaluated different supervised classification al-gorithms for this task and our best model achieves 93% accuracy.These results help us to detect incorrect mappings and achieve ahigh-quality DBpedia.CCS CONCEPTS• Information systems→ Resource Description Framework(RDF); • Computing methodologies → Cross-validation; Se-mantic networks; • Social and professional topics→ Quality as-surance;KEYWORDSLinked Data, Data Quality, Mappings, DBpedia, Machine LearningPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full cita-tion on the first page. Copyrights for components of this work owned by others thanACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SAC 2018, April 9–13, 2018, Pau, France© 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-5191-1/18/04. . . $15.00https://doi.org/10.1145/3167132.3167164ACM Reference format:Mariano Rico, Nandana Mihindukulasooriya, Dimitris Kontokostas, HeikoPaulheim, Sebastian Hellmann, and Asunción Gómez-Pérez. 2018. Predict-ing Incorrect Mappings: A Data-Driven Approach Applied to DBpedia. InProceedings of SAC 2018: Symposium on Applied Computing , Pau, France,April 9–13, 2018 (SAC 2018), 8 pages.https://doi.org/10.1145/3167132.31671641 INTRODUCTIONA large number of RDF knowledge bases are created by transform-ing non-RDF data sources into RDF. Such non-RDF formats includerelational databases, CSV files, key-value pairs, etc. A key input tothis transformation process is a mapping that defines how to trans-form the non-RDF source data into RDF. Such mapping specifieshow tomap the source schema into RDF vocabularies, and possibly",
        "publication_date": "2018-04-09",
        "authors": "Mariano Rico, Nandana Mihindukulasooriya, Dimitris Kontokostas, Heiko Paulheim, Sebastian Hellmann, Asunción Gómez‐Pérez",
        "file_name": "10!1145%3167132!3167164.pdf",
        "file_path": "output/PDFs/10!1145%3167132!3167164.pdf",
        "pdf_link": null
    },
    {
        "title": "Deliberative and Epistemic Approaches to Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_2",
        "arxiv": null,
        "abstract": "Abstract Deliberative and epistemic approaches to democracy are two importantdimensions of contemporary democratic theory. This chapter studies thesedimensions in the emerging ecosystem of civic and political participation tools, andappraises their collective value in a new distinct concept: linked democracy. Linkeddemocracy is the distributed, technology-supported collective decision-makingprocess, where data, information and knowledge are connected and shared bycitizens online. Innovation and learning are two key elements of Atheniandemocracies which can be facilitated by the new digital technologies, and across-disciplinary research involving computational scientists and democratic the-orists can lead to new theoretical insights of democracy.Keywords Deliberative democracy � Epistemic democracy � Semantic web �Institutions � Participatory ecosystems2.1 IntroductionSemantic Web engineers have often complained that building ontologies is hard. Tobuild an ontology for a given domain—for example, tort law—one needs to recruitexperts in that domain, elicit their legal knowledge, and then reach a shared, explicitconsensus of how such legal knowledge will be represented and formalised so thatcomputers can ‘understand it’. It is not an easy task, indeed, especially if ontologieshave to be designed from scratch and the subject matter is complex.If it is hard to build ontologies, mapping the conceptual domain of deliberativeand epistemic theories of democracy is not less harder. In fact, it is quite theopposite. In the last thirty years, political philosophers and scientists have producedan oceanic body of literature on the justification, mechanisms, and outcomes ofdemocracy based on a number of procedural and cognitive arguments. They havedone so at different levels: normative (discussing the foundational values), theo-retical (formulating hypothesis), and empirical (developing case studies and testingnew institutional arrangements). Successive generations of scholars have expanded,© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_227http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_2refined, or remixed their different approaches with extraordinary sophistication. Asa result, any attempt to represent the domain of contemporary models of democracywill necessarily be limited and selective. Like the making of the 19th centuryOxford English Dictionary, or the 21st century Wikipedia, the effort would requirethe involvement of hundreds if not thousands of dedicated volunteers.This chapter will take an oblique route by briefly considering the debates indemocracy theory over the last decades that have explored the meaning and practiceof democratic participation. The discussions about the role of citizen participationare sometimes structured into a binary between ‘procedural’ and ‘epistemic’accounts of democratic practice, or, with a different terminology, between ‘ma-joritarian’ and ‘populist’ approaches. Hélène Landemore has proposed a moreexpressive dichotomy: the ‘talkers’ and the ‘counters’ (Landemore 2013, 53).1 The‘talkers’ walk the path of ‘deliberation followed by majority rule as a fallible butoverall reliable way to make collective decisions’; the ‘counters’ explore ‘theepistemic properties of judgement aggregation when large groups of people are",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "20250514082305.pdf",
        "file_path": "output/PDFs/20250514082305.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_2.pdf"
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.26342/2023-70-12",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Five challenges for the Semantic Sensor Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-2010-0005",
        "arxiv": null,
        "abstract": "Abstract. The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet. Most efforts in this area focused on the provision of platforms that could be used to build sensor-based applications more efficiently, considering some of the most important challenges in sensor-based data management and sensor network configuration. The introduction of semantics into these platforms provides the opportunity of going a step forward into the understanding, management and use of sensor-based data sources, and this is a topic being ex-plored by ongoing initiatives. In this paper we go through some of the most relevant challenges of the current Sensor Web, and describe some ongoing work and open opportunities for the introduction of semantics in this context. Keywords: Sensor, ontology, query language  1. Introduction The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet [1,6,7,11,15]. Most of the work done on this topic, performed in some cases under the umbrella of the OGC Sensor Web Enablement Working Group1, focused on the creation of specifications for different functionalities related to the management of sensor-based data (observations, measurements, sensor net-work descriptions, transducers, data streaming, etc.), and for the different types of services that may han-dle these data sources (planning, alert, observation and measurement collection and management, etc.).  Some additional work has focused on the provi-sion of platforms that provide the services needed to develop sensor-based applications. These platforms include libraries for common domain-independent data management tasks, such as data cleaning, stor-age, aggregation, query processing, etc., and they are                                                            * Corresponding author. E-mail:  ocorcho@fi.upm.es. 1 http://www.opengeospatial.org/projects/groups/sensorweb used to provide domain-specific aggregated services (e.g., coastal imaging [6], patient care [15]).  Finally, centralized registries for sensor-based data have appeared (e.g., Pachube2, SensorMap3), focused on the registration of sensor-based data sources, and on the provision of access to them in multiple ways, by means of REST-based interfaces, web services, or ad-hoc query languages, to name a few. Figure 1 presents a general architecture of Sensor Web applications; which can be characterised by:   • variability and heterogeneity of data, devices and networks (including unreliable nodes and links, noise, uncertainty, etc.);  • use of rich data sources (sensors, images, GIS, etc.) in different settings (live, streaming, his-torical, and processed);  • existence of multiple administrative domains; and  ",
        "publication_date": "2010-01-01",
        "authors": "Óscar Corcho, Raúl García‐Castro",
        "file_name": "10!3233%sw-2010-0005.pdf",
        "file_path": "output/PDFs/10!3233%sw-2010-0005.pdf",
        "pdf_link": null
    },
    {
        "title": "An Intelligent and Autoadaptive System of Virtual Identities Based on Deep Learning for the Analysis of Online Advertising Networks",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-24299-2_26",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Juan David Mateos-Nobre, Alfredo Ayala-Muñoz",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Architecting Data Science Education.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Data scientists are currently among the most demanded professionals in many spheres, including industries, governments, public sector, among oth-ers. This is due to several good reasons. Probably an important one of those rea-sons is the growing demand to find proper ways to face the challenges of estab-lishing data-driven economies and societies. As academics and educationalists, but also Data Science professionals, we look at how to bring up this kind of specialists such that to meet the current shortages but also mid-term demands. In this position paper we deliberate about how to architect thematically, didacti-cally, and organizationally a university program under the thematic umbrella of Data Science. We focus on the selection of learning units or disciplines to be covered in order to produce the M.Sci. and Ph.D. graduates who will be ready to face the future challenges in the mid-term perspective. We outline our rec-ommendation on using learning tools and materials. We also concisely present the approach for stimulating competitive and cooperative atmosphere in the class that stimulates intensive collective and individual learning. We recom-mend to reinforce an academic program by involving industrial partners inten-sively in the process. We ground our deliberation on our experience in imple-menting relevant M.Sci. and Ph.D. programs in Data Science and Semantic Technologies. Keywords: Data Science education, topical scope, program structure, learning tools, didactics, collaboration with industries.  1 Introduction The boost in the abundance, complexity, and variety of data in all spheres of human activity is a phenomenon that leaves a rare information professional negligent these days. Industries are entering into data driven economy, which demands having and using data as a primary asset. On the other hand, the shift to more intensive use of data results in the increase of data generation and storage at unprecedented scales in terms of volumes and rates. A few topical examples are as follows (c.f. [1]): https://orcid.org/0000-0002-5159-254X“Exponential growth of data volumes is accelerated by the dramatic increase of social networking applications that allow non-specialist users create a huge amount of content easily and freely. Equipped with rapidly evolving mobile devices, a user is becoming a nomadic gateway boosting the generation of ad-ditional real-time sensor data. The emerging Internet of Things makes every-thing a data or content, adding billions of additional artificial and autonomic sources of data to the overall picture. Smart spaces, where people, devices, and their infrastructure are all loosely connected, also generate data of unprece-dented volumes and with velocities rarely observed before.” Hence, data generation is a phenomenon that fuels itself and so far we do not ob-serve any signs of saturation for this process. Straightforwardly, the societal demand for the professionals capable of efficient and effective processing of these data also increases at unprecedented rate. These gave rise to Data Science as a discipline and community. As denoted by Hoehndorf and Queralt-Rosinach [2]: “Data Science has as its subject matter the extraction of knowledge from data. While data has been analyzed and knowledge extracted for millennia, the rise of “Big” data has led to the emergence of Data Science as its own discipline that studies how to translate data through analytical algorithms typically taken from statistics, machine learning or data mining, and turning it into knowledge. Data Science also encompasses the study of principles and methods to store, process and communicate with data throughout its life cycle, and starts just af-",
        "publication_date": "2018-01-01",
        "authors": "Vadim Ermolayev, Mari Carmen Suárez-Figueroa, Oleksii Molchanovskyi",
        "file_name": "20250514082311.pdf",
        "file_path": "output/PDFs/20250514082311.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2104/paper_266.pdf"
    },
    {
        "title": "Reference Ontology and (ONTO)2 Agent: The Ontology Yellow Pages",
        "implementation_urls": [],
        "doi": "10.1007/pl00011649",
        "arxiv": null,
        "abstract": "Abstract. Knowledge reuse by means of ontologies faces three important problems at present: (1) there are no standardized identifying features that characterize ontologies from the user point of view; (2) there are no web sites using the same logical organization, presenting relevant information about ontologies; and (3) the search for appropriate ontologies is hard, time-consuming and usually fruitless. To solve the above problems, we present: (1) a living set of features that allow us to characterize ontologies from the user point of view and have the same logical organization; (2) a living domain ontology about ontologies (called Reference Ontology) that gathers, describes and has links to existing ontologies; and (3) (ONTO)2Agent, the ontology-based WWW broker about ontologies that uses Reference Ontology as a source of its knowledge and retrieves descriptions of ontologies that satisfy a given set of constraints. 1. Introduction and Motivation During recent years, considerable progress has been made in developing the con­ceptual bases for building technology that allows knowledge component reuse and sharing. One of the main motivations underlying both ontologies and problem-solving methods (PSM) is to enable sharing and reuse of knowledge and reasoning behavior across domains and tasks. PSMs and ontologies can be seen as com­plementary reusable components to construct knowledge systems (Gómez-Pérez and Benjamins, 1998). Ontologies are concerned with static domain knowledge and PSMs with dynamic reasoning knowledge. The integration of ontologies and PSMs is a possible solution to the 'interaction problem' (Bylander and Chan-drasekaran, 1988), which states that representing knowledge for the purpose of solving some problem is strongly affected by the nature of the problem and the inference strategy to be applied to the problem. Ontologies are defined as a formal, explicit specification of a shared conceptu­alization (G. Gruber, 1993; Borst, 1997); that is, 'Conceptualization refers to an abstract model of some phenomenon in the world by having identified the relevant concepts of that phenomenon. Explicit means that the type of concepts used, and the constraints on their use are explicitly defined. Formal refers to the fact that the ontology should be machine-readable. Shared reflects the notion that an ontology captures consensual knowledge, that is, it is not private to some individual, but accepted by a group' (Studer et al, 1998). PSMs describe the reasoning process of a knowledge-based system in an implementation- and domain-independent man­ner (Benjamins and Fensel, 1998). There are also the notions of task ontologies (Mizoguchi et al, 1995) and PSM ontologies (Chandrasekaran et al, 1998). Nowadays, it is easy to get information from organizations that have ontolo­gies and PSMs on the web. There are even accessible points that gather informa­tion about ontologies and have links to other web pages containing more explicit information about such ontologies (see The Ontology Page, also known as TOP; http ://www.medg.lcs.mit.edu/doyle/top) and there are also ontology servers, like The Ontology Server (http://www-ksl.standford.edu:5915) (Farquhar et al, 1995, 1996), Cycorp's Upper CYC Ontology Server (http://www.cyc.com) (Lenat, 1990) or Ontosaurus (http://indra.isi.edu:8000/Loom) (Swartout et al, 1997), that col­lect a huge number of very well-known ontologies. In the PSM area, there are also many PSM repositories at different locations but they are not accessible for outsiders and they are not compatible (Benjamins et al, 1998). At present, the knowledge component reuse and sharing community has identified the need to provide intelligent agents or intelligent brokering services on the WWW that ease the search for such knowledge components. In the ontology field, the need for this kind of services was identified in Fikes and Farquhar (1997) ",
        "publication_date": "2000-11-01",
        "authors": "Julio César Arpírez, Asunción Gómez‐Pérez, Adolfo Lozano-Tello, H. Sofia Pinto",
        "file_name": "10!1007%pl00011649.pdf",
        "file_path": "output/PDFs/10!1007%pl00011649.pdf",
        "pdf_link": null
    },
    {
        "title": "Exploiting Declarative Mapping Rules for Generating GraphQL Servers with Morph-GraphQL",
        "implementation_urls": [],
        "doi": "10.1142/s0218194020400070",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2020-06-01",
        "authors": "David Chaves-Fraga, Freddy Priyatna, Ahmad Alobaid, Óscar Corcho",
        "file_name": "10!1142%s0218194020400070.pdf",
        "file_path": "output/PDFs/10!1142%s0218194020400070.pdf",
        "pdf_link": null
    },
    {
        "title": "FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-62419-4_16",
        "arxiv": "2008.13482",
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, María-Esther Vidal, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Editorial: Special issue on Semantic eScience: Methods, tools and applications",
        "implementation_urls": [],
        "doi": "10.3233/sw-200380",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-08-18",
        "authors": "Daniel Garijo, Natalia Villanueva‐Rosales, Tomi Kauppinen",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Author response: Development, validation, and application of a machine learning model to estimate salt consumption in 54 countries",
        "implementation_urls": [],
        "doi": "10.7554/elife.72930.sa2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-12-07",
        "authors": "Wilmer Cristobal Guzman‐Vilca, Manuel Castillo-Cara, Rodrigo M. Carrillo‐Larco",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Secured and Trusted Demand Response system based on Blockchain technologies",
        "implementation_urls": [],
        "doi": "10.1109/inista.2018.8466303",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-07-01",
        "authors": "Apostolos C. Tsolakis, Ioannis Moschos, Konstantinos Votis, Dimosthenis Ioannidis, Dimitrios Tzovaras, Pankai Pandey, Sokratis Katsikas, Evangelos Kotsakis, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Distributed training and inference of deep learning solar energy forecasting models",
        "implementation_urls": [],
        "doi": "10.1109/pdp59025.2023.00035",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-03-01",
        "authors": "Javier Campoy, Ignacio-Iker Prado-Rujas, José L. Risco-Martı́n, Katzalin Olcoz, Marı́a S. Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Best Practices for Implementing FAIR Vocabularies and Ontologies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/ssw200034",
        "arxiv": "2003.13084",
        "abstract": "Abstract. With the adoption of Semantic Web technologies, an increas-ing number of vocabularies and ontologies have been developed in differ-ent domains, ranging from Biology to Agronomy or Geosciences. How-ever, many of these ontologies are still difficult to find, access and un-derstand by researchers due to a lack of documentation, URI resolvingissues, versioning problems, etc. In this chapter we describe guidelinesand best practices for creating accessible, understandable and reusableontologies on the Web, using standard practices and pointing to exist-ing tools and frameworks developed by the Semantic Web community.We illustrate our guidelines with concrete examples, in order to helpresearchers implement these practices in their future vocabularies.Keywords: Ontology metadata · Ontology publication · Ontology ac-cess · FAIR principles · Linked Data principles.1 IntroductionIn the last decade, a series of initiatives for open data, transparency and openscience have led to the development of a myriad of datasets and linked Knowl-edge Graphs on the Web.3 Ontologies and vocabularies have been developedaccordingly to represent the contents of these datasets and Knowledge Graphsand help in their integration and linking. However, while significant effort hasbeen spent on making data Findable, Accessible, Interoperable and Reusable(FAIR) [18], ontologies and vocabularies are often difficult to access, understandand reuse. This may be due to several reasons, including a lack of definitionsof ontology classes and properties; deprecated or unavailable imported ontolo-gies, non-resolvable ontology URIs, lack of examples and diagrams in the docu-mentation, or having scientific publications describing an ontology without anyreference to its implementation.The scientific community has started to acknowledge the need for ontologiesto be properly documented, versioned, published and maintained following the3 https://lod-cloud.net/http://arxiv.org/abs/2003.13084v1https://lod-cloud.net/2 Garijo and Poveda-VillalónLinked Data Principles [6] and adapting the FAIR principles for data [8]. Butthese recommendations do not include guidelines on how to implement them for atarget vocabulary. In this chapter we address this issue by describing how to makean ontology or vocabulary comply with the FAIR principles, including examplessummarizing best practices from the community and our own experience; andpointing to popular existing tools and frameworks.Our guidelines are aimed at ontology engineers, and therefore the paper isstructured according to their ontology development processes: Section 2 describesseveral design decisions for an ontology URI (naming conventions, versioning,permanent URIs); Section 3 describes how to create a documentation that is easyto reuse and understand by others (minimum metadata, diagrams to include);Section 4 illustrates how to make an ontology accessible and findable in the Web;Section 5 points to existing end-to-end frameworks that support the ontologypublication process; and Section 6 concludes our paper. We consider the designand development of an ontology out of the scope of this paper, as it has beencovered by methodologies for ontology development (e.g., LOT4 or NeOn [14]).2 Accessible Ontology URI DesignOntologies are digital artifacts, and therefore their URIs should follow the Linked",
        "publication_date": "2020-11-12",
        "authors": "Daniel Garijo, María Poveda‐Villalón",
        "file_name": "10!3233%ssw200034.pdf",
        "file_path": "output/PDFs/10!3233%ssw200034.pdf",
        "pdf_link": null
    },
    {
        "title": "Similar Terms Grouping Yields Faster Terminological Saturation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13929-2_3",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Natalya Keberle, Aliaksandr Birukou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.2830/446704",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Events Matter: Extraction of Events from Court Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia200847",
        "arxiv": null,
        "abstract": "Abstract. The analysis of court decisions and associated events is part of the dailylife of many legal practitioners. Unfortunately, since court decision texts can oftenbe long and complex, bringing all events relating to a case in order, to understandtheir connections and durations is a time-consuming task. Automated court decisiontimeline generation could provide a visual overview of what happened throughouta case by representing the main legal events, together with relevant temporal infor-mation. Tools and technologies to extract events from court decisions however arestill underdeveloped. To this end, in the current paper we compare the effectivenessof three different extraction mechanisms, namely deep learning, conditional randomfields, and rule-based method, to facilitate automated extraction of events and theircomponents (i.e., the event type, who was involved, and when it happened). In addi-tion, we provide a corpus of manually annotated decisions of the European Court ofHuman Rights, which shall serve as a gold standard not only for our own evaluation,but also for the research community for comparison and further experiments.Keywords. event extraction, named entity recognition, court decisions1. IntroductionCourt decisions are an important source of law information for legal practitioners: theyelaborate on the facts of a case, involved parties, interpretations of the circumstances,the applicable law and legal principles, and finally the legal assessment leading to thedecision. Legal professionals constantly extract, interpret and reason with and about priorcases whilst arguing for a decision in a current, undecided case. However, court decisionstexts can be long and complex and thus time-consuming to read. Therefore it would bebeneficial to find a means to provide a quick overview of a case, thereby helping to turndecisions into operational, consumable and actionable legal knowledge.In this work we focus specifically on using Natural Language Processing (NLP) tech-niques to automatically extract the essence of a court case. Besides extracting generallegal rules from individual cases, we aim at providing a quick overview of what hap-pened, who was involved and when the event took place. In the terminology of NLP, eventextraction can be treated as a text classification task aiming at assigning text fragments(typically, paragraphs, sentences or smaller parts of documents) to predefined (event)Legal Knowledge and Information SystemsS. Villata et al. (Eds.)© 2020 The Authors, Faculty of Law, Masaryk University and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA20084733classes [1]. Another, related NLP task is Named Entity Recognition (NER) which ex-tracts entities referred to in texts and classifies them into categories [2], for instance peo-ple, places and organizations; moreover, named entities can also be domain-specific, forinstance, courts or laws. Event extraction can benefit from NER, since it can be used toenrich events with relevant information, such as the parties involved. This paper focuseson the extraction of events and their components from court decisions of the EuropeanCourt of Human Rights (ECHR)1 based on a sample thereof.Summarizing our contributions, we: (i) provide a corpus of manually annotatedECHR decisions; (ii) perform a comparison of different approaches to automatically ex-tract events and their components – implementations as well as our evaluation results aremade available on GitHub; and (iii) introduce a prototypical web interface that can beused to display court decisions along with their extracted timelines.The remainder of this paper is structured as follows. We outline related works in Sec-",
        "publication_date": "2020-12-01",
        "authors": "Erwin Filtz, María Navas-Loro, Cristiana Santos, Axel Polleres, Sabrina Kirrane",
        "file_name": "10!3233%faia200847.pdf",
        "file_path": "output/PDFs/10!3233%faia200847.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic Discovery in the Web of Things",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-74433-9_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Fernando Serena, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "morph-GraphQL: GraphQL Servers Generation from R2RML Mappings (S)",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/morph-graphql",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!18293%seke2019-055.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "All the information about the dataset, mapping and queries and their results is available online6."
                    }
                ]
            }
        ],
        "doi": "10.18293/seke2019-055",
        "arxiv": null,
        "abstract": "Abstract—REST has become in the last decade the mostcommon manner to provide web services, yet it was not originallydesigned to handle typical modern applications (e.g., mobileapps). GraphQL was released publicly in 2015 and since thenhas gained momentum as an alternative approach to REST.However, generating and maintaining GraphQL resolvers is noteasy. First, a domain expert has to analyse a dataset, designthe corresponding GraphQL schema and map the dataset to theschema. Then, a software engineer (e.g., GraphQL developer)implements the corresponding GraphQL resolvers in a specificprogramming language. In this paper we present an approachthat generates GraphQL resolvers from declarative mappingsspecification in the W3C Recommendation R2RML, hence, canbe used both by a domain expert as without the need toinvolve software developers to implement the resolvers, and bysoftware developers as the initial version of the resolvers to beimplemented. Our approach is implemented in morph-GraphQL.Index Terms—GraphQL, R2RML, OBDAI. INTRODUCTIONIntroduced in 2000, Representational State Transfer (REST)has become the most common manner to provide web servicesin the last few years. Those web services that conform tothe REST principles, known as RESTful web services, useHTTP/S and its operations to make requests to the underlyingserver, such as GET to retrieve objects, POST to add objects,PUT to modify objects and DELETE to remove objects,among others.Over the years, the complexity of modern software concepthas evolved since the inception of REST. For example, typicalmobile applications have to take into account aspects thatreceive little attention in traditional applications, such as thesize of data being exchanged/transmitted and the number ofAPI calls being made. These aspects are relevant to the prob-lem known as over-fetching and under-fetching. Over-fetchingrefers to the situation in which a REST endpoint returns moreDOI reference number: 10.18293/SEKE2019-055data than what is required by the developer. For example, adeveloper may need some information about the name of a userso she hits the corresponding endpoint (/user). However,the endpoint may return information that is not needed by theclient, such as birth date and address. The opposite also raises aproblem, which is having the REST endpoint provide less datathan required. Such a case is called under-fetching. It refers tothe situation in which a single REST endpoint does not providesufficient information requested by the client. For example, inorder to obtain the names of all friends of a particular user,typically two endpoints may be needed: the first is the endpointthat returns the identifiers of all the friends (/friends), andthe second is the one that returns the details of each of thefriends based on the identifier (/user).",
        "publication_date": "2019-07-10",
        "authors": "Freddy Priyatna, David Chaves-Fraga, Ahmad Alobaid, Óscar Corcho",
        "file_name": "10!18293%seke2019-055.pdf",
        "file_path": "output/PDFs/10!18293%seke2019-055.pdf",
        "pdf_link": null
    },
    {
        "title": "IoT Approaches for Distributed Computing",
        "implementation_urls": [],
        "doi": "10.1155/2018/9741053",
        "arxiv": null,
        "abstract": "latest development on IoT related to new abstraction ormultiagent approaches to distribute tasks among edges andCloud; new techniques and communication standards forsharing information to increase spectrum efficiency whilekeeping data consistency and availability; and new meta-data, policies, and hardware/software capabilities to aid fog-orchestration in distributed databases.The paper “Distributed Measurement Data Gatheringabout Moving Objects” presents techniques for the acqui-sition of data related to moving objects that reduces theresources consumed by communication tasks. The methodsproposed use Fog computing and automated prediction andresult in improved network traffic.These methods can enableefficient Internet ofThings composed ofmoving vehicles withstrict communication requirements.The paper “MeReg: Managing Energy-SLA Tradeoff forGreen Mobile Cloud Computing” proposes an adaptiveheuristics energy-aware algorithm, which creates an upperCPU utilization threshold using recent CPU utilization his-tory to detect overloaded hosts and dynamic VM selectionalgorithms to consolidate the VMs from overloaded orunderloaded host. The algorithm tries to minimize totalenergy consumption and maximize Quality of Service,including the reduction of service level agreement (SLA) vio-lations.Theproposed solution contributes to reduce electricalenergy consumption, which affects businesses using mobileHindawiWireless Communications and Mobile ComputingVolume 2018, Article ID 9741053, 2 pageshttps://doi.org/10.1155/2018/9741053http://orcid.org/0000-0001-8175-2201http://orcid.org/0000-0003-0824-4133https://doi.org/10.1155/2018/97410532 Wireless Communications and Mobile Computingcloud computing (MCC) as well as the environment throughcar-bon dioxide (CO2) emissions.The paper “Distributed Image Compression Architectureover Wireless Multimedia Sensor Networks” describes tech-niques that improve the energy consumption for networksthat obtain image signals. Specifically, the paper proposestechniques for distributed compression of images, optimalcamera coverage design, and routing schemes for reducedtransmission energy. The techniques proposed are of par-ticular interest for emerging multimedia sensor networkssince both the transmission of original multimedia signalsand centralized compression require unaffordable energyconsumption.The paper “An Adaptive Joint Sparsity Recovery forCompressive Sensing Based EEG System” proposes a schemeto reduce the energy consumption associated with the trans-",
        "publication_date": "2018-01-01",
        "authors": "Javier Prieto, Abbes Amira, Javier Bajo, Santiago Mazuelas, Fernando De la Prieta",
        "file_name": "10!1155%2018%9741053.pdf",
        "file_path": "output/PDFs/10!1155%2018%9741053.pdf",
        "pdf_link": null
    },
    {
        "title": "A Dialogical Approach to Readiness for Change towards Sustainability in Higher Education Institutions: The Case of the SDGs Seminars at the Universidad Polit é cnica de Madrid",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract: The transformation for sustainability requires a paradigm shift towards systems thinkingand interdisciplinary collaboration, which entails, above all, a process of cultural change affectingindividual mindsets, organizations and society as a whole. Sustainability in higher educationinstitutions (HEIs) has been a recurrent research field in the past decades. However, little attention hasbeen paid to the processes of internal and cultural change and, in particular, to the first steps to prepareacademic communities for change. Understanding “readiness for change” as a core organizationalcompetency to overcome continuous environmental changes and considering the diluted hierarchyat HEIs, this article proposes the adoption of dialogical and developmental approaches in a singleaction case, the SDGs Seminars at the Universidad Politécnica de Madrid. This methodology wasused to diagnose organizational and individual readiness for change considering cognitive, affectiveand behavioural components, and to identify consequences in organizational structures and culture.Our findings reveal that reframing dialogical spaces in HEIs to experience a collaborative andsustainability culture can unlock change, breaking down organizational silos, reducing resistancesand engaging academic communities in the cocreation of institutional strategies. Furthermore, thecase suggests that acting at the group level has impacts both on the individual and institutional levels.Keywords: readiness for change; higher education; academic culture; collaboration; dialogical;developmental; conversations; transformation; sustainable development goals (SDGs)1. IntroductionThe 2030 Agenda for Sustainable Development [1] requires a paradigm shift towardssystems thinking, collaboration and interdisciplinarity [2]. This transformation entails,above all, a process of cultural change which affects individual mindsets, organizationsand society as a whole [3–5].Higher education institutions (HEIs) are not an exception and should transform theirorganizational structure, culture and communication practices in order to overcome disci-plinary and sectoral boundaries [2,6]. In fact, their highly fragmented and monodisciplinarystructures inherited from the 19th century, as well as a conservative and competitive culturewhich encourages individualism, hierarchy, incrementalism, bureaucracy and market-oriented strategies [7–11], hinder internal and external collaborative potential [12,13]. As aconsequence, their response to societal needs is slowed down [14].Sustainability 2021, 13, 9168. https://doi.org/10.3390/su13169168 https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.comhttps://orcid.org/0000-0003-1077-5611https://orcid.org/0000-0003-2728-5731https://orcid.org/0000-0002-7441-3241https://orcid.org/0000-0003-3444-1501https://doi.org/10.3390/su13169168https://doi.org/10.3390/su13169168https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/su13169168https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/article/10.3390/su13169168?type=check_update&version=2ze sustainabilityArticleA Dialogical Approach to Readiness for Change towardsSustainability in Higher Education Institutions: The Case of theSDGs Seminars at the Universidad Politécnica de Madrid1",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514082519.pdf",
        "file_path": "output/PDFs/20250514082519.pdf",
        "pdf_link": "https://www.mdpi.com/2071-1050/13/16/9168/pdf?version=1629191461"
    },
    {
        "title": "A Template-Based Approach for Annotating Long-Tail Datasets.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. An increasing amount of data is shared on the Web throughheterogeneous spreadsheets and CSV files. In order to homogenize andquery these data, the scientific community has developed Extract, Trans-form and Load (ETL) tools and services that help making these files ma-chine readable in Knowledge Graphs (KGs). However, tabular data maybe complex; and the level of expertise required by existing ETL toolsmakes it difficult for users to describe their own data. In this paper wepropose a simple annotation schema to guide users when transformingcomplex tables into KGs. We have implemented our approach by extend-ing T2WML, a table annotation tool designed to help users annotatetheir data and upload the results to a public KG. We have evaluated oureffort with six non-expert users, obtaining promising preliminary results.Keywords: Dataset annotation · Metadata · Knowledge Graph.1 IntroductionAn increasing amount of data is shared on the Web by multiple organizationsusing Excel and CSV formats. Content creators usually prefer to use tabulardata because it is simple to generate, manipulate and visualize by humans; andthere is a significant number of tools to help explore and edit the contents ofspreadsheets. These data need to be properly understood by others, and hencedocumentation (e.g., variables captured, provenance, usage notes, etc.) is usuallyincluded in auxiliary files or the spreadsheets themselves. As a result, many ofthese spreadsheets have comments, clarifications, notes and references to otherfiles explaining how to interpret the information contained in them.In order to convert tabular data to a machine readable format, the SemanticWeb community has created Extract, Transform and Load (ETL) tools (e.g.,[4]) and mapping languages (e.g., [1, 5]) that help translating spreadsheets intoKnowledge Graphs. However, these tools and languages require significant exper-tise when transforming heterogeneous tabular data with comments, incompletevalues or columns that are interrelated to each other, making it difficult fordomain experts to integrate their own datasets with existing KGs.? Copyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 Garijo et al.In this paper we describe an approach to help non-experts transform theirdata into a structured representation through dataset annotations. Our contri-butions include 1) a dataset annotation schema that helps generating templatesfor translating datasets into KGs; 2) an extension of the T2WML dataset an-notation tool [6] to accommodate the proposed schema; and 3) an approach toupload annotated datasets to a registry once the dataset annotation is complete.In order to assess our approach, we conducted a preliminary evaluation with 6users unfamiliar with Knowledge Representation or Semantic Web technologies,who were able to describe and integrate their annotated datasets as a KG.2 Challenges in Long-Tail Dataset AnnotationWe focus on those datasets that are not straightforward to map into a structuredrepresentation. Consider for example Table 1, which depicts the food prices indifferent regions of Ethiopia at different points in time. The table has a time seriesfor the price value of different items at different dates, a repeated column withthe item being described (ignore), the item category and different informationabout the region where that item was produced. The dataset has also somemissing values and labels marked as ”unknown”, which we may want to skip.",
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Ke-Thia Yao, Amandeep Singh, Pedro Szekely",
        "file_name": "20250514082526.pdf",
        "file_path": "output/PDFs/20250514082526.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2722/profiles2020-paper-3.pdf"
    },
    {
        "title": "Enhancing Trust in Trust Services: Towards an Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [],
        "doi": "10.24251/hicss.2022.712",
        "arxiv": null,
        "abstract": "AbstractAs their name suggests, trust is of crucialimportance in “trust service”. Nevertheless, in manycases, these services suffer from a lack transparency,documentation, traceability, and inclusive multi-lateraldecision-making mechanisms. To overcome thesechallenges, in this paper we propose an integratedframework which incorporates formal argumentationand negotiation within a blockchain environmentto make the decision-making processes of fundmanagement transparent and traceable. We introducethree possible architectures and we evaluate andcompare them considering different technical, financial,and legal aspects.1. IntroductionIt is a commonplace that trust has a specialimportance in entering into contractual relations. Thereis a domain in which the importance of trust is socrucial that the whole type of service is named afterit, i.e. trust services, where the fund managers arein the position of a fiduciary acting on behalf of theprincipals. The whole service the fiduciary providesis subject to the overall duty to act in the best interestof the client. The legislator can (and does1) declarethe principal’s right to check the fiduciary’s relevantactivities in order to give some weight to this duty byits intended controlability. On one hand, though, mostprobably there is a difference between the principal’sand the fund managers’ expertise and overview giving∗∗This work has received funding from the EU H2020 researchand innovation programme under the MSCA ITN European JointDoctorate grant agreement No 814177 LAST-JD-RIoE.1For instance, the 6:315. § of the Hungarian Civil Code (Act V of2013) says: The principal and the beneficiary shall have the right tocheck the fiduciary’s activities relating to asset management.the very reason to enter in such a relationship, and onthe other hand, the lack of the decision making processesbeing documented might limit the transparency one cangain by practicing this right.While trust companies might rely on smart contractswhen engaging in their core activities in the securitiesmarket—as suggested by scholars [1, 2] and provenby the surge of Decentralized Finance (DeFi) [3]—theinvolvement of Distributed Ledger Technologies (DLTs)for the securities transactions does not address thepossible trust issues between the principal and thefiduciary: the former does not have access to the reasonwhy the transaction took place and whether it was reallyin his interest. To this regard, trust can be understoodas a relational attribute between a social actor and",
        "publication_date": "2022-01-01",
        "authors": "Liuwen Yu, Mirko Zichichi, Réka Markovich, Amro Najjar",
        "file_name": "10!24251%hicss!2022!712.pdf",
        "file_path": "output/PDFs/10!24251%hicss!2022!712.pdf",
        "pdf_link": null
    },
    {
        "title": "Annotating OGC web feature services automatically for generating geospatial knowledge graphs",
        "implementation_urls": [],
        "doi": "10.1111/tgis.12863",
        "arxiv": null,
        "abstract": "AbstractSciVal TopicsAbstractThe Spatial Data Infrastructure initiatives are now broadly developed and deployed. However, whileplenty of tools use them, some tasks are still complex to perform by non-expert users, such as finding,accessing, and using some of their related OGC Web Services (OWS). One of the main reasons for thesechallenges is associated with semantic heterogeneity within these services. This entails a lack of properdescriptions and requires knowledge about data structure and domain-specific query languages todiscover and retrieve these services. Semantic annotations of OWS play a crucial role in achievingsemantic interoperability and addressing these associated challenges. In this article we describe anapproach for enabling the automatic generation of semantic annotations of Web Feature Services(WFS) at their three request levels (GetCapabilities, DescribeFeatureType, and GetFeature), which areused to generate knowledge graphs. This approach uses various external services, ontological resources(POSM, DBpedia, GeoSPARQL, GeoNames, and datos.ign.es vocabularies), and knowledge bases(DBpedia and datos.ign.es). Moreover, this approach enables us to validate annotations obtained as aprevious step to generating geospatial knowledge graphs. Additionally, we present our proposalthrough an application case and assess it using a representative set of 21 WFS services, achieving anaverage of 46.70% annotations, of which 22.29% and 35.52% were validated using DBpedia andCited by 0 documentsInform me when this documentis cited in Scopus:Related documents,(2019)Transactions in GIS, , (2011)GeoInformatica,(2022)Geo-Spatial InformationScienceFind more related documents inScopus based on:Set citation alert▻A framework for connecting twointeroperability universes: OGCWeb Feature Services and LinkedDataVilches-Blázquez, L.M.Saavedra, J.Integrating semantic webtechnologies and geospatialcatalog services for geospatialinformation discovery andprocessing in cyberinfrastructureYue, P. Gong, J. Di, L.A graph-based representation of",
        "publication_date": "2021-12-07",
        "authors": "Víctor Saquicela, Luis M. Vilches‐Blázquez, Renán Freire, Óscar Corcho",
        "file_name": "10!1111%tgis!12863.pdf",
        "file_path": "output/PDFs/10!1111%tgis!12863.pdf",
        "pdf_link": null
    },
    {
        "title": "Converting UML-Based Ontology Conceptualizations to OWL with Chowlk",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-80418-3_8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Serge Chávez-Feria, Raúl García‐Castro, María Poveda‐Villalón",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-62466-8_18",
        "arxiv": "2006.00088",
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Filip Ilievski, Daniel Garijo, Hans Chalupsky, Naren Teja Divvala, Yixiang Yao, Craig Milo Rogers, Rongpeng Li, Jun Liu, Amandeep Singh, Daniel Schwabe, Pedro Szekely",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Neural networks in distributed computing and artificial intelligence",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2017.06.022",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-06-17",
        "authors": "Javier Bajo, Juan M. Corchado",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
        "implementation_urls": [],
        "doi": "10.1145/3453172",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-06-30",
        "authors": "Yolanda Gil, Daniel Garijo, Deborah Khider, Craig A. Knoblock, Varun Ratnakar, Maximiliano Osorio, Hernán Vargas, Tam Minh Pham, Jay Pujara, Basel Shbita, Bình Dương Vũ, Yao‐Yi Chiang, Dan Feldman, Yijun Lin, Hae Jin Song, Vipin Kumar, Ankush Khandelwal, Michael Steinbach, Kshitij Tayal, Shaoming Xu, Suzanne A. Pierce, Lissa Pearson, Daniel Hardesty-Lewis, Ewa Deelman, Rafael Ferreira da Silva, Rajiv Mayani, Armen R. Kemanian, Yuning Shi, Lorne Leonard, S. D. Peckham, Maria Stoica, Kelly M. Cobourn, Zeya Zhang, Christopher Duffy, Lele Shu",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards Integrating Public Procurement Data into a Semantic Knowledge Graph",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Public procurement accounts for a substantial part of thepublic investment and global economy. Therefore, improving effectiveness,efficiency, transparency and accountability of government procurement isof broad interest. To this end, in this poster paper, we present our approachfor integrating procurement data, including public spending and corporatedata, from multiple sources across the EU into a semantic knowledge graph.We are aiming to improve procurement processes through supportingmultiple stake holders, such as government agencies, companies, controlauthorities, journalists, researchers, and individual citizens.Keywords: Knowledge graph · Public procurement · Ontology.1 IntroductionPublic procurement accounts for a substantial part of the public investment andglobal economy. Every year, over 250 000 public authorities in the EU spendaround 14% of GDP on the purchase of services, works and supplies1. Therefore,improving effectiveness, efficiency, transparency and accountability of governmentprocurement is of broad interest [1]. To this end, European Commission hasput several relevant directives forward, i.e., for public sector information (e.g.,Directive 2003/98/EC) and public procurement (e.g., Directive 2014/24/EU8),to improve public procurement practices. As a result of these, national publicprocurement portals have been created, which live together with regional, localas well as EU-wide public procurement portals. However, there is no commonagreement across the EU (not even, in many cases, inside the same country)on the data formats to be used for exposing such data sources and on the datamodels that need to be used for exposing such data, which leads to a largeheterogeneity in the data that is being exposed.? This work is funded by EU H2020 TheyBuyForYou project (780247).?? Corresponding author. Email: ahmet.soylu@sintef.no1 https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enIn Europe, contracting portals like Tenders Electronic Daily2 (TED) may beseen as a way to homogenise the data that is being provided, but unfortunatelythis portal is only used for those contracts that are larger than a predefinedbudget threshold, and hence this does not cover the whole richness of types ofpublic contracts nor does it force the usage of this format for those contracts thatdo not need to be published there. The only relevant data model that is gettingsome important traction worldwide is the Open Contracting Data Standard3(OCDS). However, it has been mostly developed with a focus on transparency inthe public contracting procedures. Though, several ontologies, such as LOTED2[2], PPROC [3], PCO[4] and upcoming eProcurement ontology4, are developedwith different levels of detail and focus for representing procurement data, there isno solution integrating supplier and procurement data enabling such as matchingof suppliers and buyers and advanced analytics and procurement intelligence.In this poster paper, we present our approach, in the context of TheyBuy-ForYou5 project, for integrating procurement data, including public spending andcorporate data, from multiple sources across the EU into a knowledge graph. Weare aiming to improve procurement processes through supporting multiple stakeholders, such as government agencies, companies, control authorities, journalists,researchers, and individual citizens. The proposed solution enables developersto create fully functional, robust, and scalable data integration pipelines, fromincluding sourcing the data, to pre-processing, augmenting, and interlinking it.",
        "publication_date": "2018-01-01",
        "authors": "Ahmet Soylu, Óscar Corcho, Elena Simperl, Dumitru Roman, Francisco Yedro Martínez, Chris Taggart, Ian Makgill, Brian Elvesæter, Ben Symonds, Helen McNally, George Konstantinidis, Yuchen Zhao, Till Christopher Lech",
        "file_name": "20250514082601.pdf",
        "file_path": "output/PDFs/20250514082601.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2262/ekaw-poster-01.pdf"
    },
    {
        "title": "Thesaurus Enhanced Extraction of Hohfeld’s Relations from Spanish Labour Law",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe the design of an experiment to extract Ho-hfeld’s deontic relations from legal texts. Our approach intends to minimise themanual effort in the annotation process by expanding a set of initial annotationswith the legal domain knowledge contained in thesauri represented in SemanticWeb formats. With such annotations, we perform a set of iterations to train a deeplearning relation extraction model. After analysing the results, we will adapt theprocess to work on the extraction of Hohfeld’s potestative relations. We also planto use that model to recognise relations in unseen legal sub-domains.Keywords: Relation Extraction · Thesaurus · Terminology · Semantic Web1 IntroductionNew legal documentation is being generated daily, which implies new regulations andlaws that need to be processed and, most importantly, understood. Several works havealready tackled the difficulties in legal information processing, such as [5], which identi-fies five major aggravating factors: multijurisdictionality, volume, accessibility, updatesand consolidation and vagueness of legal document classification.Natural language processing tools help solving such challenges, and they can reachgreat performance on many language understanding tasks [25]. Yet, these models re-quire significantly large annotated datasets and language resources to train. We found,however, that legal language resources are scarce, mostly monolingual, and sometimespublished in close and proprietary formats. This may be one of the reasons why most In-formation Extraction systems, and Relation Extraction tools specifically, do not handlelegal texts properly and, if they do, they tend to return very general results (see Sec-tion 2). Therefore, with the aim of making legal information understandable and easieraccessible, in this paper we describe the design of an experiment to extract relationsamongst terms in legal texts. We further represent them as part of rich domain-specificmulti-lingual resources, that can be ultimately exploited for different use cases.This work is framed within Lynx3 project, an Innovation Action funded by the Eu-ropean Union’s Horizon 2020, whose goal is to create a Knowledge Graph of legal and3 http://lynx-project.eu/2 P. Martı́n-Chozas et al.regulatory data to ease the access to information from different jurisdictions, languagesand domains. Such a Legal Knowledge Graph (LKG) could be of a great help to complywith current regulations, specially for non-legal-expert users.Amongst all legal relations the Hohfeld’s fundamental legal relations are the mostgeneral ones [10]. The Hohfeld’s relations, being the highest abstraction of all possi-ble legal relations, may serve the basis for more detailed domain-specific legal rela-tions. In other words, the legal relations appearing in legal sub-domains may be seenas sub-relation of Hohfeld’s relations. They are divided in two sets of relations: deon-tic relations (Right, Duty, No-Right and Priviledge) and potestative relations (Power,Liability, Disability and Immunity). The term “deontic” refers to a branch of the logicthat is responsible for studying the inferential relationships between normative formulasthat include the operators of permission (P), obligation (O) and prohibition (F), amongstothers [24]. While deontic relations (Figure 1) are those that modify (ordinary) actions,potestative relations modify deontic relations. In this preliminary experiment we willput the focus on the deontic relations, leaving potestative relations for future work.Right DutyNo-right Privilegeoppositeoppositecorrelativecorrelative",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514082603.pdf",
        "file_path": "output/PDFs/20250514082603.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2918/paper4.pdf"
    },
    {
        "title": "Extracting and Understanding Call-to-actions of Push-Notifications",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-08473-7_14",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Beatriz Esteves, Kieran Fraser, Shridhar Kulkarni, Owen Conlan, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Linked-Fiestas: A Knowledge Graph to Promote Cultural Tourism in Spain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_18",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Andrea Cimmino, Nandana Mihindukulasooriya, Freddy Priyatna, Mariano Rico",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A framework and computer system for knowledge-level acquisition, representation, and reasoning with process knowledge",
        "implementation_urls": [],
        "doi": "10.1016/j.ijhcs.2010.05.004",
        "arxiv": null,
        "abstract": "ABSTRACT The development of knowledge-based systems is usually approached through the combined skills of software and knowledge engineers (SEs and KEs, respectively) and of subject matter experts (SMEs). One of the most critical steps in this task aims at transferring knowledge from SMEs’ expertise to formal, machine-readable representations, which allow systems to reason with such knowledge. However, this process is costly and error prone. Alleviating such knowledge acquisition bottleneck requires enabling SMEs with the means to produce the target knowledge representations, minimizing the intervention of KEs. This is especially difficult in the case of complex knowledge types like processes. The analysis of scientific domains like Biology, Chemistry, and Physics uncovers: i) that process knowledge is the single most frequent type of knowledge occurring in such domains and ii) specific solutions need to be devised in order to allow SMEs to represent it in a computational form. We present a framework and computer system for the acquisition and representation of process knowledge in scientific domains by SMEs. We propose methods and techniques to enable SMEs to acquire process knowledge from the domains, to formally represent it, and to reason about it. We have developed an abstract process metamodel and a library of Problem Solving Methods (PSMs), which support these tasks, respectively providing the terminology for SME-tailored process diagrams and an abstract formalization of the strategies needed for reasoning about processes. We have implemented this approach as part of the DarkMatter system and formally evaluated it in the context of the intermediate evaluation of Project Halo, an initiative aiming at the creation of question answering systems by SMEs.   1. INTRODUCTION Building knowledge-based systems is an activity that has been traditionally carried out by a combination of software and knowledge engineers and of subject matter experts (SMEs), also known as domain experts. Software engineers (SEs) are focused on architectural and user interface issues related to the development of software. Knowledge engineers (KEs) are focused on knowledge acquisition and representation tasks, with the aim of building the required knowledge bases. For these tasks, they normally work in collaboration with SMEs, who act as repositories of domain knowledge to a large extent. The combination of KEs and SMEs is feasible for a number of domains. However, it has two main drawbacks, first characterized as the knowledge acquisition bottleneck by Feigenbaum in 1977: i) it is costly and ii) it can be error prone, especially in complex domains.   A large amount of work in knowledge-based systems in the past three decades has concentrated on providing frameworks and tools that support the collaboration of KEs and SMEs with the goal of alleviating the knowledge acquisition bottleneck. Despite such work, existing knowledge acquisition tools are still not effective and intuitive enough to allow SMEs to capture the knowledge from a domain by themselves.   Among the different types of knowledge that can be used in knowledge-based systems, in our work we focus on the particular case of process knowledge. Process knowledge is one of the most widely used but also complex types of knowledge across domains, posing important challenges for knowledge acquisition. A process can be considered as a special concept which encapsulates such things as preconditions, results, contents, ",
        "publication_date": "2010-06-05",
        "authors": "Asunción Gómez‐Pérez, Michael Erdmann, Mark Greaves, Óscar Corcho, Richard Benjamins",
        "file_name": "10!1016%j!ijhcs!2010!05!004.pdf",
        "file_path": "output/PDFs/10!1016%j!ijhcs!2010!05!004.pdf",
        "pdf_link": null
    },
    {
        "title": "Development, validation, and application of a machine learning model to estimate salt consumption in 54 countries",
        "implementation_urls": [],
        "doi": "10.7554/elife.72930",
        "arxiv": null,
        "abstract": "ABSTRACT Background: Global targets to reduce salt intake have been proposed but their monitoring is challenged by the lack of population-based data on salt consumption. We developed a machine learning (ML) model to predict salt consumption based on simple predictors, and applied this model to national surveys in low- and middle-income countries (LMICs).   Methods: Pooled analysis of WHO STEPS surveys. We used 19 surveys with spot urine samples for the ML model derivation and validation; we developed a supervised ML regression model based on: sex, age, weight, height, systolic and diastolic blood pressure. We applied the ML model to 49 new STEPS surveys to quantify the mean salt consumption in the population.  Results: The pooled dataset in which we developed the ML model included 45,152 people. Overall, there were no substantial differences between the observed (8.1 g/day (95% CI: 8.0-8.2 g/day)) and ML-predicted (8.1 g/day (95% CI: 8.1-8.2 g/day)) mean salt intake (p= 0.065). The pooled dataset where we applied the ML model included 157,699 people; the overall predicted mean salt consumption was 8.1 g/day (95% CI: 8.1-8.2 g/day). The countries with the highest predicted mean salt intake were in Western Pacific. The lowest predicted intake was found in Africa. The country-specific predicted mean salt intake was within reasonable difference from the best available evidence.    Conclusions: A ML model based on readily available predictors estimated daily salt consumption with good accuracy. This model could be used to predict mean salt consumption in the general population where urine samples are not available.   Funding: Wellcome Trust (214185/Z/18/Z)  Key words: artificial intelligence; deep learning; cardio-metabolic risk factors; cardiovascular health; global health; population health.    . CC-BY 4.0 International licenseIt is made available under a perpetuity.  is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint The copyright holder for thisthis version posted September 2, 2021. ; https://doi.org/10.1101/2021.08.31.21262944doi: medRxiv preprint https://doi.org/10.1101/2021.08.31.21262944http://creativecommons.org/licenses/by/4.0/3  INTRODUCTION The association between high sodium/salt intake and high blood pressure, a major risk factor of cardiovascular diseases (CVD), is well-established.1–3 More than 1.7 million CVD deaths were attributed to a diet high in sodium in 2019, with ~90% of these deaths occurring in low- and middle-income countries (LMICs).4,5 Consequently, salt reduction has been included in international goals: the World Health Organization (WHO) recommendation of limiting salt consumption to <5 g/day,2 and the agreement by the WHO state members of a 30% relative reduction in mean population salt intake by 2025.6 Because available evidence suggests that sodium/salt consumption is higher than the global targets,7–9 we need timely and consistent data of sodium/salt consumption in the general population to track progress of salt reduction targets.  Global efforts have been made to produce comparable estimates of sodium/salt intake for all countries.7 Similarly, researchers have summarized all the available evidence in specific world regions.8,9 Although the global endeavor was based on the gold standard method to assess ",
        "publication_date": "2022-01-05",
        "authors": "Wilmer Cristobal Guzman‐Vilca, Manuel Castillo-Cara, Rodrigo M. Carrillo‐Larco",
        "file_name": "10!7554%elife!72930.pdf",
        "file_path": "output/PDFs/10!7554%elife!72930.pdf",
        "pdf_link": null
    },
    {
        "title": "LUBM4OBDA: Benchmarking OBDA Systems with Inference and Meta Knowledge",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/lubm4obda",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!13052%jwe1540-9589!2284.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "All resources are available online at GitHub6 and archived at Zenodo.7 In the following we introduce these elements."
                    }
                ]
            }
        ],
        "doi": "10.13052/jwe1540-9589.2284",
        "arxiv": null,
        "abstract": "AbstractOntology-based data access focuses on enabling query evaluation over het-erogeneous relational databases according to the model represented by anontology. The relationships between the ontology and the data sources arecommonly defined with declarative mappings, which are used by systemsto perform SPARQL-to-SQL query translation or to generate RDF dumpsfrom the relational databases. Besides the potential homogenization of databecause of using an ontology, some additional advantages of this paradigmare that it may allow applying reasoning thanks to the ontology, as wellas querying for meta knowledge, which describes statements with informa-tion such as provenance or certainty. In this paper, (i) we adapt a widelyused RDF graph store benchmark, namely LUBM, for ontology-based dataaccess, (ii) extend the benchmark for the evaluation of queries that exploitmeta knowledge, and (iii) apply it for performance evaluation of state-of-the-art declarative mapping systems. Our proposal, the LUBM4OBDABenchmark, considers inference capabilities that are not covered by previousJournal of Web Engineering, Vol. 22_8, 1163–1186.doi: 10.13052/jwe1540-9589.2284© 2024 River Publishers1164 J. Arenas-Guerrero et al.ontology-based data access benchmarks, and it is the first one for the eval-uation of meta knowledge and the RDF-star data model. The experimentalevaluation shows that current virtualization systems cannot handle someadvanced inference tasks, and that optimizations are needed to scale RDF-starmaterialization.Keywords: OBDA, semantic web, ontology, data integration.1 IntroductionRelational databases (RDBs) are widely used by organizations to managetheir data. Some application scenarios can benefit from exploiting this dataas knowledge graphs [22], to potentially homogenize data using ontologies,applying reasoning over ontologies, get new insights, or for semantic dataintegration use cases [39, 41]. Ontology-based data access [40] (OBDA)is a paradigm for making data sources such as RDBs (or local schemas)available through a standardized and common view (or global schema). Thisview is given in the form of an ontology, which also enriches the databasewith context by providing background knowledge, reasoning capabilities,and interlinking with other knowledge bases. The relationships between theglobal and local schemas are usually defined through mappings [27,39] whichpopulate the ontology with instances generated from the RDB. As a result, itis possible to perform semantic queries independently of the structure of theoriginal RDB.In the Semantic Web community, a set of standards has been pro-posed: OWL [21] and RDFS [8] are the languages used to encode ontolo-gies, RDF [13] allows representing data, and queries are expressed usingSPARQL [18]. The mappings are specified using R2RML [14], the W3Crecommendation to map RDBs to RDF that follows the global as view [27]approach, in which the mappings define, for each element in the globalschema, a query over the local schemas. In addition, RML [24] is a well-known superset of R2RML to map not only RDBs, but also other types ofdata sources such as CSV, JSON or XML.",
        "publication_date": "2024-02-22",
        "authors": "Julián Arenas-Guerrero, Marı́a S. Pérez, Óscar Corcho",
        "file_name": "10!13052%jwe1540-9589!2284.pdf",
        "file_path": "output/PDFs/10!13052%jwe1540-9589!2284.pdf",
        "pdf_link": null
    },
    {
        "title": "FAIR Computational Workflows",
        "implementation_urls": [],
        "doi": "10.1162/dint_a_00033",
        "arxiv": null,
        "abstract": "ABSTRACTComputational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.†\t Corresponding author: Carole Goble (E-mail: carole.goble@manchester.ac.uk, ORCID: 0000-0003-1219-2137).© 2019 Chinese Academy of Sciences Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) licenseDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/dint_a_00033 by guest on 14 May 2025http://dx.doi.org/10.1162/dint_a_00051http://dx.doi.org/10.1162/dint_a_00030https://w3id.org/fair/principles/terms/F1https://w3id.org/fair/principles/terms/F2https://w3id.org/fair/principles/terms/F3https://w3id.org/fair/principles/terms/F4https://w3id.org/fair/principles/terms/A1https://w3id.org/fair/principles/terms/A1.1https://w3id.org/fair/principles/terms/A1.2https://w3id.org/fair/principles/terms/A2https://w3id.org/fair/principles/terms/I1https://w3id.org/fair/principles/terms/I2https://w3id.org/fair/principles/terms/I3https://w3id.org/fair/principles/terms/R1https://w3id.org/fair/principles/terms/R1.1https://w3id.org/fair/principles/terms/R1.2https://w3id.org/fair/principles/terms/R1.3http://crossmark.crossref.org/dialog/?doi=10.1162/dint_a_00033&domain=pdf&date_stamp=2020-01-31Data Intelligence\t 109FAIR Computational Workflows1.  INTRODUCTIONIn data intensive science, e-infrastructures and software tool-chains are heavily used to help scientists manage, analyze, and share increasing volumes of complex data [1]. Data processing tasks like data cleansing, normalisation and knowledge extraction need to be automated stepwise in order to foster performance, standardisation and re-usability. Increasingly complex data computations and parameter-driven simulations need reliable e-infrastructures and consistent reporting to enable systematic comparisons of alternative setups [2, 3]. As a response to these needs, the practice of performing computational processes using workflows has taken hold in different domains such as the life sciences [4, 5, 6], biodiversity [7], astronomy [8], geosciences [9], and social sciences [10]. Workflows also support the adoption of novel computational approaches, notably machine learning methods [11], due to the ease with which single components in a processing pipeline can be exchanged or updated.Generally speaking, a workflow is a precise description of a procedure – a multi-step process to coordinate multiple tasks and their data dependencies. In computational workflows each task represents the execution of a computational process, such as: running a code, the invocation of a service, the calling of a command line tool, access to a database, submission of a job to a compute cloud, or the execution of data processing script or workflow. Figure 1 gives an example of a real workflow for variant detection in genomics, represented using the Common Workflow Language open standard [12].Computational workflows promise support for automation that scale across computational infrastructures ",
        "publication_date": "2019-11-01",
        "authors": "Carole Goble, Sarah Cohen‐Boulakia, Stian Soiland‐Reyes, Daniel Garijo, Yolanda Gil, Michael R. Crusoe, Kristian Peters, Daniel Schober",
        "file_name": "10!1162%dint_a_00033.pdf",
        "file_path": "output/PDFs/10!1162%dint_a_00033.pdf",
        "pdf_link": null
    },
    {
        "title": "Introduction: Ontology Engineering in a Networked World",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_1",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez, Enrico Motta, Aldo Gangemi",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An Overview of the TBFY Knowledge Graph for Public Procurement.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. A growing amount of public procurement data is being madeavailable in the EU for the purpose of improving the effectiveness, effi-ciency, transparency, and accountability of government spending. However,there is a large heterogeneity, due to the lack of common data formats andmodels. To this end, we developed an ontology network for representingand linking tender and company data and ingested relevant data fromtwo prominent data providers into a knowledge graph, called TBFY. Inthis poster paper, we present an overview of our knowledge graph.Keywords: Public procurement · Knowledge graph · Ontology.1 IntroductionIn the EU, public authorities spend around 14% of GDP on the purchase ofservices, works, and supplies every year7. Therefore, a growing amount of publicprocurement data is being made available in the EU through public portals for thepurpose of improving the effectiveness, efficiency, transparency, and accountabilityof government spending. However, there is a large heterogeneity, due to the lackof common data formats and models for exposing such data.There are various standardization initiatives for electronic procurement, suchas Open Contracting Data Standard (OCDS)8 and TED eSenders 9. However,these are mostly oriented to achieve interoperability, document-oriented, andprovide no standardised practices to refer to third parties, companies participatingin the process, etc. This again generates a lot of heterogeneity. The Semantic Web? Copyright c© 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).7 https://ec.europa.eu/growth/single-market/public-procurement_en8 http://standard.open-contracting.org/latest/en/9 http://simap.ted.europa.eu/https://ec.europa.eu/growth/single-market/public-procurement_enhttp://standard.open-contracting.org/latest/en/http://simap.ted.europa.eu/approach has been proposed as a response [1]. For example, several ontologies havebeen developed, such as PPROC ontology [3] for describing public processes andcontracts, LOTED2 ontology [2] for public procurement notices, PCO ontology [4]for contracts in public domain, and MOLDEAS ontology [5] for announcementsabout public tenders. Each of these was developed with different concerns inmind (legal, process-oriented, etc.) without significant adoption so far.To this end, we developed an ontology network for representing and linkingtender and company data and ingested relevant data from two prominent dataproviders into a knowledge graph, called TBFY. In this poster paper, we presentan overview of our knowledge graph for public procurement.2 Knowledge GraphWe integrated two datasets according to an ontology network: tender dataprovided by OpenOpps10 in the OCDS format and company data provided byOpenCorporates11. OpenOpps has gathered over 2M tender documents frommore than 300 publishers through Web scrapping and by using open APIs, whileOpenCorporates currently has 140M entities collected from national registers.2.1 Ontology NetworkWe are currently using two main ontologies. First, an ontology for tender data(see Figure 1) that we developed using the OCDS’ data model12.Fig. 1. A fragment of OCDS ontology depicting some of the key classes.Second, we reused the euBG ontology for company data13. Both ontologies",
        "publication_date": "2019-01-01",
        "authors": "Ahmet Soylu, Brian Elvesæter, Philip Turk, Dumitru Roman, Óscar Corcho, Elena Simperl, Ian Makgill, Chris Taggart, Marko Grobelnik, Till Christopher Lech",
        "file_name": "20250514082707.pdf",
        "file_path": "output/PDFs/20250514082707.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper14.pdf"
    },
    {
        "title": "Boosting Knowledge Graph Generation from Tabular Data with RML Views",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-33455-9_29",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Julián Arenas-Guerrero, Ahmad Alobaid, María Navas-Loro, Marı́a S. Pérez, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "OpenADR Ontology: Semantic Enrichment of Demand Response Strategies in Smart Grids",
        "implementation_urls": [
            {
                "identifier": "https://github.com/albaizq/OpenADRontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%sest48500!2020!9203093.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "SHACL shapes for the proposed OpenADR ontology can be found in our Github repository23."
                    }
                ]
            }
        ],
        "doi": "10.1109/sest48500.2020.9203093",
        "arxiv": null,
        "abstract": "Abstract—Demand Response (DR) gains increasing attention asa core building block of smart grids. Advanced ICT systems havebeen made available in the last decades and have been employedalready in commercial energy markets. As more and morehardware and software solutions are flooding the market, theneed for interoperability among systems has become a necessity.Building upon OpenADR, a well-known standard for DR, thiswork presents its semantic enrichment towards transforming itinto an ontology (publicly available), which ultimately enablessemantic interoperability among various DR stakeholders andsystems and other semantic-related features like data validation,reusing terms and integration with other standard ontologies.Following the Linked Open Terms methodology, a detaileddescription of the main OpenADR services is presented, encodedin OWL, along with needed extensions that derive from otherwell-known ontologies. By introducing an OpenADR ontology,the adoption and deployment of OpenADR in both researchand industrial implementations is expected to expand, ultimatelypromoting significantly semantic interoperability in DR systems.Keywords—OpenADR, ontology, demand response, smart grid,semantic interoperabilityI. INTRODUCTIONA. MotivationDemand Response (DR) is already part of the energy marketin multiple European countries and in many more there are fu-ture plans drafted to accommodate this relatively new businessmodel [1]. An increasing number of equipment, services, roles,as well as information exchange have made their appearancein every day energy and financial transactions. Thus, althoughenabling the participation of customers in DR schemes isbecoming a commodity, besides opportunities, it also givesbirth to new challenges, one of which is interoperability [2].The EU Energy Efficiency Directive establishes the require-ments and technical modalities for the respective stakeholdersamong member states towards facilitating the uptake of DRservices [3]. Nevertheless, actual deployment of DR has anincreasing number of variations in response to different energymarket products and regulations amongst member states. Thisvariety of DR programs, where each has its own requirements,highlights the need for clearly defined standards to communi-cate DR program and market information [4]. Thus, multiplestandards have been proposed to describe this domain.The Energy Market Information Exchange (EMIX1) is astandard for price and product definition. The Universal SmartEnergy Framework (USEF [5]) describes the market for flexi-bility, enabling commoditization and market trading of flexibleenergy use. The Smart Grid Architecture Model (SGAM [6])aims to present the design of smart grid use cases from anarchitectural viewpoint. It consists of five layers represent-ing business objectives and processes, functions, information",
        "publication_date": "2020-09-01",
        "authors": "Alba Fernández-Izquierdo, Andrea Cimmino, Christos Patsonakis, Apostolos C. Tsolakis, Raúl García‐Castro, Dimosthenis Ioannidis, Dimitrios Tzovaras",
        "file_name": "10!1109%sest48500!2020!9203093.pdf",
        "file_path": "output/PDFs/10!1109%sest48500!2020!9203093.pdf",
        "pdf_link": null
    },
    {
        "title": "Efficient Clustering from Distributions over Topics",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.931305",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF",
                        "source_paragraph": ""
                    }
                ]
            }
        ],
        "doi": "10.1145/3148011.3148019",
        "arxiv": "2012.08206",
        "abstract": "ABSTRACTThere are many scenarios where we may want to find pairs of tex-tually similar documents in a large corpus (e.g. a researcher doingliterature review, or an R&D project manager analyzing projectproposals). To programmatically discover those connections canhelp experts to achieve those goals, but brute-force pairwise com-parisons are not computationally adequate when the size of thedocument corpus is too large. Some algorithms in the literaturedivide the search space into regions containing potentially simi-lar documents, which are later processed separately from the restin order to reduce the number of pairs compared. However, thiskind of unsupervised methods still incur in high temporal costs. Inthis paper, we present an approach that relies on the results of atopic modeling algorithm over the documents in a collection, asa means to identify smaller subsets of documents where the simi-larity function can then be computed. This approach has provedto obtain promising results when identifying similar documentsin the domain of scientific publications. We have compared ourapproach against state of the art clustering techniques and withdifferent configurations for the topic modeling algorithm. Resultssuggest that our approach outperforms (> 0.5) the other analyzedtechniques in terms of efficiency.CCS CONCEPTS• Mathematics of computing → Probability and statistics; • Information systems→Document topicmodels; •Appliedcomputing → Document management and text processing.KEYWORDStopicmodels; semantic similarity; large-scale text analysis; scholarlydata1 INTRODUCTIONGiven the huge amount of information about any domain that isbeing produced or captured daily, it becomes crucial to providemechanisms for automatically identifying the elements that canbring value for the involved agents (general consumers, experts,companies, investors...) and discard the noisy, non-relevant infor-mation. Much of the information is presented in the form of textualdocuments, making necessary for experts to browse through manyof these texts to find relevant data. A way to explore the knowledgeinside collection of documents is by moving from one informationelement to another based on certain criteria that relates them. Thisapproach requires to calculate a similarity matrix with all possiblecomparisons between elements, so we can later select the mostThis work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R,financed by the Spanish Ministry MINECO and co-financed by FEDER..pertinent ones. Since computing a 𝑛 × 𝑛 matrix takes 𝑂 (𝑛2) time,obtaining all possible pairs of similarities in a large collection ofdocuments can be unfeasible because of the exponential cost ofcomparing every pair of elements.Our work is derived from a real need in the domain of digitallibraries, where we targeted the task of finding relations among",
        "publication_date": "2017-12-04",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!1145%3148011!3148019.pdf",
        "file_path": "output/PDFs/10!1145%3148011!3148019.pdf",
        "pdf_link": null
    },
    {
        "title": "Ontology based document annotation: trends and open research problems",
        "implementation_urls": [],
        "doi": "10.1504/ijmso.2006.008769",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2006-01-01",
        "authors": "Óscar Corcho",
        "file_name": "ijmso.2006.008769",
        "file_path": "10.1504/ijmso.2006.008769",
        "pdf_link": null
    },
    {
        "title": "The First International Workshop on Knowledge Graphs for Sustainability – KG4S Foreword",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3589745",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-28",
        "authors": "Eva Blomqvist, María Poveda‐Villalón, Raúl García‐Castro, Pascal Hitzler, Mikael Lindecrantz",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Integration of building material databases for IFC-based building performance analysis",
        "implementation_urls": [],
        "doi": "10.22260/isarc2021/0027",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-11-02",
        "authors": "Stefan Fenz, Julia Bergmayr, Nico Plattner, Serge Chávez-Feria, María Poveda-Villalón, Georgios B. Giannakis",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Clasificación temática automática de documentos basada en vocabularios y frecuencias de uso. El caso de artículos de divulgación científica",
        "implementation_urls": [],
        "doi": "10.3989/redc.2023.3.1996",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-07-06",
        "authors": "César González-Pérez, José Ignacio Vidal Liy, Ana García‐García, Pablo Calleja Ibáñez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Experiential Observations: An Ontology Pattern-Based Study on Capturing the Potential Content within Evidences of Experiences",
        "implementation_urls": [
            {
                "identifier": "https://github.com/modellingDH/odp_experience",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1145%3586078.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "25OWL 2 profiles: https://www.w3.org/TR/owl2-profiles."
                    }
                ]
            }
        ],
        "doi": "10.1145/3586078",
        "arxiv": null,
        "abstract": "can we perform an abstraction over the cultural domain to render these data interoperable through areusable ontological framework?RQ2: If such an abstraction is possible, what will its key concepts be, so that the phenomenological coordi-nates of the experience can be extracted from the informational content, while still preserving theirdiversity?A hypothetical upper ontology of observation would abstract from the phenomenon by providing a middle-ware between phenomenological ontologies and ontologies of sources (Figure 1). On the one hand, ontologiesof sources like LRMoo/FRBRoo [4] and SPAR [5] describe objective features of sources but not their informationcontent. On the other hand, experiential ontologies describe the phenomenon in object but not the relationsbetween the content structures and features of the sources. In other words, the need for the E&O is grounded onthe need for a language to express general features of sources with a direct relation to their phenomenologicalcontent. For example, a listener may recount that hearing a particularly “groovy” riff in a rock song, or a peculiar1Submission to the ODP portal: http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation.ACM Journal on Computing and Cultural Heritage, Vol. 16, No. 3, Article 58. Publication date: August 2023.http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation58:4 • A. Antonini et al.Fig. 1. Observations are the result of a combination of activity, experience and the opportunity to express the observation(evidence).rendition of it in a live performance, inspired them to conduct research on the musicians who wrote, played orwere influenced by it. This latter example demonstrates that an abstraction over this reality into a knowledgepattern should not make assumptions on what is labelled as sources—that is, those whose consumption promptsthe observation. They could belong anywhere in the information layering of FRBR, such as works, expressionsor manifestations (which musical performances generate),2 and which ones should be treated as sources dependson the user’s ability to discern among those layers when recording their evidence.Indeed, each different experiential study adopts different models for the description of data, grounded on thenature of the phenomenon in object and relying on a set of assumptions concerning the homogeneous nature ofsources. In this scenario, the E&O pattern supports data interoperability between research use cases, clarifyingthe relation between activity, reflection and the evidences used in experiential studies.1.2 Application ScenariosThe E&O pattern is intended to support extending schemas and ontologies to encode research data from expe-riential studies to express key facts concerning sources, which would otherwise be lost with the backgroundknowledge of the experts on those sources.Interoperable research data do introduce new issues. Indeed, data reuse enables a detachment between theanalysis of sources and the use of the generated data: the in-depth understanding of sources (e.g., provenanceand limitations) is therefore no longer a requirement for using the data. For instance, a scholar in ClassicalStudies can mix and match data from heterogeneous studies, such as online book reviews or marginalia froman author’s library, without being an expert in either type of source or method. In other words, the technicalfeasibility granted by having a common schema of the research subject does remove the need for informationnecessary to the correct framing of a study. E&O provides a way to reinstate the context of datasets, which isnecessary to interpret the data correctly. For instance, is the reading experience spontaneous or guided by aquestionnaire? Is it a mature recollection years after reading or an immediate response? Is reading a free choiceor part of a school or work assignment?The rest of the article is structured as follows. Section 2 provides a brief background on experiential studiesand the issues concerning observing experience. Section 3 presents a summary of the state of the art, includingrelevant patterns and experiential ontologies. Section 4 describes the E&O pattern in detail, whereas Section 5is devoted to its evaluation. In Section 6, we discuss the application of the pattern and of the retrospective casestudies. Section 7 provides a discussion of the case studies in terms of common patterns emerging from applyingE&O to the encoding of the different types of sources. Finally, Section 8 concludes the article and includes futurelines of work.2https://www.ifla.org/publications/functional-requirements-for-bibliographic-records.",
        "publication_date": "2023-08-09",
        "authors": "Alessio Antonini, Alessandro Adamou, Mari Carmen Suárez-Figueroa, Francesca Benatti",
        "file_name": "10!1145%3586078.pdf",
        "file_path": "output/PDFs/10!1145%3586078.pdf",
        "pdf_link": null
    },
    {
        "title": "Genetic correlations and genome-wide associations of cortical structure in general population samples of 22,824 adults",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514082944.pdf",
        "file_path": "output/PDFs/20250514082944.pdf",
        "pdf_link": "https://www.nature.com/articles/s41467-020-18367-y.pdf"
    },
    {
        "title": "Design and implementation of a low-cost Universal Control for intelligent electric wheelchairs",
        "implementation_urls": [],
        "doi": "10.1109/tla.2018.8408424",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-05-01",
        "authors": "Daniel H. de la Iglesia, Gabriel Villarrubia González, Juan F. De Paz, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "CHALLENGE-BASED LEARNING IN EXPLAINABLE ARTIFICIAL INTELLIGENCE EDUCATION",
        "implementation_urls": [],
        "doi": "10.21125/edulearn.2021.0165",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-07-01",
        "authors": "Ignacio-Iker Prado-Rujas, Emilio Serrano, Damiano Zanardini, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Linked Open Vocabularies (LOV): A gateway to reusable semantic vocabularies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-160213",
        "arxiv": null,
        "abstract": "Abstract. This article addresses a number of limitations of state-of-the-art methods of Ontology Alignment: 1) they primarilyaddress concepts and entities while relations are less well-studied; 2) many build on the assumption of the ‘well-formedness’ ofontologies which is unnecessarily true in the domain of Linked Open Data; 3) few have looked at schema heterogeneity from asingle source, which is also a common issue particularly in very large Linked Dataset created automatically from heterogeneousresources, or integrated from multiple datasets. We propose a domain- and language-independent and completely unsupervisedmethod to align equivalent relations across schemata based on their shared instances. We introduce a novel similarity measureable to cope with unbalanced population of schema elements, an unsupervised technique to automatically decide similaritythreshold to assert equivalence for a pair of relations, and an unsupervised clustering process to discover groups of equivalentrelations across different schemata. Although the method is designed for aligning relations within a single dataset, it can alsobe adapted for cross-dataset alignment where sameAs links between datasets have been established. Using three gold standardscreated based on DBpedia, we obtain encouraging results from a thorough evaluation involving four baseline similarity measuresand over 15 comparative models based on variants of the proposed method. The proposed method makes significant improvementover baseline models in terms of F1 measure (mostly between 7% and 40%), and it always scores the highest precision and isalso among the top performers in terms of recall. We also make public the datasets used in this work, which we believe make thelargest collection of gold standards for evaluating relation alignment in the LOD context.Keywords: Ontology alignment, ontology mapping, Linked Data, DBpedia, similarity measure1. IntroductionThe Web of Data is currently seeing remarkablegrowth under the Linked Open Data (LOD) commu-nity effort. The LOD cloud currently contains over*Corresponding author. E-mail: ziqi.zhang@sheffield.ac.uk.9,000 datasets and more than 85 billion triples.1 Itis becoming a gigantic, constantly growing and ex-tremely valuable knowledge source useful to many ap-plications [16,30]. Following the rapid growth of theWeb of Data is the increasingly pressing issue of het-1http://stats.lod2.eu/, visited on 30-09-2015.This article is published online with Open Access and distributed under the terms of the Creative Commons Attribution Non-Commercial License.1570-0844/15/$35.00 © 2015 – IOS Press and the authors.mailto:ziqi.zhang@sheffield.ac.ukmailto:a.gentile@sheffield.ac.ukmailto:i.augenstein@sheffield.ac.ukmailto:f.ciravegna@sheffield.ac.ukmailto:eva.blomqvist@liu.semailto:ziqi.zhang@sheffield.ac.ukhttp://stats.lod2.eu/UNCORRECTED  PROOF2 Z. Zhang et al. / An unsupervised data-driven method to discover equivalent relations in large Linked Datasetserogeneity, the phenomenon that multiple vocabular-ies exist to describe overlapping or even the samedomains, and the same objects are labeled with dif-ferent identifiers. The former is usually referred toschema-level heterogeneity and the latter as data orinstance-level heterogeneity. It is widely recognizedthat currently LOD datasets are characterized by denselinks at data-level but very sparse links at schema-level[15,24,38]. This may hamper the usability of data overlarge scale and reduces interoperability between Se-mantic Web applications built on LOD datasets. This",
        "publication_date": "2016-12-06",
        "authors": "Pierre-Yves Vandenbussche, Ghislain Auguste Atemezing, María Poveda‐Villalón, Bernard Vatant",
        "file_name": "10!3233%sw-160213.pdf",
        "file_path": "output/PDFs/10!3233%sw-160213.pdf",
        "pdf_link": null
    },
    {
        "title": "NLP4Types: Predicting Types Using NLP.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Type inference for resources in Knowledge Graphs is a widelystudied problem, for which different approaches have been proposed, in-cluding reasoning, statistical analysis, and the usage of the textual in-formation related to the resources. We focus on the latter, exploitingtext classification techniques for predicting semantic types from textualdescriptions. In this paper we introduce NLP4Types, an online tool thatcombines different standard NLP techniques and classifiers for predicttypes based on DBpedia abstracts, as well as to collect feedback fromthe users for those predictions.Keywords: DBpedia, Natural Language Processing, Data Quality, Linked Data1 IntroductionType statements, that is, assertions of types for entities, are the most basic andfundamental piece of information for semantic resources. This information can begenerated by different means, including manual, automated and semi-automatedapproaches. In this paper we cover DBpedia, which is generated automaticallyfrom the information contained in Wikipedia, using a set of translation map-pings, from the entries in tabular format contained the infoboxes of each page.As not all pages contain infoboxes it is not always possible to generate type infor-mation. According to our calculation, around a 16% of resources from Wikipediado not have any type mapped to DBpedia. We have also to take into accountthat, even in those cases in which this information can be generated, it is not al-ways complete or correct, as mappings are defined manually and collaborativelyby users.In this paper we explore how textual abstracts can be exploited, using NLPtechniques, to classify entries into the DBpedia ontology. We combine document-to-term matrix and Named Entity Recognition to train a model that we lateruse to predict types from free text on our tool. We have evaluated our modelusing K-fold evaluation and a well-known gold standard, obtaining high results.The final result of this process is NLP4Types1, an online tool that allows userto explore type predictions and to collect feedback from them.∗This work was partially funded by projects RTC-2016-4952-7, TIN2013-46238-C4-2-R and TIN2016-78011-C4-4-R, from the Spanish State Investigation Agency of theMINECO and FEDER Funds.1http://nlp4types.linkeddata.es2 Related WorkTyping resources on large datasets is a widely studied problem that has beenaddressed during last decade, being SDType [7] the most prominent system.SDType exploits the statistical information of property distribution to infer newtyping statements. Other approaches have been introduced, exploiting differentNLP-based techniques for type assignment based on text [3, 2]. In [5] a hier-archy of Support Vector Machines (hSVM) is introduced for applying lexico-syntactic patterns using a bag-of-words model, extracted from short abstractsand Wikipedia categories. This work extends the Linked Hypernym DatasetFramework [4], by the same authors, for extracting these pattern-based struc-tures. These works introduce also the LHD Gold Standard dataset, which we usein this paper, to measure the performance of our system and compare it to otherexisting tools. This gold standard has been produced, as reported by authors,using experts to assign types to a subset of the English DBpedia resources. Wehave used it to evaluate our system, as it provides means for comparing ourcontribution to both, hSVM and SDType.",
        "publication_date": "2018-01-01",
        "authors": "Idafen Santana-Pérez, Mariano Rico",
        "file_name": "20250514083008.pdf",
        "file_path": "output/PDFs/20250514083008.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2262/ekaw-demo-19-poster.pdf"
    },
    {
        "title": "A Generalized Component Efficiency and Input-Data Generation Model for Creating Fleet-Representative Vehicle Simulation Cases in VECTO",
        "implementation_urls": [],
        "doi": "10.4271/2019-01-1280",
        "arxiv": null,
        "abstract": "Abstract The Vehicle Energy Consumption calculation Tool (VECTO) is used for the official calculation and reporting of CO2 emissions of HDVs in Europe. It uses certified input data in the form of energy or torque loss maps of driveline components and engine fuel consumption maps. Such data are proprietary and are not disclosed. Any further analysis of the fleet performance and CO2 emissions evolution using VECTO would require generic inputs or reconstructing realistic component input data. The current study attempts to address this issue by developing a process that would create VECTO input files based as much as possible on publicly available data. The core of the process is a series of models that calculate the vehicle component efficiency maps and produce the necessary VECTO input data. The process was applied to generate vehicle input files for rigid trucks and tractor-trailers of HDV Classes 4, 5, 9 and 10. Subsequently, evaluating the accuracy of the process, the simulation results were compared with reference VECTO results supplied by various vehicle manufacturers. The results showed that the difference between simulated and reference CO2 emissions was on average -0.6% in the Long Haul cycle and 1% in the Regional Delivery. Such a process could be a powerful tool for calculating HDV CO2 emissions for development and analysis purposes, e.g. for new vehicle prototypes or multistage vehicles, and for creating VECTO equivalent models that can be used to assess alternative operating conditions and mission profiles of existing vehicle models. The methodology was applied for creating input of various components in the US tool for HDV certification, GEM, for generic sample-vehicle models available. Introduction Heavy Duty Vehicles (HDV) account for 4% of the vehicle fleet, but despite their low share contribute to 30% of the overall greenhouse gas emissions of road transport [1]. The European Union has committed to reducing road CO2 emissions [2], and for this reason, the European Commission has introduced a CO2 emissions certification approach that is based on vehicle simulation [3] and subsequently proposed CO2 reduction targets for the decade 2020-2030 [4]. Using simulation tools for certifying CO2 emissions in the HDV sector was the solution of choice because HDV configurations are highly customizable and a laboratory measurement-based approach, like in the case of light-duty vehicles, would not be possible. A similar approach has also been adopted in other regions such as the United States and Japan [5]. In this context, the Vehicle Energy Consumption calculation Tool (VECTO) was developed as the official tool for calculating and reporting HDV CO2 emissions.  VECTO was first launched in 2012 and has since then undergone various updates in order to become in 2017 the official tool to be used for the certification of HDV CO2 emissions in the EU [6,5]. HDVs are highly customizable, but they exhibit some common characteristics based on their intended use, which enabled their classification into HDV classes. VECTO is capable of simulating all ",
        "publication_date": "2019-04-02",
        "authors": "Nikiforos Zacharof, Alessandro Tansini, Ignacio-Iker Prado-Rujas, Theodorοs Grigoratos, Georgios Fontaras",
        "file_name": "10!4271%2019-01-1280.pdf",
        "file_path": "output/PDFs/10!4271%2019-01-1280.pdf",
        "pdf_link": null
    },
    {
        "title": "Completeness and consistency analysis for evolving knowledge bases",
        "implementation_urls": [
            {
                "identifier": "http://github.com/AKSW/RDFUnit",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1016%j!websem!2018!11!004.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "@prefix sh: <http ://www.w3.org/ns/shacl# > ."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.11.004",
        "arxiv": "1811.12721",
        "abstract": "AbstractAssessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessmentprocedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curatethe data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas ofquality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolutionanalysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality,and range constraints. For completeness analysis, we use data profiling information from consecutive knowledge base releases toestimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the resultsof the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively andqualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach isevaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpediaKB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learningmodels over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the bestperforming model in our experimental setup is the Random Forest, reaching an F1 score greater than 90% for minimum andmaximum cardinality and 84% for range constraints.Keywords: Quality Assessment, Evolution Analysis, Validation, Knowledge Base, RDF Shape, Machine Learning1. IntroductionIn recent years, numerous efforts have been put towards shar-ing Knowledge Bases (KBs) in the Linked Open Data (LOD)cloud1. This has led to the creation of large corpora, makingbillions of RDF2 triples available from different domains suchas Geography, Government, Life Sciences, Media, Publication,Social Networking, and User generated data. These KBs evolveover time: their data instances and schemas are updated, ex-tended, revised and refactored [1]. Unlike in more controlledtypes of knowledge bases, the evolution of KBs exposed in theLOD cloud is usually unrestrained [2], which may cause datato suffer from a variety of quality issues, at both schema leveland data instance level. Considering the aggregated measure ofconformance, the empirical study carried out by Debattista etal. [2] shows that datasets published in the LOD cloud havereasonable overall quality, but significant issues remain con-cerning different quality metrics, such as data provenance andlicensing. Therefore, by looking at individual metrics, we canexplore certain aspects, for example data quality issues in thedata collection or integration processes.Data quality relates to the perception of the “fitness for use”in a given context [3]. One of the common tasks for data qual-1http://lod-cloud.net2https://www.w3.org/RDFity assessment is to perform a detailed data analysis with dataprofiling [4]. Data profiling is usually defined as the process ofexamining data to collect statistics and provide relevant meta-data about the data [5]. Based on the information we gatherfrom data profiling, we can thoroughly examine and understanda KB, its structure, and its properties before using the KB. Vari-ous approaches have been developed for KB quality assessmentbased on manual, semi-automatic, and automated approaches.For example, Flemming’s [6] data quality assessment approachevaluate data quality scores based on manual user input for data",
        "publication_date": "2018-11-22",
        "authors": "Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Marco Torchiano, Nandana Mihindukulasooriya, Óscar Corcho, Raúl García‐Castro",
        "file_name": "10!1016%j!websem!2018!11!004.pdf",
        "file_path": "output/PDFs/10!1016%j!websem!2018!11!004.pdf",
        "pdf_link": null
    },
    {
        "title": "A contribution-based framework for the creation of semantically-enabled web applications",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2009.07.004",
        "arxiv": null,
        "abstract": "example, the F-plugin contains one instance of the class Vpoet. Abstract classes and interfaces are written in italics. Fig. 5 shows the layers involved in the development of F-plugins. The upper layer is the API provided by JSPWiki, which provides the abstract class WikiPiugin. Below this layer, the Fortunata API provides a set of generic classes that can be exploited by specific application classes. This is the case of the semantic web application VPOET, which uses the class Vpoet, shown in the layer named \"Semantic Web application\". The last layer, named in the figure \"F-plugin\", is for specific appli­cation plugins. The class Adcivisuai izat ionis an example of the kind of classes that exist in this layer, and howit is related to the other layers. A semantically-enabled web application is represented by a class derived from the abstract class FortunataSWAppii-cat ion, which provides developers with useful methods as well as forcé developers to implement the methods c r éa t e i n ­d iv idua l () and f i i iDataModei( ) concerning semantics persistence. All the plugins in a Fortunata-based application share a semantic web application. In this example, the figure shows the class AddVisual izat ion. This class is an F-plugin, and consequently it inherits the methods implemented in the base class For tuna taP lug in and it is forced to implement three methods (one from the interface WikiPiugin and two from the class For tunataPlugin) . This plugin contains an instance of the class Vpoet, which implements two methods from the class FortunataSWApplication concerning seman­tic data management. The process to créate and contribute an F-plugin is detailed in the upper part of Fig. 6, and follows the usual procedure in any plugin-based architecture. First, the developer must créate the F-plugin locally (steps 1-3) and perform an ade-quate number of tests to check that it is working correctly (step 4). Then she must proceed to the publication (step 5) of the plugin source code and of the documentation about its usage. The bottom part of the figure depicts the process to créate new functionality by reusing the initial functionality following a \"contributively collaboration\" schema. It com-prises the following steps: installation of an existing F-plugin (step 6), reading and understanding of its associated ontol-ogies (either by manually reading the OWL files, using any off-the-shelf ontology editor, or by means of OMEMO) in order to find the elements that must be added, removed or modified, or in order to decide whether a new set of ontologies has to be imported and used (step 7), local creation of the extended plugin (steps 8-10), local tests (step 11) and publication (step 12). The purpose of this detailed explanation is to show the low complexity of the plugin reuse and contribution process. Table 1 summarizes the development tasks that are normally associated to the development of a typical semantic web application, and compares the skills that are required to perform these tasks when using a traditional development approach and a Fortunata-based approach. Traditional development requires more competencies (more development tools and roles) than Fortunata-based development. This is one of the main results of the comparison performed with real developers, which is described in Section 4. 3. OMEMO and VPOET: examples of Fortunata-based semantic web applications This section illustrates how the Fortunata framework can be used to créate two prototypical semantically-enabled web applications. These applications are not intended to be original or innovative, since similar types of applications are available in the current state of the art, but we aim at demonstrating that they are easy to implement and extend using our approach. OMEMO is focused on the HTML publication of ontologies (in a similar fashion to systems like OWLDoc (See http://www.co-ode.org/downloads/owldoc/)), and it is interesting as a case study since it exploits many features of the wiki infrastructure, such as orphan links or the simplicity of the wiki syntax. VPOET is focused on semantic data visualization, and especially http://www.coode.org/downloads/owldoc/http://www.coode.org/downloads/owldoc/http://ode.org/downloads/owldoc/)),r r OÍ Q. 3 ai o O CD ' ü i ",
        "publication_date": "2009-07-12",
        "authors": "Mariano Rico, David Camacho, Óscar Corcho",
        "file_name": "10!1016%j!ins!2009!07!004.pdf",
        "file_path": "output/PDFs/10!1016%j!ins!2009!07!004.pdf",
        "pdf_link": null
    },
    {
        "title": "Using the ODRL Profile for Access Control for Solid Pod Resource Governance",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-11609-4_3",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Beatriz Esteves, Victor Rodrı́guez-Doncel, Harshvardhan J. Pandit, Nicolas Mondada, Pat McBennett",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Is Automated Consent in Solid GDPR-Compliant? An Approach for Obtaining Valid Consent with the Solid Protocol",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/duo-odrl-dpv",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514083213.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available online: https://www.dataprotectioncontrol.org/spec/(accessed on 25 October 2023)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info14120631",
        "arxiv": null,
        "abstract": "Abstract: Personal Information Management Systems (PIMS) are acquiring a prominent role in thedata economy by promoting services that help individuals to have more control over the processing oftheir personal data, in line with the European data protection laws. One of the highlighted solutionsin this area is Solid, a new protocol that is decentralizing the storage of data, through the usage ofinteroperable web standards and semantic vocabularies, to empower its users to have more controlover the processing of data by agents and applications. However, to fulfill this vision and gatherwidespread adoption, Solid needs to be aligned with the law governing the processing of personaldata in Europe, the main piece of legislation being the General Data Protection Regulation (GDPR).To assist with this process, we analyze the current efforts to introduce a policy layer in the Solidecosystem, in particular, related to the challenge of obtaining consent for processing personal data,focusing on the GDPR. Furthermore, we investigate if, in the context of using personal data forbiomedical research, consent can be expressed in advance, and discuss the conditions for validconsent and how it can be obtained in this decentralized setting, namely through the matching ofprivacy preferences, set by the user, with requests for data and whether this can signify informedconsent. Finally, we discuss the technical challenges of an implementation that caters to the previouslyidentified legal requirements.Keywords: personal information management systems; Solid; semantic web; data protection; consent1. IntroductionThe General Data Protection Regulation (GDPR) [1] has become the gold standard inthe European Union (EU) and its effects are being globally felt in Asia, Latin America andAfrica [2].The purpose of the GDPR is twofold: on the one hand, it protects individuals in whatconcerns their human rights and, on the other hand, it enables the free flow of personaldata (Article 1 GDPR). The EU expressed a vision that encompasses the creation of a singleEuropean market for data, where access to personal and non-personal data from acrossthe world is secure and can be used by an ecosystem of companies, governments, andindividuals to provide high-quality data-driven products and services for its citizens whileensuring that “EU law can be enforced effectively” and data subjects are still in control ofwhat happens to their personal data [3].In addition to the GDPR, novel data-related legislation with new data governanceschemes, such as the Data Governance Act (DGA) [4], is being brought forward by the EUto build an infrastructure for data-sharing and to improve citizens’ trust. In particular, trusthas been proven as an important factor that positively influences the perceived usefulnessInformation 2023, 14, 631. https://doi.org/10.3390/info14120631 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14120631https://doi.org/10.3390/info14120631https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-0319-8935https://orcid.org/0000-0003-0259-7560https://doi.org/10.3390/info14120631https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14120631?type=check_update&version=1Information 2023, 14, 631 2 of 33and ease of use of digital personal datastores [5] in data-handling services and allow themto share their sensitive data for the ‘public good’.However, this has not come without challenges in its interpretation and enforcement.",
        "publication_date": "2023-11-24",
        "authors": "Marcu Florea, Beatriz Esteves",
        "file_name": "20250514083213.pdf",
        "file_path": "output/PDFs/20250514083213.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/14/12/631/pdf?version=1700836284"
    },
    {
        "title": "SLoG: Large-Scale Logging Middleware for HPC and Big Data Convergence",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00156",
        "arxiv": null,
        "abstract": "Abstract—Cloud developers traditionally rely on purpose-specific services to provide the storage model they need for anapplication. In contrast, HPC developers have a much morelimited choice, typically restricted to a centralized parallel filesystem for persistent storage. Unfortunately, these systems oftenoffer very low performance when subject to highly-concurrent,conflicting I/O patterns. This makes difficult the implementationof inherently concurrent data structures such as distributedshared logs. Yet, this data structure is key to applications suchas computational steering, data collection from physical sensorgrids or discrete event generators. In this paper we tackle thisissue. We present SLoG, a shared log middleware providing ashared log abstraction over a parallel file system, designed tocircumvent the aforementioned limitations. We evaluate SLoGdesign on up to 100,000 cores of the Theta supercomputer: itdemonstrates high append velocity at scale while also providingsubstantial benefits for other persistent backend storage systems.I. INTRODUCTIONTraditionally, cloud system designers have used purpose-specific storage services for their data storage needs. Theseinclude key-value stores [1]–[3], wide-column databases [4]–[6] or streaming message brokers [7], [8]. On the contrary,High-Performance Computing (HPC) platforms rest upon ona much more constrained set of storage primitives, typicallylimited to parallel file systems [9], [10] or transient burstbuffers [11], [12]. The low availability of local storage on thecompute nodes of most supercomputers today and the lack ofadministrative access give little opportunity for users to deploythe storage system they need.As the boundaries between HPC and Big Data Analytics(BDA) continue to blur [13], new challenges arise. A criticalobjective set in this convergence context is to foster applicationportability across platforms [14]. Let us consider an appli-cation running in a cloud context, which uses a specializedstorage service such as a distributed shared log [15]. Portingthis application to HPC (e.g., to leverage specific hardwarecapabilities) is challenging. Indeed, deploying cloud-orientedshared log services on HPC is often not possible because ofthe unique specificities of these platforms. While one couldconsider a shared log abstraction over the available parallelfile system, providing the illusion of a storage paradigm atopanother is extremely difficult considering conflicting set ofconstraints and APIs between the two models [16], [17].Shared log storage is indeed one of these storage modelsthat are both unavailable and very difficult to implement onHPC platforms using the available storage primitives. Yet,in scientific applications, distributed logs could play manyroles, e.g. for in-situ visualization of large data streams,collection of telemetry events for computational steering, ordata aggregation from arrays of physical sensors. A shared",
        "publication_date": "2018-07-01",
        "authors": "Pierre Matri, Philip Carns, Robert Ross, Alexandru Costan, Marı́a S. Pérez, Gabriel Antoniu",
        "file_name": "10!1109%icdcs!2018!00156.pdf",
        "file_path": "output/PDFs/10!1109%icdcs!2018!00156.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic Web Technologies and E-Business",
        "implementation_urls": [],
        "doi": "10.4018/978-1-59904-192-6",
        "arxiv": null,
        "abstract": "Service/Protocol/Abstract Process Mediation The parties involved in a conversation may have not only the needs for data transformation described in the previous section, but also different message interaction models. It is at this stage where service mediation (aka protocol mediation or abstract process mediation) is needed.  Let us see first how the message interaction patterns to be used in a conversation are expressed in the context of service-oriented architectures. In the Web services world, two different types of interactions are distinguished: orchestration and choreography. Web service orchestration describes how Web services can interact with each other at the message level, including the business logic and execution order of the interactions. Web service choreography tracks the sequence of messages that may involve multiple parties and multiple sources (requesters, providers, etc.). With the last enhancements and standards this distinction has almost disappeared and in many cases both aspects are generally called as Web service orchestration. Early work in Web services orchestration included eCo (from CommerceNet, focused on the integration of e-commerce services), WSCL (Web Services Conversation Language, from HP), XLANG (from Microsoft, included in the Microsoft Biztalk Server) and WSFL (Web Services Flow Language, from IBM).  Some of these early proposals were superseded by other languages, and currently the two most important proposals are the following: - BPEL4WS [BPEL4WS] (Business Process Execution Language for Web Services). It superseded XLANG and WSFL and now is being further developed with a different name, WS-BPEL [WS-BPEL], under the standardisation process in OASIS.  - WS-CDL (Web Services Choreography Description Language). It evolves from WSCI [WSCI] (Web Service Choreography Interface) and BPML [BPML] (Business Process Management Language), which in their turn evolved from WSCL. This language is now following the standardisation process in the W3C. All these languages and proposals allow establishing the order in which a set of Web services have to exchange their messages in order to create a more complex business process. In other words, they allow specifying the control flow of composite business processes by means of allowing an effective communication between the Web services involved in the process, including sequential and parallel activities, loops, etc. They provide execution engines that are able to read such specifications and drive the communication, and they also take into account aspects such as the transactionality of operations, callback mechanisms, etc. By allowing the expression of the control flow that a set of Web services must abide to in order to form a complex service, these languages aim at reducing the complexity required to orchestrate Web services, thereby reducing time-to-market and costs, and increasing the overall efficiency for building a complex service. However, the interaction patterns of several Web services involved in a communication do not always have a precise match, that is, do not always follow exactly the same pattern in realising the complex process. Sometimes there can be mismatches like the ones identified in [Cimpian and Mocan, 2005], and summarised in Figure 4: - Unexpected messages. One of the parties receives does not expect to receive a message issued by another. For instance, in a commercial transaction with a credit card a service sends the credit card type, the credit card number, the expiration date, the full name and the pin code, while the service that receives those messages does not expect to receive a pin code, since it does not use it. - Messages in different order. The parties involved in a communication send and receive messages in different orders. In the previous case the sender may send the messages in the order specified above while the receiver expects first the full name and then the rest of the messages. - Messages that need to be split. One of the parties sends a message with multiple ",
        "publication_date": "2007-01-01",
        "authors": "",
        "file_name": "10!4018%978-1-59904-192-6.pdf",
        "file_path": "output/PDFs/10!4018%978-1-59904-192-6.pdf",
        "pdf_link": null
    },
    {
        "title": "ContractFrames: Bridging the Gap Between Natural Language and Logics in Contract Law",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-31605-1_9",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "María Navas-Loro, Ken Satoh, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Technologies and Applications for Big Data Value",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_1",
        "arxiv": null,
        "abstract": "Abstract The continuous and significant growth of data, together with improvedaccess to data and the availability of powerful computing infrastructure, has ledto intensified activities around Big Data Value (BDV) and data-driven ArtificialIntelligence (AI). Powerful data techniques and tools allow collecting, storing,analysing, processing and visualising vast amounts of data, enabling data-drivendisruptive innovation within our work, business, life, industry and society.The adoption of big data technology within industrial sectors facilitates organisa-tions to gain a competitive advantage. Driving adoption is a two-sided coin. On oneside, organisations need to master the technology necessary to extract value frombig data. On the other side, they need to use the insights extracted to drive theirdigital transformation with new applications and processes that deliver real value.This book has been structured to help you understand both sides of this coin andbring together technologies and applications for Big Data Value.This chapter defines the notion of big data value, introduces the Big Data ValuePublic-Private Partnership (PPP) and gives some background on the Big Data ValueAssociation (BDVA)—the private side of the PPP. It then moves on to structure theE. Curry (�)Insight SFI Research Centre for Data Analytics, NUI Galway, Irelande-mail: edward.curry@nuigalway.ieS. AuerLeibniz Universität Hannover, Hanover, GermanyA. J. BerreSINTEF Digital, Oslo, NorwayA. MetzgerPaluno, University of Duisburg-Essen, Essen, GermanyM. S. PerezUniversidad Politécnica de Madrid, Boadilla del Monte, Madrid, SpainS. ZillnerSiemens AG, München, Germany© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_1&domain=pdfmailto:edward.curry@nuigalway.iehttps://doi.org/10.1007/978-3-030-78307-5_12 E. Curry et al.contributions of the book in terms of three key lenses: the BDV Reference Model,the Big Data and AI Pipeline, and the AI, Data and Robotics Framework.Keywords Data ecosystem · Big data value · Data-driven innovation · Big Data1 IntroductionThe continuous and significant growth of data, together with improved access todata and the availability of powerful computing infrastructure, has led to intensifiedactivities around Big Data Value (BDV) and data-driven Artificial Intelligence (AI).Powerful data techniques and tools allow collecting, storing, analysing, processingand visualising vast amounts of data, enabling data-driven disruptive innovationwithin our work, business, life, industry and society. The rapidly increasing volumesof diverse data from distributed sources create significant technical challenges forextracting valuable knowledge. Many fundamental, technological and deploymentchallenges exist in the development and application of big data and data-driven AI",
        "publication_date": "2022-01-01",
        "authors": "Edward Curry, Sören Auer, Arne J. Berre, Andreas Metzger, Marı́a S. Pérez, Sonja Zillner",
        "file_name": "10!1007%978-3-030-78307-5_1.pdf",
        "file_path": "output/PDFs/10!1007%978-3-030-78307-5_1.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-78307-5_1.pdf"
    },
    {
        "title": "PHDD: Corpus of Physical Health Data Disclosure on Twitter During COVID-19 Pandemic",
        "implementation_urls": [
            {
                "identifier": "https://github.com/echen102/COVID-19-TweetIDs",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1007%s42979-022-01097-x.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Data and Material Availability The data that support the findings of this study are openly available in zenodo at https://​doi.​org/​10.​5281/​ zenodo.​45383​59."
                    }
                ]
            }
        ],
        "doi": "10.1007/s42979-022-01097-x",
        "arxiv": null,
        "abstract": "AbstractHealth-related information is considered as ‘highly sensitive’ by the European General Data Protection Regulations (GDPR) and determining whether a text document contains health-related information or not is of interest for both individuals and companies in a number of different scenarios. Although some efforts have been made to detect different categories of per-sonal data in texts, including health information, the classification task by machines is still challenging. In this work, we aim to contribute to solving this challenge by building a corpus of tweets being shared in the current COVID-19 pandemic context. The corpus is called PHDD(Corpus of Physical Health Data Disclosure on Twitter During COVID-19 Pandemic) and contains 1,494 tweets which have been manually tagged by three taggers in three dimensions: health-sensitivity status, categories of health information, and subject of health history. Furthermore, a lightweight ontology called PTHI(Privacy Tags for Health Information), which reuses two other vocabularies, namely hl7 and dpv, is built to represent the corpus in a machine-readable format. The corpus is publicly available and can be used by NLP experts for implementation of techniques to detect sensitive health information in textual documents.Keywords  Corpus · NLP · Personal data detection · Health-related data · General data protection regulationIntroductionPeople frequently share their personal information on social media networks without considering the potential consequences and threats to their privacy. Simultaneously, the amount of textual data, of which many are sensitive or personal, collected by various organizations is overgrow-ing [7]. Whenever sensitive or personal data is involved, privacy concerns exist as well: in case of online personal data disclosure, users are exposed to a number of threats, such as identity theft, cyber fraud [9], harassment, bullying [16] discrimination in job, credit and visa applications,1 and maybe to other unknown long term consequences. When sensitive categories of personal data, such as health-related information are disclosed, the results can be even more serious: insurance companies may increase costs by finding users’ individual and family health history [3], for example. Other results such as stigma, discrimination, and prejudice are also expected, especially when highly sensitive diseases such as mental illnesses, sexually transmitted diseases, or physical disabilities are involved [1, 15].Personal data resources collected by data controllers are also at risk. This is because of two main reasons: they could be copied and disseminated by unauthorized parties, or they could be published by companies for different purposes such as research or advertisement, without conducting any saniti-zation or de-identification process. In these situations, data controllers are not protecting personal data, thus failing to comply with regulations such as the European General Data Protection Regulation (GDPR). In that case, the cost of vio-lation is high: up to 20 million euros or 4% of the total inter-national turnover, whichever is higher, would be considered This work has been supported by the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant agreement No. 813497 (PROTECT), and the European Union’s Horizon 2020 research and innovation programme under Grant agreement No. 825182 (Prêt-à-LLOD). *\t Rana Saniei ",
        "publication_date": "2022-04-06",
        "authors": "Rana Saniei, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%s42979-022-01097-x.pdf",
        "file_path": "output/PDFs/10!1007%s42979-022-01097-x.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s42979-022-01097-x.pdf"
    },
    {
        "title": "Rule extraction in unsupervised anomaly detection for model explainability: Application to OneClass SVM",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2021.116100",
        "arxiv": "1911.09315",
        "abstract": "AbstractOneClass SVM is a popular method for unsupervisedanomaly detection. As many other methods, it suffers fromthe black box problem: it is difficult to justify, in an intu-itive and simple manner, why the decision frontier is iden-tifying data points as anomalous or non anomalous. Suchtype of problem is being widely addressed for supervisedmodels. However, it is still an uncharted area for unsuper-vised learning. In this paper, we evaluate several rule ex-traction techniques over OneClass SVM models, as well aspresent alternative designs for some of those algorithms. To-gether with that, we propose algorithms to compute metricsrelated with eXplainable Artificial Intelligence (XAI) regard-ing the \"comprehensibility\", \"representativeness\", \"stability\"and \"diversity\" of the extracted rules. We evaluate our propos-als with different datasets, including real-world data comingfrom industry. With this, our proposal contributes to extendXAI techniques to unsupervised machine learning models.Keywords XAI, OneClass SVM, unsupervised learning,rule extraction, anomaly detection, metrics1. IntroductionResponsible Artificial Intelligence (RAI) is defined as thedifferent AI principles that should be considered when de-veloping and deploying real applications based on AI [1].RAI serves as a methodological framework to both iden-tify core aspects (or AI principles) that should be consideredwhen developing AI solutions while also proposing how toimplement them. These AI principles include aspects suchas Fairness, Explainability, Security, Privacy and Human-centric design [2].The AI principle of Explainability is addressed throughthe use of Explainable AI (XAI) techniques, which can beapplied to black-box models in order to obtain post-hoc ex-planations based on the information that they provide. In theliterature, there are many XAI proposals for supervised MLmodels. However, some of the most recent and thorough re-views on XAI [3, 4, 1, 5] do not mention many applicationsof those techniques to unsupervised learning.Copyright © 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.Outlier detection is one of the tasks where unsupervisedlearning is applied. It is defined as the process of detect-ing anomalous observations within a dataset, and sometimesremove it as a first step within data-mining applications[6]. There is often no prior information about outliers in adataset, hence unsupervised ML algorithms offer the chanceto infer patterns and detect anomalies. However, not only isit important to detect outliers, but also to explain them. Ex-planations can help to understand why a particular datapointhas been labelled anomalous (and what changes in the fea-",
        "publication_date": "2021-10-25",
        "authors": "Alberto Barbado, Óscar Corcho, Richard Benjamins",
        "file_name": "10!1016%j!eswa!2021!116100.pdf",
        "file_path": "output/PDFs/10!1016%j!eswa!2021!116100.pdf",
        "pdf_link": null
    },
    {
        "title": "Description of Postdata Poetry Ontology V1.0",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractOne stream of work in the digital humanities focuses on interoperability processesand the description of traditional concepts using computer-readable languages. Inthe case of literary studies, there has been some research into these topics, but thecomplexity of the knowledge domain remains an issue. This complexity is based onthe different interpretations of concepts in different traditions, the use of isolated andprivate databases, unique applications of language and, thus, the richness of poeticinformation. All of this suggests the need to explore new options to represent thecomplexity in computer-readable language. This paper presents an ontology networkdesigned to capture poetry domain knowledge. The ontologies in question relate topoetic works and their structural and prosodic components.1 IntroductionThe Poetry Standardization and Linked Open Data Project, POSTDATA, aimsto provide a means for European poetry researchers to publish and accesssemantically enriched data. To achieve this goal, it was necessary to developa poetry ontology. This ontology attempts to enhance interoperability in theEuropean poetry research community and capture the concepts and propertiesthat define the domain of European poetry knowledge. The development of theontology began with an attempt to define a domain model of poetry based on ananalysis of 23 poetry repertories (i.e. poetry research databases) (Curado Malta,González-Blanco, et al. 2016; Postdata ERC project 2021). These repertorieswere selected because of their relevance, availability in digital format (i.e. allare implemented in databases) and the rich sample they provided of multi-lingual poetry. They ranged from the classical period (e.g. Pedecerto1) to themodern era (e.g. Corpus of Spanish Golden-Age Sonnets2) to the Middle Ages(e.g. Cantigas de Santa Maria for singers.3) This first step allowed us to identifythe most significant identities and properties that define a poetic work (i.e. apoem), taking into account the different traits related to this literary genre. Theresult was a domain model that reflected poetry’s complexity and heterogene-ity. We then transformed this domain model into an ontology network, whichwould allow for its effective and extensive use in computational frameworksas a Post-data computer-aided annotation tool. In this paper, we present thefirst version of our network’s four most significant ontologies (i.e. version 1.0).4These ontologies relate to poetic works, their structural and prosodic compo-nents and information about relevant dates. To populate the ontologies, weincorporated the ontology definitions into OMEKA, a framework that facilitatesthe use of these ontologies in research tasks. This article is structured as follows:In Section 2, we present some previous results related to ontologies of literature,especially of the poetry domain. Section 3 describes the methodology that weused to develop our ontologies. Section 4 presents a detailed description ofthe most relevant ontologies that we created. Finally, Section 5 outlines ourconclusions and directions for future work.1 http://www.pedecerto.eu2 https://github.com/bncolorado/CorpusSonetosSigloDeOro3 http://www.cantigasdesantamaria.com/4 http://postdata.linhd.uned.es/results/network-of-ontologies/http://www.pedecerto.euhttps://github.com/bncolorado/CorpusSonetosSigloDeOrohttp://www.cantigasdesantamaria.com/http://postdata.linhd.uned.es/results/network-of-ontologies/",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514083501.pdf",
        "file_path": "output/PDFs/20250514083501.pdf",
        "pdf_link": "https://www.plottingpoetry.org/books/tackling_toolkit/pdf/02_diezplatas.pdf"
    },
    {
        "title": "Ontology Development by Reuse",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_7",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2011-12-01",
        "authors": "Mariano Fernández‐López, Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Knowledge Graph Construction with R2RML and RML: An ETL System-based Overview",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have proven to be a powerful technologyto integrate and structure the myriad of data available nowadays. Thesemantic web community has actively worked on data integration sys-tems, providing an important set of engines and mapping languages tofacilitate the construction of knowledge graphs. Despite these impor-tant efforts, there is a lack of objective evaluations of the capabilities ofthese engines in terms of performance, scalability, and conformance withmapping specifications. In this work, we conduct such evaluation consid-ering several R2RML and RML processors to identify their strengths andweaknesses. We (i) perform a qualitative analysis of the distinctive fea-tures of each engine, (ii) examine their conformance with the mappinglanguage specification they support, and (iii) assess their performanceand scalability using the GTFS-Madrid-Bench benchmark.Keywords: Knowledge Graphs · RML · R2RML · GTFS-Madrid-Bench1 IntroductionIn recent years, knowledge graphs (KGs) have become one of the most widelyused technologies in data integration reaching the top positions of the GartnerHype Cycle for Artificial intelligence in 20203. This popularity has resulted inopen KGs like Wikidata [25] or YAGO [12], and in the adoption of this tech-nology by major technology companies such as Facebook, Google, or eBay [19].To construct KGs from non-RDF data sources, mapping languages allow practi-tioners to define the relationships between input data sources and ontologies inCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0002-8235-7331https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-2924-7272https://orcid.org/0000-0002-5603-6390https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/Arenas-Guerrero et al.a declarative and maintainable manner [17]. Although there are several mappinglanguages in the state of the art (e.g., SPARQL-Generate [16] or ShExML [9]),there are two of them that stand out: R2RML [5], which is the W3C standardlanguage for RDB2RDF mapping, and RML [8], which is a well-known extensionof R2RML for data formats beyond relational databases (RDBs).KGs can be constructed with [R2]RML-compliant engines that can imple-ment two strategies: materialization or virtualization [20]. The former is theETL approach that generates the entire KG (i.e., all the triples), while the lat-ter generates results for SPARQL queries by translating them to the native querylanguage of the input data source (e.g., SQL queries in the case of RDBs) [3,21].Given the high number of engines available [7,13,3,23,22,10], it is easy for anypractitioner to get lost in deciding which one best fits their use case. Whilethere are comprehensive and structured evaluations for the virtualization ap-proach [4,15] that ease the user’s choice, there is a lack of such an evaluation for",
        "publication_date": "2021-03-11",
        "authors": "Julián Arenas-Guerrero, Mario Scrocca, Ana Iglesias-Molina, Jhon Toledo, Luis Pozo-Gilo, Daniel Doña, Óscar Corcho, David Chaves-Fraga",
        "file_name": "20250514083504.pdf",
        "file_path": "output/PDFs/20250514083504.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2873/paper11.pdf"
    },
    {
        "title": "Extending Ontology Engineering Practices to Facilitate Application Development",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-17105-5_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Systematic Review of Ontologies for the Water Domain",
        "implementation_urls": [],
        "doi": "10.1002/9781394171460.ch2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-09-16",
        "authors": "Shikha Mehta, Sanju Tiwari, Patrick Siarry, M. A. Jabbar",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Moderate Experiential Learning Approach Applied on Data Science",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-23990-9_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-06-24",
        "authors": "Emilio Serrano, Daniel Manrique",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Association between <i>CFH, CFB, ARMS2, SERPINF1, VEGFR1</i> and <i>VEGF</i> polymorphisms and anatomical and functional response to ranibizumab treatment in neovascular age-related macular degeneration",
        "implementation_urls": [],
        "doi": "10.1111/aos.13519",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-09-19",
        "authors": "Estefanía Cobos, Sergio Recalde, Jaouad Anter, María Hernández‐Sánchez, Carla Barreales, Leticia Olavarrieta, A. Valverde-Megías, Marta Suarez‐Figueroa, Fernando E.S. Cruz, Maximino Abraldes, Julián Pérez‐Pérez, Patricia Fernández‐Robredo, Luís Arias, Alfredo Garcı́a-Layana",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Automating the Response to GDPR’s Right of Access",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/access-right-solid",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%faia220462.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "A demonstration of the developed Solid application is available at https://protect.oeg.fi.upm.es/access-right/and the public repository of the code is accessible at https://github.com/besteves4/access-right-solid for further development."
                    }
                ]
            }
        ],
        "doi": "10.3233/faia220462",
        "arxiv": null,
        "abstract": "Abstract.With the enforcement of the European Union’s General Data Protec-tion Regulation, users of Web services – the ‘data subjects’ –, which arepowered by the intensive usage of personal data, have seen their rightsbe incremented, and the same can be said about the obligations imposedon the ‘data controllers’ responsible for these services. In particular, the‘Right of Access’, which gives users the option to obtain a copy of theirpersonal data as well as relevant details such as the categories of per-sonal data being processed or the purposes and duration of said pro-cessing, is putting increasing pressure on controllers as their executionoften requires a manual response effort, and the wait time is negativelyaffecting the data subjects. In this context, the main goal of this work isthe development of an API, which builds on the previously mentionedstructured information, to assist controllers in the automation of repliesto right of access requests. The implemented API method is then usedin the implementation of a Solid application whose main goal is to assistusers in exercising their right of access to data stored in Solid Pods.Keywords. digital rights management, GDPR, right of access, Solid1. IntroductionWith the enforcement of the European Union’s General Data Protection Reg-ulation (GDPR), users of Web services have seen their rights as GDPR ‘datasubjects’ being expanded when it comes to the processing of their personal data.On the other hand, on top of other GDPR-related obligations, ‘data controllers’,the entities that effectively process the data, have seen an increase in workloadrelated to the response to data subject’s right-related requests. GDPR’s ChapterIII2 details a set of 10 data subject rights, starting with the ‘Right to be Informed’described in Articles 13 and 14 and ending with the ‘Right to object to automateddecision making’ in Article 22. Considering this, data controllers would benefitfrom having the information they need to provide to data subjects in a structuredformat to automate the response to such requests [1]. In particular, the ‘Right of1Corresponding author: beatriz.gesteves@upm.es2https://gdpr-info.eu/chapter-3/Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220462170https://gdpr-info.eu/chapter-3/Access’3 is putting more and more pressure on controllers as they not only haveto provide the purpose for which the data is being used or the types of data beingprocessed but also need to provide a copy of said data. As this task is usuallydone manually, the wait time can negatively affect data subjects.In addition, with the emergence of decentralised data storage solutions, suchas Solid4, as an alternative to the traditional centralised data silos, new challengesappear as the data subject–data controller roles are still not adequately defined inthis decentralised contexts. In this context, the creation of Application Program-ming Interface (API) services would help with the automation of right-relatedrequests as, at their core, they are a ‘request–response’ type of software interface",
        "publication_date": "2022-12-05",
        "authors": "Beatriz Esteves, Victor Rodrı́guez-Doncel, Ricardo Longares",
        "file_name": "10!3233%faia220462.pdf",
        "file_path": "output/PDFs/10!3233%faia220462.pdf",
        "pdf_link": null
    },
    {
        "title": "A Spanish Political Tweets Fine-Tuned Sentiment Analysis Model",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-14859-0_8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-08-27",
        "authors": "Diego M. Jiménez-Bravo, Álvaro Lozano Murciego, Javier Bajo, Daniel H. de la Iglesia, Cristian Pinzón",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A novel deep learning approach using blurring image techniques for Bluetooth-based indoor localisation",
        "implementation_urls": [],
        "doi": "10.1016/j.inffus.2022.10.011",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-10-17",
        "authors": "Reewos Talla-Chumpitaz, Manuel Castillo-Cara, Luis Orozco‐Barbosa, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "RETRACTED: Applying case-based reasoning in social computing to transform colors into music",
        "implementation_urls": [],
        "doi": "10.1016/j.engappai.2018.03.007",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-03-28",
        "authors": "María Navarro-Cáceres, Sara Rodrı́guez, Javier Bajo, Juan M. Corchado",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Challenge-Based Learning in Computational Biology and Data Science.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Data Science is an interdisciplinary field devoted to extractknowledge from large amounts of data. There is a great variety of pro-grams that address the teaching of this field with a growing demand ofprofessionals. However, data science pedagogy tends to emphasize gen-eral aspects of data and the use of tools instead of the its scientificdimension. This position paper describes an ongoing educational inno-vation project for the use of the Challenge-based Learning approach toteach and learn Data Science. In this approach, students work on solv-ing complex and real world problems while the learning is obtained byiterating through three main phases: engage, investigate, and act.Keywords: Challenge-based learning, active learning, experiential learn-ing, project based learning, data science, computational biology.1 IntroductionData science (DS) is an interdisciplinary field devoted to identify patterns and ex-tract knowledge by mining large amounts of structured and unstructured data.Among others, DS includes: machine learning, data processing, statistical re-search, and their related methods. This science has become a revolution thathas changed our manner of doing business, health, politics, education and in-novation [11]. Scientific breakthroughs will be increasingly assisted by advancedcomputing capabilities and DS methods that help researchers manipulate andexplore massive datasets [9].Challenge-based learning (CBL) is a new learning approach created by AppleInc. in collaboration with teachers and leaders in the education community. CBLis “an engaging, multidisciplinary approach that starts with standards-basedcontent and lets students leverage the technology they use in their daily livesto solve complex, real-world problems” [5]. In CBL, students work with otherstudents, their teachers, and experts in their communities and around the worldto develop deeper knowledge of the subjects they are studying.Data science is in a privileged position with respect to other branches ofknowledge to articulate learning through experiences and challenges [18]. The? ORCID ID: 0000-0001-7587-0703?? ORCID ID: 0000-0002-0792-4156Kaggle platform [2] periodically releases a series of competitions on real problemssuch as “Predicting a Biological Response” [4]; which offered 20,000$ to thebest predictive model that linked a biological response of molecules to theirchemical properties. These public competitions have the potential to involveactively the student in a real, significant, and related problematic situation;including a framework for the implementation of a solution to the challenge.This position paper presents an ongoing educational innovation project fo-cusing on using the CBL approach in a DS course, as part of a ComputationalBiology master degree at the Technical University of Madrid (UPM). Studentswill work on challenges at the level of a Kaggle competition with special pref-erence for active and multidisciplinary problems. Based on the 2016 update forthe CBL framework proposed by Apple Inc. [12], students will learn by followingthree main phases: engage, investigate, and act.The paper outline is as follows: after describing the background of the pre-sented innovation project in section 2, the project details are given in section3. These include the scope and students’ profile, the project goals, the time-line and educational resources, the evaluation, resulting products, and diffusionplan. Section 4 explains the expected contribution to the improvement of learn-",
        "publication_date": "2018-01-01",
        "authors": "Emilio Serrano, Martín Molina, Daniel Manrique, Javier Bajo",
        "file_name": "20250514083525.pdf",
        "file_path": "output/PDFs/20250514083525.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2104/paper_236.pdf"
    },
    {
        "title": "Publishing Tourism Statistics as Linked Data a Case Study of Sri Lanka",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_17",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Nandana Mihindukulasooriya, Freddy Priyatna, Mariano Rico",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Graph-based root cause analysis for service-oriented and microservice architectures",
        "implementation_urls": [],
        "doi": "10.1016/j.jss.2019.110432",
        "arxiv": null,
        "abstract": "AbstractService-oriented architectures and microservices define two ways of designing software with the aim ofdividing an application into loosely-coupled services that communicate among each other. This translatesinto rapid development, where each service is developed and deployed by small teams, enabling continuousshipping of new features and fast-evolving applications. However, the underlying complexity of this typeof architecture can hinder observability and maintenance by the user. In particular, identifying the rootcause of an anomaly detected in the application can be a difficult and time-consuming task, consideringthe numerous services and connections to be examined. In this work, we present a root cause analysisframework, based on graph representations of these architectures. The graphs can be used to compareany anomalous situation that happens in the system with a library of anomalous graphs that serves as aknowledge base for the user troubleshooting those anomalies. We use the Grid’5000 testbed to deploy threedifferent architectures and inject a set of anomalies. The results show how our graph-based approach is19.41% more effective than a machine learning method that does not take into account the relationshipbetween elements.Keywords: SOA, Microservices, Root Cause Analysis, Containers, Graphs1. IntroductionAs many industries increasingly rely on information systems to operate efficiently, software developmentand architectures have evolved in different directions. Virtualisation has gained momentum in the last decadethanks to the ability of cloud providers to offer Infrastructure as a Service (IaaS) to their clients. This enablescustomers to use processing power on demand through Virtual Machines (VMs) and it eliminates the burdenof maintaining the infrastructure. It also allows the provider to optimize the utilization of the datacenterby means of VM migration and consolidation. The next step in virtualization has been containerisation. Acontainer is an executable unit of packaged software that includes all the dependencies needed. It runs onthe host operating system, sharing the kernel and using its own user space, isolating it from other containers.This eliminates any boot time overhead in comparison with VM’s, empowering the migration and scalingadvantages of virtualisation. A machine can also host dozens of containers at the same time1 thanks to theirlightweight nature. Finally, from a software development point of view, they eliminate the “it works on mymachine” problem, since the whole running environment is included inside the container.Industry has acknowledged all the containers benefits [1], specially thanks to the adoption of technolo-gies like Docker [2], with DevOps [3] and microservice architectures becoming growing trends and commonEmail addresses: abrandon@fi.upm.com (Álvaro Brandón), marc.solesimo@ca.com (Marc Solé),alberto.huelamosegura@ca.com (Alberto Huélamo), david.solans@ca.com (David Solans), mperez@fi.upm.com (Maŕıa S.Pérez), victor.muntes@ca.com (Victor Muntés-Mulero)1https://www.datadoghq.com/docker-adoption/ (last accessed Dec 2018)Preprint submitted to Elsevier October 7, 2019practices. The philosophy behind microservices follows the same principles used in service-oriented archi-tectures [4]. It consists of breaking the system logic into different, small units where each one has a singletask or responsibility. The different units or microservices communicate and cooperate with each other toprovide the system functionality as a whole. Any of the aforementioned units of logic is executed inside acontainer. In comparison with a monolithic architecture, it allows the user to scale specific components ofthe application by increasing the number of containers responsible for that part. Besides, it also enablesnew paradigms like serverless computing, where certain events trigger the allocation of new containers, ab-stracting the infrastructure needed by the user [5]. Finally, it also facilitates the development process andsoftware reuse [6].Coordinating and scaling these containers require an orchestrating unit, specially if we want to deploythem in a distributed infrastructure with several machines. Tools like Kubernetes2 or DC/OS3 have filledthis gap. These platforms have self-healing features, where unhealthy or faulty containers can be relaunched,or containers migrated to a different machine in case one of their hosts dies. But establishing the root causeof these failures can be really complex, specially when we have a network of different services that depend oneach other. The troubleshooting process normally involves a tedious search through logs across the different",
        "publication_date": "2019-10-15",
        "authors": "Álvaro Brandón, Marc Solé, Alberto Huélamo, David Noguéro, Marı́a S. Pérez, Víctor Muntés-Mulero",
        "file_name": "10!1016%j!jss!2019!110432.pdf",
        "file_path": "output/PDFs/10!1016%j!jss!2019!110432.pdf",
        "pdf_link": null
    },
    {
        "title": "Keeping up with storage: Decentralized, write-enabled dynamic geo-replication",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2017.06.009",
        "arxiv": null,
        "abstract": "AbstractLarge-scale applications are ever-increasingly geo-distributed. Maintaining the highest possible data locality is crucialto ensure high performance of such applications. Dynamic replication addresses this problem by dynamically creatingreplicas of frequently accessed data close to the clients. This data is often stored in decentralized storage systems suchas Dynamo or Voldemort, which offer support for mutable data. However, existing approaches to dynamic replicationfor such mutable data remain centralized, thus incompatible with these systems. In this paper we introduce a write-enabled dynamic replication scheme that leverages the decentralized architecture of such storage systems. We proposean algorithm enabling clients to locate tentatively the closest data replica without prior request to any metadata node.Large-scale experiments on various workloads show a read latency decrease of up to 42% compared to other state-of-the-art, caching-based solutions.Keywords: cloud, replication, geo-replication, storage, fault-tolerance, consistency, database, key-value store1. IntroductionLarge-scale applications such as social networks arebeing increasingly deployed over multiple, geograph-ically distributed datacenters (or sites). Such geo-distribution provides fast data access for end-usersworldwide while improving fault-tolerance, disaster-recovery and minimizing bandwidth costs. Today’scloud computing services [1, 2] allow a wider range ofapplications to benefit from these advantages as well.However, designing geo-distributed applications is dif-ficult due to the high and often unpredictable latencybetween sites [3].A key factor impacting application performance isdata locality, i.e. the location of the data relativelyto the application. Accessing remote data is orders ofmagnitude slower than using local data. Although suchremote accesses may be acceptable for rarely-accesseddata (cold data), they hinder application performanceEmail addresses: pmatri@fi.upm.es (Pierre Matri),mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),luc.bouge@ens.ens-rennes.fr (Luc Bougé),gabriel.antoniu@inria.fr (Gabriel Antoniu)for frequently-used data (hot data). For instance, in asocial network application, popular profiles should bereplicated at all sites whereas others can remain locatedat fewer locations. Finding the right balance betweenreplication and storage is critical: replicating too manyprofiles wastes memory, while failing to replicate popu-lar ones results in degraded application performance.Dynamic replication [4] proposes to solve this issueby dynamically replicating hot data as close as possibleto the applications that access it. This technique is lever-aged in Content Delivery Networks (CDN) to cache im-mutable data close to the final user [5, 6]. Similarily, itis used in storage systems such as GFS [7] or HDFS [8]to replicate mutable data, by relying on the centralizedmetadata management of these systems [9, 10]. Yet,such an approach contradicts the design principles of",
        "publication_date": "2017-06-29",
        "authors": "Pierre Matri, Marı́a S. Pérez, Alexandru Costan, Luc Bougé, Gabriel Antoniu",
        "file_name": "10!1016%j!future!2017!06!009.pdf",
        "file_path": "output/PDFs/10!1016%j!future!2017!06!009.pdf",
        "pdf_link": null
    },
    {
        "title": "Inferring Types on Large Datasets Applying Ontology Class Hierarchy Classifiers: The DBpedia Case",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_21",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Mariano Rico, Idafen Santana-Pérez, Pedro del Pozo-Jiménez, Asunción Gómez‐Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Tool Suite to Enable Web Designers, Web Application Developers and End-users to Handle Semantic Data1",
        "implementation_urls": [],
        "doi": "10.4018/jswis.2010070103",
        "arxiv": null,
        "abstract": "’97 extended abstracts on human factors in computing systems(pp. 168–169). New York, NY, USA: ACM. (Los Angeles, USA.April 18-23)Pietriga, E. (2001). Isaviz: A visual authoring tool for rdf.see http://www.w3.org/2001/11/isaviz/. Available from http://www.w3.org/2001/11/IsaViz/Quan, D., & Karger, D. (2004). How to Make a Semantic WebBrowser. In International world wide web conference. proceed-ings of the 13th international conference on world wide web.session: Semantic interfaces and owl tools. (ISBN:1-58113-844-X)Rico, M., Camacho, D., & Corcho Óscar. (2008). VPOET: Usinga Distributed Collaborative Platform for Semantic Web Appli-cations. In C. Badica, G. Mangioni, V. Carchiolo, & D. Bur-descu (Eds.), Intelligent distributed computing, systems and ap-plications. proc. 2nd international symposium on intelligent dis-tributed computing (idc’2008) (pp. 167–176). Springer. (ISBN:978-3-540-85256-8)Rico, M., Camacho, D., & Corcho Óscar. (2009a). Macros vs.scripting in VPOET. In 5th Workshop on Scripting and De-velopment for the Semantic Web (SFSW2009) at the 6th AnnualEuropean Semantic Web Conference (ESWC). CEUR online pro-ceedings, Volume 449.Rico, M., Camacho, D., & Corcho Óscar. (2009b). VPOET Tem-plates to Handle the Presentation of Semantic Data Sources inWikis. In Fourth Workshop on Semantic Wikis: The SemanticWiki Web (SemWiki2009) at the 6th Annual European Seman-tic Web Conference (ESWC). CEUR online proceedings, Volume464 (pp. 186–190).Rico, M., Camacho, D., & Corcho Óscar. (2010). A Contribution-based Framework for the Creation of Semantically-enabled WebApplications. Journal of Information Sciences, 180(10), 1850–1864.Rochen, R., Rosson, M., & Pérez, M. (2006). End user Devel-opment of Web Applications. In H. Lieberman, F. Paternò, &V. Wulf (Eds.), (p. 161-182). Springer.Souzis, A. (2006). Bringing the wiki-way to the seman-tic web with rhizome. In M. Völkel & S. Schaffert (Eds.),Semwiki2006, proceedings of the first workshop on semanticwikis. CEUR-WS.org. Available from http://www.ceur-ws.org/Vol-206/paper19.pdf (Budva, Montenegro, June 12,2006)",
        "publication_date": "2010-01-01",
        "authors": "Mariano Rico, Óscar Corcho, José A. Macías, David Camacho",
        "file_name": "10!4018%jswis!2010070103.pdf",
        "file_path": "output/PDFs/10!4018%jswis!2010070103.pdf",
        "pdf_link": null
    },
    {
        "title": "A Proposal for Semantic Integration of Crime Data in Mexico City",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-59872-3_3",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Francisco Carrillo-Brenes, Luis M. Vilches‐Blázquez, Félix Mata",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An extension of Thing Descriptions from the Web of Things for Digital Twins",
        "implementation_urls": [],
        "doi": "10.12688/openreseurope.15280.1",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-12-21",
        "authors": "Salvador González-Gerpe, Andrea Cimmino, Socorro Bernardos, Raúl García‐Castro, María Poveda‐Villalón, Kyriakos Katsigarakis, Georgios N. Lilis, Dimitrios Rovas",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.26342/2022-69-14",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Accepted Tutorials at The Web Conference 2022",
        "implementation_urls": [],
        "doi": "10.1145/3487553.3547182",
        "arxiv": null,
        "abstract": "ABSTRACTThis paper summarizes the content of the 20 tutorials that havebeen given at The Web Conference 2022: 85% of these tutorials arelecture style, and 15% of these are hands on.CCS CONCEPTS• Computer systems organization→ Embedded systems; Re-dundancy; Robotics; • Networks→ Network reliability.KEYWORDStutorials, the web conferenceACM Reference Format:Riccardo Tommasini, Senjuti Basu Roy, Xuan Wang, Hongwei Wang, HengJi, Jiawei Han, Preslav Nakov, Giovanni Da San Martino, Firoj Alam, MarkusSchedl, Elisabeth Lex, Akash Bharadwaj, Graham Cormode, Milan Dojchi-novski, Jan Forberg, Johannes Frey, Pieter Bonte,Marco Balduini, Matteo Bel-cao, Emanuele Della Valle, Junliang Yu, Hongzhi Yin, Tong Chen, HaochenLiu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Jamell Dacon, Lingjuan Lyu, Jil-iang Tang, Aristides Gionis, Stefan Neumann, Bruno Ordozgoiti, SimonRazniewski, Hiba Arnaout, Shrestha Ghosh, Fabian Suchanek, Lingfei Wu,Yu Chen, Yunyao Li, Bang Liu, Filip Ilievski, Daniel Garijo, Hans Chalupsky,Pedro Szekely, Ilias Kanellos, Dimitris Sacharidis, Thanasis Vergoulis, Nuren-dra Choudhary, Nikhil Rao, Karthik Subbian, Srinivasan Sengamedu, Chan-dan K. Reddy, Friedhelm Victor, Bernhard Haslhofer, George Katsogiannis-Meimarakis, Georgia Koutrika, Shengmin Jin, Danai Koutra, Reza Zafarani,Yulia Tsvetkov, Vidhisha Balachandran, Sachin Kumar, Xiangyu Zhao, BoChen, Huifeng Guo, YejingWang, Ruiming Tang, Yang Zhang,WenjieWang,Peng Wu, Fuli Feng, and Xiangnan He. 2022. Accepted Tutorials at TheWeb Conference 2022. In Companion Proceedings of the Web Conference 2022(WWW ’22 Companion), April 25–29, 2022, Virtual Event, Lyon, France. ACM,New York, NY, USA, 9 pages. https://doi.org/10.1145/3487553.35471821 INTRODUCTIONThe Web Conference is pleased to host 20 tutorials for the 2022edition chaired in Lyon, France.In the attempt to foster interesting discussions as well as sup-porting the dissemination of prominent research areas, the tutorialchairs have selected high-quality contributions that cover a varietyof topics. These range from fact-checking to natural language pro-cessing, deep learning, knowledge graphs, as well as the analysisof crypto assets. In particular, following the successful approach ofprevious years, this edition will host two tutorial formats:• lecture-style (85%) tutorials will cover the state-of-the-artresearch, development, and applications in a specific webcomputing and related area, and stimulate and facilitate fu-ture work.∗Tutorial co-chairPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or a",
        "publication_date": "2022-04-25",
        "authors": "Riccardo Tommasini, Senjuti Basu Roy, Xuan Wang, Hongwei Wang, Heng Ji, Jiawei Han, Preslav Nakov, Giovanni Da San Martino, Firoj Alam, Markus Schedl, Elisabeth Lex, Akash Bharadwaj, Graham Cormode, Milan Dojchinovski, Jan Forberg, Johannes Frey, Pieter Bonte, Marco Balduini, Matteo Belcao, Emanuele Della Valle, Junliang Yu, Hongzhi Yin, Tong Chen, Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Jamell Dacon, Lingjuan Lye, Jiliang Tang, Aristides Gionis, Stefan Neumann, Bruno Ordozgoiti, Simon Razniewski, Hiba Arnaout, Shrestha Ghosh, Fabian M. Suchanek, Lingfei Wu, Yu Chen, Yunyao Li, Bang Liu, Filip Ilievski, Daniel Garijo, Hans Chalupsky, Pedro Szekely, Ilias Kanellos, Dimitris Sacharidis, Thanasis Vergoulis, Nurendra Choudhary, Nikhil Rao, Karthik Subbian, Srinivasan H. Sengamedu, Chandan K. Reddy, Friedhelm Victor, Bernhard Haslhofer, George Katsogiannis- Meimarakis, Georgia Koutrika, Shengmin Jin, Danai Koutra, Reza Zafarani, Yulia Tsvetkov, Vidhisha Balachandran, Sachin Kumar, Xiangyu Zhao, Bo Chen, Huifeng Guo, Yejing Wang, Ruiming Tang, Yang Zhang, Wenjie Wang, Peng Wu, Fuli Feng, Xiangnan He",
        "file_name": "10!1145%3487553!3547182.pdf",
        "file_path": "output/PDFs/10!1145%3487553!3547182.pdf",
        "pdf_link": null
    },
    {
        "title": "Ontologies Supporting Research-Related Information Foraging Using Knowledge Graphs: Literature Survey and Holistic Model Mapping",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-61244-3_6",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Viet Bach Nguyen, Vojtěch Svátek, Gollam Rabby, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Polypharmacy and Drug–Drug Interactions in People Living With Human Immunodeficiency Virus in the Region of Madrid, Spain: A Population-Based Study",
        "implementation_urls": [],
        "doi": "10.1093/cid/ciz811",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2019-08-17",
        "authors": "Beatriz López Centeno, Carlos Badenes-Olmedo, Angel Mataix-San-Juan, Katie McAllister, José M. Bellón, Sara Gibbons, Pascual Balsalobre, Leire Pérez‐Latorre, Juana Benedı́, Catia Marzolini, Ainhoa Aranguren-Oyarzábal, Saye Khoo, María J Calvo-Alcántara, Juan Berenguer",
        "file_name": "10!1093%cid%ciz811.pdf",
        "file_path": "output/PDFs/10!1093%cid%ciz811.pdf",
        "pdf_link": null
    },
    {
        "title": "Practical challenges of ODRL and potential courses of action",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3587628",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-28",
        "authors": "Andrea Cimmino, Juan Cano-Benito, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Typology-based semantic labeling of numeric tabular data",
        "implementation_urls": [],
        "doi": "10.3233/sw-200397",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-09-29",
        "authors": "Ahmad Alobaid, Emilia Kacprzak, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.18293/SEKE2020-104",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards the Use of Ontologies in Remotely Piloted Aircraft Systems Conceptual Design: Opportunities and Challenges",
        "implementation_urls": [],
        "doi": "10.2514/6.2021-1061",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-04",
        "authors": "Álvaro Gómez-Rodríguez, María Poveda‐Villalón, Raúl García‐Castro, Asunción Gómez‐Pérez, Cristina Cuerno-Rejado",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "arxiv": null,
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activitieswhen browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead tounexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniquesthat uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extendingthose queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations onboth scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document Similarity, Information Search and Retrieval, Clustering, Topic Models, Hashing1. IntroductionHuge amounts of documents are publicly availableon the Web offering the possibility of extracting knowl-edge from them (e.g. scientific papers in digital jour-nals). Document similarity comparisons in many in-formation retrieval (IR) and natural language process-ing (NLP) areas are too costly to be performed in suchhuge collections of data and require more efficient ap-*Corresponding author. E-mail: cbadenes@fi.upm.es.proaches than having to calculate all pairwise similari-ties.In this paper we address the problem of programmat-ically generating annotations for each of the items in-side big collections of textual documents, in a way thatis computationally affordable and enables a semantic-aware exploration of the knowledge inside it that state-of-the-art methods relying on topic models are not ableto materialize.Most text mining algorithms represent documentsin a common feature space that abstracts the specific1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reserved2 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithmsequence of words used in each document and, withappropriate representations, facilitate the analysis ofrelationships between documents even when writtenusing different vocabularies. Although a sparse wordor n-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. LatentSemantic Indexing (LSI) [17], Probabilistic Latent Se-mantic Indexing (PLSI) [25] and more recently, LatentDirichlet Allocation (LDA) [11], which is the simplestprobabilistic topic model (PTM) [10], are algorithmsfocused on reducing feature space by annotating docu-ments with thematic information. PLSI and PTM alsoallow a better understanding of the corpus through thetopics discovered, since they use probability distribu-tions over the complete vocabulary to describe them.",
        "publication_date": "2020-05-01",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": "output/PDFs/10!3233%sw-200373.pdf",
        "pdf_link": null
    },
    {
        "title": "Extension of the BiDO Ontology to Represent Scientific Production",
        "implementation_urls": [],
        "doi": "10.1145/3318396.3318422",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-03-02",
        "authors": "Mariela Tapia-León, Idafen Santana-Pérez, María Poveda‐Villalón, Paola Espinoza-Arias, Janneth Chicaiza, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Requirements Behaviour Analysis for Ontology Testing",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "WoT Store: Enabling Things and Applications Discovery for the W3C Web of Things",
        "implementation_urls": [],
        "doi": "10.1109/ccnc.2019.8651786",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Luca Sciullo, Cristiano Aguzzi, Marco Di Felice, Tullio Salmon Cinotti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Are distributed ledger technologies ready for intelligent transportation systems?",
        "implementation_urls": [],
        "doi": "10.1145/3410699.3413789",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-09-04",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An Agent-Based Model for Exploring Pension Law and Social Security Policies",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-58790-1_4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Emilio Serrano, Ken Satoh",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies",
        "implementation_urls": [],
        "doi": "10.1145/3360901.3364444",
        "arxiv": "2101.03026",
        "abstract": "ABSTRACTWith the ongoing growth in number of digital articles in a wider setof languages and the expanding use of different languages, we needannotation methods that enable browsing multi-lingual corpora.Multilingual probabilistic topic models have recently emerged as agroup of semi-supervised machine learning models that can be usedto perform thematic explorations on collections of texts in multiplelanguages. However, these approaches require theme-aligned train-ing data to create a language-independent space. This constraintlimits the amount of scenarios that this technique can offer solu-tions to train and makes it difficult to scale up to situations wherea huge collection of multi-lingual documents are required duringthe training phase. This paper presents an unsupervised documentsimilarity algorithm that does not require parallel or comparablecorpora, or any other type of translation resource. The algorithmannotates topics automatically created from documents in a sin-gle language with cross-lingual labels and describes documents byhierarchies of multi-lingual concepts from independently-trainedmodels. Experiments performed on the English, Spanish and Frencheditions of JCR-Acquis corpora reveal promising results on classi-fying and sorting documents by similar content.CCS CONCEPTS• Information systems → Digital libraries and archives; In-formation retrieval.KEYWORDScross-lingual semantic similarity; large-scale text analysis; topicmodelsACM Reference Format:Carlos Badenes-Olmedo, José Luis Redondo-García, and Oscar Corcho. 2019.Scalable Cross-lingual Document Similarity through Language-specific Con-cept Hierarchies. In Proceedings of the 10th International Conference onKnowledge Capture (K-CAP ’19), November 19–21, 2019, Marina Del Rey, CA,USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3360901.3364444Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA© 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-7008-0/19/11. . . $15.00https://doi.org/10.1145/3360901.33644441 INTRODUCTIONCross-language information extraction deals with the retrieval ofdocuments written in languages different from the language of theuser’s query. At execution time, the query in the source language istypically translated into the target language of the documents with",
        "publication_date": "2019-09-23",
        "authors": "Carlos Badenes-Olmedo, Jose-Luis Redondo García, Óscar Corcho",
        "file_name": "10!1145%3360901!3364444.pdf",
        "file_path": "output/PDFs/10!1145%3360901!3364444.pdf",
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.15497/RDA00068",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "On The Modeling Of P2P Systems as Temporal Networks: a Case Study With Data Streaming",
        "implementation_urls": [],
        "doi": "10.23919/annsim55834.2022.9859513",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-07-18",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Why are ontologies not reused across the same domain?",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2018.12.010",
        "arxiv": null,
        "abstract": "to DOLCE, one of the main characteristics of an abstract entity is that it has neither spatial nor temporal qualities. Examples of such abstract entities are mathematical ones: a triangle, a cir­cumference, etc. Nevertheless, SUMO defines social role as a sub class of abstract. Let us note that a social role has both spatial and temporal qualities, since it may take place in a particular area along a particular time interval. Let us now suppose that ontology O1 imports SUMO and that O2 imports DOLCE. Let us also suppose that O1 imports O2. In this case, O1 includes contradictory definitions of the term abstract. Given that all the details of the natural language definitions are not formalized in the ontologies, it may be case that no inconsistency appears when a computer reasons with this ontology. However, there will be a contradiction in the intended meaning of the terms of O1. Something similar may happen if a redefinition of classes/prop­erties in the reusing ontology [19,20] takes place. Let us suppose, for example, the following axiom: reused : property rdf : subPropertyOf reusing : property http://www.sparontologies.net/ontologies/scoro/source.htmlhttp://data.semanticweb.org/ns/swc/swc_2009-05-09.htmlhttp://linkedscience.org/teach/ns/http://www.ontoware.org/index.htmlhttp://www.ontologydesignpatterns.org/ont/dul/DUL.owlhttp://www.ontologydesignpatterns.org/ont/d0.owlhttps://w3id.org/def/dul-dolce-zero-en-espannolIf reusing:property were declared as inverse functional, then reused:property would become inverse functional as well. If soft reuse had been carried out, then, formally, the only definition would be the one of the reusing ontology. However, if a third ontology imported both the reused and the reusing ones, there would be, in the third ontology, aformal definition from the reused ontology and another formal definition in the reusing ontology. Concerning the information that was obtained through the OTN use case, several heterogeneity problems were detected. First of all, there are ontologies, in the academic domain, focused on different aspects of such domain but out of the scope of OTN ontology, for example content management, social networking, etc.. Some ontologies represent the same domain as OTN, but from a different perspective. Others do not use Spanish, being the lack of localiza­tion [21]a key factor that hampers reuse. In fact, it has been proved that just 27.04% of the LOV ontologies use languages other than English. Other reasonstonot reuse LOV ontologies belonging tothe Academy category have been deficiencies in the documentation of some candidate ontologies and unreachable imported ontologies. 5. Related work One of the first analysis on ontology reuse was done by Sim-perl [22]. The author presents different use cases on ontology reuse as well as methods and tools. The main conclusion is that the reuse approach is a decision-making problem in which the developer ",
        "publication_date": "2018-12-27",
        "authors": "Mariano Fernández‐López, María Poveda‐Villalón, Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez",
        "file_name": "10!1016%j!websem!2018!12!010.pdf",
        "file_path": "output/PDFs/10!1016%j!websem!2018!12!010.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards metrics-driven ontology engineering",
        "implementation_urls": [],
        "doi": "10.1007/s10115-021-01545-9",
        "arxiv": null,
        "abstract": "AbstractThe software engineering field is continuously making an effort to improve the effectivenessof the software development process. This improvement is performed by developing quan-titative measures that can be used to enhance the quality of software products and to moreaccurately describe, better understand and manage the software development life cycle. Evenif the ontology engineering field is constantly adopting practices from software engineering,it has not yet reached a state in which metrics are an integral part of ontology engineeringprocesses and support making evidence-based decisions over the process and its outputs. Upto now, ontology metrics are mainly focused on the ontology implementation and do not takeinto account the development process or other artefacts that can help assessing the quality ofthe ontology, e.g. its requirements. This work envisions the need for a metrics-driven ontol-ogy engineering process and, as a first step, presents a set of metrics for ontology engineeringwhich are obtained from artefacts generated during the ontology development process andfrom the process itself. The approach is validated by measuring the ontology engineeringprocess carried out in a research project and by showing how the proposed metrics can beused to improve the efficiency of the process by making predictions, such as the effort neededto implement an ontology, or assessments, such as the coverage of the ontology according toits requirements.Keywords Metrics · Ontology engineering · Requirements · Ontology developmentB Alba Fernández-Izquierdoalbafernandez@fi.upm.esMaría Poveda-Villalónmpoveda@fi.upm.esAsunción Gómez-Pérezasun@fi.upm.esRaúl García-Castrorgarcia@fi.upm.es1 Ontology Engineering Group, Escuela Técnica Superior de Ingenieros Informáticos, UniversidadPolitécnica de Madrid, Madrid, Spain123http://crossmark.crossref.org/dialog/?doi=10.1007/s10115-021-01545-9&domain=pdfhttp://orcid.org/0000-0003-2011-3654https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-3037-0331https://orcid.org/0000-0002-0421-452X868 A. Fernández-Izquierdo et al.1 IntroductionSoftware metrics play an important role in the software engineering field, supporting bothdevelopment and managerial decision-making during the software life cycle. These softwaremetrics are not only related to the source code itself, but also to other artefacts that are part ofthe software main product (e.g. requirements, documentation and tests) and to the activitiesneeded to obtain such artefacts. This diversity of metrics enables software engineers to haveenough information to make different types of predictions, assessments and trade-offs, suchas effort and time predictions or software quality analysis [15].Similarly, in the ontology engineering field different metrics exist which try to assess thequality of ontologies by measuring reliability, reusability or cohesion, among other aspects.However, themetrics proposed until now aremostly focused on the ontology implementation,and they do not take into account other artefacts produced during the ontology developmentprocess or even the development process itself. Moreover, they only consider the structureof the ontology [55].",
        "publication_date": "2021-02-23",
        "authors": "Alba Fernández-Izquierdo, María Poveda‐Villalón, Asunción Gómez‐Pérez, Raúl García‐Castro",
        "file_name": "10!1007%s10115-021-01545-9.pdf",
        "file_path": "output/PDFs/10!1007%s10115-021-01545-9.pdf",
        "pdf_link": null
    },
    {
        "title": "Enhancing Public Procurement in the European Union Through Constructing and Exploiting an Integrated Knowledge Graph",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-62466-8_27",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The Challenge: From MPEG Intellectual Property Rights Ontologies to Smart Contracts and Blockchains [Standards in a Nutshell]",
        "implementation_urls": [],
        "doi": "10.1109/msp.2019.2955207",
        "arxiv": null,
        "abstract": "the abstract creation, the work, which is the result of any intellectual endeavour with enough creativity. Works are pure, abstract entities, with no material incarnation whatsoever. Derivative works are a special type of works,  that have been derived from an existing work. Works are fixated into physical manifestations, which are the very first incarnation of works. Manifestations can be instanced, and copied, or they can be transformed into commercial products. Whereas the logical schema of IP entities resembles the Functional Requirements for Bibliographic Records (FRBR) chain [13], the source is somewhat different: MVCO catering for the needs of music and media stakeholders codifies the IP entities mentioned by copyright legislation (as defined by worldwide agreed treaties such as the Berne Convention), whereas FRBR is inspired by the needs of librarians.  A user is defined as an individual or organization, acting in the media value chain. The types of roles, a user  could undertake, revolve around the IP entities, e.g., a creator is defined as the user who creates a work, an adaptor is the user who adapts a work to produce an adaptation. These roles or very similar ones are also acknowledged by copyright legislation. Other roles include producer, distributor and, finally the end-user.  The types of actions that can be performed also revolve around the IP entities. Create work is the action whose result is a new work, produce is the action whose result is a product and so forth. In addition, some other actions do not produce any new IP entity, such as, a public communication or an end-user action (e.g., play and print) but they are legal concepts with explicit mentions and provisions in copyright legislation.  The relationship between a user and a particular IP entity type (e.g., work, adaptation, product, copy) is specified through the concept of role. The actions that a user performs on a given IP entity determine the role of that user with respect to the IP entity in question. Users get roles (e.g., creator, adaptor, producer, end-user) that attribute them rights over actions (e.g., create work, make adaptation, produce, distribute, synchronise) that can be exercised on specific IP entities. Any given user may undertake any number of roles within a given value chain. Figure 1 illustrates these relationships between actions, users and IP entities.  Fig. 1. MVCO defined relationships between actions, users and IP entities.  Authorisation Model The MVCO by defining the relationships between users, actions and IP entities serves well to depict a static picture of the IP information. However, in real life rights are transferable and this dynamic nature of rights was required to be supported in the MVCO.  Transfer of rights are born with the signature of agreements or contracts which grant permissions. A permission relates an IP entity with a right in transit between the original rights owner and the new rights owner. Permissions have an intrinsic dynamic nature: they are granted, invoked and revoked. Instances of a user class will probably be actual companies or persons; instances of works will be actual works. However, instances of permissions are far more interesting due to that they could refer either to the past or in the future. That is, an instance permission (e.g., Alice’s permission to play a song) would be related to both: an end-user instance (e.g., Alice) and an action instance (e.g., play a song). However, what is the interpretation of an action instance? It might be an action effectively executed in the past (e.g., Alice played a song), but it might also be  an action to be performed in the future, as a mere possibility (e.g., Alice can play a song). This is commonly referred in the literature as event factuality, and suggests that action instances can be marked as executed acts or as possible acts. Permissions can also be granted conditionally, that is, subject to certain conditions (facts). Facts can be seen as propositions with an alethic (e.g., true or false) value. These propositions can be combined with logical operators (e.g., conjunction and disjunction) to create more complex ",
        "publication_date": "2020-02-26",
        "authors": "Panos Kudumakis, Thomas Wilmering, M. Sandler, Victor Rodrı́guez-Doncel, Laurent Boch, Jaime Delgado",
        "file_name": "10!1109%msp!2019!2955207.pdf",
        "file_path": "output/PDFs/10!1109%msp!2019!2955207.pdf",
        "pdf_link": null
    },
    {
        "title": "A Review of Bias and Fairness in Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.11.001",
        "arxiv": null,
        "abstract": "AbstractAutomating decision systems has led to hidden biases in the use of artificial intelligence (AI). Consequently, explaining these decisions and identifying responsibilities has become a challenge. As a result, a new field of research on algorithmic fairness has emerged. In this area, detecting biases and mitigating them is essential to ensure fair and discrimination-free decisions. This paper contributes with: (1) a categorization of biases and how these are associated with different phases of an AI model’s development (including the data-generation phase); (2) a revision of fairness metrics to audit the data and AI models trained with them (considering agnostic models when focusing on fairness); and, (3) a novel taxonomy of the procedures to mitigate biases in the different phases of an AI model’s development (pre-processing, training, and post-processing) with the addition of transversal actions that help to produce fairer models.DOI:  10.9781/ijimai.2023.11.001A Review of Bias and Fairness in Artificial IntelligenceRubén González-Sendino1, Emilio Serrano1, Javier Bajo1, Paulo Novais2 *1 Ontology Engineering Group, Departamento de Inteligencia Artificial, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)2 ALGORITMI Research Centre/LASI, University of Minho, Braga (Portugal)* Corresponding author: ruben.gonzalez.sendino@alumnos.upm.es (R. González-Sendino), emilio.serrano@upm.es (E. Serrano), jbajo@fi.upm.es (J. Bajo), pjon@di.uminho.pt (P. Novais).Received 16 September 2022 | Accepted 29 September 2023 | Published 10 November 2023 I.\t IntroductionThe evolution of artificial intelligence (AI) has allowed humans to be heavily supported in the decision-making process of some application domains [1]. The high degree of independence that AI is capable of exhibiting can be problematic [2], [3], especially when humans are not in the loop [4]–[6]. Automatization of decisions can come at the cost of amplifying bias and creating feedback loops [7], [8]. One of the main reasons AI can produce unfair results is due to the data with which it has been trained [9].Although the concept of bias is broad, this paper adheres to the following definition: “the systematic tendency in a model to favor one demographic group/individual over another, which can be mitigated but may well lead to unfairness” [9], [10]. Therefore, the next definition needed to understand the problem this paper studies is Fairness, which is defined as: “ the absence of prejudice or favoritism towards an individual or a group based on its inherent or acquired characteristics” [9].In the AI scope, incorrect predictions do not necessarily indicate that the model is unfair if its development was correct [11]. An unfair model is one whose decisions are biased toward a particular group of people. Moreover, biases cannot always be avoided. Thus, techniques must be used to mitigate their consequences, which aim to increase equality in the results. Data and models can be audited with fairness metrics, which are used to measure fairness between two groups or similar individuals. Furthermore, the categorization of methods for bias and unfairness mitigation depends on the phase of the AI model’s development in which they are used. These phases are typically pre-training, training, and post-training.This paper contributes with a systematic review of bias and fairness in artificial intelligence. The purpose of a systematic review is to provide a comprehensive summary of the literature available which is relevant to several research questions. The three questions addressed ",
        "publication_date": "2023-11-14",
        "authors": "Rubén González-Sendino, Emilio Serrano, Javier Bajo, Paulo Nováis",
        "file_name": "10!9781%ijimai!2023!11!001.pdf",
        "file_path": "output/PDFs/10!9781%ijimai!2023!11!001.pdf",
        "pdf_link": null
    },
    {
        "title": "Modelling of the Internet Computer Protocol Architecture: The Next Generation Blockchain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-21229-1_1",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "AoXuan Li, Luca Serena, Mirko Zichichi, Su-Kit Tang, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "MOATcoin",
        "implementation_urls": [
            {
                "identifier": "https://github.com/interwork-alliance/TokenTaxonomyFramework",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1145%3410699!3413798.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "ACM ISBN 978-1-4503-8079-9 The final published version is available online at: https://doi.org/10.1145/3410699.3413798 Rights/License: The terms and conditions for the reuse of this version of the manuscript are specified in the publishing policy."
                    }
                ]
            }
        ],
        "doi": "10.1145/3410699.3413798",
        "arxiv": null,
        "abstract": "ABSTRACT In this paper we present MOATcoin, a gamelike experiment that enabled us to practically investigate the open issues related to the governance and legal facets of smart contract based decentralized applications. After presenting the MOATcoin system architecture, we first tackle the problems of decentralized governance, its limits and the shift of trust that it entails; then, we explore the possible legal implications of the given scenario and, particularly, the consequences of code-written contracts. Finally, we offer taxonomical remarks on the concepts of “token” and “coin” and offer an insight from a regulatory perspective. CCS CONCEPTS Applied computing → Law; Computer systems organization → Distributed architectures. KEYWORDS Distributed Ledger Technologies, Smart Contract, Blockchain, Governance, Tokens, Crypto-assets, dApp, Law and Technology   mailto:biagio.distefano@univie.ac.atmailto:nadia.pocher@uab.catmailto:mirko.zichichi@upm.eshttps://www.last-jd-rioe.eu/http://lbl.cirsfid.unibo.it/1 INTRODUCTION Distributed Ledger Technologies (DLTs), originally introduced in the form of blockchain [20], enable the advent of a new vision to consider finance, trust in communication and governance. The distributed ledger ensures the immutable persistence of data, thus providing untampered data to applications when necessary. For this reason, DLTs represent an interesting technology for the development of reliable, decentralized applications (hereinafter: DApp) and services, based on Smart Contracts [3, 5]. In this paper we investigate the multidisciplinary facets, challenges and legal implications of smart contract-powered social interaction governance tools exploiting a “decentralized” game called MOATcoin1. Although this Dapp is just a game, with its simplification of a real-life scenario, it showcases crucial implications of real-world smart contract use cases. This made it possible to critically analyse what we consider to be the crucial matters of DApps that claim to have legal relevance. In order to perform this experiment, we set up a simple set of rules for a game that had to be executed on a decentralized environment. The game scenario pictures twenty Law, Science and Technology Joint Doctorate PhD candidates talking incessantly about work, even after office hours. To solve this, a simple rule was imposed: whoever pronounces certain work-related keywords (e.g., “GDPR”, “blockchain”) outside the office premises, has to buy a beer to another candidate. Since all twenty candidates are peers and no governing bodies were to be formed, we adopted a decentralized architecture based on Ethereum smart contracts [5]2. MOATcoin enables us to question the actual level of decentralization of DApps, as well as to explore governance and legal impacts of software architecture choices and the relevance of the concept of “token”, to the end of highlighting problems and possible solutions. Along with this, a complementary issue needs to be addressed. DLT-related discourse is flooded with terms such as “coins”, “cryptoassets”, “tokens”, “virtual currencies”, “cryptocurrencies”, “digital assets”, “virtual assets”, etc. From a legal and regulatory perspective, these expressions give way to endless considerations and uncertainties. From a practical perspective, it is pivotal to find a balance that avoids the loss of any technical and non-technical common ground. The remainder of this paper is organized as follows: Sections 2 and Section 3 introduce some background and related work and outline the architecture of MOATcoin. From Section 4 we start discussing the experiment results by providing an overview of MOATcoin’s governance model and highlighting weak points and possible solutions. Then, Section 5 offers a perspective on the relation between smart and legal contracts. In Section 6 we place MOATcoin within the world of tokens, by introducing taxonomy considerations and relevant implications. Finally, Section 7 provides some concluding remarks.  ",
        "publication_date": "2020-09-04",
        "authors": "Biagio Distefano, Nadia Pocher, Mirko Zichichi",
        "file_name": "10!1145%3410699!3413798.pdf",
        "file_path": "output/PDFs/10!1145%3410699!3413798.pdf",
        "pdf_link": null
    },
    {
        "title": "Grammatically uniform population initialization for grammar-guided genetic programming",
        "implementation_urls": [],
        "doi": "10.1007/s00500-020-05061-w",
        "arxiv": null,
        "abstract": "Abstract The initial population distribution is an es-sential issue in evolutionary computation performance.Population initialization methods for grammar guidedgenetic programming have some difficulties generatinga representative sample of the search space, which nega-tively affects the overall evolutionary process. This pa-per presents a grammatically uniform population ini-tialization method to address this issue by improvingthe initial population uniformity: the equiprobabilityof obtaining any individual of the search space definedby the context-free grammar. The proposed initializa-tion method assigns and updates probabilities dynam-ically to the production rules of the grammar to pur-sue uniformity, and includes a code bloat control mech-anism. We have conducted empirical experiments tocompare the proposed algorithm with a standard ini-tialization approach very often used in grammar-guidedgenetic programming. The results report that the pro-posed initialization method approximates very well auniform distribution of the individuals in the searchP. Ramos CriadoAturing Research, Salamanca, SpainE-mail: pablo.ramos@aturing.comD. Barrios RolańıaDepartamento de Matemáticas del Área IndustrialUniversidad Politécnica de Madrid, SpainE-mail: dolores.barrios.rolania@upm.esD. Manrique  Departamento de Inteligencia Artificial Universidad Politécnica de Madrid, Spain ORCID ID: 0000-0002-0792-4156 E-mail: daniel.manrique@upm.esE. SerranoDepartamento de Inteligencia Artificial Universidad Politécnica de Madrid, Spain ORCID ID: 0000-0001-7587-0703 E-mail: emilioserra@fi.upm.esspace. Moreover, the overall evolutionary process thattakes place after the population initialization performsbetter in terms of convergence speed and quality of thefinal solutions achieved when the proposed method gen-erates the initial population than when the usual ap-proach does. The results also show that these perfor-mance differences are more significant when the exper-iments involve large search spaces.Keywords Grammar-guided genetic programming ·Initialization · Genotypic uniformity · Stochasticcontext-free grammar1 IntroductionEvolutionary computation (Kari and Rozenberg, 2008)",
        "publication_date": "2020-06-12",
        "authors": "Pablo Ramos Criado, D. Barrios Rolanı́a, Daniel Manrique, Emilio Serrano",
        "file_name": "10!1007%s00500-020-05061-w.pdf",
        "file_path": "output/PDFs/10!1007%s00500-020-05061-w.pdf",
        "pdf_link": null
    },
    {
        "title": "Combination of Multi-Agent Systems and Wireless Sensor Networks for the Monitoring of Cattle",
        "implementation_urls": [],
        "doi": "10.3390/s18010108",
        "arxiv": null,
        "abstract": "Abstract: Precision breeding techniques have been widely used to optimize expenses and increaselivestock yields. Notwithstanding, the joint use of heterogeneous sensors and artificial intelligencetechniques for the simultaneous analysis or detection of different problems that cattle may presenthas not been addressed. This study arises from the necessity to obtain a technological tool thatfaces this state of the art limitation. As novelty, this work presents a multi-agent architecture basedon virtual organizations which allows to deploy a new embedded agent model in computationallylimited autonomous sensors, making use of the Platform for Automatic coNstruction of orGanizationsof intElligent Agents (PANGEA). To validate the proposed platform, different studies have beenperformed, where parameters specific to each animal are studied, such as physical activity,temperature, estrus cycle state and the moment in which the animal goes into labor. In addition, a setof applications that allow farmers to remotely monitor the livestock have been developed.Keywords: birth sensor; bovine embedded hardware; ambient intelligence; virtual organizationsof agents1. IntroductionThe last decade saw a breakthrough in the field of Ambient Intelligence (AmI), resulting in animprovement in the quality of people’s lives. The main objective of AmI is to adapt technology tothe needs of the people in a way that allows users to interact in a natural and effortless manner withthe different systems that make up the environment. Technology must act in a transparent way byadapting to the individuals and their context; simplifying the accomplishment of daily tasks and thecommunication between them and the environment. To achieve this goal, it is necessary to develop newtechnological models that allow users to interact with multiple devices simultaneously. The devicesmust collaborate in the accomplishment of daily tasks without the individuals being aware of it. Dueto the miniaturization of sensors and the reduction of costs in manufacturing processes, AmI can nowprovide new solutions to daily problems that were unthinkable just a few years ago. One field to whichAmI can be applied to is that of livestock and natural resources management.Currently in Spain, there are more than 23 million head of swine and more than 18 million ofsheep, representing 15% of the total of the European Union [1]. The continuous growth that thefarms are experiencing and the increasing demand for farm produce, make technology an essentialinstrument of continuous improvement in the development of farms. Requirements are also becomingincreasingly strict for both the breeders and the livestock, so the study of factors such as food, physicalactivity or the animal’s health is necessary. On large farms, it is difficult to devote time to observing theSensors 2018, 18, 108; doi:10.3390/s18010108 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18010108http://www.mdpi.com/journal/sensorsSensors 2018, 18, 108 2 of 27behavior of each animal. Technology must provide solutions that simplify farmers’ work, especiallyin repetitive and difficult tasks, such as those that entail the study of the factors mentioned above.The research and development of systems that detect anomalies in animals at early stages, is vitallyimportant. Numerous research groups are working on the use of electronic systems in livestock withthe aim of improving productivity and reducing operating costs. AmI allows farmers to remotelyaccess up to date information on the animals and obtain their complete traceability. The joint use ofinformation technology and electronic devices, allows to monitor and study parameters as well asthe consumption of energy on the farm, the level of food in the feeders, the lighting, climatology, thestate of animals’ health, their physical activity, etc. Smart-farming or precision farming consists inapplying information and communication technologies to livestock and agriculture. Its main objectiveis to increase the efficiency and quality of production through rapid decision making in cases where",
        "publication_date": "2018-01-02",
        "authors": "Alberto López Barriuso, Gabriel Villarrubia González, Juan F. De Paz, Álvaro Lozano Murciego, Javier Bajo",
        "file_name": "20250514083847.pdf",
        "file_path": "output/PDFs/20250514083847.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/1/108/pdf?version=1516694198"
    },
    {
        "title": "Building a Knowledge Graph from Historical Newspapers: A Study Case in Ecuador",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-32213-6_10",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Víctor Saquicela, Luis M. Vilches‐Blázquez, Mauricio Espinoza",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Analysis of ontologies and policy languages to represent information flows in GDPR",
        "implementation_urls": [],
        "doi": "10.3233/sw-223009",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-06-07",
        "authors": "Beatriz Esteves, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards CBDC-based machine-to-machine payments in consumer IoT",
        "implementation_urls": [],
        "doi": "10.1145/3477314.3507078",
        "arxiv": null,
        "abstract": "ACM must be honored. Abstracting with credit is permitted. To copy otherwise,to republish, to post on servers, or to redistribute to lists, requires prior specificpermission and/or  a fee. Request permissions from permissions@acm.org orPublicationsDept., ACM, Inc., fax +1 (212) 869-0481.https://www.acm.org/publications/policies/copyright-policyThis item was downloaded from IRIS Università di Bologna (https://cris.unibo.it/)When citing, please refer to the published version.http://dx.doi.org/10.1145/3477314.3507078https://cris.unibo.it/https://www.acm.org/publications/policies/copyright-policyTowards CBDC-based Machine-to-Machine Payments inConsumer IoTNadia Pocher∗Universitat Autònoma de BarcelonaLaw, Science and Technology RIoE EJDnadia.pocher@uab.catMirko Zichichi∗Universidad Politécnica de MadridLaw, Science and Technology RIoE EJDmirko.zichichi@upm.esABSTRACTThe technological advancement of the Internet of Things (IoT) is awell-known phenomenon that mainly affects industrial sectors butalso consumers in everyday life. The use of Consumer IoT, i.e. CIoT,devices is increasing, and they are paving the way for a Machine-to-Machine (M2M) communication that could highly enrich consumerservices. In this paper we position ourselves in the narrowing gapbetween the world of CIoT and the world of money, and we explorethe emerging interaction between the payment needs of a M2MEconomy and the “newways of payment”. Indeed, the advent of Dis-tributed Ledger Technology and cryptocurrencies has introduceda tech-oriented dynamism in the monetary and financial sphere.Accordingly, central banks all over the world have started investi-gations into digital fiat money , i.e., “retail” Central Bank DigitalCurrencies (CBDCs). Against this backdrop, we analyze the inte-gration of retail CBDC models into M2M and CIoT dynamics, whileheeding regulation-by-design and compliance-by/through-designmethodologies, and we propose a preliminary model of integrationbetween a two-tier retail CBDC architecture and CIoT.CCS CONCEPTS• Networks → Peer-to-peer networks; • Applied computing→ Law; Economics; • Human-centered computing → Ubiqui-tous and mobile devices;KEYWORDSCentral Bank Digital Currency, Machine-to-Machine, Internet ofThings, Distributed Ledger TechnologiesACM Reference Format:Nadia Pocher and Mirko Zichichi[1]. 2022. Towards CBDC-based Machine-to-Machine Payments in Consumer IoT. In The 37th ACM/SIGAPP Sympo-sium on Applied Computing (SAC ’22), April 25–29, 2022, Virtual Event, .ACM,",
        "publication_date": "2022-04-25",
        "authors": "Nadia Pocher, Mirko Zichichi",
        "file_name": "10!1145%3477314!3507078.pdf",
        "file_path": "output/PDFs/10!1145%3477314!3507078.pdf",
        "pdf_link": null
    },
    {
        "title": "Closing the Awareness Gap Between IT Practice and IT Law",
        "implementation_urls": [],
        "doi": "10.3233/faia190006",
        "arxiv": null,
        "abstract": "Abstract. Some of the ordinary activities of IT practitioners require a certain degreeof knowledge of IT law. Assuming these professionals will acquire legal knowl-edge better if expressed in terms familiar to them, this Chapter explores differentmanners of organising and presenting legal knowledge for its better cognition byIT professionals. This proposal features data models and knowledge organisationrooted in the specific legal theory of critical legal positivism of Kaarlo Tuori. It hasbeen evaluated with an experiment, where BSc students in Computer Science havebeen provided with models and reference material describing the EU legislationon cookies, and have been asked specific questions. In sight of the new theoreticalframework and the experiment results, we postulate that models and ontologies canbridge the knowledge gap and serve as lingua franca between the legal and the ITprofession.Keywords. legal knowledge, critical legal positivism, cognition of law, semi-formal models1. IntroductionA model is a representation of a reality, an abstraction, a simplification, a depiction.Modelling law can be a necessity for a number of reasons: legal drafting, analysis ofcourt cases, development of computer programs implementing or enforcing law, teach-ing law [1]. This Chapter discusses the pros and cons of using formal or semi-formalmodels for representing the law with didactic purposes, specifically when introduced toIT practitioners.The use of ontologies and semi-formal models in education environments is not new[2]. Semantic Web Technologies have been used for e-Learning [3], to align curriculumand syllabuses with learning objectives [4] or to support adaptive learning [5]. This workproposes their novel application to the legal domain – which has its own idiosincracy– and addressing a specific collective – IT experts. The contribution of this Chapter re-volves around the two proposed ideas: (i) that this target group is likely to understandbetter UML diagrams and related documentation and (ii) that legal knowledge represen-tation must consider a theoretical framework and lean on a rich tradition: languages andmethods for software and ontological engineering make explicit an idea that has beentraditionally part of legal thinking, the idea that legal concepts have a structure and arelinked one to another [6].BKnowledge of the Law in the Big Data AgeG. Peruginelli and S. Faro (Eds.)© 2019 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA19000641We engage Tuori’s critical legal positivist theory [7], which conceives law as a multi-layered phenomenon, as a feasible way to pre-conceptual modelling1. Tuori refers to an‘upper level’ concerned with legislative acts and case law, a “middle level, mediatinglevel in the law” related to the practical knowledge which lawyers and judges requireto interpret the law, and finally at the ‘lower level’, or the ‘deep structure’ of law, acompendium of the most basic principles and habits of mind by which we think andargue about the law. Critical legal positivism acknowledges the two faces of the law:“on the one hand, the law is a symbolic normative phenomenon, and on the other hand,it can be defined as a set of specific social practices”. This Chapter assumes that oneinteresting mapping from a pragmatic point of view is that of the mediating legal culture",
        "publication_date": "2019-01-01",
        "authors": "Marie François, Rodríguez-Doncel Victor, Pompeu Casanovas",
        "file_name": "10!3233%faia190006.pdf",
        "file_path": "output/PDFs/10!3233%faia190006.pdf",
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.26342/2022-69-9",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Intelligent Agent for Roadway Data Analysis",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-87687-6_10",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-10-29",
        "authors": "Gonzalo González-Moro, Gabriel Villarrubia González, Damiano Zanardini, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Legal Linked Data Ecosystems and the Rule of Law",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_5",
        "arxiv": null,
        "abstract": "Abstract This chapter introduces the notions of meta-rule of law and socio-legalecosystems to both foster and regulate linked democracy. It explores the way ofstimulating innovative regulations and building a regulatory quadrant for the rule oflaw. The chapter summarises briefly (i) the notions of responsive, better and smartregulation; (ii) requirements for legal interchange languages (legal interoperability);(iii) and cognitive ecology approaches. It shows how the protections of the sub-stantive rule of law can be embedded into the semantic languages of the web of dataand reflects on the conditions that make possible their enactment and implemen-tation as a socio-legal ecosystem. The chapter suggests in the end a reusablemulti-levelled meta-model and four notions of legal validity: positive, composite,formal, and ecological.Keywords Web of data � Socio-legal ecosystem � Rule of law �Meta-rule of law �Semantic languages � Governance � Linked democracy � Semantic web regulatorymodels � Regulatory quadrant � Legal validity5.1 Introduction: The Rule of Law in a New Brave WorldWewill expand in this chapter someways of implementing linked democracy on legaland political bases. Linked democracy is not only a theoretical approach incorpo-rating open linked data to theories of democracy. It consists of practices and the realbehaviour of people exercising their political rights on everyday bases. Thus, it alsopossesses a personal and cultural dimension that should be valued and protected. Lawis an obvious element. Behaviour on the web should be ‘fair’ and ‘legal’. What does itmean? Different states have different jurisdictions, and despite the international trendsof the global market, law has been, and still is, dependent on national states.How could we incorporate regulatory forms of empowering people on the web?How could algorithmic governance, data analytics, and semantics be used tofoster the principles of linked democracy that we have just presented at the end ofChap. 4?© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_587http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_5We will contend that there are two ways to reach such objectives: (i) embeddingthe principles of the substantive rule of law into the web of linked data (what wewill call the meta-rule of law), and (ii) incentivising the creation of socio-legalecosystems, i.e. the social conditions that are required to implement the meta-rule oflaw online and outline them among all stakeholders and users.We admit that this can be easier said than done. These two objectives might havean idealistic flavour. A few corporations have a dominant position on the web, theycan trade and invade privacy, and they usually do. As Shadbolt and Hampson(2018) have nicely put it, we live in a hyper-complex environment, shaped by ourown tools. This is a good breeding ground for elites to thrive. They also point outthat “what has changed is human potential, thanks to our transformative new tools.[…] The point is not that machines might wrest control from the elites. The problemis that most of us might never be able to wrest control of the machines from thepeople that occupy the command posts” (Shadbolt and Hampson 2018, 63).Power is certainly a problem. In our hyper-connected world, we barely know in",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "20250514083900.pdf",
        "file_path": "output/PDFs/20250514083900.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_5.pdf"
    },
    {
        "title": "OBOE: an Explainable Text Classification Framework",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2022.11.001",
        "arxiv": null,
        "abstract": "AbstractExplainable Artificial Intelligence (XAI) has recently gained visibility as one of the main topics of Artificial Intelligence research due to, among others, the need to provide a meaningful justification of the reasons behind the decision of black-box algorithms. Current approaches are based on model agnostic or ad-hoc solutions and, although there are frameworks that define workflows to generate meaningful explanations, a text classification framework that provides such explanations considering the different ingredients involved in the classification process (data, model, explanations, and users) is still missing. With the intention of covering this research gap, in this paper we present a text classification framework called OBOE (explanatiOns Based On concEpts), in which such ingredients play an active role to open the black-box. OBOE defines different components whose implementation can be customized and, thus, explanations are adapted to specific contexts. We also provide a tailored implementation to show the customization capability of OBOE. Additionally, we performed (a) a validation of the implemented framework to evaluate the performance using different corpora and (b) a user-based evaluation of the explanations provided by OBOE. The latter evaluation shows that the explanations generated in natural language express the reason for the classification results in a way that is comprehensible to non-technical users.DOI:  10.9781/ijimai.2022.11.001OBOE: an Explainable Text Classification FrameworkRaúl A. del Águila Escobar1, Mari Carmen Suárez-Figueroa2, Mariano Fernández-López3 *1 Universidad Politécnica de Madrid (UPM), Boadilla del Monte (Spain) 2 Ontology Engineering Group (OEG), Universidad Politécnica de Madrid (UPM), Boadilla del Monte (Spain)3 Department of Information Technology, Escuela Politécnica Superior, Universidad CEU-San Pablo, Boadilla del Monte (Spain)* Corresponding author: r.delaguila@alumnos.upm.es (R. A. del Águila Escobar), mcsuarez@fi.upm.es (M. C. Suárez-Figueroa), mfernandez.eps@ceu.es (M. Fernández-López).Received 18 October 2021 | Accepted 27 September 2022 | Early Access 3 November 2022 I.\t IntroductionAs a consequence of the wide use of black-box algorithms and the need to provide the justification that supports a classification result, eXplainable Artificial Intelligence (XAI), set up as an initiative, is one of the most relevant research topics in the last years.Conceptually, a text classification problem is no different from other classification problems, so the same ingredients are involved in solving the problem: data, model, users (final users or model developers) and the context of the classification problem.  Therefore, the challenges and questions that text classification tries to answer from an XAI perspective are the same: the need to specify the reasons behind the decision of the model (why question), the context of the explanation (what for question), how the model arrived at a conclusion (how question) or the data and problem of the classification (what question). However, all these ingredients and questions are not being considering together in a system to provide meaningful explanations [1]. The aim of this paper is twofold. Firstly, we present a customizable framework called OBOE (explanatiOns Based On concEpts) for explaining classification of texts. This framework defines a workflow that can be customized and allows all the ingredients to play an active role in the classification process. Furthermore, these ingredients work together to answer the questions that allow the black-box to be opened for final users and model developers by defining the following features: (a) Explanation Generation Workflow (how, why): there is an explicit and defined workflow for generating meaningful explanations for the users; (b) Data as key ingredient (what question): data is ",
        "publication_date": "2024-01-01",
        "authors": "Raúl A. del Águila Escobar, Mari Carmen Suárez-Figueroa, Mariano Fernández‐López",
        "file_name": "10!9781%ijimai!2022!11!001.pdf",
        "file_path": "output/PDFs/10!9781%ijimai!2022!11!001.pdf",
        "pdf_link": null
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%nca51143!2020!9306721.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [5] M."
                    }
                ]
            }
        ],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "abstract": "Abstract—This paper presents an architecture of a PersonalInformation Management System, in which individuals can definethe access to their personal data by means of smart contracts.These smart contracts, running on the Ethereum blockchain,implement access control lists and grant immutability, traceabil-ity and verifiability of the references to personal data, which isstored itself in a (possibly distributed) file system. A distributedauthorization mechanism is devised, where trust from multiplenetwork nodes is necessary to grant the access to the data.To this aim, two possible alternatives are described: a SecretSharing scheme and Threshold Proxy Re-Encryption scheme. Theperformance of these alternatives is experimentally comparedin terms of execution time. Threshold Proxy Re-Encryptionappears to be faster in different scenarios, in particular whenincreasing message size, number of nodes and the threshold value,i.e. number of nodes needed to grant the data disclosure.I. INTRODUCTIONThe transformation introduced by digital technologies hashad (and is having) a significant impact on economy and soci-ety. Data is at the heart of this transformation and individualsare the main sources generating more and more of it. There isan urgent need to place (again) individuals at the center and torelieve the absence of technical instruments and standards thatmake the exercise of one’s rights simple and not excessivelyburdensome [1], [2]. The EU’s GDPR 1 helps to promote thisvision and at the same time seeks to pave the way for opendata spaces for the social and economic good 2.Our aim is to seek such a technology by enabling userswith the sovereignty over their data, while guaranteeing itsconfidentiality. In our view, the data owner can define accessby limiting the scope of data utility, delegating these privilegesor giving up ownership completely, without the need to relyon (un)trusted entities to facilitate this task. The developmentof a Personal Information Management System (PIMS) 3 that∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - RIoE.1Council of European Union, Regulation 2016/679 - directive 95/462European Commission, COM(2020) 66, “A European strategy for data”3European Data Protection Supervisors, Opinion 9/2016, “EDPS Opinionon Personal Information Management Systems”fulfils these goals can be based on a distributed softwarearchitecture, where each individual is associated to a digitalspace containing personal data. This space will be used toattend the data access requests coming from data providersand data consumers. Distributed Ledger Technologies (DLT)and Decentralized File Storages (DFS) combination providesa range of features suitable for data management and sharing,such as transparency, immutability and reliability [2], [3].",
        "publication_date": "2020-11-24",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": "output/PDFs/10!1109%nca51143!2020!9306721.pdf",
        "pdf_link": null
    },
    {
        "title": "ONETT: Systematic Knowledge Graph Generation for National Access Points.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. In this paper, we describe our implemented approach for theusage and exploitation of declarative mappings for the publication ofopen transport data from transport authorities and operators into anontology based on Transmodel. This allows a homogeneous represen-tation of transport data across EU transport-related organisations andminimises the need to understand ad-hoc heterogeneous representationformats for transport data as currently published by them. We show howwe create and use RML mappings for the specific case of transformingGTFS data into a Transmodel-based ontology. In the future, such datamay be further transformed into other formats such as NeTEx.Keywords: Transmodel · GTFS · NAP · RML1 IntroductionTransport data is being currently published by transport authorities and opera-tors in many different formats, some of which are well-known de-facto standards,such as the General Transit Feed Specification or GTFS, and some others are ad-hoc data formats whose structure is decided by the data publisher (e.g., currentdatasets and APIs published by Empresa Municipal de Transportes de Madridin its open data portal1, tram information in Zaragoza2, etc.)All of these datasets have similarities, associated to the fact that they are de-scribing overlapping sets of information (schedules, stops, vehicles, lines, etc.).They are also made available, commonly, using tabular data formats. For ex-ample, GTFS feeds are essentially zip-compressed files containing sets of CSVfiles following the GTFS specification. And other data sources such as thosementioned above as examples provide the data either in CSV or JSON.Having all this data available in a homogeneous manner would actually reducethe total cost of reusing data sources, especially across operators/authorities andcities/regions. That is, developers may be able to develop one application thatwould be deployable in any city in the world with minor adaptations. This isalready happening with GTFS, which is not only being used by Google MapsCopyright c© 2019 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).1 https://opendata.emtmadrid.es/2 https://www.zaragoza.es/sede/servicio/catalogo/327https://opendata.emtmadrid.es/https://www.zaragoza.es/sede/servicio/catalogo/3272 Chaves-Fraga et al.to provide data about transport infrastructure, but also for route planning, butalso by other route planners, such as Navita.io and OpenTripPlanner.To achieve this homogeneity, there are several options that may be followed:– Transport authorities and operators may agree on using the same data for-mat and hence publish according to such data format. They know well thetype of data that they handle, the quality properties on such data, etc., sothey should be able to provide this data easily. To some extent, this is whatis happening currently with GTFS, and what should happen in the near fu-ture in the European Union with NeTex, according to directive 2010/40/EUand regulation 2017/1926 (MMTIS).– 3rd parties (as well as operators and authorities themselves) may be able tocreate transformation rules that allow transforming the original data sourcesinto other generally-agreed formats, republishing such transformed data ei-ther in the original data portals, if allowed to do so, or in other servers.",
        "publication_date": "2019-01-01",
        "authors": "David Chaves-Fraga, Adolfo Antón, Jhon Toledo, Óscar Corcho",
        "file_name": "20250514084008.pdf",
        "file_path": "output/PDFs/20250514084008.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2447/paper8.pdf"
    },
    {
        "title": "TýrFS: Increasing Small Files Access Performance with Dynamic Metadata Replication",
        "implementation_urls": [],
        "doi": "10.1109/ccgrid.2018.00072",
        "arxiv": null,
        "abstract": "Abstract—Small files are known to pose major performancechallenges for file systems. Yet, such workloads are increasinglycommon in a number of Big Data Analytics workflows or large-scale HPC simulations. These challenges are mainly caused bythe common architecture of most state-of-the-art file systemsneeding one or multiple metadata requests before being able toread from a file. Small input file size causes the overhead ofthis metadata management to gain relative importance as thesize of each file decreases. In this paper we propose a set oftechniques leveraging consistent hashing and dynamic metadatareplication to significantly reduce this metadata overhead. Weimplement such techniques inside a new file system named TýrFS,built as a thin layer above the Týr object store. We prove thatTýrFS increases small file access performance up to one order ofmagnitude compared to other state-of-the-art file systems, whileonly causing a minimal impact on file write throughput.I. INTRODUCTIONA large portion of research in data storage, managementand retrieval focuses on optimizing access performance forlarge files [1]–[5]. Yet, handling a large number of small filesraises other difficult challenges, that are partly related to thevery architecture of current file systems. Such small files,with a size inferior to a few megabytes, are very commonin large-scale facilities, as shown by multiple studies [6], [7].They can be generated by data-intensive applications suchas CM1 [8] or HACC [9], Internet of Things or StreamProcessing applications, as well as large scale workflows suchas Montage [10], CyberShake [11] or LIGO [12]. Improvingfile access performance for these applications is critical forscalability in order to handle ever-growing data sets on large-scale systems [13].As the amount of data to be transferred for storage op-erations on any single small file is intuitively small, the keyto optimizing access performance for such files lies in im-proving the efficiency of the associated metadata management.Actually, as the data size for each file decreases, the relativeoverhead of opening a file is increasingly significant. In ourexperiments, with small enough files, opening a file may takeup to an order of magnitude more time than reading the datait contains. One key cause of this behavior is the separationof data and metadata inherent to the architecture of currentfile systems. Indeed, to read a file, a client must first retrievethe metadata for all folders in its access path, that may belocated on one or more metadata servers, to check that the userhas the correct access rights or to pinpoint the location of thedata in the system. The high cost of network communicationsignificantly exceeds the cost of reading the data itself.We advocate that a different file system architecture isnecessary to reduce the cost of metadata management forsuch workloads involving many small files. While one could",
        "publication_date": "2018-05-01",
        "authors": "Pierre Matri, Marı́a S. Pérez, Alexandru Costan, Gabriel Antoniu",
        "file_name": "10!1109%ccgrid!2018!00072.pdf",
        "file_path": "output/PDFs/10!1109%ccgrid!2018!00072.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards human-guided machine learning",
        "implementation_urls": [],
        "doi": "10.1145/3301275.3302324",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-02-19",
        "authors": "Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti P. Gadewar, Qifan Yang, Neda Jahanshad",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Enhancing energy management at district and building levels via an EM-KPI ontology",
        "implementation_urls": [],
        "doi": "10.1016/j.autcon.2018.12.010",
        "arxiv": null,
        "abstract": "Abstract 1 The use of information and communication technologies facilitates energy management (EM) at 2 both district and building levels but also generates a considerable amount of data. To gain insights 3 into such data, it is essential to resolve the cross-domain data interoperability problem and 4 determine an approach to exchange performance information and insightful data amongst various 5 stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange 6 key performance information and data for districts and buildings. The ontology contains two 7 components: namely KPIs and EM master data; these, respectively, represent multi-level 8 performance information for energy performance tracking and the key data for data exploitation. 9 Through a demonstration, a sample linked dataset generated using the data correlation predefined 10 in the ontology is presented. The linked data analysis proves the feasibility of the ontology for 11 exchanging data among different stakeholders and for exploring insights in relation to 12 performance improvements.  13 Key words:  14 District; building; energy management; stakeholders; ontology; linked data. 15 1. Introduction 16 Buildings account for approximately 40% of the total final energy use in EU countries [1]. 17 However, a large portion of existing buildings are either designed or operated inefficiently [2]. 18 Energy management (EM) is a measure adopted to improve energy efficiency in buildings. 19 Furthermore, there is an increasing need to manage energy not only in a single building, but also 20 on a district scale [3]. Since the implementation of smart cities involves increasing distributed 21 electricity generation such as solar panels in energy distribution networks, EM at a district level, 22 for the purpose of combining the electricity supply and demand of buildings, is pivotal [4,5]. The 23 use of information and communication technologies (ICTs) facilitates the realisation of joint EM 24 that integrates the energy supply and demand sides.  25 Meanwhile, the use of ICTs also generates a massive amount of data and information, which could 26 provide new analysis possibilities for data-driven decision support and offer insights in relation 27 to potential performance improvement [6]. According to the National Institute of Standards and 28 Technology (NIST) in the United States, it could save up to $2 trillion in energy costs by 2030, 29 through exploiting the data from smart grids [7]. Although the expansion of data presents great 30 opportunities for energy performance improvement, there are still challenges faced in the effort 31 to make sense of this data. The problem is twofold. Firstly, there is an interoperability problem 32 between the cross-domain heterogeneous data. Secondly, the solution requires the extraction of 33 insightful data in order to avoid unnecessary analysis.  34 The extraction of core, insightful data is the primary challenge encountered when seeking to 35 access a large amount of data. Data exploitation is valuable only if they address the issues related 36 to the stakeholders. It is important to focus on data that is worth collecting, analyses which are 37 worth sharing and problems which are worth solving [8]. Master data offers a way to represent 38 key data that provides the most valuable information in an organisation [9]. In this case, master 39 data refers to the critical data objects that need to be shared across or beyond an organisation 40 which support decision-making. Master data was initially used for enterprise data management 41 due to the large volumes of data generated during business processes [10]. In the context of energy 42 management, a similar situation is encountered. Introducing the concept of master data into the 43 energy field can help make the large amount of energy-related data actionable, thus bringing 44 additional insight and value through improved decision-making.  45 The master data involved in EM should be shared among different stakeholders; therefore, it is 46 essential to support their performance concerns. In our previous study, we defined stakeholders 47 as those who have an interest in, who have influence in and who are impacted by the actions of 48 energy management; a detailed methodology was developed for selecting the KPIs (key 49 performance indicators) that underpin stakeholders’ performance goals; additionally, the use of 50 ",
        "publication_date": "2018-12-20",
        "authors": "Yehong Li, Raúl García‐Castro, Nandana Mihindukulasooriya, James O’Donnell, Sergio Vega-Sánchez",
        "file_name": "10!1016%j!autcon!2018!12!010.pdf",
        "file_path": "output/PDFs/10!1016%j!autcon!2018!12!010.pdf",
        "pdf_link": null
    },
    {
        "title": "Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2019.03.001",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2019-03-18",
        "authors": "Mariano Rico, Daniel Vila-Suero, Iuliana Botezán, Asunción Gómez‐Pérez",
        "file_name": "10!1016%j!websem!2019!03!001.pdf",
        "file_path": "output/PDFs/10!1016%j!websem!2019!03!001.pdf",
        "pdf_link": null
    },
    {
        "title": "On learning context-aware rules to link RDF datasets",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AndreaCimminoArriaga/Sorbas",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1093%jigpal%jzaa043.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "1Our prototype is available at https://github.com/AndreaCimminoArriaga/Sorbas."
                    }
                ]
            }
        ],
        "doi": "10.1093/jigpal/jzaa043",
        "arxiv": null,
        "abstract": "AbstractIntegrating RDF datasets has become a relevant problem for both researchers and practitioners. In the literature, there aremany genetic proposals that learn rules that allow to link the resources that refer to the same real-world entities, which isparamount to integrating the datasets. Unfortunately, they are context-unaware because they focus on the resources and theirattributes but forget about their neighbours. This implies that they fall short in cases in which different resources have similarattributes but refer to different real-world entities or cases in which they have dissimilar attributes but refer to the same real-world entities. In this article, we present a proposal that learns context-aware rules that take into account both the attributes ofthe resources and their neighbours. We have conducted an extensive experimentation that proves that it outperforms the mostadvanced genetic proposal. Our conclusions were checked using statistically sound methods.1 IntroductionAn RDF dataset is a collection of resources that describe real-world entities. Such datasets arecommonly used to feed a variety of automated business processes [5]. Typically, this requires tointegrate them by linking the resources that refer to the same real-world entities [1]. The resourcesare described by means of properties that can be either data properties or object properties; theformer model the attributes of the resources and the latter relate them to their neighbours.There are several state-of-the-art proposals that use genetic approaches to learn link rules [9, 10,16, 17]. Such rules basically compute the similarity of two resources by comparing their attributesusing a series of string transformations and similarity functions. If the resources are similar enough,then they are linked because they are assumed to describe the same real-world entity; otherwise, theyare kept apart. It is not difficult to realize that such context-unaware rules are imprecise in cases inwhich two resources have similar attributes but describe different real-world entities (e.g. differentpeople who have similar names or ages) or have dissimilar attributes but refer to the same real-worldentities (e.g. resources that describe different facets of a person).In this article, we present an approach to learn context-aware rules building on the context-unawarerules learnt by any of the previous proposals. By context-aware, we mean that the rule takes intoaccount the attributes of the resources being linked and the attributes of their neighbours. This is anovel approach since our analysis of the related work reveals that this is the first time that context-aware rules have been explored in this context. We have also performed an extensive experimental∗E-mail: cimmino@fi.upm.es∗∗E-mail: corchu@us.esstudy in which we sought to prove two hypothesis, namely, (i) exploring the context helps improve the effectiveness of the link rules and (ii) learning context-aware rules helps improve the efficiency of the linking process. Our experimental results and the statistical analysis that we have conducted validate these hypotheses, which prove that our proposal is very promising. Our proposal is related to a previous one in which our goal was to link two datasets on the f ly [3]; our experimentation confirms that our new proposal is as effective as the previous one but increases efficiency significantly.The rest of the article is organized as follows: Section 2 reports on the related work; Section 3 provides the details of our proposal; Section 4 presents our experimental analysis; finally, Section 5 summarizes our conclusions.2 Related workLearning link rules originated in the field of relational databases, where the problem was known as de-duplication [15], collective matching [18] or entity matching [13]. Unfortunately, it is not straightforward to adapt these results to RDF datasets because of the gap between the underlying data models.Some authors have developed a number of proposals that are specifically tailored to working with RDF datasets. Unfortunately, some of them work on a single dataset [8, 14] and others require the datasets to be modelled using OWL ontologies [4, 6, 7, 12], which hinders their general applicability. There are a few proposals that work on two RDF datasets without an explicit model [9, 10, 16, 17, 20], which makes them generally applicable. We analyse them below.Isele and Bizer [9, 10] devised GenLink, which is a genetic approach that uses a tournament ",
        "publication_date": "2020-08-06",
        "authors": "Andrea Cimmino, Rafael Corchuelo",
        "file_name": "10!1093%jigpal%jzaa043.pdf",
        "file_path": "output/PDFs/10!1093%jigpal%jzaa043.pdf",
        "pdf_link": null
    },
    {
        "title": "A High-Level Ontology Network for ICT Infrastructures",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-88361-4_26",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Óscar Corcho, David Chaves-Fraga, Jhon Toledo, Julián Arenas-Guerrero, Carlos Badenes-Olmedo, Mingxue Wang, Peng Hu, Nicholas Burrett, José Ferrater Mora, Puchao Zhang",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Standards for the IoT",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-45316-9_6",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-07-21",
        "authors": "Keith Dickerson, Raúl García‐Castro, Peter Kostelnik, Marek Paralič",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "TheyBuyForYou platform and knowledge graph: Expanding horizons in public procurement with open linked data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/data-sources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%sw-210442.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The data collected from OpenOpps and OpenCorporates is openly available under the Open Database License (ODbl).23 It is available on GitHub24 in JSON format and is updated on a monthly basis."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210442",
        "arxiv": null,
        "abstract": "Abstract. Public procurement is a large market affecting almost every organisation and individual; therefore, governments needto ensure its efficiency, transparency, and accountability, while creating healthy, competitive, and vibrant economies. In this con-text, open data initiatives and integration of data from multiple sources across national borders could transform the procurementmarket by such as lowering the barriers of entry for smaller suppliers and encouraging healthier competition, in particular byenabling cross-border bids. Increasingly more open data is published in the public sector; however, these are created and main-tained in siloes and are not straightforward to reuse or maintain because of technical heterogeneity, lack of quality, insufficientmetadata, or missing links to related domains. To this end, we developed an open linked data platform, called TheyBuyForYou,consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border,and cross-lingual procurement knowledge graph. We developed advanced tools and services on top of the knowledge graph foranomaly detection, cross-lingual document search, and data storytelling. This article describes the TheyBuyForYou platform andknowledge graph, reports their adoption by different stakeholders and challenges and experiences we went through while creatingthem, and demonstrates the usefulness of Semantic Web and Linked Data technologies for enhancing public procurement.Keywords: Public procurement, knowledge graph, linked datas, open data, ontology*Corresponding author. E-mail: ahmet.soylu@oslomet.no.1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:ahmet.soylu@oslomet.nomailto:ocorcho@fi.upm.esmailto:cbadenes@fi.upm.esmailto:fyedro@fi.upm.esmailto:brian.elvesater@sintef.nomailto:till.lech@sintef.nomailto:dumitru.roman@sintef.nomailto:t.blount@soton.ac.ukmailto:matej.kovacic@ijs.simailto:matej.posinkovic@ijs.simailto:ian@spendnetwork.commailto:chris.taggart@opencorporates.commailto:elena.simperl@kcl.ac.ukmailto:ahmet.soylu@oslomet.nohttps://creativecommons.org/licenses/by/4.0/266 A. Soylu et al. / TheyBuyForYou platform and knowledge graph1. IntroductionThe market around public procurement is large enough so as to affect almost every single citizen and organisationacross a variety of sectors. For this reason, public spending has always been a matter of interest at local, regional, andnational levels, and even more so, in times of great austerity and increased public scrutiny. Primarily, governmentsneed to be efficient in delivering services, ensure transparency, prevent fraud and corruption, and build healthy andsustainable economies [4,20]. For example, in the European Union (EU), every year, over 250.000 public authoritiesspend around 2 trillion euros (about 14% of GDP) on the purchase of services, works, and supplies;1 while theOrganisation for Economic Co-operation and Development (OECD) estimates that more than 82% of fraud andcorruption cases remain undetected across all OECD countries [27] costing as high as 990 billion euros a year in theEU alone [40]. Moreover, small and medium-sized enterprises (SMEs) are often locked out of markets and restrictedby borders due to the high cost of obtaining the required information, where larger companies can absorb the cost.This leads to a tendency for governments to rely on monolithic suppliers without adequate competition to delivergood value for the taxpayers.The availability of good quality, open, and integrated procurement data, coming from multiple sources acrossnational borders, could alleviate the aforementioned challenges [16]. This includes government agencies assessingpurchasing options, companies exploring new business contracts and placing cross-border bids, and other parties(such as journalists, researchers, local communities, business associations, transparency activists, and individualcitizens) looking for a better understanding of the intricacies of the public procurement landscape through decision-",
        "publication_date": "2021-09-03",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Tom Blount, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": "10!3233%sw-210442.pdf",
        "file_path": "output/PDFs/10!3233%sw-210442.pdf",
        "pdf_link": null
    },
    {
        "title": "TermitUp: Generation and enrichment of linked terminologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Pret-a-LLOD/termitup",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%sw-222885.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "TermitUp is available in a public GitHub reposi-tory46, as a Python project licensed under Apache Li-cense 2.0 terms."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-222885",
        "arxiv": null,
        "abstract": "Abstract. Domain-specific terminologies play a central role in many language technology solutions. Substantial manual effortis still involved in the creation of such resources, and many of them are published in proprietary formats that cannot be easilyreused in other applications. Automatic term extraction tools help alleviate this cumbersome task. However, their results areusually in the form of plain lists of terms or as unstructured data with limited linguistic information. Initiatives such as theLinguistic Linked Open Data cloud (LLOD) foster the publication of language resources in open structured formats, specificallyRDF, and their linking to other resources on the Web of Data. In order to leverage the wealth of linguistic data in the LLODand speed up the creation of linked terminological resources, we propose TermitUp, a service that generates enriched domainspecific terminologies directly from corpora, and publishes them in open and structured formats. TermitUp is composed offive modules performing terminology extraction, terminology post-processing, terminology enrichment, term relation validationand RDF publication. As part of the pipeline implemented by this service, existing resources in the LLOD are linked with theresulting terminologies, contributing in this way to the population of the LLOD cloud. TermitUp has been used in the frameworkof European projects tackling different fields, such as the legal domain, with promising results. Different alternatives on how tomodel enriched terminologies are considered and good practices illustrated with examples are proposed.Keywords: Terminology Generation, Terminology Enrichment, Linguistic Linked Data, Multilingualism1. IntroductionInternational institutions have become major pro-ducers of multilingual terminology databases, under-stood as resources that account for the specialisedwords used in a particular field in multiple languages.Since its foundation, the European Union has main-tained initiatives to cater for the collection, mainte-nance and creation of terminologies, thesauri or vocab-ularies, to cover their internal communication needsand to support translators. Some of the best known*Corresponding author. E-mail: pmchozas@fi.upm.es.resources are available from TermCoord1 (Terminol-ogy Coordination Unit of the European Parliament),in charge of the interinstitutional terminology databaseIATE2 (InterActive Terminology for Europe) since2004, or the EU Vocabularies site3, maintained by thePublications Office, that is also in charge of the upkeepof the multilingual thesaurus EuroVoc4.The creation and curation of such vocabularies hasnot only supported translators, documentalists and le-1https://termcoord.eu/2https://iate.europa.eu/3https://op.europa.eu/en/web/eu-vocabularies4http://eurovoc.europa.eu/1570-0844/0-1900/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:pmchozas@fi.upm.esmailto:kvazquez@delicias.dia.fi.upm.esmailto:pcalleja@fi.upm.esmailto:emontiel@fi.upm.esmailto:vrodriguez@fi.upm.esmailto:pmchozas@fi.upm.es2 P. Martín-Chozas et al. / TermitUp: Generation and Enrichment of Linked Terminologies1 12 23 34 4",
        "publication_date": "2022-05-27",
        "authors": "Patricia Martín-Chozas, Karen Vázquez-Flores, Pablo Calleja, Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%sw-222885.pdf",
        "file_path": "output/PDFs/10!3233%sw-222885.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards a Taxonomy of AI Risks in the Health Domain",
        "implementation_urls": [],
        "doi": "10.1109/transai54797.2022.00007",
        "arxiv": null,
        "abstract": "Abstract—The adoption of AI in the health sector has its shareof benefits and harms to various stakeholder groups and entities.There are critical risks involved in using AI systems in the healthdomain; risks that can have severe, irreversible, and life-changingimpacts on people’s lives. With the development of innovativeAI-based applications in the medical and healthcare sectors, newtypes of risks emerge. To benefit from novel AI applications inthis domain, the risks need to be managed in order to protectthe fundamental interests and rights of those affected. Thiswill increase the level to which these systems become ethicallyacceptable, legally permissible, and socially sustainable. In thispaper, we first discuss the necessity of AI risk management in thehealth domain from the ethical, legal, and societal perspectives.We then present a taxonomy of risks associated with the use ofAI systems in the health domain called HART, accessible onlineat https://w3id.org/hart. HART mirrors the risks of avariety of different real-world incidents caused by use of AIin the health sector. Lastly, we discuss the implications of thetaxonomy for different stakeholder groups and further research.Index Terms—risk, AI systems, health, AI regulation, ethics ofAI, AI public policy, taxonomyI. INTRODUCTIONApplication of AI in the health domain has great potentialfor promoting public health, improving patient care, reducingtreatment costs, assisting medics in reaching a diagnosis, anddiscovering new treatment methods and drugs. However, thereare significant risks involved in the use of AI systems such asrisk of errors which can lead to injury to patients or risk ofdisclosing patients’ sensitive data [1]. With the huge amount ofAI investment in the medical and healthcare sectors for drugs,cancer, molecular, and drug discovery [2], the uncertaintiesaround the newly developed AI systems in these sectors areincreased.This project is the result of interdisciplinary research within the PROTECT(Protecting Personal Data Amidst Big Data Innovation) project and hasreceived funding from the European Union’s Horizon 2020 research andinnovation programme under the Marie Skłodowska-Curie grant agreementNo 813497.This circumstance has led to a lively discussion within thefields of ethics, social sciences, and legal scholarship making itall the more necessary for conceptualising and identifying theexact risks to different stakeholder groups and to use this in-sight for the different contexts of risk management and impactassessment. Significantly the need for risk-based assessmentlies at the heart of four relevant legislative instruments whichapply to AI in the health sector within the European Union.However, in the current state of debate in the ethical, legal,and social science literature, it is often not clear how theaddressed risks are conceptualised. By establishing a clearunderstanding of AI risk, we aim to contribute to a better",
        "publication_date": "2022-09-01",
        "authors": "Delaram Golpayegani, Joshua Hovsha, Leon Rossmaier, Rana Saniei, Jana Mišić",
        "file_name": "10!1109%transai54797!2022!00007.pdf",
        "file_path": "output/PDFs/10!1109%transai54797!2022!00007.pdf",
        "pdf_link": null
    },
    {
        "title": "Trusting Decentralised Knowledge Graphs and Web Data at the Web Conference",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3589756",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-28",
        "authors": "John Domingue, Allan Third, María-Esther Vidal, Philipp D. Rohde, Juan Cano-Benito, Andrea Cimmino, Ruben Verborgh",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Foreword",
        "implementation_urls": [],
        "doi": "10.1038/ki.2009.188",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2009-07-31",
        "authors": "Kai‐Uwe Eckardt, Bertram L. Kasiske",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Prediction and Decision-Making in Intelligent Environments Supported by Knowledge Graphs, A Systematic Review",
        "implementation_urls": [],
        "doi": "10.3390/s19081774",
        "arxiv": null,
        "abstract": "Abstract: Ambient Intelligence is currently a lively application domain of Artificial Intelligenceand has become the central subject of multiple initiatives worldwide. Several approaches insidethis domain make use of knowledge bases or knowledge graphs, both previously existing and adhoc. This form of representation allows heterogeneous data gathered from diverse sources to becontextualized and combined to create relevant information for intelligent systems, usually followinghigher level constraints defined by an ontology. In this work, we conduct a systematic review of theexisting usages of knowledge bases in intelligent environments, as well as an in-depth study of thepredictive and decision-making models employed. Finally, we present a use case for smart homesand illustrate the use and advantages of Knowledge Graph Embeddings in this context.Keywords: knowledge base; knowledge graph; intelligent environment; ambient intelligence;reasoning model; knowledge graph embedding1. IntroductionAccording to Augusto et al.: “An Intelligent Environment is one in which the actions of numerousnetworked controllers is orchestrated by self-programming pre-emptive processes in such a way asto create an interactive holistic functionality that enhances occupants experiences” [1]. Although inthis particular context the term “environment” is popularly associated with homes, it also encompassesbroader scenarios, such as buildings, streets or other areas. Intelligent environments are technologicallybased on the combination of several socio-technical innovations such as the Internet of Things (IoT),mobile Internet access, smartphones, data analytics, open data initiatives, and sharing economymodels [2]. These advances allow intelligent environments to manage assets and resources efficientlyby services enhanced with intelligence such as traffic management or healthcare systems.Developing responsive and smarter environments is one of the main present objectives, as shownby the number of research projects developed to pursue this goal, such as Km4City [3] or RoomPathy [4].Although there exists a considerable heterogeneity among the existing works in terms of objectives,methods, and areas of application, the use of knowledge bases (KBs) or knowledge graphs (KGs) is inthe core of a large number of these works.KBs play a key role in multiple ambient intelligence applications, as they are an essential part ofthe conversion of heterogeneous, numerical data provided by sensors into contextualized and semanticinformation. The transformation procedure is usually performed using ontologies, which enable theSensors 2019, 19, 1774; doi:10.3390/s19081774 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-6838-1266https://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/0000-0002-0792-4156https://orcid.org/0000-0001-9461-7922http://www.mdpi.com/1424-8220/19/8/1774?type=check_update&version=1http://dx.doi.org/10.3390/s19081774http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticlePrediction and Decision-Making in IntelligentEnvironments Supported by Knowledge Graphs,A Systematic Review1 1,Elvira Amador-Dominguez *, Emilio Serrano , Daniel Manrique 7© and Juan F. De Paz?1 Ontology Engineering Group, Department of Artificial Intelligence, ETSI Informaticos,Universidad Politécnica de Madrid, 28660 Madrid, Spain; eamador@fi.upm.esArtificial Intelligence Lab, Department of Artificial Intelligence, ETSI Informaticos,Universidad Politécnica de Madrid, 28660 Madrid, Spain; dmanrique@fi.upm.es",
        "publication_date": "2019-04-13",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Juan F. De Paz",
        "file_name": "20250514084322.pdf",
        "file_path": "output/PDFs/20250514084322.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/19/8/1774/pdf?version=1555141210"
    },
    {
        "title": "ODRL Profile for Expressing Consent through Granular Access Control Policies in Solid",
        "implementation_urls": [],
        "doi": "10.1109/eurospw54576.2021.00038",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-09-01",
        "authors": "Beatriz Esteves, Harshvardhan J. Pandit, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Students’ Evaluation of a Virtual World for Procedural Training in a Tertiary-Education Course",
        "implementation_urls": [],
        "doi": "10.1177/0735633117706047",
        "arxiv": null,
        "abstract": "Abstract This article presents an investigation on the educational value of virtual worlds intended for the acquisition of procedural knowledge. This investigation takes as a case of study a virtual laboratory on biotechnology. A remarkable feature in this virtual laboratory is an automatic tutor that supervises student’s actions and provides tutoring feedback when it is appropriate. The study presented in this article covers two different aspects of the system. First, it analyzes the impact of this virtual world in learning some concepts related to a biotechnology practice; and second, it surveys the students’ opinion on the virtual world by means of three open questions (posi­tive, negative, and general impressions). Results demonstrated that the virtual world had a positive influence in the students’ knowledge, and it was well received by them. 1School of Computer Engineering, UPM, Spain 2Department of Artificial Intelligence, UPM, Spain 3MONTES (School of Forest Engineering and Natural Environment), UPM, Spain Corresponding Author: Jaime Ram ı́rez, Escuela Sup. de Ing. Informa t́icos, Calle de los Ciruelos Montegancedo Campus, Universidad Polite´cnica de Madrid, Building 5, Floor 1, Office 5112, 28660 Boadilla del Monte, Madrid, Spain. Email: jramirez@fi.upm.es There are many examples in the literature that show how interactive simulations are successfully employed for educational purposes. Among these interactive simulations, three-dimensional (3D) virtual environments represent a group of applications that each day is attracting more and more the interest of the edu­cational community. This is because these systems are engaging and allow students to learn new concepts and procedures by recreating situations that in the real world would be too expensive, dangerous, or simply out of reach for the students. We can see 3D virtual environments as a tool for implementing the ‘‘learning by doing’’ approach derived from the constructivist pedagogy (Huang, Rauch, & Liaw, 2010). When several users can visit a shared 3D virtual environment through Internet and interact by means of desktop devices, this 3D virtual environment is usually referred to as a virtual world. In addition to the benefits of 3D simulations, virtual worlds also provide the possibility of reproducing real scen­arios where several people (students or teachers) are able to chat and share rooms, instruments, and so forth. Moreover, as students connect to virtual worlds through Internet, they can perform virtual practices at home, without the physical presence of a teacher close to them. This opens the door to virtual practices from distant places where sufficiently equipped real labs are not available. In recent years, the development of 3D virtual worlds has become more inex­pensive thanks to platforms such as Second Life (http://secondlife.com/), OpenSimulator (OS; http://opensimulator.org/), Open Wonderland (http://open-wonderland.org/), or Bitmanagement Software Collaborate System (http://www. bitmanagement.com/), which provide developers with a basic layer of function­ality. As a result of this, more academic institutions have been able to develop and use their own virtual worlds. This article presents a study on the educational value of a virtual world for procedural learning. In the literature on virtual worlds, we can find many works on how this kind of systems can support different learning activities. However, there are just a few works that show how this kind of systems can support task ",
        "publication_date": "2017-05-08",
        "authors": "Jaime Ramírez, Mariano Rico, Diego Riofrío-Luzcando, Marta Berrocal‐Lobo, Angélica de Antonio",
        "file_name": "10!1177%0735633117706047.pdf",
        "file_path": "output/PDFs/10!1177%0735633117706047.pdf",
        "pdf_link": null
    },
    {
        "title": "Models to represent linguistic linked data",
        "implementation_urls": [],
        "doi": "10.1017/s1351324918000347",
        "arxiv": null,
        "abstract": "AbstractAs the interest of the Semantic Web and computational linguistics communities in linguisticlinked data (LLD) keeps increasing and the number of contributions that dwell on LLDrapidly grows, scholars (and linguists in particular) interested in the development of LLDresources sometimes find it difficult to determine which mechanism is suitable for their needsand which challenges have already been addressed. This review seeks to present the state ofthe art on the models, ontologies and their extensions to represent language resources as LLDby focusing on the nature of the linguistic content they aim to encode. Four basic groupsof models are distinguished in this work: models to represent the main elements of lexicalresources (group 1), vocabularies developed as extensions to models in group 1 and ontologiesthat provide more granularity on specific levels of linguistic analysis (group 2), cataloguesof linguistic data categories (group 3) and other models such as corpora models or service-oriented ones (group 4). Contributions encompassed in these four groups are described, high-lighting their reuse by the community and the modelling challenges that are still to be faced.1 Introduction1.1 Background and motivationLanguage resources (dictionaries, terminologies, corpora, etc.) developed in the fieldsof corpus linguistics, computational linguistics and natural language processing(NLP) are often encoded in heterogeneous formats and developed in isolation fromone another. This makes their discovery, reuse and integration for both the develop-ment of NLP tools and daily linguistic research a difficult and cumbersome task. In∗We are very grateful to the anonymous reviewers for their meticulous reading of the surveyand for providing us with numerous insightful and constructive suggestions to improve it.We would also like to thank Dr Guadalupe Aguado-de-Cea for her help in proofreadingthis manuscript. This work is supported by the Spanish Ministry of Education, Cultureand Sports through the Formación del Profesorado Universitario (FPU) program, and bythe Spanish Ministry of Economy and Competitiveness through the project 4V (TIN2013-46238-C4-2-R) within the FEDER funding scheme, the Juan de la Cierva program, andthe Excellence Network ReTeLe (TIN2015-68955-REDT).https://www.cambridge.org/core/terms. https://doi.org/10.1017/S1351324918000347Downloaded from https://www.cambridge.org/core. University of New England, on 06 Oct 2018 at 15:39:38, subject to the Cambridge Core terms of use, available athttp://orcid.org/0000-0001-6433-4649https://www.cambridge.org/core/termshttps://doi.org/10.1017/S1351324918000347https://www.cambridge.org/core2 J. Bosque-Gil et al.order to alleviate such an issue and to enhance interoperability of language resourceson the Web, a community of language technologies experts and practitioners hasstarted adopting techniques coming from the area of study of linked data (LD).The LD paradigm emerges as a series of best practices and principles for ‘exposing,sharing and connecting data on the Web’ (Bizer, Heath and Berners-Lee 2011),independently of the domain. These principles state that unique resource identifiersshould be used to name things in a way that allows people to look them up, finduseful information represented with standard formalisms and discover more thingsthat are linked to those resources. LD emerged in the context of the SemanticWeb, an extension of the Web ‘in which information is given well-defined meaning,better enabling computers and people to work in cooperation’ (Berners-Lee et al.2001). General and domain-specific on-line ontologies (such as the ones we revisitin this survey) provide this well-defined meaning on the Semantic Web and areused to represent the data that will be linked to other data. Following this line,",
        "publication_date": "2018-10-04",
        "authors": "Julia Bosque-Gil, Jorge Gracia, Elena Montiel-Ponsoda, Asunción Gómez‐Pérez",
        "file_name": "10!1017%s1351324918000347.pdf",
        "file_path": "output/PDFs/10!1017%s1351324918000347.pdf",
        "pdf_link": null
    },
    {
        "title": "Decentralized Personal Data Marketplaces: How Participation in a DAO Can Support the Production of Citizen-Generated Data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iotaledger/streams",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514084527.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available online: https://github.com/iotaledger/streams/blob/develop/specification/Streams_Specification_1_0A.pdf (accessed on 24 May 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/s22166260",
        "arxiv": null,
        "abstract": "Abstract: Big Tech companies operating in a data-driven economy offer services that rely on theirusers’ personal data and usually store this personal information in “data silos” that prevent trans-parency about their use and opportunities for data sharing for public interest. In this paper, we presenta solution that promotes the development of decentralized personal data marketplaces, exploiting theuse of Distributed Ledger Technologies (DLTs), Decentralized File Storages (DFS) and smart contractsfor storing personal data and managing access control in a decentralized way. Moreover, we focus onthe issue of a lack of efficient decentralized mechanisms in DLTs and DFSs for querying a certain typeof data. For this reason, we propose the use of a hypercube-structured Distributed Hash Table (DHT)on top of DLTs, organized for efficient processing of multiple keyword-based queries on the ledgerdata. We test our approach with the implementation of a use case regarding the creation of citizen-generated data based on direct participation and the involvement of a Decentralized AutonomousOrganization (DAO). The performance evaluation demonstrates the viability of our approach fordecentralized data searches, distributed authorization mechanisms and smart contract exploitation.Keywords: distributed ledger technology; decentralized file storage; distributed hash table; datamarketplace; keyword-based search; citizen-generated data1. IntroductionRecent scandals have shown the harm that current data collection, storage and sharingpractices can cause with regard to the misuse of personal data [1,2]. As the world isbecoming more “smart”, so-called smart environments, of which smart cities [3] stand outthe most, have in common the ability to transform data (in particular, personal data) intomeaningful information needed by the liveness of the ecosystem they generate. Basedon this transformation, indeed, they provide services that are becoming more and moretargeted towards individuals. For instance, it is commonly known that personal informationis used to recommend opportunities to individuals and to make their life easier. However,entities that control these data might not always operate with the aim of social good [4].Many Big Tech companies rely on data collected about their users, usually storing thispersonal information in corporate databases, i.e., data silos, and transacting it to thirdparties with not enough transparency for individuals.Meanwhile, among the many technologies used for general-purpose data manage-ment and storage, Distributed Ledger Technologies (DLTs) are rising up as powerful toolsfor avoiding control centralization. DLT and the realm of decentralized systems, suchas Decentralized File Storages (DFS), that are emerging as solutions able to tackle theissue of obtaining large amounts of data that are not of dubious or of false origin, whileproviding more disintermediated processes [5,6]. DLTs, in this context, provide a new wayof handling personal data, such as recording, storage and transfer. This can be carriedout in combination with cryptographic schemes to ensure data confidentiality. By theirSensors 2022, 22, 6260. https://doi.org/10.3390/s22166260 https://www.mdpi.com/journal/sensorshttps://doi.org/10.3390/s22166260https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/sensorshttps://www.mdpi.comhttps://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0003-1076-2511https://doi.org/10.3390/s22166260https://www.mdpi.com/journal/sensorshttps://www.mdpi.com/article/10.3390/s22166260?type=check_update&version=2Sensors 2022, 22, 6260 2 of 32",
        "publication_date": "2022-08-20",
        "authors": "Mirko Zichichi, Stefano Ferretti, Victor Rodrı́guez-Doncel",
        "file_name": "20250514084527.pdf",
        "file_path": "output/PDFs/20250514084527.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/22/16/6260/pdf?version=1661238706"
    },
    {
        "title": "Using machine learning to optimize parallelism in big data applications",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2017.07.003",
        "arxiv": null,
        "abstract": "AbstractIn-memory cluster computing platforms have gained momentum in the last years, due to their ability toanalyse big amounts of data in parallel. These platforms are complex and difficult-to-manage environments.In addition, there is a lack of tools to better understand and optimize such platforms that consequentlyform backbone of big data infrastructure and technologies. This directly leads to underutilization of avail-able resources and application failures in such environment. One of the key aspects that can address thisproblem is optimization of the task parallelism of application in such environments. In this paper, wepropose a machine learning based method that recommends optimal parameters for task parallelization inbig data workloads. By monitoring and gathering metrics at system and application level, we are able tofind statistical correlations that allow us to characterize and predict the effect of different parallelism set-tings on performance. These predictions are used to recommend an optimal configuration to users beforelaunching their workloads in the cluster, avoiding possible failures, performance degradation and wastageof resources. We evaluate our method with a benchmark of 15 Spark applications on the Grid5000 testbed.We observe up to a 51% gain on performance when using the recommended parallelism settings. The modelis also interpretable and can give insights to the user into how different metrics and parameters affect theperformance.Keywords: machine learning, spark, parallelism, big data1. IntroductionBig data technology and services market is esti-mated to grow at a CAGR1 of 22.6% from 2015 to2020 and reach $58.9 billion in 2020 [1]. Highly vis-ible early adopters such as Yahoo, eBay and Face-book have demonstrated the value of mining com-plex information sets, and now many companies areeager to unlock the value in their own data. In orderto address big data challenges, many different par-allel programming frameworks, like Map Reduce,Apache Spark or Flink have been developed [2, 3, 4].Planning big data processes effectively on theseplatforms can become problematic. They involvecomplex ecosystems where developers need to dis-cover the main causes of performance degradationin terms of time, cost or energy. However, process-ing collected logs and metrics can be a tedious andEmail address: abrandon@fi.upm.es (Álvaro BrandónHernández)1Compound Annual Growth Ratedifficult task. In addition, there are several param-eters that can be adjusted and have an importantimpact on application performance.While users have to deal with the challenge ofcontrolling this complex environment, there is afundamental lack of tools to simplify big data in-frastructure and platform management. Some toolslike YARN or Mesos [5, 6] help in decoupling theprogramming platform from the resource manage-ment. Still, they don’t tackle the problem of opti-mizing application and cluster performance.One of the most important challenges is findingthe best parallelization strategy for a particular ap-",
        "publication_date": "2017-07-17",
        "authors": "Álvaro Brandón, Marı́a S. Pérez, Smrati Gupta, Víctor Muntés-Mulero",
        "file_name": "10!1016%j!future!2017!07!003.pdf",
        "file_path": "output/PDFs/10!1016%j!future!2017!07!003.pdf",
        "pdf_link": null
    },
    {
        "title": "Systematic Construction of Knowledge Graphs for Research-Performing Organizations",
        "implementation_urls": [
            {
                "identifier": "https://github.com/RMLio/rmlmapper-java",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514084729.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available online: https://github.com/RMLio/rmlmapper-java (accessed on 1 October 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info13120562",
        "arxiv": null,
        "abstract": "Abstract: Research-Performing Organizations (e.g., research centers, universities) usually accumulatea wealth of data related to their researchers, the generated scientific results and research outputs, andpublicly and privately-funded projects that support their activities, etc. Even though the types of datahandled may look similar across organizations, it is common to see that each institution has developedits own data model to provide support for many of their administrative activities (project reporting,curriculum management, personnel management, etc.). This creates obstacles to the integration andlinking of knowledge across organizations, as well as difficulties when researchers move from oneinstitution to another. In this paper, we take advantage of the ontology network created by the SpanishHERCULES initiative to facilitate the construction of knowledge graphs from existing informationsystems, such as the one managed by the company Universitas XXI, which provides support tomore than 100 Spanish-speaking research-performing organizations worldwide. Our effort is notjust focused on following the modeling choices from that ontology, but also on demonstrating howthe use of standard declarative mapping rules (i.e., R2RML) guarantees a systematic and sustainableworkflow for constructing and maintaining a KG. We also present several real-world use casesin which the proposed workflow is adopted together with a set of lessons learned and generalrecommendations that may also apply to other domains. The next steps include researching in theautomation of the creation of the mapping rules, the enrichment of the KG with external sources, andits exploitation though distributed environments.Keywords: knowledge graph; research-performing organizations; declarative mapping rules1. IntroductionResearch-performing organizations such as universities and research centers collectand accumulate a large amount of data related to their activities (e.g., scientific results,project outputs, academic courses, etc.). Although there are some common informationmodels (e.g., EuroCRIS [1]), in this domain it is common practice for each of these organiza-tions to develop their own information system to support all their activities. This fact has anegative impact on integrating and exploiting knowledge across institutions and also makesit difficult for researchers to manage their data when they move from one organizationto another. The effectiveness of semantic web technologies and knowledge graphs [2] forcomplex data management tasks has already been demonstrated in several domains [3–5],by companies (e.g., Google [6], Amazon [7]) and public communities (e.g., DBpedia [8],Wikidata [9]).HERCULES is a multi-anual project, promoted by the main Spanish association ofuniversities (CRUE) [10], that aims to build a semantic layer to harmonize the knowl-edge and data of the information systems of Spanish research-performing organizations.The main objective is to address data interoperability problems across organizations interms of schemes and formats, ensuring efficient exploitation of combined knowledge.Information 2022, 13, 562. https://doi.org/10.3390/info13120562 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13120562https://doi.org/10.3390/info13120562https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/info13120562https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13120562?type=check_update&version=1Information 2022, 13, 562 2 of 14",
        "publication_date": "2022-11-30",
        "authors": "David Chaves-Fraga, Óscar Corcho, Francisco Yedro Martínez, Roberto Moreno, Juan Olías, Alejandro De La Azuela",
        "file_name": "20250514084729.pdf",
        "file_path": "output/PDFs/20250514084729.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/13/12/562/pdf?version=1669804108"
    },
    {
        "title": "Ontological Model for the Semantic Description of Syllabuses",
        "implementation_urls": [],
        "doi": "10.1145/3357419.3357442",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-08-23",
        "authors": "Mariela Tapia-León, Carlos Aveiga, Janneth Chicaiza, Mari Carmen Suárez-Figueroa",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific Publications",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2012.01953",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Carlos Badenes-Olmedo, David Chaves-Fraga, María Poveda‐Villalón, Ana Iglesias-Molina, Pablo Calleja, Socorro Bernardos, Patricia Martín-Chozas, Alba Fernández-Izquierdo, Elvira Amador-Domínguez, Paola Espinoza-Arias, Luis Pozo-Gilo, Edna Ruckhaus, Esteban González-Guardia, R. Cedazo, Beatriz López Centeno, Óscar Corcho",
        "file_name": "arxiv.2012.01953",
        "file_path": "10.48550/arxiv.2012.01953",
        "pdf_link": null
    },
    {
        "title": "Reuse and Reengineering of Non-ontological Resources in the Legal Domain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_24",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Cristiana Santos, Pompeu Casanovas, Victor Rodrı́guez-Doncel, Leendert van der Torre",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "XSD2SHACL: Capturing RDF Constraints from XML Schema",
        "implementation_urls": [],
        "doi": "10.1145/3587259.3627565",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-11-29",
        "authors": "X.R. Duan, David Chaves-Fraga, Anastasia Dimou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Semantic Modelling of Plans and Execution Traces for Enhancing Transparency of IoT Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TrustLens/EP-PLAN",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%iotsms48152!2019!8939260.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://www.w3.org/TR/2013/REC-prov-o-20130430/[4] D."
                    }
                ]
            }
        ],
        "doi": "10.1109/iotsms48152.2019.8939260",
        "arxiv": null,
        "abstract": "Abstract—Transparency of IoT systems is an essential require-ment for enhancing user’s trust towards such systems. Prove-nance mechanisms documenting the execution of IoT systemsare often cited as an enabler of such transparency. However,provenance records often lack detailed descriptions of a system’sexpected behaviour. Plan specifications describe the steps neededto achieve a certain goal by a human or an automated system.Once plans reach a certain level of complexity, they are typicallydecomposed in different levels of abstraction. However, thisdecomposition makes it difficult to relate high level abstract plansto their granular execution traces. This paper introduces EP-Plan, a vocabulary for linking the different levels of granularityof a plan with their respective provenance traces. EP-Plan alsoprovides the means to describe plan metadata such as constraints,policies, rationales, and expected participating agents associatedwith a plan.I. INTRODUCTIONAn increasing number of systems provide means to docu-ment provenance records of past executions [1] (i.e., executiontraces), in order to inspect and explain the creation process ofexisting results or behaviour of a system. In this paper, weargue that recording such provenance can be further enhancedby recording the set of planned steps that guided its execution,and we refer to such records as plans.Plans document intended system behaviour, which is ben-eficial at the point when no runtime provenance is available(e.g. to assess risks associated with a planned IoT deployment).Plans are also critical to understand errors, enabling a pointof reference for comparison when the execution deviates fromwhat was planned to happen.Gateway CloudPlanRelay data from sensorsPlanProcess and store dataPlanDeliver smart temperature monitoring at Alice’s homeSensorPlanProduce observationsIoT SystemExecution TraceA log of activities generating and publishing raw data values and timestampsExecution TraceA log of activities receiving and uploading sensor data  Execution Trace",
        "publication_date": "2019-10-01",
        "authors": "Milan Marković, Daniel Garijo, Peter Edwards, Wamberto Vasconcelos",
        "file_name": "10!1109%iotsms48152!2019!8939260.pdf",
        "file_path": "output/PDFs/10!1109%iotsms48152!2019!8939260.pdf",
        "pdf_link": null
    },
    {
        "title": "The landscape of multimedia ontologies in the last decade",
        "implementation_urls": [],
        "doi": "10.1007/s11042-011-0905-z",
        "arxiv": null,
        "abstract": "Abstract Many efforts have been made in the area of multimedia to bridge the so-called “semantic-gap” with the implementation of ontologies from 2001 to the present. Inthis paper, we provide a comparative study of the most well-known ontologies related tomultimedia aspects. This comparative study has been done based on a frameworkproposed in this paper and called FRAMECOMMON. This framework takes into accountprocess-oriented dimension, such as the methodological one, and outcome-orienteddimensions, like multimedia aspects, understandability, and evaluation criteria. Finally,we derive some conclusions concerning this one decade state-of-art in multimediaontologies.Keywords Ontology . Multimedia . RDF(S) . OWL . Comparative Framework1 IntroductionVision and sound are the most used senses to communicate experiences and knowledge. Theseexperiences or knowledge are normally recorded in media objects, which are generallyassociated to text, image, sound, video and animation. In this regard, a multimedia object can beconsidered as a composite media object (text, image, sound, video, or animation) that iscomposed of a combination of different media objects.Multimed Tools ApplDOI 10.1007/s11042-011-0905-zM. C. Suárez-Figueroa (*) : O. CorchoOntology Engineering Group (OEG). Facultad de Informática,Universidad Politécnica de Madrid (UPM), Madrid, Spaine-mail: mcsuarez@fi.upm.esO. Corchoe-mail: ocorcho@fi.upm.esURL: http://www.oeg-upm.net/G. A. AtemezingEurecom, MM Department, Sophia-Antipolis, Franceemail: ghislain.atemezing@gmail.comNowadays, a growing amount of multimedia data is being produced, processed, andstored digitally. We are continuously consuming multimedia contents in different formatsand from different sources using Google1, Flickr2, Picasa3, YouTube4, and so on. Theavailability of huge amounts of multimedia objects implies the need for efficientinformation retrieval systems that facilitate storage, retrieval, and browsing of not onlytextual, but also image, audio, and video objects. One potential approach can be based onthe semantic annotation of the multimedia content to be semantically described andinterpreted both by human agents (users) and machines agents (computers). Hence, there isa strong need of annotating multimedia contents to enhance the agents’ interpretation andreasoning for an efficient search.The annotation of multimedia objects is difficult because of the so-called semantic gap[24]; that is, the disparity between low level features (e.g., colour, textures, fragments) thatcan be derived automatically from the multimedia objects and high level concepts (mainlyrelated to domain content), which are typically derived based on human experience andbackground. In other words, the semantic gap refers to the lack of coincidence between theinformation that machines can extract from the visual data and the interpretation that thesame data have for a particular person in a given situation. The challenge of unifying bothlow level elements and high level descriptions of multimedia contents in a unique ontologyis one of the ways to contribute to bridge this semantic gap.The need for a high level representation that captures the true semantics of amultimedia object led at the beginning to the development of the MPEG-7 standard[9] for describing multimedia documents. This standard provides metadata descriptors",
        "publication_date": "2011-11-21",
        "authors": "Mari Carmen Suárez-Figueroa, Ghislain Auguste Atemezing, Óscar Corcho",
        "file_name": "10!1007%s11042-011-0905-z.pdf",
        "file_path": "output/PDFs/10!1007%s11042-011-0905-z.pdf",
        "pdf_link": null
    },
    {
        "title": "Creating and Querying Personalized Versions of Wikidata on a Laptop",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2108.07119",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Hans Chalupsky, Pedro Szekely, Filip Ilievski, Daniel Garijo, Kartik Shenoy",
        "file_name": "arxiv.2108.07119",
        "file_path": "10.48550/arxiv.2108.07119",
        "pdf_link": null
    },
    {
        "title": "Impact of Text Length for Information Retrieval Tasks based on Probabilistic Topics",
        "implementation_urls": [],
        "doi": "10.26342/2021-67-2",
        "arxiv": null,
        "abstract": "Abstract: Information retrieval has traditionally been approached using vectormodels to describe texts. In large document collections, these models need to reducethe dimensions of the vectors to make the operations manageable without compro-mising their performance. Probabilistic topic models (PTM) propose smaller vectorspaces. Words are organized into topics and documents are related to each otherfrom their topic distributions. As in many other AI techniques, the texts used totrain the models have an impact on their performance. Particularly, we are inter-ested on the impact that length of texts may have to create PTM. We have studiedhow it influences to semantically relate multilingual documents and to capture theknowledge derived from their relationships. The results suggest that the most ade-quate texts to train PTM should be of equal or greater length than those used tomake inferences later and documents should be related by hierarchy-based similaritymetrics at large-scale.Keywords: probabilistic topics, text similarity, hierarchical topics, document re-trieval.Resumen: La recuperación de información ha utilizado tradicionalmente modelosvectoriales para describir los textos. A gran escala, estos modelos necesitan reducirlas dimensiones de los vectores para que las operaciones sean manejables sin com-prometer su rendimiento. Los modelos probabiĺısticos de tópicos (MPT) proponenespacios vectoriales más pequeños. Las palabras se organizan en tópicos y los doc-umentos se relacionan entre śı a partir de sus distribuciones de tópicos. Como enmuchas otras técnicas de IA, los textos utilizados para entrenar los modelos influyenen su rendimiento. En particular, nos interesa el impacto de la longitud de los textosal crear MPT. Hemos estudiado cómo influye al relacionar semánticamente docu-mentos multilingües y al capturar el conocimiento derivado de sus relaciones. Losresultados sugieren que los textos más adecuados deben ser de igual o mayor longi-tud que los utilizados para hacer inferencias posteriormente y las relaciones debenbasarse en métricas de similitud jerárquicas.Palabras clave: topicos probabiĺısticos, semejanza de textos, jerarqúıa de tópicos,recuperación de documentos.1 IntroductionProbabilistic Topic Models (PTM) (Hof-mann, 2001) (Blei, Ng, and Jordan, 2003)are statistical methods based on bag-of-wordsthat analyze the words of the original textsto discover the themes that run throughthem, how those themes are connected toeach other, or how they change over time.PTM do not require any prior annotationsor labeling of the documents. The topicsemerge, as hidden structures, from the anal-ysis of the original texts. These structuresare topic distributions, per-document topicdistributions or per-document per-word topicassignments. In turn, a topic is a distribu-tion over terms that is biased around thosewords associated to a single theme. Figure 1shows some topics that have emerged whencreating a topic model with the collection ofWikipedia articles to better understand what",
        "publication_date": "2021-09-06",
        "authors": "Carlos Badenes-Olmedo, Borja Lozano-Álvarez, Óscar Corcho",
        "file_name": "20250514084836.pdf",
        "file_path": "output/PDFs/20250514084836.pdf",
        "pdf_link": "http://rua.ua.es/dspace/bitstream/10045/117485/1/PLN_67_02.pdf"
    },
    {
        "title": "Fuzzy Semantic Labeling of Semi-structured Numerical Datasets",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Ahmad Alobaid, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "What Are the Parameters that Affect the Construction of a Knowledge Graph?",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-33246-4_43",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "David Chaves-Fraga, Kemele M. Endris, Enrique Iglesias, Óscar Corcho, María-Esther Vidal",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Enhancing virtual ontology based access over tabular data with Morph-CSV",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%sw-210432.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The description and features of each query are also available online24."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210432",
        "arxiv": "2001.09052",
        "abstract": "Abstract. Ontology-Based Data Access (OBDA) has traditionally focused on providing a unified view of heterogeneous datasets(e.g., relational databases, CSV and JSON files), either by materializing integrated data into RDF or by performing on-the-fly querying via SPARQL query translation. In the specific case of tabular datasets represented as several CSV or Excel files,query translation approaches have been applied by considering each source as a single table that can be loaded into a relationaldatabase management system (RDBMS). Nevertheless, constraints over these tables are not represented (e.g., referential integrityamong sources, datatypes, or data integrity); thus, neither consistency among attributes nor indexes over tables are enforced.As a consequence, efficiency of the SPARQL-to-SQL translation process may be affected, as well as the completeness of theanswers produced during the evaluation of the generated SQL query. Our work is focused on applying implicit constraints onthe OBDA query translation process over tabular data. We propose Morph-CSV, a framework for querying tabular data thatexploits information from typical OBDA inputs (e.g., mappings, queries) to enforce constraints that can be used together withany SPARQL-to-SQL OBDA engine. Morph-CSV relies on both a constraint component and a set of constraint operators. Fora given set of constraints, the operators are applied to each type of constraint with the aim of enhancing query completenessand performance. We evaluate Morph-CSV in several domains: e-commerce with the BSBM benchmark; transportation withthe GTFS-Madrid benchmark; and biology with a use case extracted from the Bio2RDF project. We compare and report theperformance of two SPARQL-to-SQL OBDA engines, without and with the incorporation of Morph-CSV. The observed resultssuggest that Morph-CSV is able to speed up the total query execution time by up to two orders of magnitude, while it is able toproduce all the query answers.Keywords: Knowledge Graphs, Tabular Data, Mapping Languages, Constraints1. IntroductionGuided by the Open Data principles, governmentsand private organizations are regularly publishing vastamounts of public data in open data portals. For exam-ple, almost a million datasets are available in the Eu-ropean Open Data Portal (EODP)1, and many of them*Corresponding author. E-mail: dchaves@fi.upm.es.1https://www.europeandataportal.euare available in tabular formats (e.g., CSV, Excel), asobserved in Table 1. Both the simplicity of a tabularrepresentation and the variety of tools to manage a ta-ble (e.g., Excel, Calc) have influenced the popularityof tabular formats to represent open data.Albeit extensively utilized, tabular representationsimposed various data management challenges to ad-vanced users (e.g., developers, data scientists). Thelack of a unified way to query tabular data, some-thing available in other formats (e.g., RDB, JSON,1570-0844/0-1900/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:dchaves@fi.upm.esmailto:eruckhaus@fi.upm.esmailto:fpriyatna@fi.upm.esmailto:ocorcho@fi.upm.esmailto:maria.vidal@tib.eumailto:dchaves@fi.upm.eshttps://www.europeandataportal.eu2 Chaves-Fraga et al. / Enhancing Virtual OBDA over Tabular Data with Morph-CSV1 12 23 34 45 5",
        "publication_date": "2021-04-09",
        "authors": "David Chaves-Fraga, Edna Ruckhaus, Freddy Priyatna, María-Esther Vidal, Óscar Corcho",
        "file_name": "10!3233%sw-210432.pdf",
        "file_path": "output/PDFs/10!3233%sw-210432.pdf",
        "pdf_link": null
    },
    {
        "title": "Enhancing Drug Repurposing on Graphs by Integrating Drug Molecular Structure as Feature",
        "implementation_urls": [],
        "doi": "10.1109/cbms58004.2023.00215",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-06-01",
        "authors": "Adrián Ayuso-Muñoz, Lucía Prieto Santamaría, Andrea Álverez-Pérez, Belén Otero-Carrasco, Emilio Serrano, Alejandro Rodríguez‐González",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2021.02.018",
        "arxiv": null,
        "abstract": "AbstractKnowledge graphs (KGs) are one of the most common frameworks for knowl-edge representation. However, they suffer from a severe scalability problemthat hinders their usage. KG embedding aims to provide a solution to thisissue. Nonetheless, general approaches are incapable of representing and rea-soning about information not previously contained in the graph. This paperproposes to leverage semantic and ontological information for a significant ben-efit of knowledge graph completion, focusing on triple classification. The goalof this task is to determine whether a given fact holds. Furthermore, this paperalso considers the classification of facts that include entities that have not beenseen during training, denoted out-of-knowledge-base or OOKB entities. An in-cremental method is presented, composed of six stages. Although the proposalcan be applied to any KG embedding model, this work focuses on its applicationfor semantic matching models, such as ComplEx and DistMult. Compared toother approaches, our proposal is model-agnostic, computationally inexpensive,and does not require retraining. The results show that triple classification ac-curacy scales up to 15% with the proposed approach, as well as accelerating theconvergence of the model to its optimal solution. Furthermore, facts containingOOKB entities can be classified with a reasonable accuracy.Key words: Knowledge Graph Embeddings, Entity Initialization, KnowledgeGraph Completion, Word Embeddings, Ontological Information1. IntroductionKnowledge graphs (KGs) are currently one of the primary forms of knowledgerepresentation. In these structures, knowledge is stored in the form of facts, or∗Corresponding authorEmail address: emilioserra@fi.upm.es (Emilio Serrano)Preprint submitted to Information Science December 1, 2020 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ",
        "publication_date": "2021-02-20",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Patrick Hohenecker, Thomas Lukasiewicz",
        "file_name": "10!1016%j!ins!2021!02!018.pdf",
        "file_path": "output/PDFs/10!1016%j!ins!2021!02!018.pdf",
        "pdf_link": null
    },
    {
        "title": "Using LOT methodology to develop a noise pollution ontology: a Spanish use case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/opencitydata/medio-ambiente-contaminacion-acustica",
                "type": "git",
                "paper_frequency": 5,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1007%s12652-019-01561-2.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The ontology code is available in our GitHub repository, as explained in subsection 3.1.1."
                    }
                ]
            }
        ],
        "doi": "10.1007/s12652-019-01561-2",
        "arxiv": null,
        "abstract": "Abstract Local administrations generate large amounts of data due to theprocesses followed to attend administrative governance issues and the needsof its citizenry. Sadly, in most cases this data is not fully exploited and re-mains within the institutions, making their reutilization difficult. Currently,open data initiatives had gained ground worldwide and more cities are takingadvantage of adopting an open data strategy, which are visible at the orga-nizational level and at user level. In this respect, there is a need to generateguidelines that allow cities: a) to identify datasets to be shared, for examplepollution, commercial premises, public services, etc. and b) to publish qualitydata on their portals. Data should be accurate and interoperable among citiesto facilitate reuse. This work describes the development process of an ontologyto represent the acoustic pollution data collected by measurement stations lo-cated in cities, providing a common model for data publication. The developedontology reuses several well-known ontologies and includes classes, propertiesand instances specifically created to cover this domain. This work also includesreal examples about how to instantiate the ontology.Keywords noise pollution · open data · ontologyPaola Espinoza-Arias* Corresponding authorE-mail: pespinoza@fi.upm.esMaŕıa Poveda-VillalónE-mail: mpoveda@fi.upm.esOscar CorchoE-mail: ocorcho@fi.upm.es1 Ontology Engineering Group, ETSI Informáticos, Universidad Politécnica de Madrid,28660 Madrid, Spain.2 Paola Espinoza-Arias et al.1 IntroductionNoise pollution is one of the environmental problems affecting the health andwell-being of citizens in large towns and cities. According to the World HealthOrganization, noise is the second largest environmental cause of health prob-lems, just after the impact of air quality [5]. Bearing in mind this public healthissue, many cities around the world have defined several regulations in order toassess and manage environmental noise. In the case of the European Union, in2002 the Directive 2002/49/EC [10], also known as the Environmental NoiseDirective (END), was passed in order to identify noise pollution levels and totrigger the necessary action both at Member State and at European Unionlevel. The END defines the environmental noise pollution as noise caused byroad, rail and airport traffic, industry, construction, as well as some otheroutdoor activities. In accordance with one of the key areas related with thisdirective, Member States are committed to reporting their strategic noise mapsevery five years. The purpose of these maps is presenting and assessing the cal-culated/measured noise levels over a geographical area in order to determinethe population exposed by this pollution. In the context of national legislationand local regulations, in Spain the Ley del Ruido 37/2003 [3] has also beendefined for protecting citizens from excessive noise pollution.In order to detect the noise level of cities, several sensors, located at strate-gic places (e.g. train stations, airports, wide boulevards, etc.), are used to col-lect these data. These sensors, in most cases, are part of an interconnected sen-sor network deployed over the city. Although cities are reporting their strategic",
        "publication_date": "2019-10-31",
        "authors": "Paola Espinoza-Arias, María Poveda‐Villalón, Óscar Corcho",
        "file_name": "10!1007%s12652-019-01561-2.pdf",
        "file_path": "output/PDFs/10!1007%s12652-019-01561-2.pdf",
        "pdf_link": null
    },
    {
        "title": "Multi-perspective approach for curating and exploring the history of climate change in Latin America within digital newspapers",
        "implementation_urls": [],
        "doi": "10.2298/csis220110008v",
        "arxiv": null,
        "abstract": "Abstract. This paper introduces a multi-perspective approach to deal with curation14and exploration issues in historical newspapers. It has been implemented in the15platform LACLICHEV (Latin American Climate Change Evolution platform).16Exploring the history of climate change through digitalized newspapers published17around two centuries ago introduces four challenges: (1) curating content for track-18ing entries describing meteorological events; (2) processing (digging into) collo-19quial language (and its geographic variations5) for extracting meteorological events;20(3) analyzing newspapers to discover meteorological patterns possibly associated21with climate change; (4) designing tools for exploring the extracted content.22LACLICHEV provides tools for curating, exploring, and analyzing historical news-23paper articles, their description and location, and the vocabularies used for referring24to meteorological events. This platform makes it possible to understand and iden-25tify possible patterns and models that can build an empirical and social view of the26history of climate change in the Latin American region.27Keywords: data curation, metadata extraction, data collections exploration, data28analytics.291. Introduction30Ninety-seven per cent of climate scientists agree that climate-warming trends over the31past century are very likely due to human activities6. Some observation reports and studies32reveal that the planet’s average surface temperature has risen about 2.0 degrees Fahren-33heit (1.1 degrees Celsius) since the late 19th century. The hypothesis is that this change34has been mainly driven by increased carbon dioxide and other human-made atmospheric35emissions.365 In Iberoamerica, Spanish has variations in the different countries, even if all Spanish-speaking people canperfectly understand each other.6 https://climate.nasa.gov/scientific-consensus/2 Authors Suppressed Due to Excessive LengthTechnological advances have allowed understanding of phenomena and complex sys-1tems by collecting many different types of information. Data collections are exported un-2der different releases with different sizes and formats (e.g., CSV, text, excel), sometimes3with various quality features. Tools helping to understand, consolidate and correlate data4collections are crucial. Even if there is an increasing interest in analysing digital data col-5lections for performing historical studies on climatologic events, the history of climate6behaviour is still an open issue that has not revealed missing knowledge. Long histor-7ical data studies could make it possible to compute more complete models of climatic8phenomena and the conditions in which they emerged. However, meteorology is a young9science that started around the 19th century. It is supported by more or less recent data,10making it challenging to run an analysis that can give more historical pictures of climatic11evolution and its implications using observations instead of extrapolations. Those willing12to promote changes in the behaviour of society and industry to reduce emissions that have13a role in climate change must convince civil society of the importance of the challenges.14For this reason, our work addressed the problem of collecting and analyzing the history15of meteorological events to explore how they were described, lived and perceived by civil16society. In this sense, the digitalization of data collections has an increasingly vital role17in collecting vast amounts of hidden data. Thus, considering that digital archives become18more easily accessible every time and contain explicit and implicit spatio-temporal in-19formation, researchers in GIScience [18], are becoming aware of these new data sources20[10], [9], [34], [41]. Moreover, digital data collections make it possible to have an analytic21vision of the evolution of environmental, administrative, economic and social phenomena.22In this context, our work deals with data collections that report the emergence of mete-23",
        "publication_date": "2023-01-01",
        "authors": "Genoveva Vargas‐Solar, José-Luis Zechinelli-Martini, Javier A. Espinosa-Oviedo, Luis M. Vilches‐Blázquez",
        "file_name": "10!2298%csis220110008v.pdf",
        "file_path": "output/PDFs/10!2298%csis220110008v.pdf",
        "pdf_link": null
    },
    {
        "title": "A framework for connecting two interoperability universes: OGC Web Feature Services and Linked Data",
        "implementation_urls": [],
        "doi": "10.1111/tgis.12496",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-11-28",
        "authors": "Luis M. Vilches‐Blázquez, Jhonny Saavedra",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "FMonE: A Flexible Monitoring Solution at the Edge",
        "implementation_urls": [],
        "doi": "10.1155/2018/2068278",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Álvaro Brandón, Marı́a S. Pérez, Jesús Montes, Alberto Sánchez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Community Roadmap for Scientific Workflows Research and Development",
        "implementation_urls": [],
        "doi": "10.1109/works54523.2021.00016",
        "arxiv": "2110.02168",
        "abstract": "Abstract—The landscape of workflow systems for scientificapplications is notoriously convoluted with hundreds of seeminglyequivalent workflow systems, many isolated research claims, anda steep learning curve. To address some of these challenges andlay the groundwork for transforming workflows research anddevelopment, the WorkflowsRI and ExaWorks projects partneredto bring the international workflows community together. Thispaper reports on discussions and findings from two virtual“Workflows Community Summits” (January and April, 2021).The overarching goals of these workshops were to develop a viewof the state of the art, identify crucial research challenges in theworkflows community, articulate a vision for potential communityefforts, and discuss technical approaches for realizing this vision.To this end, participants identified six broad themes: FAIR com-putational workflows; AI workflows; exascale challenges; APIs,interoperability, reuse, and standards; training and education;and building a workflows community. We summarize discussionsand recommendations for each of these themes.Index Terms—Scientific workflows, community roadmap, datamanagement, AI workflows, exascale computing, interoperabilityI. INTRODUCTIONScientific workflow systems are used almost universallyacross scientific domains for solving complex and large-scale computing and data analysis problems, and have un-derpinned some of the most significant discoveries of thepast decades [1]. Many of these workflows have significantcomputational, storage, and communication demands, and thusmust execute on a wide range of large-scale platforms, fromlocal clusters over science or public clouds to upcomingexascale HPC platforms [2]. Managing these executions isoften a significant undertaking, requiring a sophisticated andversatile software infrastructure.Historically, many of these infrastructures for workflowexecution consisted of complex, integrated systems, developedin-house by workflow practitioners with strong dependencieson a range of legacy technologies—even including sets ofad-hoc scripts. Due to the increasing need to support work-flows, dedicated workflow systems were developed to provideabstractions for creating, executing, and adapting workflowsconveniently and efficiently while ensuring portability. Whilethese efforts are all worthwhile individually, there are nowhundreds of independent workflow systems [3]. These arecreated and used by thousands of researchers and developers,leading to a rapidly growing corpus of workflows researchpublications. The resulting workflow system technology land-scape is fragmented, which may present significant barriersfor future workflow users due to the tens of seemingly com-parable, yet usually mutually incompatible, systems that exist.In the current workflow research, there are conflicting theo-retical bases and abstractions for what constitutes a workflow",
        "publication_date": "2021-11-01",
        "authors": "Rafael Ferreira da Silva, Henri Casanova, Kyle Chard, İlkay Altıntaş, Rosa M. Badía, Bartosz Baliś, Tainã Coleman, Frederik Coppens, Frank Di Natale, Bjoern Enders, Thomas Fahringer, Rosa Filgueira, Grigori Fursin, Daniel Garijo, Carole Goble, Dorran Howell, Shantenu Jha, Daniel S. Katz, Daniel Laney, Ulf Leser, M. Malawski, Kshitij Mehta, Loïc Pottier, Jonathan Ozik, J. L. Peterson, Lavanya Ramakrishnan, Stian Soiland‐Reyes, Douglas Thain, Matthew Wolf",
        "file_name": "10!1109%works54523!2021!00016.pdf",
        "file_path": "output/PDFs/10!1109%works54523!2021!00016.pdf",
        "pdf_link": null
    },
    {
        "title": "Chapter 7. Terminology and ontologies",
        "implementation_urls": [],
        "doi": "10.1075/tlrp.23.07mon",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-06-03",
        "authors": "Elena Montiel-Ponsoda",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards a Knowledge Graph Based Platform for Public Procurement",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-14401-2_29",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Elena Simperl, Óscar Corcho, Marko Grobelnik, Dumitru Roman, Ahmet Soylu, María Jesús Fernández Ruiz, Stefano Gatti, Chris Taggart, Urška Skok Klima, Annie Ferrari Uliana, Ian Makgill, Till Christopher Lech",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Lynx: A knowledge-based AI service platform for content processing, enrichment and analysis for the legal domain",
        "implementation_urls": [],
        "doi": "10.1016/j.is.2021.101966",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-12-06",
        "authors": "Julián Moreno Schneider, Georg Rehm, Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel, Patricia Martín-Chozas, María Navas-Loro, Martin Kaltenböck, Artem Revenko, Sotirios Karampatakis, Christian Sageder, Jorge Gracia, Filippo Maganza, Ilan Kernerman, Dorielle Lonke, Andis Lagzdiņš, Julia Bosque-Gil, Pieter Verhoeven, Elsa Gomez Diaz, Pascual Boil Ballesteros",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The apertium bilingual dictionaries on the web of data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170258",
        "arxiv": null,
        "abstract": "Abstract. Bilingual electronic dictionaries contain collections of lexical entries in two languages, with explicitly declared trans-lation relations between such entries. Nevertheless, they are typically developed in isolation, in their own formats and accessiblethrough proprietary APIs. In this paper we propose the use of Semantic Web techniques to make translations available on theWeb to be consumed by other semantic enabled resources in a direct manner, based on standard languages and query means.In particular, we describe the conversion of the Apertium family of bilingual dictionaries and lexicons into RDF (Resource De-scription Framework) and how their data have been made accessible on the Web as linked data. As a result, all the converteddictionaries (many of them covering under-resourced languages) are connected among them and can be easily traversed from oneto another to obtain, for instance, translations between language pairs not originally connected in any of the original dictionaries.Keywords: Linguistic linked data, multilingualism, Apertium, bilingual dictionaries, lexicons, lemon, translation1. IntroductionThe publication of bilingual and multilingual lan-guage resources as linked data (LD) on the Web canlargely benefit the creation of the critical mass of cross-lingual connections required by the vision of the mul-tilingual Web of Data [9]. The benefits of sharing lin-guistic information on the Web of Data have beenrecently recognized by the language resources com-munity, which has shown increasing interest in pub-lishing their linguistic data and metadata as LD onthe Web [2]. As a result of interlinking multilingualand open language resources, the Linguistic LinkedOpen Data (LLOD) cloud is emerging,1 that is, a new*Corresponding author. E-mail: jgracia@fi.upm.es.1An updated picture of the current LLOD cloud can be found athttp://linguistic-lod.org/linguistic ecosystem based on the LD principles thatwill allow the open exploitation of such data at globalscale.In this article we will focus on the case of electronicbilingual dictionaries as a particular type of languageresources. Bilingual dictionaries are specialized dictio-naries that describe translations of words or phrasesfrom one language to another. They can be unidirec-tional or bidirectional, allowing translation, in the lat-ter case, to and from both languages. A bilingual dic-tionary can contain additional information such as partof speech, gender, declension model and other gram-matical properties.Electronic bilingual dictionaries have been typicallydeveloped in isolation, in their own formats and acces-sible through proprietary APIs. We propose the use ofSemantic Web techniques to make translations avail-mailto:jgracia@fi.upm.esmailto:asun@fi.upm.esmailto:marta.villegas@upf.edumailto:nuria.bel@upf.edumailto:jgracia@fi.upm.eshttp://linguistic-lod.org/able on the Web to be consumed by other semantic en-abled resources in a direct manner, based on standard",
        "publication_date": "2017-01-24",
        "authors": "Jorge Gracia, Marta Villegas, Asunción Gómez‐Pérez, Núria Bel",
        "file_name": "10!3233%sw-170258.pdf",
        "file_path": "output/PDFs/10!3233%sw-170258.pdf",
        "pdf_link": null
    },
    {
        "title": "Discovering Hidden Mental States in Open Multi-Agent Systems by Leveraging Multi-Protocol Regularities with Machine Learning",
        "implementation_urls": [],
        "doi": "10.3390/s20185198",
        "arxiv": null,
        "abstract": "Abstract: The agent paradigm and multi-agent systems are a perfect match for the design of smartcities because of some of their essential features such as decentralization, openness, and heterogeneity.However, these major advantages also come at a great cost. Since agents’ mental states are hiddenwhen the implementation is not known and available, intelligent services of smart cities cannotleverage information from them. We contribute with a proposal for the analysis and prediction ofhidden agents’ mental states in a multi-agent system using machine learning methods that learnfrom past agents’ interactions. The approach employs agent communication languages, which is acore property of these multi-agent systems, to infer theories and models about agents’ mental statesthat are not accessible in an open system. These mental state models can be used on their own orcombined to build protocol models, allowing agents (and their developers) to predict future agents’behavior for various tasks such as testing and debugging them or making communications moreefficient, which is essential in an ambient intelligence environment. This paper’s main contribution isto explore the problem of building these agents’ mental state models not from one, but from severalinteraction protocols, even when the protocols could have different purposes and provide distinctambient intelligence services.Keywords: open multi-agent system; smart city; agent communication languages; agent-orientedsoftware engineering1. IntroductionSmart cities are technologically based on the combination of several socio-technical innovationssuch as: the Internet of Things (IoT), mobile Internet access, smartphones, data analytics, open datainitiatives, and sharing economy models, among others [1]. This allows these cities to manage assetsand resources efficiently by services enhanced with intelligence such as: traffic management, hospitals,transportation systems, power and water plants, waste management, etcetera.The open and heterogeneous nature of multi-agent systems (MASs) addresses naturally thedynamism and scalability problems of smart cities and ambient intelligence [2]. High-level interactionprotocols and communications are a cornerstone of MASs, which are capable of establishing conversationby following these protocols, sending and receiving messages, or sharing vocabularies.However, how do we leverage information from an open MAS to provide citizens with intelligentservices? MAS platforms and frameworks usually allow developers to analyze agents’ mental statesand interactions among agents for testing, debugging, and verification purposes [3]. Regarding theSensors 2020, 20, 5198; doi:10.3390/s20185198 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/ORCID ID: 0000-0002-4392-4743http://www.mdpi.com/1424-8220/20/18/5198?type=check_update&version=1http://dx.doi.org/10.3390/s20185198http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleDiscovering Hidden Mental States in OpenMulti-Agent Systems by Leveraging Multi-ProtocolRegularities with Machine LearningtEmilio Serrano *\"\\ and Javier BajoOntology Engineering Group, Department of Artificial Intelligence, Universidad Politécnica de Madrid,28660 Madrid, Spain; jbajo@fi.upm.es* Correspondence: emilioserra@fi.upm.es+ Current address: Departamento de Inteligencia Artificial, Escuela Técnica Superior de Ingenieros Informaticos,Universidad Politécnica de Madrid, Boadilla del Monte, 28660 Madrid, Spain.",
        "publication_date": "2020-09-12",
        "authors": "Emilio Serrano, Javier Bajo",
        "file_name": "20250514085133.pdf",
        "file_path": "output/PDFs/20250514085133.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/20/18/5198/pdf?version=1599893740"
    },
    {
        "title": "Ontology Management in an Industrial Environment: The BASF Governance Operational Model for Ontologies (GOMO)",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.7007495",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-08-18",
        "authors": "Ana Iglesias-Molina, José Antonio Bernabé-Díaz, P.R. Deshmukh, Paola Espinoza-Arias, Aarón Ayllón-Benítez, Alba Fernández-Izquierdo, José María Ponce-Bernabé, Sara Isabel Pérez, Edna Ruckhaus, Óscar Corcho, José Luis Sánchez-Fernández",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "On the Use of Deep Neural Networks for Security Vulnerabilities Detection in Smart Contracts",
        "implementation_urls": [],
        "doi": "10.1109/percomworkshops56833.2023.10150302",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-03-13",
        "authors": "Martina Rossini, Mirco Zichichi, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "eWoT: A Semantic Interoperability Approach for Heterogeneous IoT Ecosystems Based on the Web of Things",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/eWoT",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514085156.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Availability: the implementation of eWoT is publicly available at https://github.com/oeg-upm/eWoT."
                    }
                ]
            }
        ],
        "doi": "10.3390/s20030822",
        "arxiv": null,
        "abstract": "Abstract: With the constant growth of Internet of Things (IoT) ecosystems, allowing them tointeract transparently has become a major issue for both the research and the software developmentcommunities. In this paper we propose a novel approach that builds semantically interoperableecosystems of IoT devices. The approach provides a SPARQL query-based mechanism to transparentlydiscover and access IoT devices that publish heterogeneous data. The approach was evaluated inorder to prove that it provides complete and correct answers without affecting the response time andthat it scales linearly in large ecosystems.Keywords: semantic interoperability; IoT devices; context-based search; content-based search1. IntroductionIn the last decade, the IoT devices available through the Web have become pervasive [1],allowing users and applications to easily monitor and interact with their devices [2]. Far fromreaching a scenario in which the number of IoT devices will stop growing, researchers foresee aconstant growth in the number of such devices available through the Web in the mid-term future [3,4].The different IoT devices are developed and distributed by different vendors [5]; as a result, these IoTdevices rely on a wide number of heterogeneous models, data formats, and APIs [6].Semantic interoperability aims at dealing with IoT device heterogeneity [7], by enablingan environment in which IoT devices can be included, forming an IoT ecosystem. The ecosystem mustenable transparent mechanisms through which such devices are discoverable [8]. Discovery shouldenable context- and content-based searching (i.e., the discovery of sensors by some meta-data(context-based) or by values that devices are capturing (content-based)). For this purpose,W3C standards have become widely adopted [9], specifically SPARQL [10], to transparently interactwith IoT ecosystems, and ontologies are used to describe the meta-data of the IoT devices. In particular,the Thing Description (TD) model defined by the W3C Web of Things working group [11] is one of themost adopted to profile IoT devices and developing discovery mechanisms [12].A thorough analysis of the current proposals that address semantic interoperability for IoTecosystems was presented by Zout et al. [9], involving approximately 50 proposals. There are mainlytwo approaches for Web-available IoT devices based on the W3C standards that perform both context-and content-based searches: federation and centralised approaches. The former requires each IoTdevice in the ecosystem to enable a SPARQL endpoint, so when a query is issued it is federatedto all the devices in the ecosystem [13]. The latter consists of storing all the sensors’ meta-data ina central triple store, and it requires the IoT devices to push their fresher data into the triple store.Sensors 2020, 20, 822; doi:10.3390/s20030822 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-1823-4484https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttp://www.mdpi.com/1424-8220/20/3/822?type=check_update&version=1http://dx.doi.org/10.3390/s20030822http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleeWoT: A Semantic Interoperability Approach forHeterogeneous IoT Ecosystems Based on the Webof ThingsAndrea Cimmino **®, Maria Poveda-Villal6n **® and Rat Garcia-Castro tOntology Engineering Group (OEG), Universidad Politécnica de Madrid, 28660 Madrid, Spain;rgarcia@fi.upm.es* Correspondence: cimmino@fi.upm.es (A.C.); mpoveda@fi.upm.es (M.P.-V.)+ These authors contributed equally to this work.",
        "publication_date": "2020-02-04",
        "authors": "Andrea Cimmino, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "20250514085156.pdf",
        "file_path": "output/PDFs/20250514085156.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/20/3/822/pdf?version=1580812003"
    },
    {
        "title": "A sustainable process and toolbox for geographical linked data generation and publication: a case study with BTN100",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/btn100",
                "type": "git",
                "paper_frequency": 10,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1186%s40965-019-0060-4.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Original and transformed files, plugin and scripts are available at https://github.com/oeg-upm/btn100."
                    }
                ]
            }
        ],
        "doi": "10.1186/s40965-019-0060-4",
        "arxiv": null,
        "abstract": "AbstractWe describe the process and tools that we have used to generate and publish the BTN100 Linked Dataset, based onthe original data from the Spanish Topographic Base (1:100.000 scale) from the Spanish Instituto Geográfico Nacional.We have taken into account the limitations and lessons learned from our initial experience on the generation andpublication of Linked Data from a range of geographical sources in Spain, in 2010, and we have now refined theprocess in order to facilitate: declarative mappings for the transformations from existing open data (shapefiles),automation of transformations whenever there are changes in the original data sources, version control, andalignment with INSPIRE URIs. As a result of this transformation and publication process we have also updated thereference ontology for geographical features and aligned with general ontologies such as GeoSPARQL.Keywords: Geospatial data, Linked data, Linked dataset, OntologyIntroductionOne of the activities of the Spanish Instituto GeográficoNacional1 (IGN) is to produce geographical informationfor all the territorial entities in Spain. IGN is responsi-ble for maintaining and making accessible cartographicand topographic databases for the representation of theSpanish territory. Their catalogs publish data related totransport networks, geodetic information, administrativeunits, etc. making it possible for everyone to downloadthem from their data portal2 under an open data licensecompatible with CC By 4.03.Governments, via their many agencies and organiza-tions, are constantly producing data that may be highlyinterrelated, but in practice become isolated data dueto lack of interoperability. Cartographic and topographicinformation from IGN may easily enrich informationfrom other government entities data, e.g. data fromthe National Institute of Statistics, Institute of CulturalHeritage, General Direction of Cadastre, Geological andMining Institute, etc.*Correspondence: pespinoza@fi.upm.es1Ontology Engineering Group, Universidad Politécnica de Madrid, Boadilla delMonte 28660, SpainFull list of author information is available at the end of the articleHowever, the generalized lack of use of semantics stan-dards in the descriptions of the data elements within thedata sources make it difficult to reuse them. Althoughprogress in data availability, there are still plenty issuesrelated to semantic interoperability; this is the ability ofinformation systems to exchange data with unambiguous,shared meaning.There are several initiatives around the world that havefocused on generating and publishing Linked Data from arange of geospatial data sources, and a W3C/OCGWork-ing Group was running between 2015 and 2017 with thetitle “Spatial Data on the Web” producing recommenda-tions on how to publish different types of geospatial, sen-sor, and temporal data on the web in a principled manner.The LinkedGeoData initiative4 aimed to make availablethe information collected by Open Street Map5 as RDF",
        "publication_date": "2019-03-13",
        "authors": "Paola Espinoza-Arias, Miguel Ángel García-Delgado, Óscar Corcho, Pedro Vivas-White, Hugo Potti-Manjavacas",
        "file_name": "10!1186%s40965-019-0060-4.pdf",
        "file_path": "output/PDFs/10!1186%s40965-019-0060-4.pdf",
        "pdf_link": "https://opengeospatialdata.springeropen.com/track/pdf/10.1186/s40965-019-0060-4"
    },
    {
        "title": "Semantic Web and Databases",
        "implementation_urls": [],
        "doi": "10.1007/b106149",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2005-01-01",
        "authors": "Christoph Bußler, Val Tannen, Irini Fundulaki",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Analysis of the Confidence in the Prediction of the Protein Folding by Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-38079-2_9",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Paloma Tejera-Nevado, Emilio Serrano, Ana González-Herrero, Rodrigo Bermejo-Moreno, Alejandro Rodríguez‐González",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The Zaragoza’s Knowledge Graph: Open Data to Harness the City Knowledge",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract: Public administrations handle large amounts of data in relation to their internal processesas well as to the services that they offer. Following public-sector information reuse regulations andworldwide open data publication trends, these administrations are increasingly publishing their dataas open data. However, open data are often released without agreed data models and in non-reusableformats, reducing interoperability and efficiency in data reuse. These aspects hinder interoperabilitywith other administrations and do not allow taking advantage of the associated knowledge in anefficient manner. This paper presents the continued work performed by the Zaragoza city councilover more than 15 years in order to generate its knowledge graph, which constitutes the key piece oftheir data management system, whose main strengthen is the open-data-by-default policy. The mainfunctionalities that have been developed for the internal and external exploitation of the city’s opendata are also presented. Finally, some city council experiences and lessons learned during this processare also explained.Keywords: public administration; linked open data; knowledge graph; interoperability; ontologies;open government data; vocabularies; API1. IntroductionPublic administrations are large data producers and collectors due to the need to provide supportfor their internal processes, as well as for services offered to citizens. These data are traditionallyavailable in several formats, according to their own information models, and are managed in different,sometimes isolated, data sources. From the point of view of the organization, the importance of datais undeniable. A proper organizational data management improves decision-making, operationalefficiency and service provision. From the side of citizens, the public administration’s data are anessential asset that must be made available in order to enhance transparency and the accountability [1].From the point of view of re-users, having these data available enables developing value-addedservices and applications, consequently stimulating innovation and economy [2]. Having all thesebenefits in mind, public administrations are increasingly publishing their data as Open Data, definedas data that can be freely used, re-used and redistributed by anyone—subject only, at most, to therequirement to attribute and sharealike [3]. An Open Government Data strategy ensures that dataare made available across city departments and to third parties, contributes to citizen engagement,increases democracy and serves to drive economic growth and social improvement [4,5].However, these data are mostly represented with information models defined without previousagreements or consensus processes with other institutions, which hinders the possibility of using andInformation 2020, 11, 129; doi:10.3390/info11030129 www.mdpi.com/journal/informationhttp://www.mdpi.com/journal/informationhttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0002-9260-0753http://dx.doi.org/10.3390/info11030129http://www.mdpi.com/journal/informationhttps://www.mdpi.com/2078-2489/11/3/129?type=check_update&version=20101001010 information fmoPrZ01010ArticleThe Zaragoza’s Knowledge Graph: Open Data toHarness the City KnowledgePaola Espinoza-Arias ’*, Maria Jestis Fernandez-Ruiz 2, Victor Morlan-Plo 2,Rubén Notivol-Bezares 2 and Oscar Corcho !1 Ontology Engineering Group, ETSI Informaticos, Universidad Politécnica de Madrid, 28660 Madrid, Spain;ocorcho@fi.upm.es",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514085317.pdf",
        "file_path": "output/PDFs/20250514085317.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/11/3/129/pdf"
    },
    {
        "title": "Multiagent Systems on Virtual Games: A Systematic Mapping Study",
        "implementation_urls": [],
        "doi": "10.1109/tg.2022.3214154",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-10-12",
        "authors": "Jose Barambones, Juan Cano-Benito, Ismael Sánchez-Rivero, Ricardo Imbert, Florian Richoux",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "SAREF4INMA: A SAREF extension for the industry and manufacturing domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/idafensp/ar2dtool",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%sw-200402.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "More details related to LOT are available online in its website.12 The following sections present the main definitions and guidelines provided by the methodology for each of the above-mentioned activities."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-200402",
        "arxiv": null,
        "abstract": "Abstract. The IoT landscape is characterized by a fragmentation of standards, platforms and technologies, often scattered amongdifferent vertical domains. To prevent the market to continue to be fragmented and power-less, a protocol-independent semanticlayer can serve as enabler of interoperability among the various smart devices from different manufacturers that co-exist in aspecific industry domain, but also across different domains. To that end, the SAREF ontology was created in 2015 with theintention to interconnect data, enabling the communication between IoT devices that use different protocols and standards. Anumber of industrial sectors consequently expressed their interest to extend SAREF into their domains in order to fill the gaps ofthe semantics not yet covered by their communication protocols. Therefore, the SAREF4INMA ontology was recently createdto extend SAREF for describing the Smart Industry & Manufacturing domain. SAREF4INMA is based on several standards andIoT initiatives, as well as on real use cases, and includes classes, properties and instances specifically created to cover the industryand manufacturing domain. This work describes the approach followed to develop this ontology, specifies its requirements andalso includes a practical example of how to use it.Keywords: industry 4.0, ontology, standard, SAREF, SAREF4INMA1. IntroductionThis paper presents the resulting model after extending the Smart Applications REFerence ontology (SAREF)for the Industry & Manufacturing domain1 together with the methodology followed and modelling decisions takenduring the development. This paper builds on the success achieved in the past years with SAREF2, which is areference ontology for IoT created in close interaction with the industry [1] during a study requested by the EuropeanCommission in 20153. SAREF is published as an ETSI Technical Specification series that also includes dedicatedextensions to specific domains (TS 103 410, parts 1-6). A proof-of-concept solution based on SAREF in the energydomain and implemented on existing commercial products4 was demonstrated in 2017 [2].The motivation behind SAREF is that the IoT landscape is characterized by a fragmentation of standards, plat-forms and technologies, often scattered among different vertical domains [3, 4] . To prevent the market to continue*Corresponding author. E-mail: mike.deroode@tno.nl.1https://portal.etsi.org/STF/stfs/STFHomePages/STF5342https://ec.europa.eu/digital-single-market/en/blog/new-standard-smart-appliances-smart-home3https://sites.google.com/site/smartappliancesproject4https://ec.europa.eu/digital-single-market/en/news/digitalising-energy-sector-common-language-consumer-centric-world1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:mike.deroode@tno.nlmailto:laura.daniele@tno.nlmailto:albafernandez@fi.upm.esmailto:mpoveda@fi.upm.esmailto:rgarcia@fi.upm.esmailto:mike.deroode@tno.nl2 M.A.W. de Roode et al. / SAREF for Industry and Manufacturing1 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 15",
        "publication_date": "2020-10-16",
        "authors": "Mike de Roode, Alba Fernández-Izquierdo, Laura Daniele, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "10!3233%sw-200402.pdf",
        "file_path": "output/PDFs/10!3233%sw-200402.pdf",
        "pdf_link": null
    },
    {
        "title": "Survey of agent-based cloud computing applications",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2019.04.037",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-03",
        "authors": "Fernando De la Prieta, Sara Rodrı́guez, Pablo Chamoso, Juan M. Corchado, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "On The Modeling Of P2P Systems as Temporal Networks: a Case Study With Data Streaming",
        "implementation_urls": [],
        "doi": "10.23919/annsim55834.2022.9859513",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-07-18",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Ontologies for IoT Semantic Interoperability",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-45316-9_5",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-07-21",
        "authors": "Andrea Cimmino, Alba Fernández-Izquierdo, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "PaCTS 1.0: A Crowdsourced Reporting Standard for Paleoclimate Data",
        "implementation_urls": [],
        "doi": "10.1029/2019pa003632",
        "arxiv": null,
        "abstract": "Abstract The progress of science is tied to the standardization of measurements, instruments, and data.This is especially true in the Big Data age, where analyzing large data volumes critically hinges on the databeing standardized. Accordingly, the lack of community‐sanctioned data standards in paleoclimatologyhas largely precluded the benefits of Big Data advances in the field. Building upon recent efforts tostandardize the format and terminology of paleoclimate data, this article describes the PaleoclimateCommunity reporTing Standard (PaCTS), a crowdsourced reporting standard for such data. PaCTS captureswhich information should be included when reporting paleoclimate data, with the goal of maximizing thereuse value of paleoclimate data sets, particularly for synthesis work and comparison to climate modelsimulations. Initiated by the LinkedEarth project, the process to elicit a reporting standard involved aninternational workshop in 2016, various forms of digital community engagement over the next few years,and grassroots working groups. Participants in this process identified important properties acrosspaleoclimate archives, in addition to the reporting of uncertainties and chronologies; they also identifiedarchive‐specific properties and distinguished reporting standards for new versus legacy data sets. This workshows that at least 135 respondents overwhelmingly support a drastic increase in the amount of metadataaccompanying paleoclimate data sets. Since such goals are at odds with present practices, we discuss atransparent path toward implementing or revising these recommendations in the near future, using bothbottom‐up and top‐down approaches.Plain Language Summary Standardizing the way data are described and shared is key toaccelerating the progress of science. Building on recent advances in paleoceanography andpaleoclimatology, we present the first community‐led reporting standard for such datasets. The PaleoclimateCommunity reporTing Standard (PaCTS) provides guidelines as to which information should be includedwhen reporting data from various paleoclimate archives, as well as themes common to many fields, likeuncertainty and other site‐specific information. The ultimate goal of this effort is to (1) make these datasetsmore re‐usable over the long term, and (2) provide a roadmap for implementing and revising the standard,as the field of paleoclimatology and its practitioners both evolve. The requirements are driven by thediffering needs of data producers and the data consumers, who often have different goals in mind. Thus,agreeing on and writing up these requirements involves building consensus among the community to decideon their present and future goals.10.1029/2019PA003632Paleoceanography and PaleoclimatologyKHIDER ET AL. 21. IntroductionPaleoclimatology is a highly integrative discipline, often requiring the comparison of multiple data setsand model simulations to reach fundamental insights about the climate system. Currently, suchsyntheses are hampered by the time and effort required to transform the data into a usable formatfor each application. This task, called data wrangling, is estimated to consume up to 80% of researchertime in some scientific fields (Dasu & Johnson, 2003), an estimate commensurate with the experienceof many paleoclimatologists, particularly at the early‐career stage. Wrangling involves not onlyidentifying missing values or outliers in the time series but also searching multiple databases forthe scattered records, contacting the original investigators for the missing data and metadata, andorganizing the data into a machine‐readable format. Further, this wrangling requires an understandingof each data set's originating field and its unspoken practices and so cannot be easily automated or out-sourced to unskilled labor or software. There is therefore an acute need for standardizing paleoclimatedata sets.Indeed, standardization accelerates scientific progress, particularly in the era of Big Data, where data shouldbe Findable, Accessible, Interoperable, and Reusable (FAIR; Wilkinson et al., 2016). Standardization iscritical to many scientific endeavors: efficiently querying databases, analyzing the data and visualizingthe results; removing participation barriers for early‐careers scientists or people outside the field; redu-cing unintended errors in data management; and ensuring appropriate credit of the original authors.While the paleoclimate community has made great strides in this direction (e.g., Williams et al., 2018),much work remains. The recent adoption of the FAIR data principles (Wilkinson et al., 2016) by the",
        "publication_date": "2019-09-03",
        "authors": "Deborah Khider, Julien Emile‐Geay, Nicholas P. McKay, Yolanda Gil, Daniel Garijo, Varun Ratnakar, Montserrat Alonso‐García, Sébastien Bertrand, Oliver Bothe, Peter W. Brewer, Andrew G. Bunn, Manuel Chevalier, Laia Comas‐Bru, Adam Csank, Émilie Pauline Dassié, Kristine L. DeLong, Thomas Felis, Pierre Francus, Amy Frappier, William R. Gray, Simon Goring, Lukas Jonkers, Michael Kahle, Darrell S. Kaufman, Natalie Kehrwald, Belén Martrat, Helen McGregor, Julie N. Richey, Andreas Schmittner, Nick Scroxton, Elaine Kennedy Sutherland, Kaustubh Thirumalai, Kathryn Allen, Fabien Arnaud, Yarrow Axford, Timothy T. Barrows, Lucie Bazin, Suzanne E. Pilaar Birch, Elizabeth Bradley, Joshua C. Bregy, Émilie Capron, Olivier Cartapanis, Hong‐Wei Chiang, K. M. Cobb, Maxime Debret, René Dommain, Jianghui Du, Kelsey A. Dyez, Suellyn Emerick, Michael P. Erb, Georgina Falster, Walter Finsinger, Daniel Fortier, Nicolas Gauthier, S. E. George, Eric C. Grimm, J. E. Hertzberg, Fiona Hibbert, Aubrey L. Hillman, Will Hobbs, Matthew Huber, Anna L.C. Hughes, Samuel L. Jaccard, Jiaoyang Ruan, Markus Kienast, Bronwen Konecky, Gaël Le Roux, Vyacheslav Lyubchich, Valdir F. Novello, Lydia Olaka, J. W. Partin, Christof Pearce, Steven J. Phipps, Cécile Pignol, Natalia Piotrowska, Maria-Serena Poli, Alexander A. Prokopenko, Franciéle Schwanck, Christian Stepanek, George E. A. Swann, Richard J. Telford, Elizabeth R. Thomas, Zoë Thomas, S. A. Truebe, Lucien von Gunten, A. J. Waite, Nils Weitzel, Bruno Wilhelm, John W. Williams, Joseph W. Williams, Mai Winstrup, Ning Zhao, Yuxin Zhou",
        "file_name": "10!1029%2019pa003632.pdf",
        "file_path": "output/PDFs/10!1029%2019pa003632.pdf",
        "pdf_link": null
    },
    {
        "title": "Introduction to Linked Data and Its Lifecycle on the Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-39784-4_1",
        "arxiv": null,
        "abstract": "Abstract This chapter presents Linked Data, a new form of distributed data on theweb which is especially suitable to be manipulated by machines and to shareknowledge. By adopting the linked data publication paradigm, anybody can publishdata on the web, relate it to data resources published by others and run artificialintelligence algorithms in a smooth manner. Open linked data resources maydemocratize the future access to knowledge by the mass of internet users, eitherdirectly or mediated through algorithms. Governments have enthusiasticallyadopted these ideas, which is in harmony with the broader open data movement.Keywords Linked data � Semantic web � Democracy � Ontologies � Knowledgerepresentation � eDemocracy1.1 IntroductionMore than half of the world’s population has access to the Internet. Vast amounts ofknowledge accumulated in roughly 2 billion websites are available to anyone whois able to read and can afford an internet connection.Entertainment habits, interpersonal human relations and almost any conceivableaspect of human life have been profoundly transformed with the arrival of theinternet. Yet modern democracies have remained relatively unaffected. It is true thatpropaganda techniques have undergone changes, political parties organize theircampaign strategies differently and the idea of eDemocracy is perhaps about tohatch; but the public institutions, the habits of citizens and the overall political gameare all apparently the same.We have to indulge—Internet is a new thing. But a careful observation of theevolution of technologies and the new organizational forms they enable revealdiscrete signs of change, now with little effect but potentially of much impact.This chapter introduces some new technologies and ideas which may seemirrelevant today, but which will probably exert a powerful influence on the forth-coming transformations of the concept of democracy.© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_11.2 The World Wide Web as a Source of Dataand Knowledge1.2.1 Data, Information and KnowledgeMarshall McLuhan described technology as extensions of man (McLuhan 1964),whereby our bodies and our senses are extended beyond their natural limits.Certainly, a shovel is an improvement of our hands when we dig a trench andtelescopes are augmented eyes when we look at the stars. In top level chess tour-naments, chess players prepare their games and study their opponents with a jointteam of humans and machine—machines also extend human’s capabilities forthinking.In order to make a value judgement, we need data—this is a truism. But todaywe also need machines which need data. Whenever we take an important decision,we usually google for some related information. Our decisions are mediated byinformation provided by a company, or a handful of companies, whose interestsmay not match our interests. Maybe in the future we will have a wider range of",
        "publication_date": "2013-01-01",
        "authors": "Sören Auer, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Amrapali Zaveri",
        "file_name": "20250514085516.pdf",
        "file_path": "output/PDFs/20250514085516.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_1.pdf"
    },
    {
        "title": "DLT-based Data Mules for Smart Territories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AnaNSi-research/TruDaMul",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%icccn54977!2022!9868916.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/AnaNSi-research/TruDaMul https://polygon.technology/papers/10.1049/ntw2.12037 https://github.com/luca-Serena/lunes-tdm-islands https://github.com/AnaNSi-research/TruDaMul https://github.com/AnaNSi-research/TruDaMul"
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn54977.2022.9868916",
        "arxiv": null,
        "abstract": "Abstract—Many services that are taken for granted in smartcities are not even remotely available in dislocated areas yet, dueto the lack of or too costly wide area network connectivity. Withthe aim to offer a practical and secure way to transport dataand allow for communications in such constrained scenarios, wefocus on the problem of incentivizing to data mules, i.e. devicesdedicated to enable the data transfer even in the absence of theInternet. Our solution combines the use of several distributedtechnologies for verifying the correct behavior of all the partici-pants and incentivize them. We focus on the use of state channelsto support the flow of smart-contract-based tokens as a form ofpayment, in a condition where participants communicate onlywith others in physical proximity. Furthermore, we validate theviability of the application through the simulation of peer-to-peerinteractions between the participants. In this work we achievepositive results in terms of communication latency and percentageof client nodes which are able to benefit from the system.Index Terms—Data Mules, Distributed Ledger Technologies,State Channels, Smart TerritoriesI. INTRODUCTIONNowadays, we are at a crossroad. People are to some extentbeing constrained to move to big, smart cities, due to thehigher opportunities in terms of work and offered services.Conversely, recent events, such as the COVID-19 pandemic,have shown how this trend can be reversed, with an increasingnumber of people deciding to move to the countryside andrural areas. Almost, since for sure many services that are takenfor granted in (smart) cities, are not even remotely available indislocated areas yet. For some underprivileged territories it isnot possible to implement (costly) smart cities services due tothe very different economic circumstances or due to unavail-able, unreliable or too expensive network infrastructures [1].We argue that what is needed is a set of novel opportunisticsolutions, which allows us to share and reuse data, services,computation and bandwidth. Such a solution would simplifythe development of new services and the integration of legacytechnologies into new ones. Well-known examples consist oftechnologies such as multi-homing mobile services, mobilead-hoc networks, opportunistic networks, peer-to-peer andfog computing systems [1]. In this novel “smart territory”case, however, such applications might not be supported bya wide area network connectivity, and certain networkingsolutions might result as too costly (e.g. satellite connections).This work has received funding from the EU H2020 research and innovationprogramme under the MSCA ITN European Joint Doctorate grant agreementNo 814177 LAST-JD - RIoE and from the University of Urbino through the“Bit4Food” research project.Data Mules (that is an acronym for Mobile Ubiquitous LANExtensions [2]) allow for communication and data transfereven in the absence of Internet, and they can be important tools",
        "publication_date": "2022-07-01",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%icccn54977!2022!9868916.pdf",
        "file_path": "output/PDFs/10!1109%icccn54977!2022!9868916.pdf",
        "pdf_link": null
    },
    {
        "title": "Understanding the phenomenology of reading through modelling",
        "implementation_urls": [],
        "doi": "10.3233/sw-200396",
        "arxiv": null,
        "abstract": "Abstract. Large scale cultural heritage datasets and computational methods for the Humanities research framework are the two pillars of Digital Humanities (DH), a research field aiming to expand Humanities studies beyond specific sources and periods to address macro-scale research questions on broad human phenomena. In this regard, the development of machine-readable semantically enriched data models based on a cross-disciplinary “language” of phenomena is critical for achieving the interoperability of research data. This paper reports on, documents, and discusses the development of a model for the study of reading experiences as part of the EU JPI-CH project Reading Europe Advanced Data Investigation Tool (READ-IT). Through the discussion of the READ-IT ontology of reading experience, this contribution will highlight and address three challenges emerging from the development of a conceptual model for the support of research on cultural heritage. Firstly, this contribution addresses modelling for multi-disciplinary research. Secondly, this work describes the development of an ontology of reading experience, under the light of the experience of previous projects, and of ongoing and future research developments. Lastly, this contribution addresses the validation of a conceptual model in the context of ongoing research, the lack of a consolidated set of theories and of a consensus of domain experts.  Keywords: Reading Experience, Conceptual Modelling, Experience Ontology, Digital Humanities, Modelling Methods  1*Corresponding author. E-mail: alessio.antonini@open.ac.uk.  1.  Introduction The combination of digital sources and computational methods is at the centre of a change of paradigm and of research breakthroughs on cultural heritage. Firstly, the discoverability of sources described and enriched through the Semantic Web enables the construction of integrated datasets of sources based on different archives. Secondly, data integration, as well as quantitative and qualitative studies, complement the in-depth analysis of individual sources. The use of large-scale datasets and computational methods applied within a Humanities research framework is the pillar of the revolution of the Digital Humanities (DH).  The current challenge for the Digital Humanities is how to scale up from the established paradigm of focused studies of specific sources and periods, to macro-scale research addressing broad human phenomena over the longue durée, as represented in cultural heritage [1]. While the humanistic research of the past has focused on scarce and hence exceptional case studies, the radical digital reconstruction of the cultural heritage archive permits for the first time the study of more extensive contexts or ideas [2]. In this vision, the systematic study of data generated by research case studies that focus on the human phenomenon of reading (see Section 2) could unlock advancements on macro-scale questions related to understanding the human condition through time [3]. To realise this vision, a crucial issue to be addressed is the development of a shared “language” for the formalisation of the phenomenon of reading to be used in the production of computable research data [4]. ",
        "publication_date": "2020-09-29",
        "authors": "Alessio Antonini, Mari Carmen Suárez-Figueroa, Alessandro Adamou, Francesca Benatti, François Vignale, Guillaume Gravier, Lucia Lupi",
        "file_name": "10!3233%sw-200396.pdf",
        "file_path": "output/PDFs/10!3233%sw-200396.pdf",
        "pdf_link": null
    },
    {
        "title": "A Linked Data Terminology for Copyright Based on Ontolex-Lemon",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_28",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Victor Rodrı́guez-Doncel, Cristiana Santos, Pompeu Casanovas, Asunción Gómez‐Pérez, Jorge Gracia",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Ontological Representation of Smart City Data: From Devices to Cities",
        "implementation_urls": [],
        "doi": "10.3390/app9010032",
        "arxiv": null,
        "abstract": "Abstract: Existing smart city ontologies allow representing different types of city-related data fromcities. They have been developed according to different ontological commitments and hence do notshare a minimum core model that would facilitate interoperability among smart city informationsystems. In this work, a survey has been carried out in order to study available smart city ontologiesand to identify the domains they are representing. Taking into account the findings of the survey anda set of ontological requirements for smart city data, a list of ontology design patterns is proposed.These patterns aim to be easily replicated and provide a minimum set of core concepts in order toguide the development of smart city ontologies.Keywords: ontology; smart cities; ontology design patterns1. IntroductionThe term smart city refers to a city that manages, in an intelligent way, all its associated resourceswith the aim to enhance the quality of the services provided to citizens and to improve their qualityof life [1,2]. The smart city domain has been a topic of interest in many sectors around the world.Standardization bodies, for example, the International Telecommunications Union (https://www.itu.int) and the International Standards Organization (https://www.iso.org), have been working ondefining standards and recommendations in order to provide a unified way to refer to and managethis particular field. In addition, several initiatives and projects in the smart city field have emerged,which denote the efforts and investments that industries, countries, and regions are making in orderto manage the city resources in a better manner. In the case of city coalitions or research groups(e.g., Smart Cities-European Medium-Sized Cities (http://smart-cities.eu), Open and Agile Smart Cities(http://www.oascities.org), Smart Cities Council (http://smartcitiescouncil.com), etc.), their studiesand business reports mention more than 300 smart cities involved [3] with an increasing need for andinterest in exploring solutions in order to improve their city processes. In the case of projects (e.g.,ESPRESSO [4], CityPulse [5], SmartSantander [6], etc.), which are financed by public, private, or a mixof both funds, they aim, in most cases, to provide technological tools to solve requirements in severalcity challenges.In this respect, there is wide agreement about the fact that smart cities are characterized by apervasive use of information and communication technologies [7–9], which, in various urban domains,may help cities make better use of their resources [10]. Some of these technologies include open datainfrastructures, mobile applications, public participation tools, Internet of Things (IoT) platforms, etc.The data handled or produced by all these technologies is very heterogeneous in terms of formats,structure, and delivery mechanisms, both inside the same city and across different cities. Hence,this opens up the opportunity to create common models to allow interoperability inside cities. In thiscontext, ontologies, understood as formal specifications of shared conceptualizations [11], can be usedAppl. Sci. 2019, 9, 32; doi:10.3390/app9010032 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttps://orcid.org/0000-0002-9260-0753https://www.itu.inthttps://www.itu.inthttps://www.iso.orghttp://smart-cities.euhttp://www.oascities.orghttp://smartcitiescouncil.comhttp://dx.doi.org/10.3390/app9010032http://www.mdpi.com/journal/applscihttp://www.mdpi.com/2076-3417/9/1/32?type=check_update&version=2",
        "publication_date": "2018-12-22",
        "authors": "Paola Espinoza-Arias, María Poveda‐Villalón, Raúl García‐Castro, Óscar Corcho",
        "file_name": "20250514085647.pdf",
        "file_path": "output/PDFs/20250514085647.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/9/1/32/pdf?version=1545833153"
    },
    {
        "title": "Coming to Terms with FAIR Ontologies",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-61244-3_18",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "María Poveda‐Villalón, Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Legal Ontologies for the Spanish e-Government",
        "implementation_urls": [],
        "doi": "10.1007/11881216_32",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2006-01-01",
        "authors": "Asunción Gómez‐Pérez, Fernando Ortiz, Boris Villazón-Terrazas",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Ontology evolution for personalised and adaptive activity recognition",
        "implementation_urls": [],
        "doi": "10.1049/iet-wss.2018.5209",
        "arxiv": null,
        "abstract": "Abstract: Ontology-based knowledge driven Activity Recognition (AR) models play a vital role in realm of Internet of Things (IoTs). However, these models suffer the shortcomings of static nature, inability of self-evolution and lack of adaptivity. Also, AR models cannot be made comprehensive enough to cater all the activities and smart home inhabitants may not be restricted to only those activities contained in AR model. So, AR models may not rightly recognize or infer new activities. In this paper, a framework has been proposed for dynamically capturing the new knowledge from activity patterns to evolve behavioural changes in AR model (i.e. ontology based model). This ontology based framework adapts by learning the specialized and extended activities from existing user-performed activity patterns. Moreover, it can identify new activity patterns previously unknown in AR model, adapt the new properties in existing activity models and enrich ontology model by capturing change representation to enrich ontology model. The proposed framework has been evaluated comprehensively over the metrics of accuracy, statistical heuristics and Kappa Coefficient. A well-known dataset named DAMSH has been used for having an empirical insight to the effectiveness of proposed framework that shows a significant level of accuracy for AR models.  1. Introduction Human Activity Recognition (HAR) determines the activities that have been performed by humans based upon certain knowledge and context. Earlier, Activity Recognition (AR) was performed by observing and analyzing human activities through surveillance cameras. Such manual observation driven AR seemed cost-intensive and demanding around the clock e.g. personnel deployment in home care services was infeasible financially. However, automated HAR systems resolved the issues by providing efficient and cost-effective measures instead of human-centered observations and analysis. Continuous scientific and technical progress has directed the human expectation from HAR toward Personalized HAR [2] for personalized service provision. A rich growth of data-driven and knowledge-driven modeling techniques have been proposed in [3] [4] [5]. Limitations of data-driven problems are cold start problem and non-reusability [1] [3]. Whereas, knowledge-driven techniques are static in nature, incomplete, and non-adaptable [1] [4]. One recent contribution in knowledge-driven techniques is based upon ontologies [5] [6]. Compared with the rest of the approaches, ontology-based models provide a higher degree of automation, better reasoning ability and solid technological foundations but still lacking the self-evolution. In this paper, we extend our work described in [7] for ontology evolution. Proposed ontological model for Activity Recognition (AR) adopts hybrid activity modeling approach (knowledge-driven and data-driven) in which seed knowledge about activities is modeled in an ontology. Seed knowledge comprises of a set of actions necessary to perform an activity called Perceptible Activity Models (PAMs). Model described in [7] transfer the sensor stream into action properties. This sensor stream is used with different ontological contexts such as duration, location, object type, temporal dependencies among actions and feature-based semantic similarities [7] to recognize personalized activity patterns.  Practically, in activity modeling, it is not possible to ",
        "publication_date": "2019-03-02",
        "authors": "Muhammad Safyan, Zia Ul Qayyum, Sohail Sarwar, Muddesar Iqbal, Raúl García Castro, Anwer Al‐Dulaimi",
        "file_name": "10!1049%iet-wss!2018!5209.pdf",
        "file_path": "output/PDFs/10!1049%iet-wss!2018!5209.pdf",
        "pdf_link": null
    },
    {
        "title": "Smart Contracts Vulnerability Classification through Deep Learning",
        "implementation_urls": [],
        "doi": "10.1145/3560905.3568175",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-11-06",
        "authors": "Martina Rossini, Mirko Zichichi, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "GEnI: A framework for the generation of explanations and insights of knowledge graph embedding predictions",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2022.12.010",
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are among the most commonly used knowledgerepresentation paradigms, being at the core of tasks such as question an-swering or recommendation systems. Knowledge Graph Completion (KGC)is one of the key tasks concerning KGs, where the goal is to mine new el-ements from the existing information. Different approaches have been pro-posed through the years to tackle this challenge. Among them, two analo-gous categories can be distinguished: Rule-Learning and Knowledge GraphEmbeddings (KGE). Different methods have been subsequently proposed tounify both types under a single framework, such that the benefits of bothproposals can be exploited. However, none of these methods consider usingrule-learning models not as a boosting agent for the KGE model but as anexplainability tool. This work presents GEnI, a framework capable of gener-ating insights and explanations for KGE models. GEnI follows a three-phasesequential process, yielding a proper insight for a given prediction. Possi-ble outcomes are rules, correlations, and influence detection. Moreover, theoutput is expressed in natural language, to extend the explainability of theproposal further. GEnI has been successfully evaluated under three criteria:coherence, the meaningfulness of the output, and reliability. Moreover, itcan be used by both translational and bilinear KGE models, offering a broadcoverage. Furthermore, this work also presents an in-depth revision of ex-isting integrative approaches between rule-learning and embedding models,providing a comparative framework between them.This document is a Preprint. The final version has been published in NeurocomputingGEnI: Generation of Explanations of KGE predictions1. IntroductionKnowledge Graphs (KG) are one of the cornerstones of knowledge rep-resentation. They are widely used for tasks such as question answering orrecommendation. YAGO [1], Freebase [2], and WordNet [3] are exampleswhere the information is stored in the form of facts or triples. Each triplefollows the schema (s, r, o) and models the interactions between a subject andan object entity through a relation. KGs are constantly growing, making itnecessary to develop methods that can support the constant introduction ofnew elements into the KG.Knowledge Graph Completion (KGC) methods provide a solution to thisissue, automatically mining new facts from the information contained in thegraph. Amongst the existing KGC proposals, KG embeddings (KGE) are thepredominant option. These models embed the KG into a continuous vectorspace, then using the computed representations to infer new facts.Recent KGE models rely on increasingly complex paradigms, such asGraph Neural Networks [4], operations [5], and representation spaces [6, 7].While the usage of these paradigms benefits the model performance, it in-creases the complexity, thus hindering its interpretability.Rule-based learning models provide an alternative to KGE models, wherethe inference of new facts is performed from a set of mined rules from thedata. While these methods solve the interpretability issue existing withinKGE approaches, they can not encode statistical properties. Moreover, thesemodels can face scalability problems as the search space grows proportionallywith the KG size.Several approaches develop a hybrid KGC solution, unifying both em-",
        "publication_date": "2022-12-07",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique",
        "file_name": "10!1016%j!neucom!2022!12!010.pdf",
        "file_path": "output/PDFs/10!1016%j!neucom!2022!12!010.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards an Ontology for Public Procurement Based on the Open Contracting Data Standard",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-29374-1_19",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Ahmet Soylu, Brian Elvesæter, Philip Turk, Dumitru Roman, Óscar Corcho, Elena Simperl, George Konstantinidis, Till Christopher Lech",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The Use of Decentralized and Semantic Web Technologies for Personal Data Protection and Interoperability",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_23",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Mirko Zichichi, Victor Rodrı́guez-Doncel, Stefano Ferretti",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Refining Terminological Saturation using String Similarity Measures.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. This paper reports on the refinement of the THD algorithm, developed in the OntoElect framework. This baseline THD algorithm used exact string matches for key term comparison. It has been refined by introducing an appro-priate string similarity metric for grouping the terms having similar meaning and looking similar as text strings. To choose the most appropriate metric, several existing metrics have been cross-evaluated on the developed test set of multi-word terms in English. The rationale for creating this test set is also presented. Further, the refined algorithm for measuring terminological difference has been cross-evaluated with the baseline THD algorithm. For this cross-evaluation, the bags of terms extracted from the TIME collection of scientific papers were used. The experiment revealed that using the refined algorithm yielded better and quicker terminological saturation, compared to the baseline. Keywords: Automated Term Extraction, OntoElect, Terminological Difference, Key Term, Linguistic Similarity Metric, Bag of Terms, Terminological Satura-tion. 1 Introduction The research presented in this paper is the part of the development of the methodolog-ical and instrumental components for extracting representative (complete) sets of sig-nificant terms from the representative sub-collections of textual documents having min-imal possible size. These terms are further interpreted as the required features for engi-neering an ontology in a particular domain of interest. Therefore, it is assumed that the documents in a collection cover a single and well circumscribed domain. The main hypothesis, put forward in this work, is that a sub-collection can be considered as rep-resentative to describe the domain, in terms of its terminological footprint, if any addi-tions of extra documents from the entire collection to this sub-collection do not notice-ably change this footprint. Such a sub-collection is further considered as complete and mailto:aluonac@i.uamailto:rodeonpopov@gmail.com therefore yields a representative bag of significant terms describing its domain. The approach to assess the representativeness does so by evaluating terminological satura-tion in a document (sub-)collection [1], [31]. Detecting saturation is done by measuring terminological difference (thd) among the pairs of the consecutive incrementally enlarged datasets, as described in Section 4. This set measure is of course based on measuring differences between individual terms.   A (baseline) THD algorithm [1] has been developed and implemented in the OntoElect project1. This THD algorithm, however, uses a simple string equivalence check for de-tecting similar individual terms. The objective of the research presented in this paper was to find out if it is possible to achieve better performance in measuring terminolog-ical difference by using a proper string similarity measure to compare individual terms.  The remainder of the paper is structured as follows. Section 2 reviews the related work. Section 3 reports on the implementation of the chosen string similarity measures and selecting the proper term similarity thresholds for their use. Section 4 sketches out the approach of OntoElect for measuring thd and our refinement of the baseline THD algorithm. Section 5 presents the set-up and results of our evaluation experiments. Our conclusions and plans for the future work are given in Section 6.  2 Related Work The work reported in this paper aims at improving the measures of terminological dif-ference between the bags of terms extracted from textual documents. The improvement is sought via the proper choice and use of existing string metrics for measuring linguis-",
        "publication_date": "2018-01-01",
        "authors": "Alyona Chugunenko, Victoria Kosa, Rodion Popov, David Chaves-Fraga, Vadim Ermolayev",
        "file_name": "20250514085718.pdf",
        "file_path": "output/PDFs/20250514085718.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2105/10000003.pdf"
    },
    {
        "title": "LYNX: Towards a Legal Knowledge Graph for Multilingual Europe",
        "implementation_urls": [],
        "doi": "10.26826/law-in-context.v37i1.129",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-12-20",
        "authors": "Victor Rodrı́guez-Doncel, Elena Montiel-Ponsoda",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Handling qualitative preferences in SPARQL over virtual ontology-based data access",
        "implementation_urls": [],
        "doi": "10.3233/sw-212895",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-14",
        "authors": "Marlene Goncalves, David Chaves-Fraga, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Drivers, standards and platforms for the IoT: Towards a digital VICINITY",
        "implementation_urls": [],
        "doi": "10.1109/intellisys.2017.8324287",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-09-01",
        "authors": "Aida Mynzhasova, Carna Radojicic, Christopher Heinz, Johannes Kölsch, Christoph Grimm, Juan Rico, Keith Dickerson, Raúl García‐Castro, Victor Oravec",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Review of the state of the art: discovering and associating semantics to tags in folksonomies",
        "implementation_urls": [],
        "doi": "10.1017/s026988891100018x",
        "arxiv": null,
        "abstract": "AbstractThis paper describes and compares the most relevant approaches for associating tags with semantics inorder to make explicit the meaning of those tags. We identify a common set of steps that are usuallyconsidered across all these approaches and frame our descriptions according to them, providing aunified view of how each approach tackles the different problems that appear during the semanticassociation process. Furthermore, we provide some recommendations on (a) how and when to use eachof the approaches according to the characteristics of the data source, and (b) how to improve results byleveraging the strengths of the different approaches.Keywords: folksonomies, ontologies, tagging, semantics, clustering.1 IntroductionIn recent years we have witnessed the transition from a Web where the content is generated mainly bythe owners of websites to a more open and social Web where users are not only information consumersbut also producers (prosumers - Tapscott and Williams 2006). This new age of the Web, also known asWeb 2.01, has brought a diversity of new social applications like wikis, blogs, social networks, socialbookmarks, and photo, music and video sharing sites. These applications made it possible for all Webusers to contribute and share huge amounts of multimedia content, and to tag these content resources withfree-form keywords.Tags serve multiple purposes, such as content organisation, description, and searching. In 2003,Delicious2 was released as a social bookmarking tool where users are able to assign tags to urls in acollaborative manner. One year later, Flickr3 was presented as a social network for photo sharing whereusers can assign tags to their own photos or to other photos from their colleagues. Nowadays, tagging ispart of many popular applications such as Amazon, YouTube and Last.Fm, to name a few, where users canassign tags to products, videos and songs respectively.In 2004, Vander Wal4 coined the term Folksonomy to describe the new structure of users, tags andobjects. Folksonomy is defined as the result of personal free tagging of information and objects (anything1http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html2http://delicious.com/3http://www.flickr.com/4http://www.vanderwal.net/folksonomy.html58 A. GARCIA-SILVA ET AL.with a URL) for one’s own retrieval. The tagging is done in a social environment (usually shared and opento others). Folksonomies are good sources of terminology frequently updated by large communities ofusers. This contrasts with other classification schemes, such as thesauri or taxonomies, which are generallycreated and maintained by controlled user groups. Hence one of the advantages of folksonomies is theirability to rapidly adapt to new changes in terminologies and domains. Furthermore, as time goes by userstend to stabilize the vocabulary used to tag a resource (Golder & Huberman, 2006). This stabilization is theresult of several user iterations and of a tag recommendation strategy based on previously assigned tags.These vocabularies can be seen as shared conceptualizations by groups of users with respect to groups ofresources.The success of tagging is attributed to two main factors; (a) they are very easy to create, where usersdo not need any special skills or experience to tag, and (b) the benefits of tagging are immediate (Hothoet al., 2006). However, current tagging technology suffers from two main problems. First problem isthat folksonomies lack a uniform representation to facilitate their sharing and reuse. Some Web 2.0applications provide APIs to export their folksonomies. However, they do it in proprietary formats. Toovercome this problem, ontologies have been proposed to model the tagging activities in folksonomies,with semantic concepts to represent users, tags, resources, etc. (Echarte et al., 2007; Gruber, 2005; Kimet al., 2008a; Knerr, 2006; Newman, 2005; Passant, 2008; Scerri et al., 2007). One example of theseontologies is the SCOT ontology (Kim et al., 2008a), which is depicted in figure 1. This ontologymodels tagging information, and includes concepts such as User, Item, Tag and Tag Cloud as well as",
        "publication_date": "2012-02-22",
        "authors": "Andrés García-Silva, Óscar Corcho, Harith Alani, Asunción Gómez‐Pérez",
        "file_name": "10!1017%s026988891100018x.pdf",
        "file_path": "output/PDFs/10!1017%s026988891100018x.pdf",
        "pdf_link": null
    },
    {
        "title": "Proof of Location through a Blockchain Agnostic Smart Contract Language",
        "implementation_urls": [],
        "doi": "10.1109/icdcsw60045.2023.00016",
        "arxiv": null,
        "abstract": "Abstract—Location-based services are at the heart of manyapplications that individuals use every day. However, there isoften no guarantee of the truthfulness of users’ location data,since this information can be easily spoofed without a proof mech-anism. In distributed system applications, preventing users fromsubmitting counterfeit locations becomes even more challengingbecause of the lack of a central authority that monitors dataprovenance. In this work, we propose a decentralized architecturebased on blockchains and decentralized technologies, offering atransparent solution for Proof of Location (PoL). We specificallyaddress two main challenges, i.e., the issuing process of the PoLand the proof verification. We describe a smart contract basedimplementation in Reach, a blockchain-agnostic smart contractlanguage, and the tests we conducted on different blockchains,i.e. Ethereum, Polygon, and Algorand, measuring latency andcosts due to the payment of fees. Results confirm the viability ofthe proposal.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Keyword Search, SmartContractsI. INTRODUCTIONNowadays, many users’ activities are supported by a differ-ent number of mobile applications, leveraging their positionto offer specific location-based services. For example, trust-worthy crowd-sourcing of urban or environmental obstaclesfor accessibility purposes [1], [2], customer-loyalty rewardsystems that offer discounts to users who frequently visit theshop, privacy-preserving contact tracing [3].These kinds of systems have at least three issues to copewith. First, some level of trust is needed in the user that crowd-sources some data related to a certain position. This led to theidea of a Proof-of-Location (PoL) because the location couldbe easily spoofed [4]. Second, there is the need to ensure,on the other hand, some privacy guarantees to the users thatgenerate data, so as to avoid everyone being entitled to knowa specific user location at a certain time. Third, the usualapproach is to resort to a centralized system, where a singleentity is responsible for collecting and storing data, users thatgenerated them, and their associated position representing,somehow, a PoL. While this solution can help in dealing withthe two issues above, it raises some concerns on personal dataThis work received funding from the EU H2020 R&D programme under theMSCA ITN EJD grant agreement No 814177 Law Science and TechnologyJoint Doctorate - RIoE.sovereignty, as location data is one of the most sensitive caseswith respect to users’ data exploitation [5], [6].With this in view, in this paper we propose a decentralizedProof of Location (PoL) system, based on blockchain anddistributed storage technologies. Our system is designed to bedecentralized, so as to avoid the presence of single point of",
        "publication_date": "2023-07-18",
        "authors": "Michele Bonini, Mirko Zichichi, Stefano Ferrettiv, Gabriele D’Angelo",
        "file_name": "10!1109%icdcsw60045!2023!00016.pdf",
        "file_path": "output/PDFs/10!1109%icdcsw60045!2023!00016.pdf",
        "pdf_link": null
    },
    {
        "title": "Street images classification according to COVID-19 risk in Lima, Peru: a convolutional neural networks feasibility analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/jmcastagnetto/lima-atu-covid19-paraderos",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514085751.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Outcome (ie, labels: moderate and extreme COVID-19 risk) data are available online: https://sistemas.atu.gob.pe/paraderosCOVID; this information was systematised at https://github.com/jmcastagnetto/lima-atu-covid19-paraderos."
                    }
                ]
            }
        ],
        "doi": "10.1136/bmjopen-2022-063411",
        "arxiv": null,
        "abstract": "ABSTRACTObjectives  During the COVID-19 pandemic, convolutional neural networks (CNNs) have been used in clinical medicine (eg, X-rays classification). Whether CNNs could inform the epidemiology of COVID-19 classifying street images according to COVID-19 risk is unknown, yet it could pinpoint high-risk places and relevant features of the built environment. In a feasibility study, we trained CNNs to classify the area surrounding bus stops (Lima, Peru) into moderate or extreme COVID-19 risk.Design  CNN analysis based on images from bus stops and the surrounding area. We used transfer learning and updated the output layer of five CNNs: NASNetLarge, InceptionResNetV2, Xception, ResNet152V2 and ResNet101V2. We chose the best performing CNN, which was further tuned. We used GradCam to understand the classification process.Setting  Bus stops from Lima, Peru. We used five images per bus stop.Primary and secondary outcome measures  Bus stop images were classified according to COVID-19 risk into two labels: moderate or extreme.Results  NASNetLarge outperformed the other CNNs except in the recall metric for the moderate label and in the precision metric for the extreme label; the ResNet152V2 performed better in these two metrics (85% vs 76% and 63% vs 60%, respectively). The NASNetLarge was further tuned. The best recall (75%) and F1 score (65%) for the extreme label were reached with data augmentation techniques. Areas close to buildings or with people were often classified as extreme risk.Conclusions  This feasibility study showed that CNNs have the potential to classify street images according to levels of COVID-19 risk. In addition to applications in clinical medicine, CNNs and street images could advance the epidemiology of COVID-19 at the population level.INTRODUCTIONIn COVID-19 research, deep learning tools applied to image analysis (ie, computer vision) have informed the diagnosis and prog-nosis of patients through the classification of X-ray and computer tomography images of the chest.1–3 These tools have helped practi-tioners treating COVID-19 patients.On the other hand, the application of computer vision to study the epidemiology of COVID-19 has been limited. One relevant example is the use of Google Street View images to extract features of the built envi-ronment and associate these with COVID-19 ",
        "publication_date": "2022-09-01",
        "authors": "Rodrigo M. Carrillo‐Larco, Manuel Castillo-Cara, Jose Francisco Hernández Santa Cruz",
        "file_name": "20250514085751.pdf",
        "file_path": "output/PDFs/20250514085751.pdf",
        "pdf_link": "https://bmjopen.bmj.com/content/bmjopen/12/9/e063411.full.pdf"
    },
    {
        "title": "Multi-Agent System for Demand Prediction and Trip Visualization in Bike Sharing Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/novafloss/workalendar",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514085815.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available online: https://github.com/novafloss/workalendar (accessed on 5 October 2017)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app8010067",
        "arxiv": null,
        "abstract": "Abstract: This paper proposes a multi agent system that provides visualization and prediction toolsfor bike sharing systems (BSS). The presented multi-agent system includes an agent that performsdata collection and cleaning processes, it is also capable of creating demand forecasting models foreach bicycle station. Moreover, the architecture offers API (Application Programming Interface)services and provides a web application for visualization and forecasting. This work aims to make thesystem generic enough for it to be able to integrate data from different types of bike sharing systems.Thus, in future studies it will be possible to employ the proposed system in different types of bikesharing systems. This article contains a literature review, a section on the process of developing thesystem and the built-in prediction models. Moreover, a case study which validates the proposedsystem by implementing it in a public bicycle sharing system in Salamanca, called SalenBici. It alsoincludes an outline of the results and conclusions, a discussion on the challenges encountered in thisdomain, as well as possibilities for future work.Keywords: bike sharing systems (BSS); regression models; open data; data visualization; multi agentsystems; organizations and institutions; socio-technical systems1. IntroductionThere is a consensus in the literature [1,2] which states that bicycles are one of the most sustainablemodes of urban transport and they are suitable for both short trips and medium distance trips.Riding a bicycle does not have any negative impact on the environment [3], it promotes physicalactivity and improves health. Furthermore, its use is cost-effective from the perspective of usersand infrastructure.Moreover, due to the increased CO2 levels, the European Union and other states are takingmeasures to reduce greenhouse gas emissions in every sector of the economy [4].These facts explain the growing popularity of sustainable means of transport such as bike sharingsystems. From 1965 when they came into use in Amsterdam to 2001, there were only few systemsaround the world. Bike sharing systems (BSS) began to spread in 2012, when their number increased toover 400 [5]. By 2014 this number had doubled [6] and nowadays there are approximately 1175 cities,municipalities or district jurisdictions in 63 different countries where these systems are in active use,according to BikeSharingMap [7].Appl. Sci. 2018, 8, 67; doi:10.3390/app8010067 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-0493-4471https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/app8010067http://www.mdpi.com/journal/applsciAppl. Sci. 2018, 8, 67 2 of 21Bike sharing systems allow users to travel in the city at a low cost or even for free. They can pickup a bicycle at one of the stations distributed across the city and leave it at another. These systemshave evolved over time [8] and today the vast majority include sensors that provide information on theinteraction of users with the system. However, the management of these systems and the data collectedby them, is often poor and as a result the numbers of bicycles available at stations are not sufficient.These are the reasons as to why bike sharing systems should be improved with data producedby the systems themselves. They should include predictive models for user behaviour and demand,which will notify the system administrator of the stations where more bicycles are required forsatisfying user demand. This will also allow to set up new stations in places where the demand is highor, on the contrary, to close down the stations at which the demand is too low.This article presents a multi-agent system which collects bike sharing system data together withother useful data. The system uses these data to create demand prediction models and to offer services",
        "publication_date": "2018-01-05",
        "authors": "Álvaro Lozano Murciego, Juan F. De Paz, Gabriel Villarrubia González, Daniel H. de la Iglesia, Javier Bajo",
        "file_name": "20250514085815.pdf",
        "file_path": "output/PDFs/20250514085815.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/8/1/67/pdf?version=1515142581"
    },
    {
        "title": "A quality assessment approach for evolving knowledge bases",
        "implementation_urls": [],
        "doi": "10.3233/sw-180324",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-09-11",
        "authors": "Mohammad Rifat Ahmmad Rashid, Marco Torchiano, Giuseppe Rizzo, Nandana Mihindukulasooriya, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Chowlk: from UML-Based Ontology Conceptualizations to OWL",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-06981-9_20",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Serge Chávez-Feria, Raúl García‐Castro, María Poveda‐Villalón",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A graph-based representation of knowledge for managing land administration data from distributed agencies – A case study of Colombia",
        "implementation_urls": [],
        "doi": "10.1080/10095020.2021.2015250",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-07",
        "authors": "Luis M. Vilches‐Blázquez, Jhonny Saavedra",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Deep neural network architectures for social services diagnosis in smart cities",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2019.05.034",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-17",
        "authors": "Emilio Serrano, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Dealing with Demand in Electric Grids with an Adaptive Consumption Management Platform",
        "implementation_urls": [],
        "doi": "10.1155/2018/4012740",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Diego M. Jiménez-Bravo, Juan F. De Paz, Gabriel Villarrubia González, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Multiagent System for the Prediction of Road Maintenance Actions",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-87687-6_11",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-10-29",
        "authors": "Pablo Galcerán, Juan F. De Paz, Jacinto González‐Pachón, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Ontology based E-learning framework: A personalized, adaptive and context aware model",
        "implementation_urls": [],
        "doi": "10.1007/s11042-019-08125-8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-09-13",
        "authors": "Sohail Sarwar, Zia Ul Qayyum, Raúl García‐Castro, Muhammad Safyan, Rana Faisal Munir",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An Intelligent System to Generate Chord Progressions from Colors with an Artificial Immune System",
        "implementation_urls": [],
        "doi": "10.1007/s00354-020-00100-4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-06-25",
        "authors": "María Navarro-Cáceres, José A. Castellanos-Garzón, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Introduction: A Hybrid Regulatory Framework and Technical Architecture for a Human-Centered and Explainable AI",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_1",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Victor Rodrı́guez-Doncel, Monica Palmirani, Michał Araszkiewicz, Pompeu Casanovas, Ugo Pagallo, Giovanni Sartor",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Data governance through a multi-DLT architecture in view of the GDPR",
        "implementation_urls": [],
        "doi": "10.1007/s10586-022-03691-3",
        "arxiv": null,
        "abstract": "AbstractThe centralization of control over the processing of personal data threatens the privacy of individuals due to the lack oftransparency and the obstruction of easy access to their data. Individuals need the tools to effectively exercise their rights,enshrined in regulations such as the European Union General Data Protection Regulation (GDPR). Having direct controlover the flow of their personal data would not only favor their privacy but also a ‘‘data altruism’’, as supported by the newEuropean proposal for a Data Governance Act. In this work, we propose a multi-layered architecture for the managementof personal information based on the use of distributed ledger technologies (DLTs). After an in-depth analysis of thetensions between the GDPR and DLTs, we propose the following components: (1) a personal data storage based on a(possibly decentralized) file storage (DFS) to guarantee data sovereignty to individuals, confidentiality and data portability;(2) a DLT-based authorization system to control access to data through two distributed mechanisms, i.e. secret sharing (SS)and threshold proxy re-encryption (TPRE); (3) an audit system based on a second DLT. Furthermore, we provide aprototype implementation built upon an Ethereum private blockchain, InterPlanetary File System (IPFS) and Sia and weevaluate its performance in terms of response time.Keywords Distributed Ledger Technology � GDPR � Smart Contracts � Personal Data � Decentralized File Storage �Data Governance1 IntroductionThe control, direct or indirect, that individuals currentlyexercise over their personal data is conditioned by thecentralized platform-based personal information manage-ment techniques, which are then concentrated in a fewinternet service providers (ISPs) for the purpose ofexploring, filtering and obtaining data of interest [1]. Thelack of control by individuals over access to their data is ofgrowing concern and, as a result, several regulations havebeen enacted with the aim of addressing this need. TheGeneral Data Protection Regulation (GDPR) [2] is a prin-cipal example, designed for European citizens to helppromote a view in favor of the interests of individuals,instead of large corporations. It has been followed by otherregulations around the world, such as the California Con-sumer Privacy Act [3] in the USA. The GDPR conveysdata control by imposing a number of accountabilitymeasures on the responsible actors and by assigning a setof rights to individuals, i.e. as ‘‘natural persons should havecontrol of their own personal data’’ (Recital 7). Dedicatedtechnologies can help either companies to comply withGDPR (and similar) and individuals to exercise their rights,with particular regard to address two main issues: the lackof transparency in the management of personal informationand the inability to access and make interoperable personaldata.& Stefano Ferrettistefano.ferretti@uniurb.itMirko Zichichimirko.zichichi@upm.esGabriele D’Angelog.dangelo@unibo.itVı́ctor Rodrı́guez-Doncelvrodriguez@fi.upm.es1 Ontology Engineering Group, Universidad Politécnica de",
        "publication_date": "2022-08-10",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%s10586-022-03691-3.pdf",
        "file_path": "output/PDFs/10!1007%s10586-022-03691-3.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s10586-022-03691-3.pdf"
    },
    {
        "title": "Editorial of transport data on the web",
        "implementation_urls": [],
        "doi": "10.3233/sw-223278",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2022-10-14",
        "authors": "David Chaves-Fraga, Pieter Colpaert, Mersedeh Sadeghi, Marco Comerio",
        "file_name": "10!3233%sw-223278.pdf",
        "file_path": "output/PDFs/10!3233%sw-223278.pdf",
        "pdf_link": null
    },
    {
        "title": "Enhancing the Maintainability of the Bio2RDF Project Using Declarative Mappings.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Bio2RDF is one of the most popular projects that integratesand publishes biomedical datasets as Linked Data. The community hasactively contributed to the generation of these datasets using ad-hoc pro-grammed scripts. In the context of the Semantic Web, Ontology-BasedData Access (OBDA) approaches have been proposed to provide dataaccess and transformation in a more standardized way, using declara-tive mapping languages. In this paper, we propose the use of an OBDAapproach to provide an alternative to the way in which transformationsinto RDF are currently done in the Bio2RDF project, with the aim of en-hancing its methodology in terms of understandability, reusability andmaintainability. We describe the proposed methodology together withthe declarative mappings creation process aiming to improve the afore-mentioned features. We compare the RDF dataset generated using ourproposal with the latest release of Bio2RDF for a subset of the datasources that we have dealt with. Finally, we discuss the set of challengesthat we face with this approach.Keywords: Bio2RDF · OBDA · RML1 IntroductionIn the last decades, the amount of databases that have been created to store andshare biological knowledge has heavily increased [1,2]. According to [3], there aremore than 1600 biological databases that are publicly accessible online, includingwell-known examples, such as PubMed1, UniProt2 or KEGG3. Nowadays, theseresources have become essential for researchers, as they rely on them to conductmuch of their work.Each biological data source contains information specific to its domain. Thismeans that the knowledge of a concept (e.g. enzyme, transcription factor) isdistributed in multiple data sources that are created by different institutions,usually represented in different formats and terminologies. A relevant challengein this domain is how to integrate these data sources in order to provide a1 https://www.ncbi.nlm.nih.gov/pubmed/2 https://www.uniprot.org/3 https://www.genome.jp/kegg/https://www.ncbi.nlm.nih.gov/pubmed/https://www.uniprot.org/https://www.genome.jp/kegg/2 Iglesias-Molina et al.Table 1: Comparison of the methodology of Bio2RDF in its differentreleases and the proposed approach with declarative mappings. Thefeatures compared are the type of tool, how many can be used, and if it allowsmaterialisation or virtualization.FeatureBio2RDFRelease 1Bio2RDFReleases 2 & 3DeclarativeMappingsTool Type Ad-hoc solution Ad-hoc solution General Purpose# Tools 1 (myBio2RDF app) 1 (PHP scripts) ManyMaterialization Yes Yes Yes",
        "publication_date": "2019-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho",
        "file_name": "20250514090054.pdf",
        "file_path": "output/PDFs/20250514090054.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2849/paper-01.pdf"
    },
    {
        "title": "LOT: An industrial oriented ontology engineering framework",
        "implementation_urls": [
            {
                "identifier": "https://github.com/SmartDeveloperHub/sdh-vocabulary",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1016%j!engappai!2022!104755.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Users Experts activity flow X (input) Artefact reference Figure 1: LOT methodology base workflow Even though there are many roles which can be involved in ontology development projects, the LOT methodology classifies them into the following groups: 5Online documentation of the methodology is available at https://lot.linkeddata.es/5 https://lot.linkeddata.es/• Ontology developer: An ontology developer is a member of the ontology development team who has high knowledge about ontology development and knowledge representation."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.engappai.2022.104755",
        "arxiv": null,
        "abstract": "AbstractOntology Engineering has captured much attention during the last decades leading to the prolif-eration of numerous works regarding methodologies, guidelines, tools, resources, etc. includingtopics which are still being investigated. Even though, there are still many open questions whenaddressing a new ontology development project, regarding how to manage the overall projectand articulate transitions between activities or which tasks and tools are recommended for eachstep. In this work we propose the Linked Open Terms (LOT) methodology, an overall and light-weight methodology for building ontologies based on existing methodologies and oriented tosemantic web developments and technologies. The LOT methodology focuses on the alignmentwith industrial development, in addition to academic and research projects, and software devel-opment, that is making ontology development part of the software industry. This methodologyincludes lessons learnt from more than 20 years in ontological engineering and its application on18 projects is reported.Keywords: Ontology engineering; Ontology development methodology; Ontology developmentsoftware support; Collaborative ontology development; Ontology industrial development1. IntroductionQ11: Should you always write ontology functional requirements in the form of CompetencyQuestions (CQs) (Grüninger and Fox, 1995) when you elaborate the requirements specificationfor an ontology? Q2: Which is the best way to communicate requirements to software engineers?Q3: And to domain experts? Q4: Which are the main problems that you will find when you reusean ontology? Q5: Which is the sequence of activities that has been shown to be successful inpractice in ontology development? Q6: What tools can you use for each activity in ontologydevelopment? Q7: Are there some tools that aid you to identify typical mistakes in modelling?Q8: What do you have to do to transform your Web Ontology Language (OWL) ontology intoan HTML document?Email addresses: mpoveda@fi.upm.es (María Poveda-Villalón), albafernandez@fi.upm.es (AlbaFernández-Izquierdo), mfernandez.eps@ceu.es (Mariano Fernández-López), rgarcia@fi.upm.es (RaúlGarcía-Castro)1The following questions are identified by “QX” to ease the reference to the questions from other sections of thepaper.Preprint submitted to Journal of LATEX Templates 24th January 2022Although it is true that a lot of useful work has been carried out on ontology engineering overthe years, such as the proposal of multiple ontology development methodologies to systematisethe development process and the alignment with agile practices, as of today, there are importantquestions on ontology development that have not been answered, This issue has been exposed bythe recent analysis of the current state, challenges and future directions in ontology engineeringpresented by Tudorache (Tudorache, 2020).The aim of the work presented in this paper is to respond to this situation and answer thequestions presented in the first paragraph by proposing the Linked Open Terms (LOT) methodo-logy, which not only presents the activities to be performed in the ontology development process,but also proposes recommendations, tips and tools to support them. The LOT methodology isbased on the experience of, at least, 18 projects where ontologies have been developed, both bythis paper authors and by external teams, involving both domain experts and software engineers.Our experience is also diverse in other senses, for example, there are projects where the creationof linked open data has been an important result, others where the ontology has been an object-ive itself, others where the ontology has been an standard schema for communication betweensystems, etc. In addition, one of the authors has contributed in the past to two of the most well-known methodologies for building ontologies which brings not only an extensive experience inpractical matters but also a broader view and knowledge about the evolution of the ontology en-gineering field during the last decades. The conclusions and lessons learnt from our experience",
        "publication_date": "2022-03-03",
        "authors": "María Poveda‐Villalón, Alba Fernández-Izquierdo, Mariano Fernández‐López, Raúl García‐Castro",
        "file_name": "10!1016%j!engappai!2022!104755.pdf",
        "file_path": "output/PDFs/10!1016%j!engappai!2022!104755.pdf",
        "pdf_link": null
    },
    {
        "title": "A guideline for reporting experimental protocols in life sciences",
        "implementation_urls": [],
        "doi": "10.7717/peerj.4795",
        "arxiv": null,
        "abstract": "ABSTRACTExperimental protocols are key when planning, performing and publishing researchin many disciplines, especially in relation to the reporting of materials and methods.However, they vary in their content, structure and associated data elements. This articlepresents a guideline for describing key content for reporting experimental protocolsin the domain of life sciences, together with the methodology followed in order todevelop such guideline. As part of our work, we propose a checklist that contains 17 dataelements that we consider fundamental to facilitate the execution of the protocol. Thesedata elements are formally described in the SMART Protocols ontology. By providingguidance for the key content to be reported, we aim (1) to make it easier for authorsto report experimental protocols with necessary and sufficient information that allowothers to reproduce an experiment, (2) to promote consistency across laboratories bydelivering an adaptable set of data elements, and (3) to make it easier for reviewers andeditors to measure the quality of submitted manuscripts against an established criteria.Our checklist focuses on the content, what should be included. Rather than advocatinga specific format for protocols in life sciences, the checklist includes a full descriptionof the key data elements that facilitate the execution of the protocol.Subjects Biochemistry, Biotechnology, Cell Biology, Molecular Biology, Plant ScienceKeywords Checklist, Experimental protocols, Guidelines, Recommendations, Good practices forreporting protocols, Open science, ReproducibilityINTRODUCTIONExperimental protocols are fundamental information structures that support thedescription of the processes by means of which results are generated in experimentalresearch (Giraldo et al., 2017; Freedman, Venugopalan & Wisman, 2017). Experimentalprotocols, often as part of ‘‘Materials and Methods’’ in scientific publications, are centralfor reproducibility; they should include all the necessary information for obtainingconsistent results (Casadevall & Fang, 2010; Festing & Altman, 2002). Although protocolsare an important component when reporting experimental activities, their descriptionsare often incomplete and vary across publishers and laboratories. For instance, whenreporting reagents and equipment, researchers sometimes include catalog numbersand experimental parameters; they may also refer to these items in a generic manner,e.g., ‘‘Dextran sulfate, Sigma-Aldrich’’ (Karlgren et al., 2009). Having this information isimportant because reagents usually vary in terms of purity, yield, pH, hydration state,How to cite this article Giraldo et al. (2018), A guideline for reporting experimental protocols in life sciences. PeerJ 6:e4795; DOI10.7717/peerj.4795https://peerj.commailto:ogiraldo@fi.upm.eshttps://peerj.com/academic-boards/editors/https://peerj.com/academic-boards/editors/http://dx.doi.org/10.7717/peerj.4795http://creativecommons.org/licenses/by/4.0/http://creativecommons.org/licenses/by/4.0/http://dx.doi.org/10.7717/peerj.4795grade, and possibly additional biochemical or biophysical features. Similarly, experimentalprotocols often include ambiguities such as ‘‘Store the samples at room temperature untilsample digestion’’ (Brandenburg et al., 2002); but, how many Celsius degrees? What is theestimated time for digesting the sample? Having this information available not only savestime and effort, it also makes it easier for researchers to reproduce experimental results;adequate and comprehensive reporting facilitates reproducibility (Freedman, Venugopalan& Wisman, 2017; Baker, 2016).",
        "publication_date": "2018-05-28",
        "authors": "Olga Giraldo, Alexander García, Óscar Corcho",
        "file_name": "10!7717%peerj!4795.pdf",
        "file_path": "output/PDFs/10!7717%peerj!4795.pdf",
        "pdf_link": null
    },
    {
        "title": "An intelligent interface for integrating climate, hydrology, agriculture, and socioeconomic models",
        "implementation_urls": [],
        "doi": "10.1145/3308557.3308711",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-02-28",
        "authors": "Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Ewa Deelman, Rafael Ferreira da Silva, Craig A. Knoblock, Yao‐Yi Chiang, Tam Minh Pham, Jay Pujara, Bình Dương Vũ, Dan Feldman, Rajiv Mayani, Kelly M. Cobourn, Christopher Duffy, Armen R. Kemanian, Lele Shu, Vipin Kumar, Ankush Khandelwal, Kshitij Tayal, S. D. Peckham, Maria Stoica, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne A. Pierce",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Reliability and correlation analysis of computed methods to convert conventional 2D radiological hindfoot measurements to a 3D setting using weightbearing CT",
        "implementation_urls": [],
        "doi": "10.1007/s11548-018-1727-5",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-03-09",
        "authors": "Arne Burssens, Johannes M. Peeters, Matthias Peiffer, R. Marien, Tom Lenaerts, Geoffroy Vandeputte, Jan Victor",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "ENIGMA and global neuroscience: A decade of large-scale studies of the brain in health and disease across more than 40 countries",
        "implementation_urls": [
            {
                "identifier": "https://github.com/npnl/PALS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514090341.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Preprint at https://doi.org/10.31234/osf.io/jnsb2."
                    }
                ]
            }
        ],
        "doi": "10.1038/s41398-020-0705-1",
        "arxiv": null,
        "abstract": "AbstractThis review summarizes the last decade of work by the ENIGMA (Enhancing NeuroImaging Genetics through MetaAnalysis) Consortium, a global alliance of over 1400 scientists across 43 countries, studying the human brain in healthand disease. Building on large-scale genetic studies that discovered the first robustly replicated genetic loci associatedwith brain metrics, ENIGMA has diversified into over 50 working groups (WGs), pooling worldwide data and expertiseto answer fundamental questions in neuroscience, psychiatry, neurology, and genetics. Most ENIGMA WGs focus onspecific psychiatric and neurological conditions, other WGs study normal variation due to sex and gender differences,or development and aging; still other WGs develop methodological pipelines and tools to facilitate harmonizedanalyses of “big data” (i.e., genetic and epigenetic data, multimodal MRI, and electroencephalography data). Theseinternational efforts have yielded the largest neuroimaging studies to date in schizophrenia, bipolar disorder, majordepressive disorder, post-traumatic stress disorder, substance use disorders, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, autism spectrum disorders, epilepsy, and 22q11.2 deletion syndrome. More recentENIGMA WGs have formed to study anxiety disorders, suicidal thoughts and behavior, sleep and insomnia, eatingdisorders, irritability, brain injury, antisocial personality and conduct disorder, and dissociative identity disorder. Here,we summarize the first decade of ENIGMA’s activities and ongoing projects, and describe the successes and challengesencountered along the way. We highlight the advantages of collaborative large-scale coordinated data analyses fortesting reproducibility and robustness of findings, offering the opportunity to identify brain systems involved in clinicalsyndromes across diverse samples and associated genetic, environmental, demographic, cognitive, and psychosocialfactors.IntroductionThe ENIGMA (Enhancing NeuroImaging Geneticsthrough Meta Analysis) Consortium is a collaboration ofmore than 1400 scientists from 43 countries studying thehuman brain. ENIGMA started 10 years ago, in 2009, withthe initial aim of performing a large-scale neuroimaginggenetic study, and has since diversified into 50 workinggroups (WGs), pooling worldwide data, resources andexpertise to answer fundamental questions in neu-roscience, psychiatry, neurology, and genetics (Fig. 1shows a world map of participating sites, broken down byworking group). Thirty of the ENIGMA WGs focus onspecific psychiatric and neurologic conditions. Four studydifferent aspects of development and aging. Others studykey transdiagnostic constructs, such as irritability, and theimportance of evolutionarily interesting genomic regionsin shaping human brain structure and function. Central tothe success of these WGs are the efforts of dedicatedmethods development groups within ENIGMA. There arecurrently 12 WGs that develop and disseminate multi-scale and ‘big data’ analysis pipelines to facilitate harmo-nized analyses using genetic and epigenetic data,multimodal (anatomical, diffusion, functional) magnetic© The Author(s) 2020OpenAccessThis article is licensedunder aCreativeCommonsAttribution 4.0 International License,whichpermits use, sharing, adaptation, distribution and reproductionin any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate ifchangesweremade. The images or other third partymaterial in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to thematerial. Ifmaterial is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtainpermission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.Correspondence: Paul M. Thompson (pthomp@usc.edu)Full list of author information is available at the end of the article.",
        "publication_date": "2020-03-20",
        "authors": "Paul M. Thompson, Neda Jahanshad, Christopher R. K. Ching, Lauren E. Salminen, Sophia I. Thomopoulos, Joanna K. Bright, Bernhard T. Baune, Sara Bertolín, Janita Bralten, Willem B. Bruin, Robin Bülow, Jian Chen, Yann Chye, Udo Dannlowski, Carolien G. F. de Kovel, Gary Donohoe, Lisa T. Eyler, Stephen V. Faraone, Pauline Favre, Courtney A. Filippi, Thomas Frodl, Daniel Garijo, Yolanda Gil, Hans J. Grabe, Katrina L. Grasby, Tomáš Hájek, Laura K. M. Han, Sean N. Hatton, Kevin Hilbert, Tiffany C. Ho, Laurena Holleran, Georg Homuth, Norbert Hosten, Josselin Houenou, Iliyan Ivanov, Tianye Jia, Sinéad Kelly, Marieke Klein, Jun Soo Kwon, Max A. Laansma, Jeanne Leerssen, Ulrike Lueken, Abraham Nunes, Joseph O' Neill, Nils Opel, Fabrizio Piras, Federica Piras, Merel C. Postema, Elena Pozzi, Natalia Shatokhina, Carles Soriano‐Mas, Gianfranco Spalletta, Daqiang Sun, Alexander Teumer, Amanda K. Tilot, Leonardo Tozzi, Celia van der Merwe, Eus J.W. Van Someren, Guido van Wingen, Henry Völzke, Esther Walton, Lei Wang, Anderson M. Winkler, Katharina Wittfeld, Margaret J. Wright, Je‐Yeon Yun, Guohao Zhang, Yanli Zhang‐James, Bhim M. Adhikari, Ingrid Agartz, Moji Aghajani, André Alemán, Robert R. Althoff, André Altmann, Ole A. Andreassen, David Baron, Brenda Bartnik‐Olson, Janna Marie Bas‐Hoogendam, Arielle Baskin–Sommers, Carrie E. Bearden, Laura A. Berner, Premika S.W. Boedhoe, Rachel M. Brouwer, Jan K. Buitelaar, Karen Caeyenberghs, Charlotte A. M. Cecil, Ronald A. Cohen, James H. Cole, Patricia Conrod, Stéphane A. De Brito, Sonja M. C. de Zwarte, Emily L. Dennis, Sylvane Desrivières, Danai Dima, Stefan Ehrlich, Carrie Esopenko, Graeme Fairchild, Simon E. Fisher, Jean‐Paul Fouché, Clyde Francks",
        "file_name": "20250514090341.pdf",
        "file_path": "output/PDFs/20250514090341.pdf",
        "pdf_link": "https://www.nature.com/articles/s41398-020-0705-1.pdf"
    },
    {
        "title": "Optimized Term Extraction Method Based on Computing Merged Partial C-Values",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-39459-2_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Hennadii Dobrovolskyi, Vadim Ermolayev",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "publication_date": "2021-07-19",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": "output/PDFs/10!23919%annsim52504!2021!9552126.pdf",
        "pdf_link": null
    },
    {
        "title": "Mission possible: Unify HPC and Big Data stacks towards application-defined blobs at the storage layer",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2018.07.035",
        "arxiv": null,
        "abstract": "AbstractHPC and Big Data stacks are completely separated today. The storage layer offers opportunities for convergence,as the challenges associated with HPC and Big Data storage are similar: trading versatility for performance. Thismotivates a global move towards dropping file-based, POSIX-IO compliance systems. However, on HPC platformsthis is made difficult by the centralized storage architecture using file-based storage. In this paper we advocate that thegrowing trend of equipping HPC compute nodes with local storage redistributes the cards by enabling object storageto be deployed alongside the application on the compute nodes. Such integration of application and storage not onlyallows fine-grained configuration of the storage system, but also improves application portability across platforms. Inaddition, the single-user nature of such application-specific storage obviates the need for resource-consuming storagefeatures like permissions or file hierarchies offered by traditional file systems. In this article we propose and evaluateBlobs (Binary Large Objects) as an alternative to distributed file systems. We factually demonstrate that it offersdrop-in compatibility with a variety of existing applications while improving storage throughput by up to 28%.1. IntroductionHPC and Big Data platforms are carving new datastorage models. This is made necessary by the ever-increasing scale of the computation and of the datasetsingested and produced by large-scale applications. Thesuccess of key-value stores [1, 2] or block storagesystems [3, 4] on Clouds, and the advent of burstbuffers [5, 6] or advanced I/O libraries [7, 8] for HPCclearly highlight this need.At the heart of these different methods is the movefrom legacy POSIX-compliant storage systems towardssimple storage paradigms designed especially for onepurpose, trading versatility for performance. Indeed,Email addresses: pmatri@fi.upm.es (Pierre Matri),alforov@dkrz.de (Yevhen Alforov), abrandon@fi.upm.es(Álvaro Brandon), mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),gabriel.antoniu@inria.fr (Gabriel Antoniu),michael.kuhn@informatik.uni-hamburg.de (Michael Kuhn),carns@mcs.anl.gov (Philip Carns), ludwig@dkrz.de(Thomas Ludwig)POSIX-IO imposes functionality such as hierarchicalnamespaces or file permissions. While these featuresare often provided for convenience, they are in practicerarely needed by modern applications and can signifi-cantly hinder the storage performance. Indeed, the li-braries and frameworks commonly used to access thestorage on HPC [9] and Big Data platforms [10, 11] pro-vide relaxed semantics (i.e., the set of rules and guaran-tees provided by the system regarding the behavior of itsstorage operations) compared to those of the underlyingfile system.Yet, deploying new storage models on HPC platformsused to be hard or simply impossible. Indeed, paral-lel file systems such as Lustre or GPFS on HPC havebeen the cornerstone of HPC storage for decades and arelikely to remain so in the next few years. This is largelyexplained by the high level of versatility and support for",
        "publication_date": "2018-07-26",
        "authors": "Pierre Matri, Yevhen Alforov, Álvaro Brandón, Marı́a S. Pérez, Alexandru Costan, Gabriel Antoniu, Michael Kühn, Philip Carns, Thomas Ludwig",
        "file_name": "10!1016%j!future!2018!07!035.pdf",
        "file_path": "output/PDFs/10!1016%j!future!2018!07!035.pdf",
        "pdf_link": null
    },
    {
        "title": "DBtravel: A Tourism-Oriented Semantic Graph",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_19",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Pablo Calleja, Freddy Priyatna, Nandana Mihindukulasooriya, Mariano Rico",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A Flexible and Robust Deep Learning-Based System for Solar Irradiance Forecasting",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iipr/solar-irradiance",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%access!2021!3051839.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/iipr/solar-irradiance [24] Y."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2021.3051839",
        "arxiv": null,
        "abstract": "ABSTRACT Most studies about the solar forecasting topic do not analyze and exploit the temporal andspatial components that are inherent to such a task. Furthermore, they mostly focus just on precision and noton other meaningful features, such as flexibility and robustness. With the current energy production trends,where many solar panels are distributed across city rooftops, there is a need to manage all this informationsimultaneously and to be able to add and remove sensors as needed. Likewise, robust models need to beable to cope with (inevitable) sensor failure and continue producing reliable predictions. Due to all of this,solar forecasting models need to be as decoupled as possible from the number of data sources that feedthem and their geographical distribution, enabling also the reusability of the models. This article contributeswith a family of Deep Learning models for solar irradiance forecasting complying with the aforementionedfeatures, i.e. flexibility and robustness. In the first stage, several Artificial Neural Networks are trained as abasis for predicting solar irradiance on several locations at the same time. Thereupon, a family of models thatwork with irradiance maps thanks to Convolutional Long Short-TermMemory layers is presented, obtainingforecast skills between 7.4% and 41% (depending on the location and horizon) compared to the baseline.The latter family comes with flexibility and robustness features, which are required in large-scale IntelligentEnvironments, such as Smart Cities. Working with irradiance maps means that new sensors can be added(or removed) as needed, without requiring rebuilding the model. Experiments carried out show that sensorfailures have a mild impact on the prediction error for several forecast horizons.INDEX TERMS Convolutional long short-termmemory (Conv-LSTM), deep learning, irradiance map, solarirradiance, time series forecasting.I. INTRODUCTIONA. BACKGROUND AND MOTIVATIONDuring the past decade, the field of solar forecasting hasemerged due to the increasing energy demands by indus-try and research institutions in the context of the so-calledSustainable Development Goals (SDGs). Obtaining reliable,fine grain predictions is crucial for the development of manyrenewable energy systems. An example is the optimization ofthe layout and orientation of the solar panels of a Photovoltaic(PV) plant, which can have a big impact on the overall amountof energy produced during its lifetime. As the number of PVplants grows vastly and they are integrated into the electricThe associate editor coordinating the review of this manuscript andapproving it for publication was Frederico Guimarães .grid, anticipating the amount of energy produced is neededto avoid overloads and foresee energy shortage. Furthermore,the management of batteries to accumulate the producedenergy requires accurate forecasts.Several methods are being explored in the field to thisaim, which could be grouped as: Numerical Weather Predic-tion (NWP), image-based, statistical, and Machine Learning(ML). They could also be classified based on their character-istics, meaning if a method takes into account spatio-temporalfeatures, if it is deterministic or probabilistic, if it considersexogenous inputs (other inputs such as physical variables) orjust its data features, etc.The task of solar forecasting inherently presents manyangles of complexity. On the one hand, time granularitydirectly impacts on the variability of the measurements, in the12348 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 9, 2021https://orcid.org/0000-0003-4018-8725",
        "publication_date": "2021-01-01",
        "authors": "Ignacio-Iker Prado-Rujas, Antonio García-Dopico, Emilio Serrano, Marı́a S. Pérez",
        "file_name": "10!1109%access!2021!3051839.pdf",
        "file_path": "output/PDFs/10!1109%access!2021!3051839.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic Software Metadata for Workflow Exploration and Evolution",
        "implementation_urls": [],
        "doi": "10.1109/escience.2018.00132",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-10-01",
        "authors": "Lucas Carvalho, Daniel Garijo, Cláudia Bauzer Medeiros, Yolanda Gil",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Accountable Clouds Through Blockchain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/cloud-chain",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%access!2023!3276240.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/miker83z/cloud-chain [39] E."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2023.3276240",
        "arxiv": null,
        "abstract": "ABSTRACT We present a solution for accountability in Cloud infrastructures based on blockchain.We showthat, through smart contracts, it is possible to create an unforgeable log that can be used for auditing andautomatic Service Level Agreement (SLA) verification. As a practical case study, we consider Cloud storageservices and define interaction protocols for registering the outcome of each file operation in the blockchain.We developed a prototype implementation that runs on the GoQuorum, Hyperledger Besu, and Polygonblockchains, using different consensus protocols. Using a dedicated testbed, we discuss the performanceof our implementation in terms of latencies, error rates and gas usage. Results demonstrate the viabilityof our approach over permissioned blockchains, with better performance for the Polygon and GoQuorumRaft decentralized systems. Our implementation enables interoperability, given that it is supported by theEthereum Virtual Machine which currently is underlying several blockchain platforms.INDEX TERMS Blockchain, smart contracts, cloud computing.I. INTRODUCTIONCloud computing is a well-established paradigm for provid-ing computation and storage resources according to a ‘‘payas you go’’ model. In Cloud computing, service providersown computing resources and provide remote access to thoseresources to customers for a fee [2].The level of abstraction at which a customer interactswith a Cloud infrastructure is defined by the servicemodel. In a Software as a Service (SaaS) Cloud, cus-tomers are provided with application services running in theCloud infrastructure. ‘‘Google Workspace’’ and ‘‘MicrosoftOffice Online’’ are examples of widely used SaaS Clouds.A Platform as a Service (PaaS) Cloud provides program-ming languages, tools, and a hosting environment for appli-cations developed by the customer. Examples of PaaS solu-tions are AppEngine by Google, Force.com from SalesForce,The associate editor coordinating the review of this manuscript andapproving it for publication was Nitin Gupta .Microsoft’s Azure, and Amazon’s Elastic Beanstalk. Finally,an Infrastructure as a Service (IaaS) Cloud provides low-levelcomputing capabilities such as processing, storage, and net-works where the customer can run arbitrary software, includ-ing operating systems and applications. Amazon EC2 is anexample of IaaS Cloud.The mode of operation of a Cloud defines its deploymentmodel. A Private Cloud is operated exclusively for a cus-tomer organization; it might be managed or owned by thatorganization, although this is not required. A CommunityCloud is shared by several organizations and supports aspecific community with common concerns (e.g., regulatoryrequirements). A Public Cloud is made available to the gen-eral public and is owned by an organization selling Cloudservices. Finally, a Hybrid Cloud is built upon a combinationof private, public, and community Clouds.Cloud computing allows separation between constructionand operation of the infrastructure and providing end-userservices. This opportunity enables the existence of at least48358This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",
        "publication_date": "2023-01-01",
        "authors": "Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti, Moreno Marzolla",
        "file_name": "10!1109%access!2023!3276240.pdf",
        "file_path": "output/PDFs/10!1109%access!2023!3276240.pdf",
        "pdf_link": null
    },
    {
        "title": "Ontology-driven semantic unified modelling for concurrent activity recognition (OSCAR)",
        "implementation_urls": [],
        "doi": "10.1007/s11042-018-6318-5",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-07-05",
        "authors": "Muhammad Safyan, Zia Ul Qayyum, Sohail Sarwar, Raúl García‐Castro, Mehtab Ahmed",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Knowledge maps: An essential technique for conceptualisation",
        "implementation_urls": [],
        "doi": "10.1016/s0169-023x(99)00050-6",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2000-05-01",
        "authors": "A. Gómez, Ana M. Moreno, J. Pazos, Almudena Sierra-Alonso",
        "file_name": "10!1016%s0169-023x(99)00050-6.pdf",
        "file_path": "output/PDFs/10!1016%s0169-023x(99)00050-6.pdf",
        "pdf_link": null
    },
    {
        "title": "Data Management Documentation in Citizen Science Projects: Bringing Formalisation and Transparency Together",
        "implementation_urls": [],
        "doi": "10.5334/cstp.538",
        "arxiv": null,
        "abstract": "ABSTRACTCitizen science (CS) is a way to open up the scientific process, to make it more accessible and inclusive, and to bring professional scientists and the public together in shared endeavours to advance knowledge. Many initiatives engage citizens in the collection or curation of data, but do not state what happens with such data. Making data open is increasingly common and compulsory in professional science. To conduct transparent, open science with citizens, citizens need to be able to understand what happens with the data they contribute. Data management documentation (DMD) can increase understanding of and trust in citizen science data, improve data quality and accessibility, and increase the reproducibility of experiments. However, such documentation is often designed for specialists rather than amateurs.This paper analyses the use of DMD in CS projects. We present analysis of a qualitative survey and assessment of projects’ DMD, and four vignettes of data management practices. Since most projects in our sample did not have DMD, we further analyse their reasons for not doing so. We discuss the benefits and challenges of different forms of DMD, and barriers to having it, which include a lack of resources, a lack of awareness of tools to support DMD development, and the inaccessibility of existing tools to citizen scientists without formal scientific education. We conclude that, to maximise the inclusivity of citizen science, tools and templates need to be made more accessible for non-experts in data management.mailto:gefion.thuermer@kcl.ac.ukhttps://doi.org/10.5334/cstp.538https://orcid.org/0000-0001-7345-0000https://orcid.org/0000-0003-4112-6825https://orcid.org/0000-0002-1044-3943https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-1722-947X2Thuermer et al. Citizen Science: Theory and Practice DOI: 10.5334/cstp.538Citizen science projects can help advance scientific knowledge, and educate participants about specific topics and the scientific process in general (Bonney et al. 2009). These projects occur at different scales, from local, such as the iSPEX project (http://ispex-eu.org), where citizen scientists use sensors to measure air quality (Volten et al. 2018), to international, such as eBird (https://ebird.org), an online platform used globally to record bird observations (Lagoze 2014). Citizens may create such projects from the bottom up, with or without the support of professional scientists; conduct data collection or analysis in scientist-led projects (Wiggins and Crowston 2011); or contribute to scientific publications (Tinati et al. 2015).The implementation of data management policies can make data and projects more scientifically sound, improve data quality and accessibility, and increase reproducibility. In CS projects, data management is an essential activity that enables citizen scientists to produce data that can be relevant and useful for, and trusted by, researchers (Hunter, Alabri and Ingen 2013). However, in many projects, data management policies or documentation are not systematically applied, leading to the perception that ",
        "publication_date": "2023-06-05",
        "authors": "Gefion Thuermer, Esteban González, Neal Reeves, Óscar Corcho, Elena Simperl",
        "file_name": "20250514090815.pdf",
        "file_path": "output/PDFs/20250514090815.pdf",
        "pdf_link": "https://storage.googleapis.com/jnl-up-j-cstp-files/journals/1/articles/538/647dce1d15fad.pdf"
    },
    {
        "title": "OKG-Soft: An Open Knowledge Graph with Machine Readable Scientific Software Metadata",
        "implementation_urls": [],
        "doi": "10.1109/escience.2019.00046",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-09-01",
        "authors": "Daniel Garijo, Maximiliano Osorio, Deborah Khider, Varun Ratnakar, Yolanda Gil",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome",
        "implementation_urls": [],
        "doi": "10.1371/journal.pone.0080278",
        "arxiv": null,
        "abstract": "AbstractHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience orintuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify thisdifficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertiseto domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimatingthe time required to reproduce each of the steps in the method described in the original paper and make them part of anexplicit workflow that reproduces the original results. Reproducing the method took several months of effort, and requiredusing new versions and new software that posed challenges to reconstructing and validating the results. The quantificationleads to ‘‘reproducibility maps’’ that reveal that novice researchers would only be able to reproduce a few of the steps in themethod, and that only expert researchers with advance knowledge of the domain would be able to reproduce the methodin its entirety. The workflow itself is published as an online resource together with supporting software and data. The paperconcludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and adesiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducingthe work of others from published papers, but reproducing work from one’s own laboratory.Citation: Garijo D, Kinnings S, Xie L, Xie L, Zhang Y, et al. (2013) Quantifying Reproducibility in Computational Biology: The Case of the TuberculosisDrugome. PLoS ONE 8(11): e80278. doi:10.1371/journal.pone.0080278Editor: Christos A. Ouzounis, The Centre for Research and Technology, Hellas, GreeceReceived September 18, 2012; Accepted October 10, 2013; Published November 27, 2013Copyright: � 2013 Garijo et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permitsunrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Funding: This research is sponsored by Elsevier Labs, the National Science Foundation with award number -0 , the Air Force Office of ScientificResearch with award number FA9550-11-1-0104, internal funds from the University of Southern California’s Information Sciences Institute and from the Universityof California, San Diego, and by a Formacistudy design, data collection and analysis, decision to publish, or preparation of the manuscript.Competing Interests: The research presented here has been sponsored partly by Elsevier Labs. This does not alter the authors’ adherence to all the PLOS ONEpolicies on sharing data and materials.* E-mail: pbourne@ucsd.edu (PEB); gil@isi.edu (YG)IntroductionComputation is now an integral part of the biological scienceseither applied as a technique or as a science in its own right -bioinformatics. As a technique, software becomes an instrument toanalyze data and uncover new biological insights. By reading thepublished article describing these insights, another researcherhopes to understand what computations were carried out, replicatethe software apparatus originally used and reproduce theexperiment. This is rarely the case without significant effort, andsometimes impossible without asking the original authors. In short,reproducibility in computational biology is aspired to, but rarelyachieved. This is unfortunate since the quantitative nature of thescience makes reproducibility more obtainable than in cases whereexperiments are qualitative and hard to describe explicitly.An intriguing possibility where potential quantification exists isto extend articles through the inclusion of scientific workflows thatrepresent computations carried out to obtain the published results,thereby capturing data analysis methods explicitly [1]. This wouldmake scientific results more reproducible because articles wouldhave not only a textual description of the computational processdescribed in the article but also a workflow that, as acomputational artifact, could be analyzed and re-run automati-cally. Consequently, workflows can make scientists more produc-",
        "publication_date": "2013-11-27",
        "authors": "Daniel Garijo, Sarah Kinnings, Li Xie, Lei Xie, Yinliang Zhang, Philip E. Bourne, Yolanda Gil",
        "file_name": "10!1371%journal!pone!0080278.pdf",
        "file_path": "output/PDFs/10!1371%journal!pone!0080278.pdf",
        "pdf_link": null
    },
    {
        "title": "Nine best practices for research software registries and repositories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/codemeta/codemeta",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!7717%peerj-cs!1023.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available at https://github.com/codemeta/codemeta."
                    }
                ]
            }
        ],
        "doi": "10.7717/peerj-cs.1023",
        "arxiv": null,
        "abstract": "ABSTRACTScientific software registries and repositories improve software findability andresearch transparency, provide information for software citations, and fosterpreservation of computational methods in a wide range of disciplines. Registries andrepositories play a critical role by supporting research reproducibility andreplicability, but developing them takes effort and few guidelines are available to helpprospective creators of these resources. To address this need, the FORCE11 SoftwareCitation Implementation Working Group convened a Task Force to distill theexperiences of the managers of existing resources in setting expectations for allstakeholders. In this article, we describe the resultant best practices which includedefining the scope, policies, and rules that govern individual registries andrepositories, along with the background, examples, and collaborative work that wentinto their development. We believe that establishing specific policies such as thosepresented here will help other scientific software registries and repositories betterserve their users and their disciplines.Subjects Computer Education, Databases, Digital LibrariesKeywords Best practices, Research software repository, Research software registry, Softwaremetadata, Repository policies, Research software registry guidelinesINTRODUCTIONResearch software is an essential constituent in scientific investigations (Wilson et al., 2014;Momcheva & Tollerud, 2015; Hettrick, 2018; Lamprecht et al., 2020), as it is often used totransform and prepare data, perform novel analyses on data, automate manual processes,and visualize results reported in scientific publications (Howison & Herbsleb, 2011).Research software is thus crucial for reproducibility and has been recognized by thescientific community as a research product in its own right—one that should be properlydescribed, accessible, and credited by others (Smith, Katz & Niemeyer, 2016; Chue Honget al., 2021). As a result of the increasing importance of computational methods,communities such as Research Data Alliance (RDA) (Berman & Crosas, 2020) (https://www.rd-alliance.org/) and FORCE11 (Bourne et al., 2012) (https://www.force11.org/)How to cite this article Garijo D, Ménager H, Hwang L, Trisovic A, Hucka M, Morrell T, Allen A. et al., 2022. Nine best practices forresearch software registries and repositories. PeerJ Comput. Sci. 8:e1023 DOI 10.7717/peerj-cs.1023Submitted 28 September 2021Accepted 9 June 2022Published 8 August 2022Corresponding authorDaniel Garijo, daniel.garijo@upm.esAcademic editorVarun GuptaAdditional Information andDeclarations can be found onpage 24DOI 10.7717/peerj-cs.1023Copyright2022 Garijo et al.Distributed underCreative Commons CC-BY 4.0https://github.com/force11/force11-sciwghttps://github.com/force11/force11-sciwghttps://www.rd-alliance.org/https://www.rd-alliance.org/",
        "publication_date": "2022-08-08",
        "authors": "Daniel Garijo, Hervé Ménager, Lorraine Hwang, A. Trisovic, Michael Hucka, Thomas E. Morrell, Alice Allen",
        "file_name": "10!7717%peerj-cs!1023.pdf",
        "file_path": "output/PDFs/10!7717%peerj-cs!1023.pdf",
        "pdf_link": null
    },
    {
        "title": "Latest enhancements in the Spanish DBpedia.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. The Spanish DBpedia is a data source used initially to sup-port the Spanish community. However, our logs show that the Spanishlanguage goes beyond Spanish speakers and many non-Spanish speakersuse the Spanish DBpedia on a daily basis. In the last months we havemade two important enhancements to the Spanish DBpedia: (1) we pub-lish a nonstandard dataset containing the type of resources that in thestandard distribution have no type, and (2) we update automatically ourdata every week by using the DBpedia databus. In this way, we satisfya frequent request made by companies and we foster the usage of theSpanish language, the second mother language by the number of speak-ers (after Chinese), and the second in scientific papers (after English).Keywords: Spanish DBpedia · Resource type · DBpedia data bus.1 Introduction1.1 The rising of the Spanish languageThe data published by the Cervantes Institute in its 2020 report [4] are over-whelming: Spanish speakers have increased by 30% in the last decade, and thenumber of foreigners who study it has grown by 60%. More than 585 millionpeople speak Spanish. Of these, almost 489 million are native Spanish speakers.Furthermore, Spanish is the second mother tongue by number of speakers afterMandarin Chinese, and the third language in the global count of users after En-glish and Mandarin Chinese. On the Internet, it is the third most used and isthe second language, behind English, publishing scientific texts.The DBpedia project has long generated semantic information from EnglishWikipedia. Since June 2011, the information generation process has extractedinformation from Wikipedia in 111 of its languages, but only 18 languages have aDBpedia chapter with a website. One of them is Spanish. The DBpedia Interna-tionalization Committee has assigned a website and a SPARQL [7] endpoint for? Partially supported by HcommonK (RTC2019-007134-7) and Datos4.0 (TIN2013-46238-C4-3-R) projects.Copyright ©2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 S. Sanz-Lucio et al.each of these languages1. In the case of Spanish (with website es.dbpedia.org),the extraction process produces more than 100 million RDF triples from theSpanish Wikipedia. All these triples are available on the SPARQL endpointes.dbpedia.org/sparql using Semantic Web [2] and Linked Data [1] technolo-gies.1.2 The DBpedia datasetsAs we have mentioned previously, DBpedia extracts data from 111 differentlanguage editions of Wikipedia. Then, for each language we have a knowledgebase (a “Knowledge Graph” in modern terminology, abbreviated as KG). Thelargest DBpedia KG is extracted from the English edition of Wikipedia, witharound 400 million facts (triples) that describe 3.7 million resources (Wikipediaentries). The DBpedia knowledge graphs that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 millionadditional resources. Therefore, two-thirds of the information in DBpedia comesfrom non-English Wikipedias.From a technical perspective, the DBpedia project maps Wikipedia infoboxes [12]from 27 different language editions into the DBpedia ontology, a single shared on-tology consisting of 320 classes and 1,650 properties. The mappings are created",
        "publication_date": "2021-01-01",
        "authors": "Sara Sanz-Lucio, Oussama Tahiri-Alaoui, Mariano Rico",
        "file_name": "20250514091010.pdf",
        "file_path": "output/PDFs/20250514091010.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2941/paper8.pdf"
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "arxiv": null,
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "publication_date": "2021-05-07",
        "authors": "",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": "output/PDFs/10!3233%ssw51.pdf",
        "pdf_link": null
    },
    {
        "title": "Knowledge Base Evolution Analysis: A Case Study in the Tourism Domain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_26",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Marco Torchiano, Nandana Mihindukulasooriya, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A framework for the broad dissemination of hydrological models for non-expert users",
        "implementation_urls": [],
        "doi": "10.1016/j.envsoft.2023.105695",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-10",
        "authors": "Timo Schaffhauser, Daniel Garijo, Maximiliano Osorio, Daniel Bittner, Suzanne A. Pierce, Hernán Vargas, Markus Disse, Yolanda Gil",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "VICINITY: IoT Semantic Interoperability Based on the Web of Things",
        "implementation_urls": [],
        "doi": "10.1109/dcoss.2019.00061",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-01",
        "authors": "Andrea Cimmino, Viktor Oravec, Fernando Serena, Peter Kostelnik, María Poveda‐Villalón, Athanasios Tryferidis, Raúl García‐Castro, Stefan Vanya, Dimitrios Tzovaras, Christoph Grimm",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "RDF shape induction using knowledge base profiling",
        "implementation_urls": [],
        "doi": "10.1145/3167132.3167341",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-04-09",
        "authors": "Nandana Mihindukulasooriya, Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Raúl García‐Castro, Óscar Corcho, Marco Torchiano",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Cross-Evaluation of Automated Term Extraction Tools by Measuring Terminological Saturation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-76168-8_7",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Dmitriy Naumenko, Eugene Yuschenko, Carlos Badenes-Olmedo, Vadim Ermolayev, Aliaksandr Birukou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Estimating Time Lost on Semaphores with Deep Learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-87687-6_4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-10-29",
        "authors": "Francisco García Encinas, Helena Hernández Payo, Juan F. De Paz, Marı́a N. Moreno Garcı́a, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Ontology verification testing using lexico-syntactic patterns",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2021.09.011",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-09-08",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Challenges in the Implementation of Privacy Enhancing Semantic Technologies (PESTs) Supporting GDPR",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_20",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Rana Saniei",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "TempCourt: evaluation of temporal taggers on a new corpus of court decisions",
        "implementation_urls": [],
        "doi": "10.1017/s0269888919000195",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "María Navas-Loro, Erwin Filtz, Victor Rodrı́guez-Doncel, Axel Polleres, Sabrina Kirrane",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Smart Waste Collection System with Low Consumption LoRaWAN Nodes and Route Optimization",
        "implementation_urls": [],
        "doi": "10.3390/s18051465",
        "arxiv": null,
        "abstract": "Abstract: New solutions for managing waste have emerged due to the rise of Smart Cities and theInternet of Things. These solutions can also be applied in rural environments, but they require thedeployment of a low cost and low consumption sensor network which can be used by differentapplications. Wireless technologies such as LoRa and low consumption microcontrollers, such as theSAM L21 family make the implementation and deployment of this kind of sensor network possible.This paper introduces a waste monitoring and management platform used in rural environments.A prototype of a low consumption wireless node is developed to obtain measurements of theweight, filling volume and temperature of a waste container. This monitoring allows the progressivefilling data of every town container to be gathered and analysed as well as creating alerts in caseof incidence. The platform features a module for optimising waste collection routes. This moduledynamically generates routes from data obtained through the deployed nodes to save energy, time andconsequently, costs. It also features a mobile application for the collection fleet which guides everydriver through the best route—previously calculated for each journey. This paper presents a casestudy performed in the region of Salamanca to evaluate the efficiency and the viability of the system’simplementation. Data used for this case study come from open data sources, the report of the Castillay León waste management plan and data from public tender procedures in the region of Salamanca.The results of the case study show a developed node with a great lifetime of operation, a largecoverage with small deployment of antennas in the region, and a route optimization system whichuses weight and volume measured by the node, and provides savings in cost, time and workforcecompared to a static collection route approach.Keywords: smart waste collection; low powered sensors; wireless sensor networks; data analytics;decision-making; LoRaWAN; open data; VRP; CVRP1. IntroductionNowadays, more and more cities are implementing new systems based on the Internet of Things(IoT) to obtain new data about the city, offer new services and optimize the energetic efficiency.These cities use a Smart Cities model [1] which aims to achieve more sustainable cities and makecities better places to live in. Applications developed for Smart Cities include applications for citizensecurity and control of people flow in cities [2], vehicle parking [3], getting information about accessibleplaces [4], managing energy of houses [5] and public lighting [6] together with the use of smart grids,water management, waste management, health services, logistics, and a long list of other domains.Sensors 2018, 18, 1465; doi:10.3390/s18051465 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-0493-4471https://orcid.org/0000-0002-0020-827Xhttps://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://www.mdpi.com/1424-8220/18/5/1465?type=check_update&version=1http://dx.doi.org/10.3390/s18051465http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleSmart Waste Collection System with LowConsumption LoORaWAN Nodes andRoute OptimizationAlvaro Lozano '*, Javier Caridad 1“, Juan Francisco De Paz 1,Gabriel Villarrubia Gonzalez 1“ and Javier Bajo ”1 Faculty of Science, University of Salamanca, Plaza de la Merced s/n, 37002 Salamanca, Spain;jch@usal.es (J.C.); fcofds@usal.es (J.F.D.P.); gvg@usal.es (G.V.G.)Department of Artificial Intelligence, Polytechnic University of Madrid, Campus Montegancedo s/n,",
        "publication_date": "2018-05-08",
        "authors": "Álvaro Lozano Murciego, Javier Caridad, Juan F. De Paz, Gabriel Villarrubia González, Javier Bajo",
        "file_name": "20250514091031.pdf",
        "file_path": "output/PDFs/20250514091031.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/5/1465/pdf"
    },
    {
        "title": "The RML Ontology: A Community-Driven Modular Redesign After a Decade of Experience in Mapping Heterogeneous Data to RDF",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OP-TED/ePO",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1007%978-3-031-47243-5_9.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "We publish the complete ontol-ogy at http://w3id.org/rml/, and a summary of all modules with links to all their related resources (i.e."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_9",
        "arxiv": null,
        "abstract": "Abstract. The Relational to RDF Mapping Language (R2RML)became a W3C Recommendation a decade ago. Despite its wide adop-tion, its potential applicability beyond relational databases was swiftlyexplored. As a result, several extensions and new mapping languages wereproposed to tackle the limitations that surfaced as R2RML was appliedin real-world use cases. Over the years, one of these languages, the RDFMapping Language (RML), has gathered a large community of contribu-tors, users, and compliant tools. So far, there has been no well-defined setof features for the mapping language, nor was there a consensus-markingontology. Consequently, it has become challenging for non-experts tofully comprehend and utilize the full range of the language’s capabilities.After three years of work, the W3C Community Group on KnowledgeGraph Construction proposes a new specification for RML. This paperpresents the new modular RML ontology and the accompanying SHACLshapes that complement the specification. We discuss the motivationsand challenges that emerged when extending R2RML, the methodologywe followed to design the new ontology while ensuring its backward com-patibility with R2RML, and the novel features which increase its expres-siveness. The new ontology consolidates the potential of RML, empowerspractitioners to define mapping rules for constructing RDF graphs thatwere previously unattainable, and allows developers to implement sys-tems in adherence with [R2]RML.c© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14266, pp. 152–175, 2023.https://doi.org/10.1007/978-3-031-47243-5_9http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47243-5_9&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-7195-9935http://orcid.org/0000-0002-3029-6469http://orcid.org/0000-0003-0248-0987http://orcid.org/0000-0003-4734-3847http://orcid.org/0000-0003-1702-8707http://orcid.org/0000-0001-9064-0463http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0003-2138-7972https://doi.org/10.1007/978-3-031-47243-5_9The RML Ontology: A Community-Driven Modular Redesign 153Resource type: Ontology/License: CC BY 4.0 InternationalDOI: 10.5281/zenodo.7918478/URL: http://w3id.org/rml/portal/Keywords: Declarative Language · R2RML · RML · KnowledgeGraph1 IntroductionIn 2012, the Relational to RDF Mapping Language (R2RML) [37] was releasedas a W3C Recommendation. The R2RML ontology [8] provides a vocabularyto describe how an RDF graph should be generated from data in a relationaldatabase (RDB). Although R2RML gained wide adoption, its potential applica-bility beyond RDBs quickly appeared as a salient need [49,63,76,87].Targeting the generation of RDF from heterogeneous data sources other thanRDBs, several extensions [49,76,87] preserving R2RML’s core structure wereproposed. As R2RML and the growing number of extensions were applied in a",
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, Dylan Van Assche, Julián Arenas-Guerrero, Ben De Meester, Christophe Debruyne, Samaneh Jozashoori, Pano Maria, Franck Michel, David Chaves-Fraga, Anastasia Dimou",
        "file_name": "10!1007%978-3-031-47243-5_9.pdf",
        "file_path": "output/PDFs/10!1007%978-3-031-47243-5_9.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-031-47243-5_9.pdf"
    },
    {
        "title": "FOOPS!: An Ontology Pitfall Scanner for the FAIR principles.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. This paper presents FOOPS!, a web service designed to as-sess the compliance of vocabularies or ontologies against the FAIR princi-ples. FOOPS! performs a total of 24 different checks from the four FAIRdimensions, reflecting the best practices and latest community discus-sions to adapt FAIR to semantic artefacts. The web service not onlydetect best practices according to each principle, but also offers an ex-planation of why a particular principle fails, and helpful suggestions toovercome common issues.Keywords: Ontology development · FAIR principles · FAIR semanticsPaper type: Demo (available at https://w3id.org/foops)1 IntroductionThe Findable, Accessible, Interoperable, Reusable (FAIR) data principles [6]have become increasingly relevant in the context of research data managementand reproducibility; being a main subject of discussion and adoption in commu-nity initiatives such as the Research Data Alliance, FORCE 11 and the EuropeanOpen Science Cloud. As a result, the FAIR principles have been adapted to otherresearch artifacts, such as software,1 and semantic resources such as ontologies.2In order to help researchers adopt best practices around FAIR, the scien-tific community has developed self-assessment tools and validators that helpresearchers assess the FAIRness of their resources. These are typically targeted? Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).?? The authors would like to thank Raúl Alcazar and Jacobo Mata for their help.This work has been supported by the Madrid Government under the MultiannualAgreement with Universidad Politécnica de Madrid in the line Support for R&Dprojects for Beatriz Galindo researchers, the HORIZON2020 project OntoCommons:Ontology-driven data documentation for Industry Commons (H2020-958371) and byKnowledgeSpaces: Técnicas y herramientas para la gestión de grafos de conocimien-tos para dar soporte a espacios de datos (PID2020-118274RB-I00).1 https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg2 https://www.fairsfair.eu/fair-semantics-interoperability-and-services-0Daniel Garijo, Oscar Corcho, and Maŕıa Poveda-Villalóntowards research data, such as AmIFAIR [7],3 F-UJI,4 or fair-checker,5 withsome recent additions for research software (e.g., howfairis6). However, there isno FAIR validator specifically targeted towards ontologies.In this demo we present FOOPS!, an ontology pitfall scanner for the FAIRprinciples. FOOPS! works for both OWL and SKOS vocabularies, and distin-guishes itself from existing services such as Vapour7 (focused on the quality ofthe content negotiation of resources) and OOPS! [3] (focused on common pitfallson the ontology itself); to provide a comprehensive overview of how a vocabularycomplies with current FAIR best practices for ontologies [4, 2].2 FOOPS! featuresFOOPS! is a web service and application that takes as input an OWL ontologyor SKOS thesauri and runs 24 different checks distributed across the FAIR di-mensions. These checks are based on the best practices and recommendations in[1], [4], [2], and can be summarized as follows:– Findable (9 checks): the service assesses whether the ontology URI is persis-tent, resolvable, has a resolvable version IRI, and whether that IRI is uniquefor that version. FOOPS! will also assess if minimum descriptive metadatais included (e.g., title, description, etc.) and whether the ontology prefix and",
        "publication_date": "2021-01-01",
        "authors": "Daniel Garijo, Óscar Corcho, María Poveda‐Villalón",
        "file_name": "20250514091135.pdf",
        "file_path": "output/PDFs/20250514091135.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper321.pdf"
    },
    {
        "title": "Uncovering hidden therapeutic indications through drug repurposing with graph neural networks and heterogeneous data",
        "implementation_urls": [],
        "doi": "10.1016/j.artmed.2023.102687",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-10-21",
        "authors": "Adrián Ayuso-Muñoz, Lucía Prieto Santamaría, Esther Ugarte-Carro, Emilio Serrano, Alejandro Rodríguez‐González",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Mapping the Web Ontology Language to the OpenAPI Specification",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-65847-2_11",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "T2WML: Table To Wikidata Mapping Language",
        "implementation_urls": [],
        "doi": "10.1145/3360901.3364448",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-09-23",
        "authors": "Pedro Szekely, Daniel Garijo, Divij Bhatia, Jiasheng Wu, Yixiang Yao, Jay Pujara",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Crossing the chasm between ontology engineering and application development: A survey",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2021.100655",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-06-24",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "ALSPAC Grant Acknowledgements",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514091146.pdf",
        "file_path": "output/PDFs/20250514091146.pdf",
        "pdf_link": "http://www.bristol.ac.uk/alspac/external/documents/grant-acknowledgements.pdf"
    },
    {
        "title": "Best practices for publishing, retrieving, and using spatial data on the web",
        "implementation_urls": [],
        "doi": "10.3233/sw-180305",
        "arxiv": null,
        "abstract": "Abstract. Data owners are creating an ever richer set of information resources online, and these are being used for more and moreapplications. Spatial data on the Web is becoming ubiquitous and voluminous with the rapid growth of location-based services,spatial technologies, dynamic location-based data and services published by different organizations. However, the heterogeneityand the peculiarities of spatial data, such as the use of different coordinate reference systems, make it difficult for data users,Web applications, and services to discover, interpret and use the information in the large and distributed system that is the Web.To make spatial data more effectively available, this paper summarizes the work of the joint W3C/OGC Working Group onSpatial Data on the Web that identifies 14 best practices for publishing spatial data on the Web. The paper extends that work bypresenting the identified challenges and rationale for selection of the recommended best practices, framed by the set of principlesthat guided the selection. It describes best practices that are employed to enable publishing, discovery and retrieving (querying)spatial data on the Web, and identifies some areas where a best practice has not yet emerged.Keywords: Geographic information systems, Spatial data, Web technologies, World Wide Web, W3C, Open GeospatialConsortium, OGC*Corresponding author. E-mail: l.vandenbrink@geonovum.nl.**The views expressed are purely those of the author and may notin any circumstances be regarded as stating an official position of theEuropean Commission.1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:l.vandenbrink@geonovum.nl2 L. van den Brink et al. / Best Practices for Publishing, Retrieving, and Using Spatial Data on the Web1. IntroductionSpatial data is important. Firstly, because it has be-come ubiquitous with the explosive growth in position-ing technologies attached to mobile vehicles, portabledevices, and autonomous systems. Secondly, becauseit is fundamentally useful for countless convenientconsumer services like transport planning, or for solv-ing the biggest global challenges like climate changeadaptation [1]. Historically, sourcing, managing andusing high-quality spatial data has largely been thepreserve of military, government and scientific enter-prises. These groups have long recognized the im-portance and value that can be obtained by sharingtheir own specialized data with others to achieve cross-theme interoperability, increased usability and betterspatial awareness, but they have struggled to achievethe cross-community uptake they would like. SpatialData Infrastructures (SDIs) [2], which commonly em-ploy the mature representation and access standards ofthe Open Geospatial Consortium (OGC), are now welldeveloped, but have become a part of the “deep Web”that is hidden for most Web search engines and humaninformation-seekers. Even geospatial experts still donot know where to start looking for what they need orhow to use it when they find it. The integration of spa-tial data from different sources offers possibilities toinfer and gain new information; however, spatial dataon the Web is published in various structures, formatsand with different granularities. This makes publish-ing, discovering, retrieving, and interpreting the spatialdata on the Web a challenging task. By contrast, the",
        "publication_date": "2018-08-10",
        "authors": "L. van den Brink, Payam Barnaghi, Jeremy Tandy, Ghislain Auguste Atemezing, Rob Atkinson, Byron Cochrane, Yasmin Fathy, Raúl García Castro, Armin Haller, Andreas Harth, Krzysztof Janowicz, Şefki Kolozali, Bart van Leeuwen, Maxime Lefrançois, Joshua Lieberman, Andrea Perego, Danh Le-Phuoc, Bill Roberts, Kerry Taylor, Raphaël Troncy",
        "file_name": "10!3233%sw-180305.pdf",
        "file_path": "output/PDFs/10!3233%sw-180305.pdf",
        "pdf_link": null
    },
    {
        "title": "Distributing Text Mining tasks with <i>librAIry</i>",
        "implementation_urls": [],
        "doi": "10.1145/3103010.3121040",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-08-31",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Building the Legal Knowledge Graph for Smart Compliance Services in Multilingual Europe.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract.  This position paper describes the vision, objectives and methodology of the LYNX project. The aim of Lynx is to create services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law and standards. Keywords. Compliance, Legal Knowledge Graph, regtech, regulatory compliance, semantic web Introduction The term compliance is widely used to refer “to the conformance to a set of laws, regulations, policies, or best practices” [1]. Every company performing almost any activity has some concern with compliance-related problems.  These problems are a bigger concern for small and medium-sized enterprises (SMEs), which cannot afford expensive consultancy services and they are a bigger hurdle for companies trying to sell abroad, as they usually lack the knowledge on the applicable conditions in the target country. According to the European Commission1, only 7% of European SMEs sell across borders, but those who do, exhibit 7% job growth and 26% innovate in their offering, greater numbers than the 1% and 8% of SMEs that do not go outside their local markets.  The European Commission is aware of these legal and language barriers and is trying to build a single market with less entry barriers. The LYNX project is a European research project funded as an H2020 Innovation Action covering the topic ICT-14: Big Data PPP: cross-sectorial and cross-lingual data integration and experimentation. The project is expected to last three years, starting in December 2017. This position paper briefly presents this research project. 1 http://europa.eu/rapid/press-release_IP-10-895_en.htm Proceedings of the 1st Workshop on Technologies for Regulatory Compliance151. The LYNX project Having identified these compliance problem, and having assumed that technology can help lowering these legal and language barriers, the main objective of Lynx can be stated as “to create an ecosystem of cloud services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law, standards and other aspects”. Other specific objectives follow:  to provide a platform that helps companies solving questions and cases related to compliance in different sectors and jurisdictions.  to help European SMEs to reduce costs and effort in organising and monitoring legislation, regulations, and sectorial good practices.  to provide citizens a better access to legal and regulatory information from multiple jurisdictions and to facilitate their in the legislative processes and in the writing process of standards.  to draw a legal knowledge graph across different jurisdictions, comprising legislation, case law, doctrine, standards, norms, and other documents.  to deliver the necessary domain-neutral common services (such as document annotation, interlinking, etc.) as building blocks to orchestrate aggregated business-oriented services.  to cover some particular business case necessities, validating the project’s developed ecosystem.  The referred legal knowledge graph is a collection of structured data and ",
        "publication_date": "2017-01-01",
        "authors": "Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel, Jorge Gracia",
        "file_name": "20250514091315.pdf",
        "file_path": "output/PDFs/20250514091315.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2049/02paper.pdf"
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.4126/FRL01-006444989",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "On the Efficiency of Decentralized File Storage for Personal Information Management Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/testingIPFS",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%iscc50000!2020!9219623.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/miker83z/testingIPFS"
                    }
                ]
            }
        ],
        "doi": "10.1109/iscc50000.2020.9219623",
        "arxiv": "2007.03505",
        "abstract": "Abstract—This paper presents an architecture, based on Dis-tributed Ledger Technologies (DLTs) and Decentralized File Stor-age (DFS) systems, to support the use of Personal InformationManagement Systems (PIMS). DLT and DFS are used to managedata sensed by mobile users equipped with devices with sensingcapability. DLTs guarantee the immutability, traceability andverifiability of references to personal data, that are stored inDFS. In fact, the inclusion of data digests in the DLT makes itpossible to obtain an unalterable reference and a tamper-prooflog, while remaining compliant with the regulations on personaldata, i.e. GDPR. We provide an experimental evaluation on thefeasibility of the use of DFS. Three different scenarios have beenstudied: i) a proprietary IPFS approach with a dedicated nodeinterfacing with the data producers, ii) a public IPFS service andiii) Sia Skynet. Results show that through proper configurationof the system infrastructure, it is viable to build a decentralizedPersonal Data Storage (PDS).Index Terms—Personal Information Management System, Dis-tributed Ledger Technologies, Decentralized File Storage, Sensingas a Service.I. INTRODUCTIONThe advent of social media and Web 2.0 favoured a processthat broke the boundaries between authorship and readership:users produce the data that is consumed by other users.This has increased the privacy threats of applications thatare shaped by user-generated content, as it often consists ofhighly personal data. In general, the economics of personalinformation is helped by the more pervasive nature of to-day’s digital world. This information enables organizations toprovide personalized or more useful services in digital andphysical spaces, but it could also have potentially harmfulconsequences for the privacy and autonomy of users andsociety at large. Current platform-centered data managementtechniques threaten the control that individuals exercise overtheir personal information and give to few companies thepower to necessarily rely on them to explore, filter and obtain∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieITN EJD grant agreement No 814177 Law, Science and Technology JointDoctorate - Rights of Internet of Everythingdata of interest. Not mentioning the fact that some of thesecentral entities can operate without any transparency on theuse of users’ data.An individual digital counterpart can be depicted not onlyby using his own personal information, but also that of hissocial links (e.g. friends, family, colleagues) . Thus, it becomeseasier to understand users activity choice and lifestyle patterns[1] and to make more intrusive recommendations using thisdata [2], [3]. Lack of privacy control, for instance, leads anindividual being thrown into a “filter bubble” that can affect",
        "publication_date": "2020-07-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%iscc50000!2020!9219623.pdf",
        "file_path": "output/PDFs/10!1109%iscc50000!2020!9219623.pdf",
        "pdf_link": null
    },
    {
        "title": "Preface: International Conference on Smart Materials and Structures (ICSMS-2022)",
        "implementation_urls": [],
        "doi": "10.1063/12.0017296",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Multi-object tracking in traffic environments: A systematic literature review",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2022.04.087",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-04-21",
        "authors": "Diego M. Jiménez-Bravo, Álvaro Lozano Murciego, André Sales Mendes, Héctor Sánchez San Blas, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "TEC: Transparent Emissions Calculation Toolkit",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-47243-5_5",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Milan Marković, Daniel Garijo, Stefano Germano, Iman Naja",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Combining heterogeneous data sources for spatio-temporal mobility demand forecasting",
        "implementation_urls": [],
        "doi": "10.1016/j.inffus.2022.09.028",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-10-07",
        "authors": "Ignacio-Iker Prado-Rujas, Emilio Serrano, Antonio García-Dopico, M. Luisa Córdoba, Marı́a S. Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Challenges for FAIR Digital Object Assessment",
        "implementation_urls": [],
        "doi": "10.3897/rio.8.e95943",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "publication_date": "2022-10-12",
        "authors": "Esteban González, Daniel Garijo, Óscar Corcho",
        "file_name": "10!3897%rio!8!e95943.pdf",
        "file_path": "output/PDFs/10!3897%rio!8!e95943.pdf",
        "pdf_link": null
    },
    {
        "title": "Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [],
        "doi": "10.5220/0010945300003116",
        "arxiv": null,
        "abstract": "Abstract: The advent of Distributed Ledger Technologies (DLTs) has paved the way for a new paradigm of traceabilityin all information systems areas. In the context of decision-making processes, however, DLTs are generallyused only to trace the end results. In this work we argue that a reasoning system can be put in place formaking these decisions, in order to enhance auditability, transparency, and finally to provide explainability.We propose the Intelligent Human-input-based Blockchain Oracle (IHiBO), a cross-chain oracle that enablesthe execution and traceability of formal argumentation and negotiation processes, involving the intervention ofhuman experts. We take as reference the decision-making processes of fund managements, as trust is of crucialimportance in such “trust services”. The architecture and implementation of IHiBO are based on leveragingtwo-layer DLTs, smart contracts, argumentation and negotiation in a multi-agent setup. Finally, we providesome experimental results that support our discussion, namely that in the use-case we have considered ourmethodology can increase trust from principals to trusted services.1 INTRODUCTIONIn situations where trust plays a significant role, thedecision-making process might be considered as thepinnacle of the engagement between parties. In thecase of funds management, for instance, investorschoose managers based not only on forecasts of futureperformance but also on factors such as trust and reli-ability (Kostovetsky, 2016). Indeed, in these so called“trust services” the fund managers are in the positionof a fiduciary acting on behalf of the principal, sub-ject to the overall duty to act in the best interest ofthe client, i.e. the principal. Fund managers primar-ily research and determine the best stocks, bonds, orother securities to fit the strategy of the fund, then buyand sell them. The decisions taken by managers af-fect the principals directly, thus the legislator can anddoes declare the principal’s right to check the fidu-ciary’s relevant activities in order to give some weightto this duty by its intended controlability. However,this might not be so straightforward, as these activi-aThese authors contributed equally.b https://orcid.org/0000-0002-7200-6001c https://orcid.org/0000-0002-4159-4269d https://orcid.org/0000-0002-2488-2293e https://orcid.org/0000-0001-7784-6176ties, e.g. securities transactions, are increasingly exe-cuted as a collaborative process that involves not onlya single fund manager but also other managers, ana-lysts, and external entities that maintain business re-lationships. The beliefs and assumptions of this di-verse group of participants can be influenced by a va-riety of different background knowledge and in turnshape the decision that leads to the execution of afund activity. The fund management decision processis characterized by uncertain and changing informa-tion, dynamic opportunities, multiple goals and strate-gic considerations, interdependence among projects,and multiple decision-makers and locations (Cooperet al., 1997). This necessitates a collaborative pro-",
        "publication_date": "2022-01-01",
        "authors": "Liuwen Yu, Mirko Zichichi, Réka Markovich, Amro Najjar",
        "file_name": "10!5220%0010945300003116.pdf",
        "file_path": "output/PDFs/10!5220%0010945300003116.pdf",
        "pdf_link": null
    },
    {
        "title": "A Framework Based on Distributed Ledger Technologies for Data Management and Services in Intelligent Transportation Systems",
        "implementation_urls": [],
        "doi": "10.1109/access.2020.2998012",
        "arxiv": null,
        "abstract": "ABSTRACT Data are becoming the cornerstone of many businesses and entire systems infrastructure.Intelligent Transportation Systems (ITS) are no different. The ability of intelligent vehicles and devices toacquire and share environmental measurements in the form of data is leading to the creation of smart servicesfor the benefit of individuals. In this paper, we present a system architecture to promote the developmentof ITS using distributed ledgers and related technologies. Thanks to these, it becomes possible to create,store and share data generated by users through the sensors on their devices or vehicles, while on themove. We propose an architecture based on Distributed Ledger Technologies (DLTs) to offer features suchas immutability, traceability and verifiability of data. IOTA, a promising DLT for IoT, is used togetherwith Decentralized File Storages (DFSes) to store and certify data (and their related metadata) comingfrom vehicles or by the users’ devices themselves (smartphones). Ethereum is then exploited as the smartcontract platform that coordinates the data sharing through access control mechanisms. Privacy guaranteesare provided by the usage of distributed key management systems and Zero Knowledge Proof. We provideexperimental results of a testbed based on real traces, in order to understand if DLT and DFS technologiesare ready to support complex services, such as those that pertain to ITS. Results clearly show that, whilethe viability of the proposal cannot be rejected, further work is needed on the responsiveness of DLTinfrastructures.INDEX TERMS Intelligent transportation systems, distributed ledger technologies, blockchain, smartcontracts, decentralized file storage, sensing as a service.I. INTRODUCTIONThe future of Intelligent Transportation Systems (ITS) will bebased on the ability of vehicles to sense, store and exchangebig data. Vehicles will be more and more equipped withsensors that track information about the vehicle internals,as well as information about the surrounding environment androad conditions. Moreover, ubiquitous connectivity allowsindividuals to post crowdsensed information through theirsmartphones and mobile devices. This makes them becomeThe associate editor coordinating the review of this manuscript andapproving it for publication was Yanli Xu .an active part of ITS. Such crowd-sensed information isessential for building sophisticated smart services that aimat improving traffic management, transportation efficiencyand safety, raising awareness about the environment, and thusimproving the liveability and health status of the communityof a given territory. We thus envisage that vehicles and theirusers will be able to record data, store them in some databoxes, and communicate with other vehicles or users as well.There are numerous examples of Vehicle-to-Vehicle (V2V)and Vehicle-to-Infrastructure (V2I) based applications, suchas notification services [1], [2], as well as standards forcommunication messages, e.g., ETSI Cooperative Awareness100384 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020https://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0002-3690-6651https://orcid.org/0000-0001-8218-7195M. Zichichi et al.: Framework Based on DLTs for Data Management and Services in ITSMessages (CAM) [3]. In these contexts, one of the mainissues is the unreliability of the exchanged information. Thisproblem is typically due to the physical errors of the sensors,",
        "publication_date": "2020-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%access!2020!2998012.pdf",
        "file_path": "output/PDFs/10!1109%access!2020!2998012.pdf",
        "pdf_link": null
    },
    {
        "title": "On the Decentralization of Health Systems for Data Availability: a DLT-based Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ccnc51644.2023.10059701",
        "arxiv": null,
        "abstract": "Abstract—Mobile devices entered people’s lives by leaps andbounds, offering various applications relying on private third-party entities to manage their users’ data. Centralized control ofpersonal health data endangers the privacy of the users directlyinvolved. In the future, there will likely be a trend towarddecentralizing the health data collection, relieving central entitiesof this task. This comes with several challenges in a decentralizedenvironment, such as avoiding a single point of failure to guaran-tee data availability. The following work proposes an architecturebased on Distributed Ledger Technology to allow users to decideon their data while ensuring availability by employing socialnetworks. We will outline the mechanisms behind data storageand the implications of using smart contracts in the architecture.In concluding the work, we show the developed architecture andresults deriving from its assessment, highlighting possible usecases applied to the specific health data management context.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed Storage, Social NetworksI. INTRODUCTIONPersonal digital technologies are constantly evolving and arethe primary source of information generation. They broughta fundamental transformation in people’s lives, but the datagenerated usually ends up in private use for reasons relatedto the privacy of an individual. The shift to a decentralizedparadigm now seems immediate, not only to protect theindividual but, more importantly, to enable new technologiesrevolving around data management. Storing data in centralizeddata silos makes it inaccessible to the public and discon-nected from other data [1], [2]. This mechanism hampersinnovation above all. Based on this, interest in decentralizingdata management is proliferating and with great promise toenable these conditions. The healthcare sector can benefitsignificantly from decentralizing information from centralizedsystems. This is because there are so many new implications ofdoing this, ranging from contributing personally to advancingnew medical studies in a disintermediate way and gettingnew solutions on your own directly from devices that becomeincredibly efficient and intelligent [3], [4]. Indeed, this pathis not easy, there are considerable barriers to be addressed interms of security and privacy, but it is the most suitable visionto enable the digital health field.This work has received funding from Regione Marche with DDPF n. 1189,the EU’s Horizon 2020 research and innovation programme under the MSCAITN grant agreement No 814177 LAST-JD-RIoE and from the University ofUrbino through the “Bit4Food” research project.Assuming we had such a decentralized approach available,we could enable a new way of exploiting data in the healthcarespace by storing our data ourselves. However, new issueswould be introduced, such as the data availability problem.The availability of a piece of data indicates its ability to be",
        "publication_date": "2023-01-08",
        "authors": "Gioele Bigini, Mirko Zichichi, Emanuele Lattanzi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%ccnc51644!2023!10059701.pdf",
        "file_path": "output/PDFs/10!1109%ccnc51644!2023!10059701.pdf",
        "pdf_link": null
    },
    {
        "title": "CREATING AND IMPROVING EDUCATIONAL MATERIALS: AN APPROACH BASED ON CROWDSOURCING",
        "implementation_urls": [],
        "doi": "10.21125/iceri.2018.1854",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-11-01",
        "authors": "Mari Carmen Suárez-Figueroa, Edna Ruckhaus, Óscar Corcho, Martín Molina, Emilio Serrano, Damiano Zanardini",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A scalable, secure, and semantically interoperable client for cloud-enabled Demand Response",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2022.11.004",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-11-09",
        "authors": "Andrea Cimmino, Juan Cano-Benito, Alba Fernández-Izquierdo, Christos Patsonakis, Apostolos C. Tsolakis, Raúl García‐Castro, Dimosthenis Ioannidis, Dimitrios Tzovaras",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards Decentralized Complex Queries over Distributed Ledgers: a Data Marketplace Use-case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%icccn52240!2021!9522165.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [6] J."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522165",
        "arxiv": "2104.13819",
        "abstract": "Abstract—Distributed Ledger Technologies (DLT) and Decen-tralized File Storages (DFS) are becoming increasingly used tocreate common, decentralized and trustless infrastructures whereparticipants interact and collaborate in Peer-to-Peer interactions.A prominent use case is represented by decentralized datamarketplaces, where users are consumers and providers at thesame time, and trustless interactions are required. However, datain DLTs and DFS are usually unstructured and there are noefficient mechanisms to query a certain type of data for thesearch in the market. In this paper, we propose the use of aDistributed Hash Table (DHT) as a layer on top of DLTs where,once the data are acquired and stored in the ledger, these can besearched through multiple keyword based queries, thanks to thelookup functionalities offered by the DHT. The DHT network is ahypercube overlay structure, organized for an efficient processingof multiple keyword-based queries. We provide the architectureof such solution for a decentralized data marketplace and ananalysis based on a simulation that proves the viability of theproposed approach.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Data Marketplace, Key-word SearchI. INTRODUCTIONThe transformation brought about by digital technologieshas data at its core and has had a significant impact oneconomies and societies around the world. The ability to easilyget hold of data has the potential to create a data market wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is not ofdubious or false origin is often a challenge.In order to tackle this issue, Distributed Ledger Technolo-gies (DLT) and the realm of decentralized technologies (e.g.Decentralized File Storages (DFS)), that are emerging aroundthem, come to the rescue [1], [2]. By creating a common,decentralized and trustless infrastructure, i.e. a decentralizeddata marketplace, it will be possible for data consumers andproviders to interact and collaborate in Peer-to-Peer interac-tions [3], [4]. DLTs enable peers to engage in financial transac-tions without establishing a trust relationship. Benefits oftencited of DLTs, indeed, include enabling secure transactionsThis work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.between untrusted parties through consensus mechanisms,high availability, and the ability to automate and enforceprocesses through smart contracts [5].With the management of market interactions based on theuse of DLT and decentralized technologies, what remains is",
        "publication_date": "2021-07-01",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%icccn52240!2021!9522165.pdf",
        "file_path": "output/PDFs/10!1109%icccn52240!2021!9522165.pdf",
        "pdf_link": null
    },
    {
        "title": "WhenTheFact: Extracting Events from European Legal Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia220470",
        "arxiv": null,
        "abstract": "Abstract. This paper presents WhenTheFact, a tool that identifies relevant eventsfrom European judgments. It is able to extract the structure of the document, aswell as when the event happened and who carried it out. WhenTheFact builds thena timeline that allows the user to navigate through the annotations in the document.Keywords. event extraction, visualization, NLP, legal domain, timeline generation1. IntroductionEvents and their logical sequence are key to understanding legal decisions, being thestoryline of pivotal importance. We therefore assume that a judgment can be describedas a series of time-marked happenings (events) instead of focusing on the other entities(things), and to this aim we must be able to extract these events in an automatic fashion.Before undertaking the event extraction task itself, discourse extraction is required;since legal decisions are long and complex, where the event is detected within the docu-ment is crucial regarding its relevance. Once the relevant parts of the document are de-termined, the next step involves training a system using documents annotated manuallywith relevant events, as well as the semantic resources available. Finally, the system isable to annotate different documents, allowing to visualize the relevant events in it. Ad-ditionally, in the online demo2, a timeline with these relevant events is generated, easingnavigation through the document.The paper is organized as follows. Section 2 explores previous related work in lit-erature. Section 3 introduces the system created to extract relevant events from legal de-cisions, explaining its different stages: document structure extraction, training strategiesand extraction itself. Section 4 presents the evaluation of the system. Finally, Section 5summarizes the main contributions and the future research lines to explore.2. Related workBeside generic efforts in event extraction such as the carried out by temporal taggersfollowing TimeML [1,2] or related tasks such as frame-semantic parsing [3,4], semantic1Corresponding Author: Marı́a Navas-Loro, Ontology Engineering Group, Universidad Politécnica deMadrid, Spain; E-mail: mnavas@fi.upm.es. This work was funded partially by the project Knowledge Spa-ces:  Tecnicas y herramientas  para la gestion de grafos de conocimientos para dar soporte a  espacios de datos  (Grant  PID2020-118274RB-I00,  funded  by  MCIN/AEI/10.13039/501100011033)  and  by  H2020 MSCA PROTECT (813497).2https://whenthefact.oeg.fi.upm.es/´´Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220470219https://whenthefact.oeg.fi.upm.es/role labeling (SRL) [5,6] or open information extraction3, some proposals have beenmade specifically in the legal domain. These works often involve ad hoc definitions ofevents, ignoring general event annotation schemes.In the context of legal information retrieval, events can be considered as temporallybounded objects that have entities important as participants that played a significant rolein a case. To this aim, Lagos et al. [7] propose an NLP semi automatic approach toenable the use of entity related information corresponding to the relations among the keyplayers of a case, extracted in the form of events. They are interested in the topic, theroles, the location and the time, and consider different types of events. On the other hand,",
        "publication_date": "2022-12-05",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%faia220470.pdf",
        "file_path": "output/PDFs/10!3233%faia220470.pdf",
        "pdf_link": null
    },
    {
        "title": "FAIROs: Towards FAIR Assessment in Research Objects",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-16802-4_6",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Esteban González, Alejandro Benítez, Daniel Garijo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Link maintenance for integrity in linked open data evolution: Literature survey and open challenges",
        "implementation_urls": [],
        "doi": "10.3233/sw-200398",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-09-29",
        "authors": "André Gomes Regino, Júlio Cesar dos Reis, Rodrigo Bonacin, Ahsan Morshed, Timos Sellis",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Unveiling the diversity of spatial data infrastructures in Latin America: evidence from an exploratory inquiry",
        "implementation_urls": [],
        "doi": "10.1080/15230406.2020.1772113",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-07-10",
        "authors": "Luis M. Vilches‐Blázquez, Daniela Ballari",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Fostering trust with transparency in the data economy era",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/ppop",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1145%3565011!3569061.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The templates are available at https://github.com/besteves4/ppop/tree/main/templates."
                    }
                ]
            }
        ],
        "doi": "10.1145/3565011.3569061",
        "arxiv": null,
        "abstract": "ABSTRACTWhy is it hard for online users to trust service providers when itcomes to their personal data? While users might give away theirdata when using their services, this does not mean that they nec-essarily trust these companies. Building trust in online services isparticularly relevant as digital economy policy strategies, such asthe EU Data Strategy, deposit a considerable amount of faith in thebenefits of a data-driven society. To achieve this goal, transparencyshould be considered a necessary feature, on which trust can bebuilt. According to scholarly literature, the more information pro-vided to data subjects, the less power asymmetry, caused by a lackof knowledge, between them and data controllers will exist. In thisrespect, transparency around data processing has been, and stillis, conveyed through privacy notices. But these are far from beingused as helpful tools to navigate complex data-intensive environ-ments. Technical developments, such as Solid personal datastores,provide a fertile ground for the negotiation of privacy terms be-tween the involved parties. But to do so, it is necessary to haveclear and transparent processing conditions. However, while cer-tain specifications have been developed to accommodate for therepresentation of privacy terms, there is still a lack of developedsolutions to address this problem. With this in mind, we proposethe usage of the Privacy Paradigm ODRL Profile (PPOP), whichextends ODRL and DPV to specify data processing requirementsfor personal datastores envisaged as key core elements of the dataeconomy. To demonstrate the usage of PPOP, a set of policy exam-ples will be provided, as well as a prototype implementation of agenerator of machine and human-readable PPOP policies.CCS CONCEPTS• Information systems → World Wide Web; Ontologies; • Secu-rity and privacy→Human and societal aspects of securityand privacy; Access control; Social aspects of security and privacy;Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.DE ’22, December 9, 2022, Roma, Italy© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9923-4/22/12. . . $15.00https://doi.org/10.1145/3565011.3569061Privacy protections; • Applied computing→ Law; • Social andprofessional topics → Centralization / decentralization; Cen-tralization / decentralization; Privacy policies.KEYWORDStrust, transparency, data economy, data protection, ethics, knowl-edge engineering, personal information management systemsACM Reference Format:",
        "publication_date": "2022-12-01",
        "authors": "Beatriz Esteves, Haleh Asgarinia, Andrés Chomczyk Penedo, Blessing Mutiro, David Lewis",
        "file_name": "10!1145%3565011!3569061.pdf",
        "file_path": "output/PDFs/10!1145%3565011!3569061.pdf",
        "pdf_link": null
    },
    {
        "title": "Automatic Topic Label Generation using Conversational Models",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.8018043",
                "type": "zenodo",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF",
                        "source_paragraph": ""
                    }
                ]
            }
        ],
        "doi": "10.1145/3587259.3627574",
        "arxiv": null,
        "abstract": "AbstractProbabilistic topic modelling is an unsupervised machine learning technique that, given a set ofdocuments, is capable of scanning, detecting patterns of words and phrases, and automaticallygrouping words that best characterize a topic. Many times, however, we are interested in know-ing what relates these documents beyond the most characteristic patterns or sets of words inthe set. Consequently, the generation of topic labels appeared, which sought to generate a labelthat would characterize the set of documents in a more interpretable way than having a groupof words that we, a priori, do not know the relationship they have with each other. Currently,new ways of generating these topic labels that are easily understandable automatically are stillbeing investigated.At the same time, Neural Language Models based on Neural Networks with conversationalpurpose have recently emerged, which are trained to understand and generate dialogues be-tween humans and machines. These models possess capabilities beyond the ability to engage inconversation, such as ChatGPT, which has demonstrated the ability to autonomously composeemails or write about a specific topic, for example.Conversational models present an apparent potential to not only have recreational applica-tions, but can also be useful for other tasks, as stated in Sallam’s publication \"ChatGPT Utilityin Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspec-tives and Valid Concerns\" [1], where the author analysed 60 publications that talked about thebenefits of using ChatGPT in different tasks, such as efficient analysis of datasets or code gen-eration for health care research. Given this fact, the purpose of this Final Master’s Project is tostudy the capacity that conversational models may have to automatically and unsupervisedlygenerate tags for probabilistic topics given a set of keywords representative of the topic, follow-ing a methodology which we will refer as Conversational Probabilistic Topic Labelling (CPTL).We also compare the performance of these conversational models with the performance of atask-specific language model trained to generate topic labels.iiiContents1 Introduction 12 Related work 52.1 Probabilistic topic modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52.2 Topic label generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1 Supervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1.1 Term lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92.2.1.2 Term hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . 102.2.1.3 External knowledge sources . . . . . . . . . . . . . . . . . . . . . 102.2.2 Unsupervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112.3 Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 Approach 153.1 Topic Labelling system based on Conversational Models . . . . . . . . . . . . . . 154 Evaluation 194.1 Modules selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.2 Question templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.1.3 Topic words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.1.4 QA models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.2 Modules evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274.2.1 Top words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2 Language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . 28",
        "publication_date": "2023-11-28",
        "authors": "Virginia Ramón-Ferrer, Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": "10!1145%3587259!3627574.pdf",
        "file_path": "output/PDFs/10!1145%3587259!3627574.pdf",
        "pdf_link": null
    },
    {
        "title": "“Who Should I Trust with My Data?” Ethical and Legal Challenges for Innovation in New Decentralized Data Management Technologies",
        "implementation_urls": [],
        "doi": "10.3390/info14070351",
        "arxiv": null,
        "abstract": "Abstract: News about personal data breaches or data abusive practices, such as Cambridge Analytica,has questioned the trustworthiness of certain actors in the control of personal data. Innovations inthe field of personal information management systems to address this issue have regained traction inrecent years, also coinciding with the emergence of new decentralized technologies. However, onlywith ethically and legally responsible developments will the mistakes of the past be avoided. Thiscontribution explores how current data management schemes are insufficient to adequately safeguarddata subjects, and in particular, it focuses on making these data flows transparent to provide anadequate level of accountability. To showcase this, and with the goal of enhancing transparency tofoster trust, this paper investigates solutions for standardizing machine-readable policies to expresspersonal data processing activities and their application to decentralized personal data stores as anexample of ethical, legal, and technical responsible innovation in this field.Keywords: data governance; digital age; transparency; personal data management; identity management1. IntroductionData-driven innovations are expected to deliver further economic and societal develop-ment [1]. Through the analysis, sharing, and (re-)use of data, business models and govern-ments’ processes have been transformed to benefit from those practices [2]. The emergenceof a data-driven society is being fostered by policy actions from different governments on aworldwide scale. The European Union (EU) is no exception to this, as the European Com-mission has put on its agenda the development of “A Europe fit for the Digital Age”. The Eu-ropean Commission’s strategy and related policy documents can be located at the followinglink: https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_en(accessed on 26 May 2023). Regardless of whether it is a Big Tech company based in theUnited States (US), a large data broker in the EU, or a Chinese government-controlledentity, current data practices have been questioned by different societal sectors, from in-dividuals to nongovernmental organizations (NGOs) or from academics to governments.Trust in many digital services has been compromised [3], which has left individuals askingthemselves “who should I trust with my data”.In response to this trust crisis, technology has been looked upon to provide answers.Applied to the field of (personal) data, self-sovereign identity models [4] — as improve-ments over existing Personal Information Management Systems (PIMS) — have been putunder the spotlight due to their potential, but they are also taken with “a grain of salt”, asInformation 2023, 14, 351. https://doi.org/10.3390/info14070351 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14070351https://doi.org/10.3390/info14070351https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-2344-4061https://orcid.org/0000-0002-6820-999Xhttps://orcid.org/0000-0003-0259-7560https://orcid.org/0000-0002-3503-4644https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_enhttps://doi.org/10.3390/info14070351https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14070351?type=check_update&version=1Information 2023, 14, 351 2 of 17they are not free from shortcomings [5]. Through them, users would be in direct control oftheir information and decide when, how, and who can access such information. Certain",
        "publication_date": "2023-06-21",
        "authors": "Haleh Asgarinia, Andrés Chomczyk Penedo, Beatriz Esteves, David Lewis",
        "file_name": "20250514091829.pdf",
        "file_path": "output/PDFs/20250514091829.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/14/7/351/pdf?version=1687339409"
    },
    {
        "title": "Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature",
        "implementation_urls": [
            {
                "identifier": "https://github.com/drugs4covid/knowledge-acquisition",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1016%j!jbi!2023!104382.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The source code of our pre-processing pipeline is available online.14 5."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.jbi.2023.104382",
        "arxiv": null,
        "abstract": "to facilitate the use of the KG by end user. This interface abstractsthe complexity of formal query languages to simplify the retrieval ofinformation from the generated KG.https://drugs4covid.oeg.fi.upm.es/rdfhttps://drugs4covid.oeg.fi.upm.es/sparqlhttps://drugs4covid.oeg.fi.upm.es/services/bio-qaJournal of Biomedical Informatics 142 (2023) 104382C. Badenes-Olmedo and O. CorchoFig. 1. Workflow proposed for the creation of KGs from scientific publications.The following sections detail each of the steps in the context ofthe Drugs4COVID initiative: how have they been addressed, whatchallenges have arisen and what are our recommendations to build aknowledge graph from biomedical literature on coronavirus.3. Harvesting research literature on coronavirus (step 1)In this first stage, the initial objective is to identify the most appro-priate sources of information and validate the availability of their data.Once they have been discovered, the data is downloaded and furtherdownloads are planned, depending on the periodicity of the changes.We performed a systematic literature review to identify the datasetsused when creating coronavirus knowledge graphs [13]. The digitalrepositories that were explored were Pubmed, BioRxiv, MedRxiv andScopus. Preprint databases were also considered to explore works pend-ing of publication. The searches were conducted by searching terms intitle or abstract. Specifically, the inclusion criteria can be summarizedas follows:• IC1: Text in English or Spanish.• IC2: The words ‘‘covid’’/’’coronavirus’’/’’SARS-Cov’’ (regardlessof upper or lower case) appears in either title or abstract.• IC3: Text published between 2019 and 2022.• IC4: The words ‘‘Knowledge Graph’’ (regardless of upper or lowercase, or abbreviations) appear in either title or abstract.• IC5: Text in the biomedical/computational biology/computerscience fields.Metadata of the documents returned in the search, such as title, doi,authors, venue, etc, was downloaded as csv files from the repositories.A total of 218 documents were retrieved. Then, they were scannedfor duplicates by looking at duplicates either in the title or in theDOI through an R script. 13 duplicated articles were removed so 205documents were left for further studying. Manual filtering was thencarried out by removing duplicates and by a manual reviewing of thetitles and abstracts in order to discard articles out of the scope of thisreview. The availability of the datasets mentioned in the articles wasalso checked. By following these criteria, from the 205 documents, only14 texts were selected and subject of a narrower reading. Althoughselected, the texts could still be out of the scope of this review dueto low information provided about the dataset used.The sources for input covid publication data were CORD-19 cor-pus (50%), Pubmed (14%), LitCovid (14%), Targeting2019-nCoV (byGHDDI) (7%), University of Luxembourg Covid corpus (7%) and Eu-ropePMC (7%). Moreover, additional unstructured data sources that donot contain covid-19 publications were also considered, these include",
        "publication_date": "2023-05-06",
        "authors": "Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": "10!1016%j!jbi!2023!104382.pdf",
        "file_path": "output/PDFs/10!1016%j!jbi!2023!104382.pdf",
        "pdf_link": null
    },
    {
        "title": "Morph-Skyline: Virtual Ontology-Based Data Access for Skyline Queries",
        "implementation_urls": [],
        "doi": "10.1109/wiiat50758.2020.00043",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-12-01",
        "authors": "Marlene Goncalves, David Chaves-Fraga, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Phenotypes of non-alcoholic fatty liver disease (NAFLD) and all-cause mortality: unsupervised machine learning analysis of NHANES III",
        "implementation_urls": [],
        "doi": "10.1136/bmjopen-2022-067203",
        "arxiv": null,
        "abstract": "ABSTRACTObjectives  Non-alcoholic fatty liver disease (NAFLD) is a non-communicable disease with a rising prevalence worldwide and with large burden for patients and health systems. To date, the presence of unique phenotypes in patients with NAFLD has not been studied, and their identification could inform precision medicine and public health with pragmatic implications in personalised management and care for patients with NAFLD.Design  Cross-sectional and prospective (up to 31 December 2019) analysis of National Health and Nutrition Examination Survey III (1988–1994).Primary and secondary outcomes measures  NAFLD diagnosis was based on liver ultrasound. The following predictors informed an unsupervised machine learning algorithm (k-means): body mass index, waist circumference, systolic blood pressure (SBP), plasma glucose, total cholesterol, triglycerides, liver enzymes alanine aminotransferase, aspartate aminotransferase and gamma glutamyl transferase. We summarised (means) and compared the predictors across clusters. We used Cox proportional hazard models to quantify the all-cause mortality risk associated with each cluster.Results  1652 patients with NAFLD (mean age 47.2 years and 51.5% women) were grouped into 3 clusters: anthro-SBP-glucose (6.36%; highest levels of anthropometrics, SBP and glucose), lipid-liver (10.35%; highest levels of lipid and liver enzymes) and average (83.29%; predictors at average levels). Compared with the average phenotype, the anthro-SBP-glucose phenotype had higher all-cause mortality risk (aHR=2.88; 95% CI: 2.26 to 3.67); the lipid-liver phenotype was not associated with higher all-cause mortality risk (aHR=1.11; 95% CI: 0.86 to 1.42).Conclusions  There is heterogeneity in patients with NAFLD, whom can be divided into three phenotypes with different mortality risk. These phenotypes could guide specific interventions and management plans, thus advancing precision medicine and public health for patients with NAFLD.INTRODUCTIONThe epidemiology of non-communicable diseases (NCDs) is largely driven by cardiometabolic risk factors and diseases, namely dyslipidaemias, type 2 diabetes mellitus (T2DM), hypertension and cardio-vascular diseases. Nonetheless, there are other NCDs rapidly growing along with, and as a consequence of,1 2 the afore-mentioned cardiometabolic conditions. Non-alcoholic fatty liver disease (NAFLD) is an outstanding ",
        "publication_date": "2022-11-01",
        "authors": "Rodrigo M. Carrillo‐Larco, Wilmer Cristobal Guzman‐Vilca, Manuel Castillo-Cara, Claudia Alvizuri-Gómez, Saleh A. Alqahtani, Vanessa García-Larsen",
        "file_name": "20250514091917.pdf",
        "file_path": "output/PDFs/20250514091917.pdf",
        "pdf_link": "https://bmjopen.bmj.com/content/bmjopen/12/11/e067203.full.pdf"
    },
    {
        "title": "Bilingual Dataset for Information Retrieval and Question Answering over the Spanish Workers Statute",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.4256718",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-11-07",
        "authors": "Pablo Calleja Ibáñez, Patricia Martín-Chozas, Elena Montiel-Ponsoda, Víctor Rodríguez-Doncel, Elsa Gómez, Pascual Boil",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Packaging research artefacts with RO-Crate",
        "implementation_urls": [],
        "doi": "10.3233/ds-210053",
        "arxiv": "2108.06503",
        "abstract": "Abstract This report describes the initial phase of an experimental project to increase Web visibility of the Neil Armstrong Commemorative Archive, a digital collection of archival materials concerning astronaut Neil Armstrong’s tenure at the University of Cincinnati. The project description includes explanation of the mapping process from Qualified Dublin Core to BIBFRAME as well as data reconciliation and linking to external authorities such as id.loc.gov, VIAF, and Wikipedia. Next steps in the project, such as integrating related MARC datasets from local library catalogs, are also discussed. Keywords: linked data, BIBFRAME, Dublin Core, metadata, digital collections  1. Introduction Neil Armstrong, celebrated astronaut and the first person to walk on the moon, was also a professor of aerospace engineering at the University of Cincinnati (UC). In October 2013, the UC Libraries' Digital Collections and Repositories Department published the Neil Armstrong Commemorative Archive, a digital collection of unique archival materials concerning Armstrong's tenure at UC.1 The collection contained two hundred and eighteen items, including letters, photographs, artifacts, and ephemera. Although the collection was extensively described using established information standards such as the Qualified Dublin Core (DC) metadata standard and Library of Congress Name and Subject Authority Headings (LCNAF and LCSH, respectively), its discoverability outside of library catalogs and repositories was limited by the structured metadata schemas that those systems required. In order to capitalize on the power of linked open data to improve the collection's visibility on the Web, an experimental project was undertaken by UC library faculty to map the original DC metadata to the Bibliographic Framework (BIBFRAME) data model, reconcile and link the data to external authorities using the OpenRefine application, and publish the data as expressed in the Resource Description Framework (RDF). This report will describe the initial phase of the project, including explanation of the mapping process from Qualified Dublin Core to BIBFRAME as well as data reconciliation and linking to external authorities such as id.loc.gov, the Virtual International Authority File (VIAF), and Wikipedia. In addition, next steps in the project, such as integrating related MARC datasets from local library catalogs, will be discussed. 2. Methodology: Metadata for Discovery Although data is often considered the unbiased product of research, the environment in which it is created and stored impacts its content, structure, and meaning. In this project, the original dataset consisted of Qualified DC records created for UC’s DSpace repository, the Digital Resource Commons (DRC).2 For purposes of this project, the original records were                                                         1 https://drc.libraries.uc.edu/handle/2374.UC/713357  2 https://drc.libraries.uc.edu/  181This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and cite the source. https://doi.org/10.23106/dcmi.952137143Proc. Int’l Conf. on Dublin Core and Metadata Applications 2015  conceptualized as both metadata (abstract representations of digital objects) and data (a set of elements and values generated during the cataloging process). Viewing the metadata in the context of the larger dataset impacted the mapping approach from DC to BIBFRAME; specifically, a lossless migration between standards was not sought. Instead of focusing on comprehensive or archival mapping that preserved the authenticity and content of the original data in a one-to-one mapping, a flexible approach was taken in which a core set of properties (see Table 1) needed for discovery were identified and mapped.  ",
        "publication_date": "2022-01-04",
        "authors": "Stian Soiland‐Reyes, Peter Sefton, Mercè Crosas, Leyla Jael Castro, Frederik Coppens, José M. Fernández, Daniel Garijo, Björn Grüning, M. Rosa, Simone Leo, Eoghan Ó Carragáin, Marc Portier, A. Trisovic, RO-Crate Community, Paul Groth, Carole Goble",
        "file_name": "10!3233%ds-210053.pdf",
        "file_path": "output/PDFs/10!3233%ds-210053.pdf",
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.26342/2023-71-15",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Astrea: Automatic Generation of SHACL Shapes from Ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Astrea",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1007%978-3-030-49461-2_29.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Furthermore, the code of the tool is also available in GitHub6 under the Apache 2.0 licence7."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.3571009",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF",
                        "source_paragraph": ""
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-49461-2_29",
        "arxiv": null,
        "abstract": "Abstract. Knowledge Graphs (KGs) that publish RDF data modelledusing ontologies in a wide range of domains have populated the Web. TheSHACL language is a W3C recommendation that has been endowed toencode a set of either value or model data restrictions that aim at validat-ing KG data, ensuring data quality. Developing shapes is a complex andtime consuming task that is not feasible to achieve manually. This arti-cle presents two resources that aim at generating automatically SHACLshapes for a set of ontologies: (1) Astrea-KG, a KG that publishes a setof mappings that encode the equivalent conceptual restrictions amongontology constraint patterns and SHACL constraint patterns, and (2)Astrea, a tool that automatically generates SHACL shapes from a setof ontologies by executing the mappings from the Astrea-KG. These tworesources are openly available at Zenodo, GitHub, and a web application.In contrast to other proposals, these resources cover a large number ofSHACL restrictions producing both value and model data restrictions,whereas other proposals consider only a limited number of restrictionsor focus only on value or model restrictions.Keywords: SHACL shapes · RDF validation · OntologyResource type: Dataset & SoftwareAstrea-KG: http://astrea.helio.linkeddata.es/Astrea-KG DOI: https://doi.org/10.5281/zenodo.3571009Astrea application: http://astrea.linkeddata.es/1 IntroductionKnowledge Graphs (KGs) are becoming pervasive on the Web [5]. Since 2014there is a growing number of KGs from different domains that publish a quitelarge amount of data using RDF and modelled with ontologies [19]. As a result,in the last decade a considerable effort has been put in developing ontologiesfor specific domains [21]. Due to the growth of these public available KGs, theW3C has promoted a recommendation called SHACL (Shapes Constraint Lan-guage) to validate the RDF graphs [2]. In the last years KGs validation by meansof SHACL shapes has gained momentum and has become a relevant researchtopic [14].c© Springer Nature Switzerland AG 2020A. Harth et al. (Eds.): ESWC 2020, LNCS 12123, pp. 497–513, 2020.https://doi.org/10.1007/978-3-030-49461-2_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-49461-2_29&domain=pdfhttp://orcid.org/0000-0002-1823-4484http://orcid.org/0000-0003-2011-3654http://orcid.org/0000-0002-0421-452Xhttp://astrea.helio.linkeddata.es/https://doi.org/10.5281/zenodo.3571009http://astrea.linkeddata.es/https://doi.org/10.1007/978-3-030-49461-2_29498 A. Cimmino et al.A shape defines a set of restrictions that data from a KG must fulfil. There aretwo kinds of restrictions [15], those that refer to the data model, e.g., cardinality,and those that apply to the data values, e.g., string patterns. Due to this reasondeveloping shapes has become the cornerstone solution to validate KG data.Nevertheless, developing data shapes is a complex task due to the potentialsize of the data and all the available restrictions that require a deep domain",
        "publication_date": "2020-01-01",
        "authors": "Andrea Cimmino, Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "10!1007%978-3-030-49461-2_29.pdf",
        "file_path": "output/PDFs/10!1007%978-3-030-49461-2_29.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-49461-2_29.pdf"
    },
    {
        "title": "Event Extraction and Semantic Representation from Spanish Workers’ Statute Using Large Language Models",
        "implementation_urls": [],
        "doi": "10.3233/faia230983",
        "arxiv": null,
        "abstract": "Abstract. This work uses Large Language Models to process an important piece ofSpanish legislation: the Workers’ Statute. The proposed method extracts the rele-vant events in its articles using a GPT-3.5 model and represents the entities involvedin the events and the relationships between them as RDF triples. The experimentscarried out to select a high-performance strategy include both zero- and few-shotlearning tests. Finally, this work proposes a strategy to uplift the extracted legalrelations into a legal knowledge graph.Keywords. Spanish Workers’ Statute, Large Language Models, Knowledge Graph,Legal Domain, Event Extraction1. IntroductionThe legal domain is a complex and dynamic field that involves interpreting and applyinglaws and regulations. Legal data (court cases, legislations, contracts, etc.) is becominga valuable source to push forward intelligent legal tools [1]. This work proposes an ap-proach using event extraction and semantic graph modeling to bring these systems closerto the public. The event extraction task is being tackled in the state-of-the-art using deeplearning models. However, it presents numerous challenges, especially for Spanish texts,including ambiguity, polysemy, and domain-specific terminology [2].The recent development of Large Language Models (LLMs) [3, 4] has proven tobe an excellent approach to mitigate these problems and an important tool to deal withlimited data [5, 6] through natural language instructions, called prompts.This research aims to improve the performance of the event extraction task withinthe legal domain and to link the information into a semantic graph representation. Thedata to be used will be the Spanish Workers’ Statute, given its importance for legislatorsand the general public, and the availability of an annotated corpus of 133 sentences fromthe Statute gathered by Revenko and Martı́n-Chozas [7]. The low amount of tagged datawill be tackled using the GPT-3.5 model, as it has been proven the high performanceon Natural Language Processing (NLP) tasks like event extraction [3, 5]. Finally, theextracted events will be represented in a knowledge graph.Legal Knowledge and Information SystemsG. Sileno et al. (Eds.)© 2023 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA2309833292. Related WorkThis work defines an event as a textual region likely to compact relevant legal informationencapsulated by the articles on the law. The most common event structure is formed by anevent mention, an event trigger, an event argument, and an argument role. The argumentrole is the relationship between an argument and the event it participates in. The basicargument roles are subject, object, and complement. To classify the legal relations, manyworks [7, 8] use the Hohfeldian classes Right, Duty, No-Right, and Privilege [9].State-of-the-art event extraction relies nowadays on deep learning models like graphneural networks (GAN) [10] and attention mechanisms [11]. However, these models relyon huge amounts of labeled data to improve the model’s performance and are mainly usedfor English corpora. This research uses only 133 annotated sentences from the SpanishWorkers’ Statute [7], which are insufficient to achieve high-performance models.To tackle this issue, common approaches use data augmentation techniques [5],transfer learning [7], and active learning [12]. In recent years, language models (LM)have also been used for this and other NLP tasks [6, 3]. In 2021, the work [13] presented",
        "publication_date": "2023-12-07",
        "authors": "Gabriela Argüelles Terrón, Patricia Martín Chozas, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%faia230983.pdf",
        "file_path": "output/PDFs/10!3233%faia230983.pdf",
        "pdf_link": null
    },
    {
        "title": "Inspect4py",
        "implementation_urls": [],
        "doi": "10.1145/3524842.3528497",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-05-23",
        "authors": "Rosa Filgueira, Daniel Garijo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Human-Friendly RDF Graph Construction: Which One Do You Chose?",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-34444-2_19",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Ioannis Dasoulas, Anastasia Dimou",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "TINTO: Converting Tidy Data into image for classification with 2-Dimensional Convolutional Neural Networks",
        "implementation_urls": [],
        "doi": "10.1016/j.softx.2023.101391",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-05-01",
        "authors": "Manuel Castillo-Cara, Reewos Talla-Chumpitaz, Raúl García‐Castro, Luis Orozco‐Barbosa",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Comparison of Knowledge Graph Representations for Consumer Scenarios",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk-browser",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1007%978-3-031-47240-4_15.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "All resources are accessible online for the participants.3,4 2 https://github.com/usc-isi-i2/kgtk-browser/."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.7443836",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF",
                        "source_paragraph": ""
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47240-4_15",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have been widely adopted across organi-zations and research domains, fueling applications that span interactivebrowsing to large-scale analysis and data science. One design decisionin knowledge graph deployment is choosing a representation that opti-mally supports the application’s consumers. Currently, however, there isno consensus on which representations best support each consumer sce-nario. In this work, we analyze the fitness of popular knowledge graphrepresentations for three consumer scenarios: knowledge exploration, sys-tematic querying, and graph completion. We compare the accessibilityfor knowledge exploration through a user study with dedicated brows-ing interfaces and query endpoints. We assess systematic querying withSPARQL in terms of time and query complexity on both synthetic andreal-world datasets. We measure the impact of various representations onthe popular graph completion task by training graph embedding modelsper representation. We experiment with four representations: StandardReification, N-Ary Relationships, Wikidata qualifiers, and RDF-star. Wefind that Qualifiers and RDF-star are better suited to support use casesof knowledge exploration and systematic querying, while Standard Reifi-cation models perform most consistently for embedding model inferencetasks but may become cumbersome for users. With this study, we aimto provide novel insights into the relevance of the representation choiceand its impact on common knowledge graph consumption scenarios.Keywords: Knowledge Graphs · Knowledge Representation · UserStudy · Graph Completion1 IntroductionThe growth of the knowledge graph (KG) user base has triggered the emergenceof new representational requirements. While RDF is the traditional and standardmodel for KG representation, alternative models such as property graphs [25], theWikidata model [34], and RDF-star [12] have also become recently popular. Thec© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14265, pp. 271–289, 2023.https://doi.org/10.1007/978-3-031-47240-4 15http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47240-4_15&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-6606-9735http://orcid.org/0000-0002-1735-0686http://orcid.org/0000-0001-6921-1744http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-031-47240-4_15272 A. Iglesias-Molina et al.promise of these alternative and complementary representation models is thatthey can provide more flexibility to address certain use cases, such as statementannotation, for which RDF-based representations are not straightforward [17].While the plurality of knowledge representation (KR) models provides the meansto address a wider range of possibilities in consumer scenarios, there is currentlyno consensus nor sufficient empirical evidence on which representations are mostsuitable for different KG consumer tasks [16].Previous studies comparing knowledge representations have focused primar-ily on query performance [2,6,14,26,28] and graph interoperability [3,4]. Forthis scenario, the representations need to ensure efficiency to minimize perfor-",
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, Kian Ahrabian, Filip Ilievski, Jay Pujara, Óscar Corcho",
        "file_name": "10!1007%978-3-031-47240-4_15.pdf",
        "file_path": "output/PDFs/10!1007%978-3-031-47240-4_15.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-031-47240-4_15.pdf"
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "arxiv": null,
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activitieswhen browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead tounexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniquesthat uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extendingthose queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations onboth scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document Similarity, Information Search and Retrieval, Clustering, Topic Models, Hashing1. IntroductionHuge amounts of documents are publicly availableon the Web offering the possibility of extracting knowl-edge from them (e.g. scientific papers in digital jour-nals). Document similarity comparisons in many in-formation retrieval (IR) and natural language process-ing (NLP) areas are too costly to be performed in suchhuge collections of data and require more efficient ap-*Corresponding author. E-mail: cbadenes@fi.upm.es.proaches than having to calculate all pairwise similari-ties.In this paper we address the problem of programmat-ically generating annotations for each of the items in-side big collections of textual documents, in a way thatis computationally affordable and enables a semantic-aware exploration of the knowledge inside it that state-of-the-art methods relying on topic models are not ableto materialize.Most text mining algorithms represent documentsin a common feature space that abstracts the specific1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reserved2 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithmsequence of words used in each document and, withappropriate representations, facilitate the analysis ofrelationships between documents even when writtenusing different vocabularies. Although a sparse wordor n-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. LatentSemantic Indexing (LSI) [17], Probabilistic Latent Se-mantic Indexing (PLSI) [25] and more recently, LatentDirichlet Allocation (LDA) [11], which is the simplestprobabilistic topic model (PTM) [10], are algorithmsfocused on reducing feature space by annotating docu-ments with thematic information. PLSI and PTM alsoallow a better understanding of the corpus through thetopics discovered, since they use probability distribu-tions over the complete vocabulary to describe them.",
        "publication_date": "2020-05-01",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": "output/PDFs/10!3233%sw-200373.pdf",
        "pdf_link": null
    },
    {
        "title": "TimeLex: A Suite of Tools for Processing Temporal Information in Legal Texts",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_18",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Automatic Music Generation by Deep Learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-94649-8_34",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-07-04",
        "authors": "Juan C. Yelmo, Emilio Serrano",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Mapeathor: Simplifying the Specification of Declarative Rules for Knowledge Graph Construction.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. In recent years we have observed an increasing interest bythe scientific community, from social sciences to biomedicine, in the gen-eration and publication of RDF-based knowledge graphs. One possibilityfor creating knowledge graphs consists in using declarative mappings to-gether with their associated parsers. These mappings describe the rela-tionship between the source data and a reference ontology. However, thelearning curve to create these mapping files is steep, hindering its use bya wider community. In this paper we present a user-friendly mapping-language-independent tool, Mapeathor, to declare transformation rulesbased on spreadsheets and translate them into two different mappinglanguages with the purpose of easing the mappings creation process.Keywords: Knowledge Graph · Declarative mapping · Spreadsheet1 IntroductionIn the last few decades, we have seen a significant increase in the publicationof data in a machine understandable manner following Linked Data principles1(e.g., DBpedia2, Wikidata3). Knowledge Graph construction requires integratingdifferent data sources in a structured way, usually following the schema of anontology or group of ontologies. This facilitates the posterior task of mining theknowledge graph with several applications, such as searching recommendationsand learning implicit data patterns.Knowledge graphs can be built in diverse ways. One option is creating ad-hocscripts to transform data, which requires the user to repeat the process of scriptCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).1 https://5stardata.info/en/2 https://wiki.dbpedia.org/3 https://www.wikidata.org/A. Iglesias-Molina et al.writing in every specific use case. Another option is using tools like OpenRe-fine4 to perform data transformation through the creation of an RDF skeleton,which includes proprietary transformation rules and a functionality for knowl-edge graph construction. Lastly, there is an option to keep the transformationrules in specific files that can be later processed by engines that either transformthe data to RDF or create a virtual knowledge graph that can be queried with-out transforming the source data. These rules can be written in a wide variety oflanguages (e.g., R2RML [2], RML [3]) that cover different user’s needs (e.g., thesource data format or the engine that will be used). Although the use of thesemapping files is more flexible and independent, since they can be processed bya wide variety of engines, their creation is still not easy for new users. Expertsare usually needed to carry out these tasks, hindering the use of semantic webtechnologies across the scientific community. That is why it is necessary to lowerthe learning curve and improve mapping reuse and reproducibility.Since mapping languages started to be used by the community, there havebeen multiple approaches for the development of editors to ease their specifica-tion. Most of them enable editing through graphical visualization [4, 6], othersprovide a writing environment (e.g. the Protégé extension OntopPro). Theseeditors are language-oriented, they help to create one kind of mapping, not tak-ing into account the wide variety of mapping languages that currently exist.Moreover, when managing a considerable amount of mapping rules, a graphicalapproach may not be easily handled.",
        "publication_date": "2020-01-01",
        "authors": "Ana Iglesias-Molina, Luis Pozo-Gilo, Daniel Doña, Edna Ruckhaus, David Chaves-Fraga, Óscar Corcho",
        "file_name": "20250514092129.pdf",
        "file_path": "output/PDFs/20250514092129.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2721/paper488.pdf"
    },
    {
        "title": "A comprehensive quality model for Linked Data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170267",
        "arxiv": null,
        "abstract": "classes and properties in daQ are defined as abstract and, therefore, they are not directly used. Instead, the intended use of this ontology implies the creation of specific classes and properties defined as subclasses and sub-properties of those defined in daQ. This means that, unlike in Q M O and E V A L , where the elements such as measures and characteristics are defined as in­stances, when using daQ these elements are mainly de­fined as classes. In the context of the conceptual model, daQ is equivalent to Q M O , with the difference of the usage of different terminology. The Data Quality Vocabulary ̂ ( D Q V ) is an ontol­ogy for representing the quality of dataseis that is be­ing developed by the W 3 C . Similarly as daQ, and un­like Q M O and E V A L , D Q V is an ontology specifically developed having in mind dataseis. Currently, D Q V provides classes and properties for capturing informa­tion about quality categories, dimensions and metrics of a dataset, as well as about quality certificates, stan­dards and provenance related to a dataset. However, at this point in time D Q V is still under development and changes to the current design can be expected in the future. In the context of the conceptual model, D Q V tends to provide the means for capturing both the de­tails about quality (i.e., characteristics and measures) and about quality values (results of evaluation). Fur­thermore, although D Q V is specifically designed for dataseis, it does not provide the means to describe some specific aspects of Linked Data. With respect to the state of the art described in this section, this paper contributes with the extension of ex­isting ontologies in order to enable capturing informa­tion related to Linked Data which is not covered by the existing ontologies, as well as with bringing existing ontologies under unique umbrella by connecting their semantically related concepts. 3. Quality model for Linked Data This section describes a quality model for Linked Data and how it was defined using the bottom-up methodology proposed by Radulovic et al. [11]. The starting point for the definition of the quality model was the state of the art in Linked Data quality assess-'http://purl.org/net/EvaluatlonResult# http://www.w3 . org/IR/vocab-dgv/ http://purl.org/net/EvaluatlonResult%23http://www.w3ment and specification, and in particular the work done by Zaveri et al. [16]. Since the quality model presented in this section describes a classification of quality mea­sures (i.e., base measures, derived measures and indi­cators), w e have decided to adopt the terminology as ",
        "publication_date": "2017-01-31",
        "authors": "Filip Radulović, Nandana Mihindukulasooriya, Raúl García‐Castro, Asunción Gómez‐Pérez",
        "file_name": "10!3233%sw-170267.pdf",
        "file_path": "output/PDFs/10!3233%sw-170267.pdf",
        "pdf_link": null
    },
    {
        "title": "A study of the quality of Wikidata",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2021.100679",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-12-05",
        "authors": "Kartik Shenoy, Filip Ilievski, Daniel Garijo, Daniel Schwabe, Pedro Szekely",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An Abstract Framework for Non-Cooperative Multi-Agent Planning",
        "implementation_urls": [],
        "doi": "10.3390/app9235180",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-11-29",
        "authors": "Jaume Jordán, Javier Bajo, Vicente Botti, Vicente Julián",
        "file_name": "20250514092151.pdf",
        "file_path": "output/PDFs/20250514092151.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/9/23/5180/pdf?version=1575008421"
    },
    {
        "title": "Simulation of Hybrid Edge Computing Architectures",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt52167.2021.9576121",
        "arxiv": "2108.12592",
        "abstract": "Abstract—Dealing with a growing amount of data is a crucialchallenge for the future of information and communicationtechnologies. More and more devices are expected to transfer datathrough the Internet, therefore new solutions have to be designedin order to guarantee low latency and efficient traffic manage-ment. In this paper, we propose a solution that combines theedge computing paradigm with a decentralized communicationapproach based on Peer-to-Peer (P2P). According to the proposedscheme, participants to the system are employed to relay messagesof other devices, so as to reach a destination (usually a serverat the edge of the network) even in absence of an Internetconnection. This approach can be useful in dynamic and crowdedenvironments, allowing the system to outsource part of the trafficmanagement from the Cloud servers to end-devices. To evaluateour proposal, we carry out some experiments with the help ofLUNES, an open source discrete events simulator specificallydesigned for distributed environments. In our simulations, wetested several system configurations in order to understand theimpact of the algorithms involved in the data dissemination andsome possible network arrangements.Index Terms—simulation, edge computing, peer-to-peer, com-munication, performance evaluationI. INTRODUCTIONWe are living in an era in which digital services areconstantly transformed and revised. All the tools that peopleuse are now digital, producing some kind of data that is notnecessarily stored in a local storage, but that often needs to beuploaded to distributed systems, through some communicationmeans. With the rise of the Internet of Things (IoT), anincreasingly higher number of devices is expected to join theInternet in a near future, interacting with some form of Cloudor decentralized platforms [1]. In order to manage the growingamount of traffic different novel technological solutions arebeing proposed. Among them, 5G stands out, which is capableof offering Internet access to a significantly higher number ofmobile devices with an improved efficiency [2].In this context, smart cities and smart shires [3] are supposedto emerge, with the employment of hybrid physical-digital andintelligent infrastructures that use data-driven technologies toadapt to changes in the physical environment [4]. However,the growth of data exchanges between devices needs to bemanaged not only from a network infrastructure point of view,but also from the perspective of Cloud platforms, in order toavoid an overload of requests to the servers and the resultingincreased latencies or service unavailability [5].Edge computing thus emerges as a paradigm for improvingthe efficiency of the content delivery, by decentralizing themanagement of the system and bringing computation and datastorage in locations geographically closer to the users. Withan edge computing approach, most of the activities usually",
        "publication_date": "2021-09-27",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!1109%ds-rt52167!2021!9576121.pdf",
        "file_path": "output/PDFs/10!1109%ds-rt52167!2021!9576121.pdf",
        "pdf_link": null
    },
    {
        "title": "Accessibility and Personalization in OpenCourseWare : An Inclusive Development Approach",
        "implementation_urls": [],
        "doi": "10.1109/icalt49669.2020.00091",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-07-01",
        "authors": "Mirette Elias, Edna Ruckhaus, E.A. Draffan, Abi James, Mari Carmen Suárez-Figueroa, Steffen Lohmann, Abderrahmane Khiat, Sören Auer",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "PROFILES &amp; DATA",
        "implementation_urls": [],
        "doi": "10.1145/3184558.3192316",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Laura Koesten, Elena Demidova, Vadim Savenkov, John G. Breslin, Óscar Corcho, Stefan Dietze, Elena Simperl",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Simulation of the Internet Computer Protocol: the Next Generation Multi-Blockchain Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt55542.2022.9932122",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-09-26",
        "authors": "Luca Serena, AoXuan Li, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti, Su-Kit Tang",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.1109/ICSC50631.2021.00089",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "WDPlus: Leveraging Wikidata to Link and Extend Tabular Data (short paper).",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "ABSTRACTScientific observations and other open data are usually made avail-able online in a tabular manner as CSVs and spreadsheets. However,users of these data face three main challenges when attempting touse these products: finding which datasets are related to a topic ofinterest; determining which existing information can be used toextend a given dataset; and how to share their integrated datasetresults with the rest of the community. In this paper we presentWDPlus, a framework designed to address these challenges byleveraging Wikidata. WDPlus allows searching for heterogeneousdatasets, facilitates completing tabular data usingWikidata and pro-poses a mechanism to extend Wikidata in a decentralized manner.KEYWORDSKnowledge Graphs, Entity Linking, Wikidata, RDF1 INTRODUCTIONToday, data about any domain can be found on the web in datarepositories, web APIs and millions of spreadsheets and CSV files.These data comes in a myriad of formats, layouts, terminology andcleanliness that make them difficult to integrate together.Users of these data face three main challenges. The first one isfinding datasets related to a feature or topic of interest. For example,climate scientists often look for years of observational data fromauthoritative sources when estimating the climate of a region. Thesecond challenge is how to complete a given dataset with existingknowledge: machine learning applications are data hungry andrequire asmany data points and features as possible to improve theirpredictions, which often requires integrating data from differentsources. The final challenge is sharing integrated results: onceseveral datasets have been merged together, how to make themavailable to the rest of the community?Knowledge graphs have become the preferred technology to ad-dress these challenges. Large organizations, including search engineproviders, shopping giants and finance institutions are investing inlarge knowledge graphs to integrate and retrieve heterogeneousdata. However, data integration pipelines are usually created man-ually, require significant expertise, and are seldom available to thegeneral public. Similarly, linking to existing datasets in the theLinked Open Data Cloud1usually requires the expertise of a knowl-edge engineer to properly identify the appropriate target instancesto link to in other datasets.1https://lod-cloud.net/Copyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).Recent initiatives such as Data.world,2Google data search [2]and DataCommons",
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Pedro Szekely",
        "file_name": "20250514092207.pdf",
        "file_path": "output/PDFs/20250514092207.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short4.pdf"
    },
    {
        "title": "429646_1_En_6_Chapter 127..128",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "",
        "publication_date": null,
        "authors": null,
        "file_name": "20250514092209.pdf",
        "file_path": "output/PDFs/20250514092209.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_6.pdf"
    },
    {
        "title": "A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2021.115731",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-08-10",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Using a Legal Knowledge Graph for Multilingual Compliance Services in Labor Law, Contract Management, and Geothermal Energy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_12",
        "arxiv": null,
        "abstract": "Abstract This chapter provides insights about the work done and the resultsachieved by the Horizon 2020-funded Innovation Action “Lynx—Building theLegal Knowledge Graph for Smart Compliance Services in Multilingual Europe.”The main objective of Lynx is to create an ecosystem of multilingual, smart cloudservices to manage compliance based on a Legal Knowledge Graph (LKG), whichintegrates and links heterogeneous compliance data sources including legislation,case law, regulations, standards, and private contracts. The chapter provides a shortintroduction in regards to the market needs, gives an overview of the Lynx servicesavailable on the Lynx Services Platform (LySP), and provides valuable insightsinto three real-world compliance solutions developed on top of LySP togetherwith Lynx’s industry partners, namely, (1) Labor Law (Cuatrecasas, Spain), (2)Contract Management (Cybly, Austria), and (3) Geothermal Energy (DNV.GL, theNetherlands).Keywords Compliance · Legal knowledge graph · Multilingualism · NLP ·Labor law · Contract management · Geothermal energyM. Kaltenboeck (�)Semantic Web Company, Vienna, Austriae-mail: martin.kaltenboeck@semantic-web.comP. BoilCuatrecasas, Barcelona, SpainP. VerhoevenDNV, Utrecht, The NetherlandsC. SagederCybly, Salzburg, AustriaE. Montiel-Ponsoda · P. Calleja-IbáñezUniversidad Politécnica de Madrid, Madrid, Spain© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_12253http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_12&domain=pdfmailto:martin.kaltenboeck@semantic-web.comhttps://doi.org/10.1007/978-3-030-78307-5_12254 M. Kaltenboeck et al.Insights into the industry solutions include the concrete business cases, the problemstatements and requirements, the relevant data identified and used, and the LySPAI services combined to realize powerful multilingual compliance solutions inthe respective fields. The chapter closes with findings and learnings from theimplementation phase and a future outlook for further developments, specificallyfor the three vertical solutions and LySP.The chapter relates to the technical priorities of Data Management and DataAnalytics of the European Big Data Value Strategic Research and InnovationAgenda [1]. It addresses all challenges of the horizontal concern Data Managementand some of the challenges of the horizontal concern Data Analytics of the BDVTechnical Reference Model. It addresses the vertical concerns: (a) Big Data Typesand Semantics (with a focus on Text data, including Natural Language Processingdata and Graph data, Network/Web data and Metadata) as well as (b) Standards(standardization of Big Data technology areas to facilitate data integration, sharing,and interoperability). The chapter relates to the Reasoning and Decision Makingcross-sectorial technology enablers of the AI, Data and Robotics Strategic Research,",
        "publication_date": "2022-01-01",
        "authors": "Martin Kaltenboeck, Pascual Boil, Pieter Verhoeven, Christian Sageder, Elena Montiel-Ponsoda, Pablo Calleja-Ibáñez",
        "file_name": "10!1007%978-3-030-78307-5_12.pdf",
        "file_path": "output/PDFs/10!1007%978-3-030-78307-5_12.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-78307-5_12.pdf"
    },
    {
        "title": "LawORDate: a Service for Distinguishing Legal References from Temporal Expressions.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. References to documents in the legal domain usually followpatterns containing temporal information in different forms (e.g. ’Di-rective 2001/29’). These references mislead algorithms detecting puretemporal references, and false positives occur in named entity recogni-tion algorithms searching dates or intervals. This paper presents meth-ods and techniques to identify these references, applied to two differentdomains. The first domain is that of news, where the temporal infor-mation plays a crucial role for their understanding and automaticallybuilding timelines can be hampered by the errors induced from these le-gal references. The second domain is that dataset descriptions. Datasetdescriptions sometimes contain temporal information, not only in theirdedicated metadata fields (e.g. dataset creation) but also within the textof their description. LawORDate, the system presented in this paper, isa web service able to detect legal references with temporal informationin Spanish texts. The service identifies these references, avoiding theirannotation by temporal taggers and enabling a further step of linkingthe references to the original sources and building co-reference graphs.Keywords: legal references, temporal expressions, news, dataset de-scription1 IntroductionTemporal expressions detection, mainly focused on news, is a emerging fieldgaining more and more importance in NLP. Efforts such as the NewsReaderproject1 and the TempEval [1, 2] initiatives in SemEval, along with subsequentmore specific temporal tasks [3, 4] show the interest in processing the temporaldimension on all kind of texts. Usually processing of temporal expressions isdone regarding the concrete type of text being faced, both depending on its field(such as news, clinical domain or historical texts) or extension (free texts orlength-limited tweets). Due to this specialization, systems do not usually reactwell when they find expressions from other fields, such as is the case of legalreferences in news or dataset description.1 http://www.newsreader-project.eu/results/data/wikinews/Proceedings of the 1st Workshop on Technologies for Regulatory Compliance25The boom of open data portals also present this kind of mixed information.Thousands of datasets become publicly available everyday, sometimes presentingjust basic scarce metadata such as title and description. Being able to extractadditional information and new search parameters from them, such as named en-tities or temporal references, would facilitate managing them, along with linkingthem resources or queries.To this end, a system2 was built to extract temporal coverage from bothnews and related datasets in Spanish, some of them in the legal domain, and beable to link them based in the temporal dimension. This system calls an existingtemporal tagger, HeidelTime [5], able to detect temporal expressions in texts inSpanish and tag them following the TIMEX3 annotation standard. Nevertheless,this tagger happened to tag as temporal expressions references to Spanish lawsand legal documents that led to false positives, such as shown in the exampleexposed in Fig. 1, extracted from a real article3. The result of the tagging byHeidelTime can be found in Fig. 2.Estas actividades están reguladas por Real Decreto 1341/2007, de 11 de octubresobre la gestión de la calidad de las aguas de baño, incorporando al derecho español",
        "publication_date": "2017-01-01",
        "authors": "María Navas-Loro",
        "file_name": "20250514092216.pdf",
        "file_path": "output/PDFs/20250514092216.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2049/04paper.pdf"
    },
    {
        "title": "Challenges in the Digital Representation of Privacy Terms",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_22",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-01",
        "authors": "Beatriz Esteves",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Dimensionality-reducing classifiers for Spanish winter maintenance of roadways",
        "implementation_urls": [],
        "doi": "10.1109/ciot57267.2023.10084897",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-03-20",
        "authors": "Diego M. Jiménez-Bravo, Javier Bajo, E. Dopazo, Juan F. De Paz, Valderi Reis Quietinho Leithardt",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Morph-CSV: Virtual Knowledge Graph Access for Tabular Data.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Virtual knowledge graph access has traditionally focused onproviding ontology-based access to relational databases (RDB) propos-ing SPARQL-to-SQL query translation techniques and optimizations.With the advent of mapping languages or annotations such as RML orCSVW, these techniques have been applied over tabular data by con-sidering each source as a single table that can be loaded into an RDB.However, such techniques do not take into account those characteristicsthat are normally present in real-world CSV files (e.g., normalization,constraints, joins). In this paper we present Morph-CSV, a frameworkfor enhancing virtual knowledge graph access over a set of CSV files byusing a combination of CSVW annotations and RML mappings withFnO transformation functions. Exploiting these inputs, the frameworkcreates an enriched RDB representation of the CSV files together withthe corresponding R2RML mappings, enabling the use of existing querytranslation (SPARQL-to-SQL) techniques and tools.Keywords: Knowledge Graphs · CSV · RML · CSVW1 IntroductionSemi-structured data formats, and particularly spreadsheets in the form of CSVor Excel files, are one of the most widely-used formats to publish data on theWeb. There are several reasons why tabular formats are so popular for datapublication. First, they are easy to generate by data providers. In many cases,they are even used as one of the main ways to manage data inside organiza-tions. Second, they are easy to consume with common office tools (e.g., Excel,LibreOffice) and there are advanced tools that can be used to process them (e.g.,OpenRefine, Tableau). However, more advanced consumers (e.g., application de-velopers, knowledge workers) often have to face some relevant challenges whenconsuming tabular data: there is no standard way to query data in them as itcan be done with other types of data formats, such as RDB, JSON or XML; dataCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).Chaves-Fraga et al.are difficult to integrate since data constraints and relationships across differentfiles are not explicit; data are often difficult to understand since column namesare generally heterogeneous.Some of these challenges may be dealt following a Semantic Web approach.Virtual knowledge graphs (VKG) provide a unified view and common access toa set of data sources based on ontologies and mappings, translating SPARQLqueries into queries that are supported by the underlying source. Although cur-rent proposals [7] provide support for querying this kind of formats, they treateach source as if it was a single not-normalized RDB table with no keys orintegrity constraints, important elements that are used by SPARQL-to-SQL en-gines for efficient querying. Several languages have been proposed to specifyannotations to deal with the heterogeneity of tabular datasets such as CSVW[8] metadata and RML+FnO [5] mapping rules, but engines or systems have totake them into account in their VKG access pipeline.In this demo we present Morph-CSV, an open source engine1 that extendsthe typical VKG workflow to enhance performance and query completeness overtabular datasets. Our approach exploits the information from CSVW anno-tations and RML+FnO mappings so as to obtain details on the underlyingschema, required transformation functions, missing information, etc., pushing",
        "publication_date": "2020-01-01",
        "authors": "David Chaves-Fraga, Luis Pozo-Gilo, Jhon Toledo, Edna Ruckhaus, Óscar Corcho",
        "file_name": "20250514092221.pdf",
        "file_path": "output/PDFs/20250514092221.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2721/paper478.pdf"
    },
    {
        "title": "Characterize a Human-Robot Interaction: Robot Personal Assistance",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-62530-0_8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-08-31",
        "authors": "Dalila Durães, Javier Bajo, Paulo Nováis",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Interoperability of Heterogeneous Systems of Systems: Review of Challenges, Emerging Requirements and Options",
        "implementation_urls": [],
        "doi": "10.1145/3555776.3577692",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-03-27",
        "authors": "Mersedeh Sadeghi, Alessio Carenini, Óscar Corcho, Matteo Rossi, Riccardo Santoro, Andreas Vogelsang",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Integrating WordNet and Wiktionary with lemon",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-28249-2_3",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2012-01-01",
        "authors": "John P. McCrae, Elena Montiel-Ponsoda, Philipp Cimiano",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "VICINITY: IoT Semantic Interoperability Based on the Web of Things",
        "implementation_urls": [],
        "doi": "10.1109/dcoss.2019.00061",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-01",
        "authors": "Andrea Cimmino, Viktor Oravec, Fernando Serena, Peter Kostelnik, María Poveda‐Villalón, Athanasios Tryferidis, Raúl García‐Castro, Stefan Vanya, Dimitrios Tzovaras, Christoph Grimm",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Now, Later, Never: A Study of Urgency in Mobile Push-Notifications",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-20436-4_4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Beatriz Esteves, Kieran Fraser, Shridhar Kulkarni, Owen Conlan, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Balancing coverage and specificity for semantic labelling of subject columns",
        "implementation_urls": [],
        "doi": "10.1016/j.knosys.2021.108092",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-13",
        "authors": "Ahmad Alobaid, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "MuHeQA: Zero-shot question answering over multiple and heterogeneous knowledge bases",
        "implementation_urls": [],
        "doi": "10.3233/sw-233379",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-06-09",
        "authors": "Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "esT5s: A Spanish Model for Text Summarization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/t5-spanish-news-summarization",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%ssw220020.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "For the sake of reproducibility, the source code is available 3."
                    }
                ]
            }
        ],
        "doi": "10.3233/ssw220020",
        "arxiv": null,
        "abstract": "Abstract. Deep Learning models based on the Transformer architecture have rev-olutionized the state of the art of NLP tasks. As English is the language in whichmost significant advances are made, languages like Spanish require specific train-ing, but this training has a computational cost so high that only big corporationswith servers and GPUs are capable of generating them. This work has exploredhow to create a model for the Spanish language from a big multilingual model.Specifically, a model aimed at creating text summarization, a very common taskin NLP. The results, concerning the quality of the summarization (ROUGE score),point out that these small models, for a specific language, achieve similar resultsthan much bigger models, with a reasonable training in terms of time required andcomputational power, and are significantly faster at inference.Keywords. Deep learning, T5, Spanish, Text summarization1. IntroductionSummarization is a Natural Language Processing task that consists of condensing themost relevant information from a document. This task can be divided into two categories:extractive summarization and abstractive summarization. Extractive summarization con-sists of identifying and copying the most relevant and useful information pieces (typicallysentences) from the original content. In contrast, abstractive summarization requires adeeper understanding of the language to summarize the most relevant content, paraphras-ing the original sentences, combining and using synonyms or new words, without losinginformation and preserving cohesion and coherence [1]. Thus, abstractive summarizationis a difficult task in natural language processing.Currently, the state-of-the-art of language models based on transformers [2] havereached a high level of language comprehension. However, all research is mainly focusedon the English language and then applied to other languages. Even important languagessuch as Spanish, which is the second language spoken in the world, has an enormous1Corresponding Author: Mariano Rico; E-mail:mariano.rico@upm.es.The authors gratefully acknowledge the computer resources at Artemisa, funded by the European UnionERDF and Comunitat Valenciana as well as the technical support provided by the Instituto de Fı́sicaCorpuscular, IFIC (CSIC-UV). Also we acknowledge the Universidad Politécnica de Madrid for providingcomputing resources on Magerit Supercomputer. This work was funded partially by the project KnowledgeSpaces (PID2020-118274RB-I00), funded by MCIN/AEI/ 10.13039/501100011033; and project HCommonK(RTC2019-007134-7, funded by MCIN/AEI/ 10.13039/501100011033).Towards a Knowledge-Aware AIA. Dimou et al. (Eds.)© 2022 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution License 4.0 (CC BY 4.0).doi:10.3233/SSW220020184gap in their language models compared to English [3]. For example, the T5 model [4]is one of the best language models, which exploits the features of text-to-text transferlearning and it is usually used for the text summarization task, it is only trained forEnglish language, and there is no Spanish version yet.Despite this lack of models for non-English languages, there are multilingual mod-els (also including English). This is the case of the multilingual T5 (mT5) [5] which istrained in 101 languages, including English and Spanish among them. These multilin-gual approaches outperform monolingual models because similar languages have posi-tive transfer between them [6]. However, the effort required to create (and also execute)these multilingual models is very high. Our approach takes advantage of these multi-",
        "publication_date": "2022-09-06",
        "authors": "Adrian Vogel-Fernandez, Pablo Calleja, Mariano Rico",
        "file_name": "10!3233%ssw220020.pdf",
        "file_path": "output/PDFs/10!3233%ssw220020.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic Web-Based Information Systems",
        "implementation_urls": [],
        "doi": "10.4018/978-1-59904-426-2",
        "arxiv": null,
        "abstract": "ABSTRACT In this paper we present a model for building ontology translation systems between ontology languages and/or ontology tools, where translation decisions are defmed at four different layers: lexical, syntax, semantic, and pragmatic. This layered approach provides a major contribution to the current state of the art in ontology translation, since it makes ontology translation systems easier to build and understand and, consequently to maintain and reuse. As part of this model, we propose a method that guides in the process of developing ontology translation systems according to this approach. The method identifies four main activities: feasibility study, analysis ofsource, and target formats, design, and implementation of the translation system, with their decomposition in tasks, and recommends the techniques to be used inside each of them. Keywords: ontologies; semiotics; transformation languages; transformation models INTRODUCTION An ontology is defined as a \"formal explicit specification of a shared conceptualization\" (Studer etal., 1998); that is, an ontology must be machine read-able (it is formal), all its components must be described clearly (it is explicit), it de­scribes an abstract model of a domain (it is a conceptualization), and it is the prod-uct of a consensus (it is shared). Ontologies can be implemented in varied ontology languages, which are usu-ally divided in two groups: classical and ontology markup languages. Among the classical languages used for ontology con-struction, we can cite (in alphabetical or-der): CycL (Lenat& Guha, 1990), FLogic (Kifer et al., 1995), KIF (Genesereth & Fikes, 1992), LOOM (MacGregor, 1991), OCML (Motta, 1999), and Ontolingua (Gruber, 1992). Among the ontology markup languages used in the context of the Semantic Web, we can cite (in alphabetical order): DAML+OIL (Horrocks and vanHarmelen, 2001), OIL (Horrocks et al., 2000), OWL (Dean & Schreiber, 2004), RDF (Lassila& Swick, 1999), RDF Schema (Brickley & Guha, 2004), SHOE (Luke & Hefflin, 2000), and XOL (Karp et al., 1999). Each of these languages has its own syntax, its own expressiveness, and its own reasoning capabilities provided by different inference engines. Languages also are based on dif­ferent knowledge representation para-digms and combinations of them (frames, first order logic, description logic, seman­",
        "publication_date": "2007-01-01",
        "authors": "",
        "file_name": "10!4018%978-1-59904-426-2.pdf",
        "file_path": "output/PDFs/10!4018%978-1-59904-426-2.pdf",
        "pdf_link": null
    },
    {
        "title": "Personal eBanking Solutions based on Semantic Web Services",
        "implementation_urls": [],
        "doi": "10.1007/978-3-540-37017-8_13",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2007-01-01",
        "authors": "Óscar Corcho, Silvestre Losada, Richard Benjamins, José Luis Bas, Sergio Bellido",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Architectural Patterns for the Semantic Grid",
        "implementation_urls": [],
        "doi": "10.1007/978-0-387-37831-2_8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2007-02-11",
        "authors": "Ioannis Kotsiopoulos, Paolo Missier, Pinar Alper, Óscar Corcho, Sean Bechhofer, Carole Goble",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Blockchain-Based Data Management for Smart Transportation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-07535-3_20",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-42280-5_12",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Isam Diab, Álvaro González, Jesica Rivero-Espinosa",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Complex queries over decentralised systems for geodata retrieval",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1049%ntw2!12037.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "DATA AVAILABILITY STATEMENT The data that support the findings of this study are openly available in Zenodo at https://doi.org/10.5281/zenodo."
                    }
                ]
            }
        ],
        "doi": "10.1049/ntw2.12037",
        "arxiv": null,
        "abstract": "AbstractDecentralised systems have been proved to be quite effective to allow for trusted andaccountable data sharing, without the need to resort to a centralised party that collects allthe information. While complete decentralisation provides important advantages in termsof data sovereignty, absence of bottlenecks and reliability, it also adds some issues con-cerned with efficient data lookup and the possibility to implement complex querieswithout reintroducing centralised components. In this paper, we describe a system thatcopes with these issues, thanks to a multi‐layer lookup scheme based on Distributed HashTables that allows for multiple keyword‐based searches. The service of peer nodesparticipating in this discovery service is controlled and rewarded for their contribution.Moreover, the governance of this process is completely automated through the use ofsmart contracts, thus building a Decentralised Autonomous Organization (DAO). Finally,we present a use case where road hazards are collected in order to test the goodness ofour system for geodata retrieval. Then, we show results from a performance evaluationthat confirm the viability of the proposal.1 | INTRODUCTIONThe digitalisation process, which has been ongoing over thelast decades, has seen data management and data deliverybecome crucial issues. The transformation brought about bydigital technologies has data at its core and it had a significantimpact on economies and societies around the world. Theability to easily get hold of data has the potential to createseveral new services based on data and new markets wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is notfrom a dubious (or false) origin is often a challenge. In order tocope with the increasingly higher number of content that isdemanded through the Web, multiple solutions for efficient useof the Internet have been designed. In particular, thanks to thedecentralisation of content storage and delivery, it is possible toavoid the single point of failure, while reducing the workload atdata centres and allowing a distribution of data that remains‘closer’ to the original data producer. Decentralisation alsofosters the creation of open systems, where participants canfreely join the system and then contribute to its functioning.Recently, Distributed Ledger Technologies (DLTs) and arealm of decentralised systems, for example, Decentralised FileStorages (DFS), have emerged as Peer‐to‐Peer (P2P) technol-ogies capable of offering interesting features related to datavalidation and trustfulness [2, 3]. DLTs have gained popularitywith the advent of cryptocurrencies, which allow users to tradecrypto‐assets without any central entity being involved,ensuring transparency and data integrity. By creating a com-mon, decentralised and trustless infrastructure, it will bepossible for data consumers and providers to interact andcollaborate in P2P interactions [4, 5]. Benefits often cited ofDLTs, indeed, include the enabling of secure transactions be-tween untrusted parties through consensus mechanisms, highavailability, and the ability to automate and enforce processesthrough smart contracts [6]. Besides the financial use case,",
        "publication_date": "2022-03-26",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1049%ntw2!12037.pdf",
        "file_path": "output/PDFs/10!1049%ntw2!12037.pdf",
        "pdf_link": null
    },
    {
        "title": "MOVO: a dApp for DLT-based Smart Mobility",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/movoApp",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%icccn52240!2021!9522257.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The source code of Movo is available on GitHub: https://github.com/miker83z/movoApp."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522257",
        "arxiv": "2104.13813",
        "abstract": "Abstract—Plenty of research on smart mobility is currentlydevoted to the inclusion of novel decentralized software archi-tectures to these systems, due to the inherent advantages interms of transparency, traceability, trustworthiness. MOVO is adecentralized application (dApp) for smart mobility. It includes:(i) a module for collecting data from vehicles and smartphonessensors; (ii) a component for interacting with Distributed LedgerTechnologies (DLT) and Decentralized File Storages (DFS), forstoring and validating sensor data; (iii) a module for \"offline\"interaction between devices. The dApp consists of an Androidapplication intended for use inside a vehicle, which helps theuser/driver collect contextually generated data (e.g. a driver’sstress level, an electric vehicle’s battery level), which can then beshared through the use of DLT (i.e., IOTA DLT and Ethereumsmart contracts) and DFS (i.e., IPFS). The third module consistsof an implementation of a communication channel that, via Wi-FiDirect, allows two devices to exchange data and payment informa-tion with respect to DLT (i.e. cryptocurrency and token) assets.In this paper, we describe the main software components andprovide an experimental evaluation that confirms the viability ofthe MOVO dApp in real mobility scenarios.Index Terms—Smart Mobility, Distributed Ledger Technolo-gies, Decentralized Application, VANETI. INTRODUCTIONIn the last decade smart mobility has emerged as way toefficiently improve mobility, travel security and increase theoptions for travellers. The general idea is usually that ofdevising a sort of middleware to build advanced applicationsfor the provision of innovative transport and traffic manage-ment services, with the aim of enabling users “to be betterinformed and make safer, more coordinated and ‘smarter’ useof transport networks” [1].Vehicles and infrastructures are becoming increasingly“smarter”, which means that they are equipped with sensorsthat track a huge amount of different types of information,e.g. data sensed by the interior of the vehicle, the surroundingenvironment, road conditions, etc. In addition, the growthof smartphones and Internet-of-Things (IoT) devices enablesindividuals’ ubiquitous connectivity and the ability to collectpersonal information or crowdsensing data. All of this consti-tute a network of devices that is usually referred as VANET(Vehicular Ad-hoc NETwork) [2], [3].This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.In this context, a middleware platform can be designedto share and reuse data, services and computation, simpli-fying the development of new services and the integration",
        "publication_date": "2021-07-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%icccn52240!2021!9522257.pdf",
        "file_path": "output/PDFs/10!1109%icccn52240!2021!9522257.pdf",
        "pdf_link": null
    },
    {
        "title": "Scheduling Ontology Engineering Projects Using gOntt",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_14",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez, Óscar Muñoz-García",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Data Quality Barriers for Transparency in Public Procurement",
        "implementation_urls": [],
        "doi": "10.3390/info13020099",
        "arxiv": null,
        "abstract": "Abstract: Governments need to be accountable and transparent for their public spending decisionsin order to prevent losses through fraud and corruption as well as to build healthy and sustainableeconomies. Open data act as a major instrument in this respect by enabling public administrations,service providers, data journalists, transparency activists, and regular citizens to identify fraud oruncompetitive markets through connecting related, heterogeneous, and originally unconnected datasources. To this end, in this article, we present our experience in the case of Slovenia, where wesuccessfully applied a number of anomaly detection techniques over a set of open disparate data setsintegrated into a Knowledge Graph, including procurement, company, and spending data, through alinked data-based platform called TheyBuyForYou. We then report a set of guidelines for publishinghigh quality procurement data for better procurement analytics, since our experience has shown usthat there are significant shortcomings in the quality of data being published. This article contributesto enhanced policy making by guiding public administrations at local, regional, and national levels onhow to improve the way they publish and use procurement-related data; developing technologies andsolutions that buyers in the public and private sectors can use and adapt to become more transparent,make markets more competitive, and reduce waste and fraud; and providing a Knowledge Graph,which is a data resource that is designed to facilitate integration across multiple data silos by showinghow it adds context and domain knowledge to machine-learning-based procurement analytics.Keywords: public procurement; fraud and corruption; data integration; knowledge graph; linkedopen data; anomaly detection1. IntroductionPublic procurement is a business impacting the lives of millions. Globally governmentsspend trillions of dollars a year on public contracts for goods, services, and works (https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/ (ac-cessed on 19 February 2022)); for example, public authorities in the European Union(EU) spend around 14% of GDP every year (https://ec.europa.eu/growth/single-market/public-procurement_en (accessed on 19 February 2022)). A market of such a size hassubstantial challenges, such as delivering quality services with greatly reduced bud-gets; preventing losses through fraud and corruption; and building healthy and sus-tainable economies. Even a small percentage of cost increase can easily have a massiveInformation 2022, 13, 99. https://doi.org/10.3390/info13020099 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13020099https://doi.org/10.3390/info13020099https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0002-2753-9917https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enhttps://doi.org/10.3390/info13020099https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13020099?type=check_update&version=20101001010Pred formationArticle",
        "publication_date": "2022-02-20",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Mitja Medvešček, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": "20250514092403.pdf",
        "file_path": "output/PDFs/20250514092403.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/13/2/99/pdf?version=1645523351"
    },
    {
        "title": "Preface",
        "implementation_urls": [],
        "doi": "10.1016/s0304-0208(06)80001-0",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2006-01-01",
        "authors": "Anatoly A. Kilbas, H. M. Srivastava, Juan J. Trujillo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Toward the Ontological Modeling of Smart Contracts: A Solidity Use Case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ConsenSys/EthOn",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%access!2021!3115577.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: http://arxiv.org/abs/1809.09805 [14] P."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2021.3115577",
        "arxiv": null,
        "abstract": "ABSTRACT Blockchain has become a pervasive technology in a wide number of sectors like industry,research, and academy. With the emergence of blockchain, new solutions with this technology to existingproblems were devised, leading to the introduction of smart contracts. Smart contracts are similar totraditional contracts with the benefits provided by blockchain, such as immutability, privacy, and decen-tralisation. These contracts are usually defined based on a specific domain, and this domain knowledge canbe represented through an ontology. Researches have explored the benefits of using domain ontologies withsmart contracts, such as code generation, discovering other contracts in the network, or interaction with othercontracts. Notwithstanding, the representation of smart contract languages themselves has not been studied.In this paper, we present an ontology for a well-known smart contract language, Solidity, defining all entitiesneeded to cover the whole language and aligning it to other standardised ontologies such as EthOn, in a wayto improve the knowledge of the ontology developed. Furthermore, the ontology has also been validatedwith already deployed contracts in the Ethereum blockchain. Thus, Solidity will be able to benefit from theadvantages provided by ontologies, such as interoperability and the use of semantic web technologies.INDEX TERMS Ontology, smart contracts, solidity, blockchain.I. INTRODUCTIONIn the last decade, blockchain technologies have becomepervasive in our world [1]. The reason is due to some keycharacteristics of these technologies such as decentralisation,immutability, transparency, privacy, and auditability [2].Withthe growth in the adoption of these technologies, the conceptof smart contract was introduced and has become a relevantcharacteristic of blockchain.Smart contracts find their roots in the formal contracts inwhich two or more trusted entities perform an agreement onsomething. In order to ensure that the agreement is fulfilledand the derived obligations are preserved, a trusted third-partyentity acts as a referee. Smart contracts are computer pro-grams in which a blockchain eliminates the requirement forthese third-party entities by ensuring that all ledger partici-pants know the details of the contracts and, due to the pro-gramming logic of smart contracts, the terms and conditionsThe associate editor coordinating the review of this manuscript andapproving it for publication was Fabrizio Messina .of the contract are automatically applied once the conditionsare met.Sectors like industry, government, or research have ben-efited from blockchain and smart contracts’ advantages.Notable companies such as Walmart [3], [4] use blockchainand smart contracts for keeping a reliable record of foodwaste, supply chain, food chain, food origin, or inventory.Even governments have used smart contracts for managingidentities and credentials in the context of governmentaldata [5], [6].Smart contracts are usually part of a domain [7], [8] suchas agreements on music rights, video games, supply chains,buying and selling, etc. In programming languages, thesedomains can be represented using Knowledge Representa-tion (a field of artificial intelligence dedicated to represent-ing information of a domain and usually use to captureinformation about the world for solving complex problems),",
        "publication_date": "2021-01-01",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": "10!1109%access!2021!3115577.pdf",
        "file_path": "output/PDFs/10!1109%access!2021!3115577.pdf",
        "pdf_link": null
    },
    {
        "title": "Developing accessibility multimedia services",
        "implementation_urls": [],
        "doi": "10.1145/3389189.3397973",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-06-29",
        "authors": "Dimitrios Konstantinidis, Kosmas Dimitropoulos, Kiriakos Stefanidis, Thanassis Kalvourtzis, Salim Gannoum, Nikolaos Kaklanis, Konstantinos Votis, Petros Daras, Sara Rovira-Esteva, Pilar Orero, Silvia Uribe, Francisco Moreno, A. Llorente, Pablo Calleja, María Poveda‐Villalón, Pasquale Andriani, Giuseppe Vitolo, Giuseppa Caruso, Nicolamaria Manes, Fabrizio Giacomelli, Jordi Fabregat, Francesc Mas, Jordi Mata, Stavros Skourtis, Chrysostomos Bourlis, Giuliano Frittelli, Emilio Ferreiro Lago, Federico Álvarez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards a Linked Democracy Model",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_4",
        "arxiv": null,
        "abstract": "Abstract In this chapter we lay out the properties of participatory ecosystems aslinked democracy ecosystems. The goal is to provide a conceptual roadmap thathelps us to ground the theoretical foundations for a meso-level, institutional theoryof democracy. The identification of the basic properties of a linked democracyeco-system draws from different empirical examples that, to some extent, exhibitsome of these properties. We then correlate these properties with Ostrom’s designprinciples for the management of common-pool resources (as generalised to groupscooperating and coordinating to achieve shared goals) to open up the question ofhow linked democracy ecosystems can be governed.Keywords Linked democracy � Common-Pool resources4.1 IntroductionIn previous chapters we have suggested that our model of linked democracy can berepresented as a three-layered, overlapping structure of Linked Open Data (LOD),linked platforms, and linked ecosystems. A linked democracy model represents thedistributed interplay between people, digital technologies, and data (see Fig. 4.1).We have also provided examples of digital platforms and ecosystems that exhibit acertain degree of connectedness by tapping on LOD, on open data, or on crowd-sourced data produced elsewhere.Breaking silos down is a common, distinctive feature of the examples we havereviewed. But are there any other properties than we can distill from these exam-ples? Moreover, is it possible to turn those properties into design principles thathelp to orchestrate a linked democracy model? Design principles should guide theimplementation of a linked democracy model; they should also capture the insti-tutional arrangements needed to produce aligned decision making in a givendomain, either local or global. As we have seen with the Icelandic or Mexico Cityexamples, a lack of institutional endorsement of carefully designed participatory© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_475http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_4outcomes can bring crowdsourced constitutional processes to a deadlock. Linkeddemocracy is about finding ways out of locked democracy.We are fully aware that generalizing specific design principles for the efficientfunctioning of a linked democracy would require an exhaustive, large-scale surveyof case studies. We have examined some illustrative examples in the previouschapters, but this falls short of providing a comprehensive panorama. Therefore, inthis chapter we will first identify some distinctive properties of a linked democracymodel based on our previous examples. Second, we will map these properties ontothe well-established set of design principles that Elinor Ostrom identified asenabling effective management of ‘common-pool resources’ (CPR) groups (Ostrom1990, 90–102). Recently, David Wilson et al. reviewed Ostrom’s principles from anevolutionary perspective to argue that they ‘have a wider range of application thanCPR groups and are relevant to nearly any situation where people must cooperateand coordinate to achieve shared goals’ (Wilson et al. 2013, 522). We considerlinked democracy ecosystems to be one of those situations involving cooperation—in performing a wide range of tasks—and coordination—of large groups of indi-",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "20250514092552.pdf",
        "file_path": "output/PDFs/20250514092552.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_4.pdf"
    },
    {
        "title": "An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science",
        "implementation_urls": [
            {
                "identifier": "https://github.com/actionprojecteu/dmptool",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1108%dta-10-2020-0253.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "A technical description can be found at https://zenodo.org/record/3885566 and the software is available with an open source license at https://github.com/actionprojecteu/dmptool and https://github.com/actionprojecteu/dmptool-generator."
                    }
                ]
            }
        ],
        "doi": "10.1108/dta-10-2020-0253",
        "arxiv": null,
        "abstract": "Abstract. Purpose: Citizen Science – public participation in scientific projects – is becoming a global practice engaging volunteer participants, often non-scientists, with scientific research. Citizen Science is facing major challenges, such as quality and consistency, to reap open the full potential of its outputs and outcomes, including data, software, and results. In this context, the principles put forth by Data Science and Open Science domains are essential for alleviating these challenges, which have been addressed at length in these domains. The purpose of this study is to explore the extent to which Citizen Science initiatives capitalise on Data Science and Open Science principles. Approach: We analysed 48 Citizen Science projects related to pollution and its effects. We compared each project against a set of Data Science and Open Science indicators, exploring how each project defines, collects, analyses and exploits data to present results and contribute to knowledge. Roman, D., Reeves, N., Gonzalez, E., Celino, I., Abd El Kader, S., Turk, P., Soylu, A., Corcho, O., Cedazo, R., Re Calegari, G., Scandolari, D. and Simperl, E. (2021), \"An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science\",Data Technologies and Applications, Vol. ahead-of-print No. ahead-of-print. https://doi.org/10.1108/DTA-10-2020-0253This author accepted manuscript is deposited under a Creative Commons Attribution Non-commercial 4.0 International (CC BY-NC) licence. This means that anyone may distribute, adapt, and build upon the work for non-commercial purposes, subject to full attribution. If you wish to use this manuscript for commercial purposes, please contact permissions@emerald.com.https://creativecommons.org/licenses/by-nc/4.0/2   Findings: The results indicate several shortcomings with respect to commonly accepted Data Science principles, including lack of a clear definition of research problems and limited description of data management and analysis processes, and Open Science principles, including lack of the necessary contextual information for reusing project outcomes.  Originality: In the light of this analysis, we provide a set of guidelines and recommendations for better adoption of Data Science and Open Science principles in Citizen Science projects, and introduce a software tool to support this adoption, with a focus on preparation of data management plans in Citizen Science projects.  Keywords: Citizen Science, Data Science, Open Science, pollution projects, Data Management Plan, software 1.  Introduction Citizen Science (CS) describes the active engagement of volunteer participants within scientific research. Projects vary greatly in terms of the role that volunteers play and the degree of agency that they have, from more passive models where volunteers install software and sensors to more collaborative models where volunteers actively define problem spaces and research topics (Haklay, 2013). Nevertheless, CS commonly entails the gathering of data by volunteers for later dissemination and publication (Haklay, 2013; Pocock et al., 2017).  CS data are of significant value not only in the projects in which they are gathered, but for subsequent analysis and re-use (Wang et al., 2015). Volunteer-contributed data may complement and offer the opportunity to contextualise and expand upon existing monitoring efforts by professional organisations, without significant added cost (Hadj-Hammou et al., 2017). In some contexts, the vast majority of available data are from CS sources (Groom et al., 2017; Poisson et al., 2020). This is equally true of software, with CS projects developing a wide variety of software ",
        "publication_date": "2021-05-04",
        "authors": "Dumitru Roman, Neal Reeves, Esteban González, Irene Celino, Shady Abd El Kader, Philip Turk, Ahmet Soylu, Óscar Corcho, R. Cedazo, Gloria Re Calegari, Damiano Scandolari, Elena Simperl",
        "file_name": "10!1108%dta-10-2020-0253.pdf",
        "file_path": "output/PDFs/10!1108%dta-10-2020-0253.pdf",
        "pdf_link": null
    },
    {
        "title": "Increasing the Intensity over Time of an Electric-Assist Bike Based on the User and Route: The Bike Becomes the Gym",
        "implementation_urls": [],
        "doi": "10.3390/s18010220",
        "arxiv": null,
        "abstract": "Abstract: Nowadays, many citizens have busy days that make finding time for physical activitydifficult. Thus, it is important to provide citizens with tools that allow them to introduce physicalactivity into their lives as part of the day’s routine. This article proposes an app for an electricpedal-assist-system (PAS) bicycle that increases the pedaling intensity so the bicyclist can achievehigher and higher levels of physical activity. The app includes personalized assist levels that havebeen adapted to the user’s strength/ability and a profile of the route, segmented according to itsslopes. Additionally, a social component motivates interaction and competition between users basedon a scoring system that shows the level of their performances. To test the training module, acase study in three different European countries lasted four months and included nine people whotraveled 551 routes. The electric PAS bicycle with the app that increases intensity of physical activityshows promise for increasing levels of physical activity as a regular part of the day.Keywords: personalized assistance level; coaching; physical activity; electric bicycles1. IntroductionAdvances in the field of technology and developments in common transport systems have greatlyreduced people’s physical activity [1]. In developed countries, the majority of people travel to andfrom work using motor transport systems, e.g., statistics from 2011 show that, in England and Wales,85% of the population used motorized transport as their usual commute mode [2]. Nowadays, citizensspend much more time at sedentary activities, such as working in front of the computer. In 2012, datafrom 66 high and low income countries show that the percentage of adults who spent four or morehours sitting each day was 41.5% [1]. Due to these changes in our lifestyle, the risk of suffering healthproblems as a result of physical activity is increasingly high [3]. Experts from diverse entities, suchas WHO (World Health Organization), recommend an average of 150 min of exercise per week or30 min daily [4]. Physical activity provides a well-known set of health benefits [5]. Exercise has beenproven to reduce the risk of suffering from high blood pressure, stroke and others [4]. It increasescardiorespiratory and muscular fitness, bone health or increased functional health. Moreover, it canhelp prevent depression [4]. In 2010, the Global Health Observatory (GHO) estimated that the dailyphysical activity of more than 20% of adults is insufficient [6]. The low exercise, combined with thedaily ingestion of fat and calorie rich foods, is leading our society to an obesity epidemic [7].Nowadays, people who wish to make exercise a part of their daily routine usually go to gymsor sign up for different sports. This commitment implies an economic cost of registration and sportsequipment, travel to sports centers, as well as the necessary free time to carry out the activity and aSensors 2018, 18, 220; doi:10.3390/s18010220 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743https://orcid.org/0000-0002-2829-1829http://dx.doi.org/10.3390/s18010220http://www.mdpi.com/journal/sensorsSensors 2018, 18, 220 2 of 21willingness to attend regularly. Performing a team sport (e.g., basketball, football or volleyball) alsorequires adequate sports facilities, a group of people to carry out the activity and a skill in that sport.For these reasons, many people do not perform physical activity regularly [8,9]. As a result, the dailyuse of a bicycle in routine trips is an important alternative to the gym or other sports [10].One of the most widespread ways of fostering active transport among users is by promoting theuse of bicycles in the city [11,12]. Biking will not only help people to get fit but will also reduce trafficcongestion, environmental contamination, climate change and energetic sustainability [13,14]. Theupgrading of infrastructures for cyclists helps provide a positive experience and as a result increases theuse of bicycles in the city [15,16]. In recent years, cities have been promoting the use of bicycles through",
        "publication_date": "2018-01-14",
        "authors": "Daniel H. de la Iglesia, Juan F. De Paz, Gabriel Villarrubia González, Alberto López Barriuso, Javier Bajo, Juan M. Corchado",
        "file_name": "20250514092636.pdf",
        "file_path": "output/PDFs/20250514092636.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/1/220/pdf?version=1515922049"
    },
    {
        "title": "SPARQL2Flink: Evaluation of SPARQL Queries on Apache Flink",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oscarceballos/sparql2flink",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514092641.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available online: https://github.com/oscarceballos/sparql2flink (accessed on 24 March 2020)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app11157033",
        "arxiv": null,
        "abstract": "Abstract: Existing SPARQL query engines and triple stores are continuously improved to handlemore massive datasets. Several approaches have been developed in this context proposing the storageand querying of RDF data in a distributed fashion, mainly using the MapReduce ProgrammingModel and Hadoop-based ecosystems. New trends in Big Data technologies have also emerged (e.g.,Apache Spark, Apache Flink); they use distributed in-memory processing and promise to deliverhigher data processing performance. In this paper, we present a formal interpretation of some PACTtransformations implemented in the Apache Flink DataSet API. We use this formalization to providea mapping to translate a SPARQL query to a Flink program. The mapping was implemented in aprototype used to determine the correctness and performance of the solution. The source code of theproject is available in Github under the MIT license.Keywords: massive static RDF data; SPARQL; PACT Programming Model; Apache Flink1. IntroductionThe amount and size of datasets represented in the Resource Description Framework(RDF) [1] language are increasing; this leads to challenging the limits of existing triple storesand SPARQL query evaluation technologies, requiring more efficient query evaluationtechniques. Several proposals have been documented in state of the art use of Big Datatechnologies for storing and querying RDF data [2–6]. Some of these proposals havefocused on executing SPARQL queries on the MapReduce Programming Model [7] and itsimplementation, Hadoop [8]. However, more recent Big Data technologies have emerged(e.g., Apache Spark [9], Apache Flink [10], Google DataFlow [11]). They use distributedin-memory processing and promise to deliver higher data processing performance thantraditional MapReduce platforms [12]. These technologies are widely used in researchprojects and all kinds of companies (e.g., Google, Twitter, and Netflix, or even by smallstart-ups).To analyze whether or not we can use these technologies to provide query evaluationover large RDF datasets, we will work with Apache Flink, an open-source platform fordistributed stream and batch data processing. One of the essential components of theFlink framework is the Flink optimizer called Nephele [13]. Nephele is based on the Paral-lelization Contracts (PACTs) Programming Model [14] which is in turn a generalization ofthe well-known MapReduce Programming Model. The output of the Flink optimizer is acompiled and optimized PACT program which is a Directed Acyclic Graphs (DAG)-baseddataflow program. At a high level, Flink programs are regular programs written in Java,Appl. Sci. 2021, 11, 7033. https://doi.org/10.3390/app11157033 https://www.mdpi.com/journal/applscihttps://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0001-5214-4127https://orcid.org/0000-0002-6488-8649https://orcid.org/0000-0002-2858-0276https://orcid.org/0000-0002-0236-4284https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/app11157033https://doi.org/10.3390/app11157033https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/app11157033https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app11157033?type=check_update&version=2ririrg appliedA sciences",
        "publication_date": "2021-07-30",
        "authors": "Oscar Ceballos, Carlos Alberto Ramírez Restrepo, María Constanza Pabón, Andrés M. Castillo, Óscar Corcho",
        "file_name": "20250514092641.pdf",
        "file_path": "output/PDFs/20250514092641.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/11/15/7033/pdf?version=1628044316"
    },
    {
        "title": "Assigning Creative Commons Licenses to Research Metadata: Issues and Cases",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_16",
        "arxiv": "1609.05700",
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Marta Poblet, Amir Aryani, Paolo Manghi, Kathryn Unsworth, Jingbo Wang, Brigitte Hausstein, Sünje Dallmeier-Tiessen, Claus-Peter Klas, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Assess and Enhancing Attention in Learning Activities",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-73210-7_93",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-12-26",
        "authors": "Dalila Durães, Javier Bajo, Paulo Nováis",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Development Experience of a Context-Aware System for Smart Irrigation Using CASO and IRRIG Ontologies",
        "implementation_urls": [],
        "doi": "10.3390/app10051803",
        "arxiv": null,
        "abstract": "Abstract: The rapid development of information and communication technologies and wirelesssensor networks has transformed agriculture practices. New tools and methods are used to supportfarmers in their activities. This paper presents a context-aware system that automates irrigationdecisions based on sensor measurements. Automatic irrigation overcomes the water shortageproblem, and automatic sensor measurements reduce the observational work of farmers. This paperfocuses on a method for developing context-aware systems using ontologies. Ontologies are usedto solve heterogeneity issues in sensor measurements. Their main goal is to propose a shared dataschema that precisely describes measurements to ease their interpretations. These descriptions arereusable by any machine and understandable by humans. The context-aware system also containsa decision support system based on a rules inference engine. We propose two new ontologies:The Context-Aware System Ontology addresses the development of the context-aware systemin general. The Irrigation ontology automates a manual irrigation method named IRRINOV®.These ontologies reuse well-known ontologies such as the Semantic Sensor Network (SSN) and SmartAppliance REFerence (SAREF). The decision support system uses a set of rules with ontologies toinfer daily irrigation decisions for farmers. This project uses real experimental data to evaluate theimplementation of the decision support system.Keywords: agriculture; smart irrigation; context-aware system; ontology; rules1. IntroductionIn the agricultural domain, farmers need to observe natural phenomena to engage in appropriateactivities on their fields. For example, in traditional irrigation, farmers go to their fields to examineAppl. Sci. 2020, 10, 1803; doi:10.3390/app10051803 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3517-0945https://orcid.org/0000-0002-3076-5499https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-3459-8568https://orcid.org/0000-0002-7011-4535http://dx.doi.org/10.3390/app10051803http://www.mdpi.com/journal/applsciAppl. Sci. 2020, 10, 1803 2 of 41the crop development stage and measure the soil moisture provided by probes in the soil. Then, theyuse practical experience or follow an irrigation method to estimate manually the water needs of theircrops. Based on their estimations, the farmers decide whether to irrigate the fields. This conventionalapproach has two significant drawbacks. First, irrigation requires daily observations, often made byfarmers. Manual observations are influenced by other factors such as the weather or the situation offarmers. For example, a daily observation could be skipped if the farmer is sick. Second, the resourceshortage problem demands that farmers use water sparingly.Context-aware systems (CASs) can overcome the above-mentioned situations. A CAS uses awireless sensor network (WSN) to monitor environmental phenomena and uses those measurementsfor further processes. In a CAS, context refers to “any information that can characterize the situation ofan entity. An entity could be a person, a place or an object that is considered relevant to the interactionbetween a user and an application, including the user and the application themselves” [1]. A CAShas two contexts: a low-level context and a high-level context [2]. The low-level context containsquantitative data. The high-level context contains qualitative data that synthesize a situation and easethe decision. For example, the statement “soil moisture is 160 centibar (cbar)” presents a low-levelcontext; however, the statement “soil is dry” presents a high-level context.The CAS has some peculiar characteristics. First, sensors in the system are heterogeneous. Eachtype of phenomenon demands a different type of sensor. For example, in agriculture, pluviometersmeasure rain quantity, and tensiometers measure soil moisture. Thus, the CAS should address",
        "publication_date": "2020-03-05",
        "authors": "Quang-Duy Nguyen, Catherine Roussey, María Poveda‐Villalón, Christophe de Vaulx, Jean-Pierre Chanet",
        "file_name": "20250514092819.pdf",
        "file_path": "output/PDFs/20250514092819.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/10/5/1803/pdf?version=1583938924"
    },
    {
        "title": "Applying the LOT Methodology to a Public Bus Transport Ontology aligned with Transmodel: Challenges and Results",
        "implementation_urls": [],
        "doi": "10.3233/sw-210451",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-11-23",
        "authors": "Edna Ruckhaus, Adolfo Antón Bravo, Mario Scrocca, Óscar Corcho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Incentivized Data Mules Based on State-Channels",
        "implementation_urls": [],
        "doi": "10.1109/icbc54727.2022.9805515",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-05-02",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele DrAngelo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Language Resources as Linked Data for the Legal Domain",
        "implementation_urls": [],
        "doi": "10.3233/faia190019",
        "arxiv": null,
        "abstract": "Abstract. This Chapter describes a four-stage methodology to generate LinguisticLinked Data for the legal domain: identification, creation, transformation (to RDF)and linking. The goal of this process is to enhance the presence of legal languageresources in the Linguistic Linked Open Data cloud. Since this Chapter is framedwithin the H2020 LYNX project, aimed at creating a Legal Knowledge Graph, aparallel objective is to employ the resources generated as a linguistic foundation toannotate, classify and translate the legal resources represented in this graph.Keywords. legal language resources, terminology extraction, linked data, legalknowledge graph1. IntroductionOriginally, language resources have been considered as works that collect any type oflinguistic information. For the purposes of this work, we define language resources aspieces of data containing linguistic information in machine readable forms. There areseveral types of language resources depending on their format and the type of infor-mation represented: glossaries and terminologies (specialised terms), lexical databases(linguistic knowledge for computers), dictionaries (general terms), thesauri (hierarchi-cal controlled vocabularies), etc. Many general dictionaries are available online, such asMerriam Webster1 and Oxford Dictionary2; other terminological resources containingspecialised knowledge can also be found on the Internet, such as TermSciences3 andUNterm4. However, language resources for the legal domain are not that present in theWeb, since they tend to be owned by legal publishers, thus, not accessible and some-times published in obsolete and proprietary formats. Moreover, legal jargon is intricateand the meaning of terms varies as the legal framework changes. Updating non-machinereadable legal glossaries is a time-consuming and difficult task to accomplish. On theother hand, a good understanding of legal terminology is essential to comprehend legaldocumentation, which also tends to be outdated.To soften the mentioned hindrances regarding legal terminology and legal documen-tation, the LYNX project aims at creating a Legal Knowledge Graph (LKG), that is in-1https://www.merriam-webster.com/.2https://www.oxforddictionaries.com/.3http://www.termsciences.fr/termsciences/?lang=en.4https://unterm.un.org/UNTERM/portal/welcome.aKnowledge of the Law in the Big Data AgeG. Peruginelli and S. Faro (Eds.)© 2019 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA190019170terlinking public and private legal resources, metadata, standards and general open datafrom the legal domain. The idea is to offer access to updated multilingual and multi-jurisdictional legal information. For that purpose, a steady open-access legal languagefoundation is required.Linked Data [1] is a particularly convenient form to create such a language cloud,since it is intended to publish interlinked machine-readable data in open-source and non-proprietary formats. In fact, the Linguistic Linked Open Data (LLOD)5 cloud gatherslanguage resources published according to the Linked Data Principles [2], following theRDF set of W3C specifications. Again, within this cloud, legal knowledge is underrep-resented. The objective of this contribution is to create the Linguistic Legal Linked Open",
        "publication_date": "2019-01-01",
        "authors": "Martín-Chozas Patricia, M. Silva Elena, Rodríguez-Doncel Víctor",
        "file_name": "10!3233%faia190019.pdf",
        "file_path": "output/PDFs/10!3233%faia190019.pdf",
        "pdf_link": null
    },
    {
        "title": "An estimation of heavy-duty vehicle fleet CO2 emissions based on sampled data",
        "implementation_urls": [],
        "doi": "10.1016/j.trd.2021.102784",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-03-25",
        "authors": "Nikiforos Zacharof, Georgios Fontaras, Biagio Ciuffo, Alessandro Tansini, Ignacio-Iker Prado-Rujas",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards Automated Hypothesis Testing in Neuroscience",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-33752-0_18",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Shobeir Fakhraei, Varun Ratnakar, Qifan Yang, Hanna Endrias, Yibo Ma, Regina Wang, Michael M. Bornstein, Joanna K. Bright, Yolanda Gil, Neda Jahanshad",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards the Assessment of Easy-to-Read Guidelines Using Artificial Intelligence Techniques",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-58796-3_10",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Edna Ruckhaus, Jorge López-Guerrero, Maria Isabel Nogueira Cano, A. S. J. Cervera",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "PSM-Flow: Probabilistic Subgraph Mining for Discovering Reusable Fragments in Workflows",
        "implementation_urls": [],
        "doi": "10.1109/wi.2018.00-93",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-12-01",
        "authors": "Chin Wang Cheong, Daniel Garijo, Cheung Kwok Wai, Yolanda Gil",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Taxi dispatching strategies with compensations",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2019.01.001",
        "arxiv": "2401.11553",
        "abstract": "Abstract Urban mobility efficiency is of utmost importance in big cities. Taxi vehicles are key elements in daily traffic activity. The advance of ICT and geo-positioning systems has given rise to new opportunities for improving the efficiency of taxi fleets in terms of waiting times of passengers, cost and time for drivers, traffic density, CO2 emissions, etc., by using more informed, intelligent dispatching. Still, the explicit spatial and temporal components, as well as the scale and, in particular, the dynamicity of the problem of pairing passengers and taxis in big towns, render traditional approaches for solving standard assignment problem useless for this purpose, and call for intelligent approximation strategies based on domain-specific heuristics. Furthermore, taxi drivers are often autonomous actors and may not agree to participate in assignments that, though globally efficient, may not be sufficently beneficial for them individually. This paper presents a new heuristic algorithm for taxi assignment to customers that considers taxi reassignments if this may lead to globally better solutions. In addition, as such new assignments may reduce the expected revenues of individual drivers, we propose an economic compensation scheme to make individually rational drivers agree to proposed modifications in their assigned clients. We carried out a set of experiments, where several commonly used assignment strategies are compared to three different instantiations of our heuristic algorithm. The results indicate that our proposal has the potential to reduce customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point of view.  Keywords: Coordination, dynamic fleet management, dynamic optimization, multi-agent systems, open systems, taxi assignment.  1. Introduction Urban mobility is one of the main concerns that public managers face in big cities nowadays. Traffic congestions generate a high quantity of CO2 emissions and cause extra time spent by travelers. One of the main actors involved in the daily traffic activity in urban areas are taxi fleets. They consist of several thousands of vehicles in big cities (e.g. about 15,000 taxis in Madrid, Spain). They are usually affiliated to mediator services, which coordinate service calls and taxi dispatching. Lately, new mobility systems that benefit from the advances in information and communication technologies have emerged, such as Uber1, Lyft2 or Liftago3 among others. Two of the main goals of a taxi fleet are (i) to reduce the response time (e.g., the time between a customer call and the moment a taxi arrives at the customer’s location) and (ii) reduce costs of empty movements (e.g., movements taxis have to make in order to get to the location of customers). The provision of efficient methods for taxi assignment to customers is a challenge that can contribute to reducing distances of empty trips with the resulting decrease of traffic flow, pollution, time and so on. Typically, taxi fleet coordination companies apply the first-come first-serve strategy to assign taxis to customers. Once the taxi accepts the passenger, the dispatching is irreversible. This method is known to be inefficient (Egbelu & Tanchoco, 1984).  1 http://www.uber.com 2 http://www.lyft.com 3 http://www.liftago.com https://doi.org/10.1016/j.eswa.2019.01.001 2 The aforementioned case falls into a specific class of assignment problems which is characterized by a dynamic demand in time and space. To efficiently solve such problems, dynamic algorithms are required instead of classical assignment optimization methods. For this purpose, techniques from the field of intelligent systems are promising, because they allow for developing heuristics-based algorithms that intelligently prune the search space, so as to reduce the computational complexity and to support a sufficient degree of scalability. Furthermore, taxi drivers are usually autonomous actors, i.e. they can freely choose whether to accept or to reject a recommendation proposed by the mediator service, which puts additional ",
        "publication_date": "2019-01-03",
        "authors": "Holger Billhardt, Alberto Fernández, Sascha Ossowskí, Javier Palanca, Javier Bajo",
        "file_name": "10!1016%j!eswa!2019!01!001.pdf",
        "file_path": "output/PDFs/10!1016%j!eswa!2019!01!001.pdf",
        "pdf_link": null
    },
    {
        "title": "Improving the Results of Citizen Science Projects Through Reputation Systems: The Case of Wolf’s Number Experiment",
        "implementation_urls": [],
        "doi": "10.1109/access.2020.3030006",
        "arxiv": null,
        "abstract": "ABSTRACT Collective intelligence projects based on citizen participation are gaining momentum in today’ssociety. Citizen science applies crowdsourcing techniques to produce reliable data, quickly and easily. Theseprojects allow getting new knowledge and help professional scientists to come to real conclusions. This paperproposes that the use of reputation systems improves the results obtained in citizen science projects. To provethis hypothesis, a reputation system is applied to a real experiment and the results are analyzed. The goal ofthe experiment is to calculate the real-time solar activity, known as the Wolf number, using the infrastructureand user community of the GLORIA project (a set of professional robotic telescopes running since 2013).The sample size of the study are 196 end-users and 2,108 executions of the experiment. The key findingspresented in the paper are: 1) the online experiment with volunteers correctly reproduces the traditionalmethod of the year 1848 performed by astronomers or advanced amateurs, 2) the model is contrasted andvalidated with the values published by the official organization, and 3) the reputation system reduces theerror in calculations by more than half, discarding the contributions of the users with lowest karma.INDEX TERMS Citizen science, collective intelligence, crowdsourcing, reputation system, solar activity.I. INTRODUCTIONIn 1848, the Swiss astronomer Rudolf Wolf introduced amethod for registering solar activity by counting the numberof visible sunspots, known as the Wolf number, also knownworldwide as the International Sunspot Index. This method,updated in 2014 by several authors [1], has been used in thesolar activity records for the last 400 years [2] and continuesto be a rigorous method used in much research today [3], [4].Since 1981, the Solar Influences Data Analysis Center(SIDC) is the world data center for the Wolf number. Its mis-sion, as is indicated in its webpage,1 is to advance knowledgeabout the Sun and its influence on the solar system, throughresearch and observations.The associate editor coordinating the review of this manuscript andapproving it for publication was Feng Xia .1Website of the Solar Influences Data Analysis Center (SIDC):www.sidc.beThe SIDC produces the longest running time-series ofsolar activity, under the SILSO (Sunspot Index and Long-termSolar Observations) project.2 The data are freely availablefrom their website, as shown in Fig. 1, where is representedthe daily sunspot number (yellow), monthly mean sunspotnumber (blue), smoothed monthly sunspot number (red) forthe last 13 years and 12-month ahead predictions. It servesas a reference input to multiple applications in a wide rangeof scientific disciplines, such as studies of the solar cyclemechanism and of the solar forcing on the Earth’s climate.An important community is subscribed to its services, amongthem more than 500 for sunspot products; individuals (e.g.wider public, radio amateurs, space scientists, meteorologistsand paleo-climatologists); and institutions (e.g. public orga-nizations like the International Astronomical Union (IAU),2SILSO, World Data Center - Sunspot Number and Long-term SolarObservations, Royal Observatory of Belgium, on-line Sunspot Number cat-alogue: http://www.sidc.be/SILSO/, 2020.186026 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020https://orcid.org/0000-0002-4361-4331",
        "publication_date": "2020-01-01",
        "authors": "R. Cedazo, Esteban González, M. Serra‐Ricart, Alberto Brunete",
        "file_name": "10!1109%access!2020!3030006.pdf",
        "file_path": "output/PDFs/10!1109%access!2020!3030006.pdf",
        "pdf_link": null
    },
    {
        "title": "A review of mobile sensing systems, applications, and opportunities",
        "implementation_urls": [],
        "doi": "10.1007/s10115-019-01346-1",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-03-08",
        "authors": "Francisco Laport, Emilio Serrano, Javier Bajo, Andrew T. Campbell",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Traffic Optimization Through Waiting Prediction and Evolutive Algorithms",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.12.001",
        "arxiv": null,
        "abstract": "AbstractTraffic optimization systems require optimization procedures to optimize traffic light timing settings in order to improve pedestrian and vehicle mobility. Traffic simulators allow obtaining accurate estimates of traffic behavior by applying different timing configurations, but require considerable computational time to perform validation tests. For this reason, this project proposes the development of traffic optimizations based on the estimation of vehicle waiting times through the use of different prediction techniques and the use of this estimation to subsequently apply evolutionary algorithms that allow the optimizations to be carried out. The combination of these two techniques leads to a considerable reduction in calculation time, which makes it possible to apply this system at runtime. The tests have been carried out on a real traffic junction on which different traffic volumes have been applied to analyze the performance of the system.DOI:  10.9781/ijimai.2023.12.001Traffic Optimization Through Waiting Prediction and Evolutive AlgorithmsFrancisco García1, Helena Hernández2, María N. Moreno-García2, Juan F. De Paz1*, Vivian F. López2, Javier Bajo31 Expert Systems and Applications Lab. University of Salamanca. Plaza de los Caídos s/n. Salamanca (Spain)2 Data Mining Research Group. University of Salamanca Plaza de los Caídos s/n. Salamanca (Spain)3 Department of Artificial Intelligence, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)Received 10 May 2022 | Accepted 22 September 2023 | Early Access 5 December 2023 I.\t IntroductionACCORDING to United Nations data, in 2018 55% of the population was living in urban spaces, the distribution of the urban population varies considerably by region: Northern America 82%, Latin America and the Caribbean 81%, Europe 74%, Oceania 68%, Asia 60% and Africa 43%. The urban population is continuously increasing; it is estimated that 66% of the population will live in urban areas by 2050, an increase of 16% compared to 2008 [1]. These data are very similar to those provided by the United Nations organization since in 2018 it estimated that 68% of the population will live in urban areas in 2050. This increase implies greater traffic congestion in cities due to both the increase in traffic and the unsuitable infrastructures [1].  For this reason, programs such as Horizonte Europa have analyzed global challenges such as climate, energy and mobility, and in particular, intelligent mobility through the optimization of infrastructures. Due to this increase in population and the need to improve infrastructure management, there is a demand to create systems capable of improving traffic efficiency, which will be applied in this project.Traditional operational research incorporates the use of queuing theory to make predictions about different parameters such as waiting times [2]. The queuing theory approach in which there are usually M/G/s models [3] where M refers to the arrival of vehicles which is represented by a poisson, G the service rate which in certain cases can be modeled by an exponential and finally, s represents the number of servers.  From these definitions, it is possible to determine parameters such as waiting times, which will be the object of study of this project. However, classical queuing theory would not take into account parameters that need to be considered, such as the time lost from the moment a traffic light turns green until the cars start moving. For these reasons, simulators such as SUMO [4] are currently being used ",
        "publication_date": "2023-01-01",
        "authors": "Francisco García, Helena Hernández, Marı́a N. Moreno Garcı́a, Juan F. De Paz, Vivian F. López, Javier Bajo",
        "file_name": "10!9781%ijimai!2023!12!001.pdf",
        "file_path": "output/PDFs/10!9781%ijimai!2023!12!001.pdf",
        "pdf_link": null
    },
    {
        "title": "Developing Ontologies within Decentralised Settings",
        "implementation_urls": [],
        "doi": "10.1007/978-1-4419-5908-9_4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2010-01-01",
        "authors": "Alexander García, Kieran O’Neill, Leyla García, Phillip Lord, Robert Stevens, Óscar Corcho, F. Gibson",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Mind the gap: The AURORAL ecosystem for the digital transformation of smart communities and rural areas",
        "implementation_urls": [],
        "doi": "10.1016/j.techsoc.2023.102304",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-07-08",
        "authors": "Oihane Gómez–Carmona, David Buján, Diego Casado–Mansilla, Diego López–de–Ipiña, Juan Cano-Benito, Andrea Cimmino, María Poveda‐Villalón, Raúl García‐Castro, Jorge Almela-Miralles, Dimitris Apostolidis, Anastasios Drosou, Dimitrios Tzovaras, Martin Wagner, María Guadalupe-Rodriguez, Diego Salinas, David Esteller, Martí Riera-Rovira, Arnau González, Jaime Clavijo-Ágreda, Alberto Díez-Frias, María del Carmen Bocanegra-Yáñez, Rui Pedro-Henriques, Elsa Ferreira-Nunes, Marian Lux, Nikol Bujalkova",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Governing Decentralized Complex Queries Through a DAO",
        "implementation_urls": [],
        "doi": "10.1145/3462203.3475910",
        "arxiv": "2107.06790",
        "abstract": null,
        "publication_date": "2021-08-19",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Bringing Federated Semantic Queries to the GIS-Based Scenario",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/20250514093022.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "A detailed explanation of connecting our approach and QGIS is available on https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS (accessed on 23 November 2021)."
                    }
                ]
            }
        ],
        "doi": "10.3390/ijgi11020086",
        "arxiv": null,
        "abstract": "Abstract: Geospatial data is increasingly being made available on the Web as knowledge graphsusing Linked Data principles. This entails adopting the best practices for publishing, retrieving, andusing data, providing relevant initiatives that play a prominent role in the Web of Data. Despitethe appropriate progress related to the amount of geospatial data available, knowledge graphsstill face significant limitations in the GIScience community since their use, consumption, andexploitation are scarce, especially considering that just a few developments retrieve and consumegeospatial knowledge graphs from within GIS. To overcome these limitations and address somecritical challenges of GIScience, standards and specific best practices for publishing, retrieving, andusing geospatial data on the Web have appeared. Nevertheless, there are few developments andexperiences that support the possibility of expressing queries across diverse knowledge graphs toretrieve and process geospatial data from different and distributed sources. In this scenario, wepresent an approach to request, retrieve, and consume (geospatial) knowledge graphs available atdiverse and distributed platforms, prototypically implemented on Apache Marmotta, supportingSPARQL 1.1 and GeoSPARQL standards. Moreover, our approach enables the consumption ofgeospatial knowledge graphs through a lightweight web application or QGIS. The potential of thiswork is shown with two examples that use GeoSPARQL-based knowledge graphs.Keywords: GeoSPARQL; SPARQL; federated query; knowledge graph; geospatial data1. IntroductionGeospatial data is increasingly being made available on the Web [1] in the form ofknowledge graphs in the Semantic Web, often using Linked Data principles [2]. To achievethese knowledge graphs, (geospatial) resources need to be identified using HTTP URIs,indexed by search engines, and connected, or linked, to other resources [3]. Therefore, thisentails adopting the best practices for publishing, retrieving, and using data on the Web [2,3].These best practices are being embraced by a growing number of data providers, leading tobuilding a global data space containing billions of assertions—the Web of Data [3].The transformation and publication of geospatial data as knowledge graphs were pioneeredby initiatives such as GeoNames (http://www.geonames.org/ontology/documentation.html(accessed on 23 November 2021)), OpenStreetMap [4], and Ordnance Survey [5]. After theseinitiatives, many geospatial datasets have been published in the Web of Data (http://lod-cloud.net/ (accessed on 23 November 2021)). This has entailed geospatial data playing a pre-eminentrole in the Web of Data cloud, operating as central nexuses that interconnect events, people, andobjects [6] and offering an ever-growing semantic representation of the geospatial informationwealth [7].Despite the relevant progress related to the amount of data available, geospatial knowl-edge graphs still face significant limitations in the GIScience community since their use,consumption, and exploitation are really scarce in this community, especially consideringISPRS Int. J. Geo-Inf. 2022, 11, 86. https://doi.org/10.3390/ijgi11020086 https://www.mdpi.com/journal/ijgihttps://doi.org/10.3390/ijgi11020086https://doi.org/10.3390/ijgi11020086https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-5799-469Xhttp://www.geonames.org/ontology/documentation.htmlhttp://lod-cloud.nethttp://lod-cloud.nethttps://doi.org/10.3390/ijgi11020086https://www.mdpi.com/journal/ijgi",
        "publication_date": "2022-01-25",
        "authors": "Oswaldo Páez, Luis M. Vilches‐Blázquez",
        "file_name": "20250514093022.pdf",
        "file_path": "output/PDFs/20250514093022.pdf",
        "pdf_link": "https://www.mdpi.com/2220-9964/11/2/86/pdf?version=1643204664"
    },
    {
        "title": "Annotador: a temporal tagger for Spanish",
        "implementation_urls": [],
        "doi": "10.3233/jifs-179865",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-08-31",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Modelling a smart environment for nonintrusive analysis of attention in the workplace",
        "implementation_urls": [],
        "doi": "10.1111/exsy.12275",
        "arxiv": null,
        "abstract": "Abstract Nowadays, the world is getting increasingly competitive and the quality and the amount of the work presented is one of the decisive factors when choosing an employee. It is no longer necessary to only perform but, to achieve a product with quality, on time, at the lowest possible cost and with the minimum resources. For this reason, the employee must have a high score of attention when performing a task and the factors that influence attention negatively must be reduced. This is true in many different domains, from the workplace to the classroom. In this paper we present a nonintrusive smart environment for monitoring people’s attention when working in teams. The presented system provides real-time information about each individual, as well as information about the team. It can be very useful for team managers to identify potentially distracting events or individuals since when the attention of an individual is not at its best when performing the proposed task her/his performance will be negatively affected, with consequences for the individual as well as for the organization. Keywords Smart Environment, Attention Behavior, Non-intrusive, Distributed Computing.  1. INTRODUCTION The rapid progress of wireless communication and sensing technologies enabled the development of smart learning environments, which are able to detect the environmental context as well as quantifying the attention of a worker in his workplace. For this reason, making intelligent learning systems has been the objective of many researchers in the field of computer science. In the field of computer science, a smart environment is a digitally augmented physical world where sensor-enabled and networked devices work continuously and collaboratively to make the lives of the inhabitants more comfortable. Indeed, significant advances in smart devices, wireless mobile communications, sensor networks, pervasive computing, machine learning, robotics, middleware and agent technologies, and human computer interfaces have made the dream of smart environments a reality. In this concept, the word “smart” means the ability to autonomously acquire and apply knowledge, and the word “environment” means our surroundings (Cook and Das, 2005). With this technological evolution, job offers have changed, bringing along many significant and broad changes. Some of the most notorious ones can be pointed out by the emergence of indicators such as attentiveness which, in extreme cases, can compromise the life and well-being of the workers. In more moderate cases it will impair attention, general cognitive skills and productivity. In addition to these factors, many of these jobs are the so-called desk-jobs, in which people frequently sit for more than 8 hours (Liao and Drury, 2000). Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. While the true nature of this relationship is yet to be thoroughly studied (properly contextualized in each work domain), there are other ",
        "publication_date": "2018-06-05",
        "authors": "Dalila Durães, Davide Carneiro, Javier Bajo, Paulo Nováis",
        "file_name": "10!1111%exsy!12275.pdf",
        "file_path": "output/PDFs/10!1111%exsy!12275.pdf",
        "pdf_link": null
    },
    {
        "title": "Agent-based tool to reduce the maintenance cost of energy distribution networks",
        "implementation_urls": [],
        "doi": "10.1007/s10115-017-1120-7",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2017-10-23",
        "authors": "Pablo Chamoso, Juan F. De Paz, Javier Bajo, Gabriel Villarrubia González",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Potentially inappropriate medications in older adults living with HIV",
        "implementation_urls": [],
        "doi": "10.1111/hiv.12883",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2020-06-09",
        "authors": "Beatriz López Centeno, Carlos Badenes-Olmedo, Angel Mataix-San-Juan, José M. Bellón, Leire Pérez‐Latorre, JC López, Juana Benedı́, SH Khoo, Catia Marzolini, MJ Calvo‐Alcántara, Juan Berenguer",
        "file_name": "10!1111%hiv!12883.pdf",
        "file_path": "output/PDFs/10!1111%hiv!12883.pdf",
        "pdf_link": null
    },
    {
        "title": "A Case-Based Reasoning Model Powered by Deep Learning for Radiology Report Recommendation",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2021.08.011",
        "arxiv": null,
        "abstract": "AbstractCase-Based Reasoning models are one of the most used reasoning paradigms in expert-knowledge-driven areas. One of the most prominent fields of use of these systems is the medical sector, where explainable models are required. However, these models are considerably reliant on user input and the introduction of relevant curated data. Deep learning approaches offer an analogous solution, where user input is not required. This paper proposes a hybrid Case-Based Reasoning, Deep Learning framework for medical-related applications, focusing on the generation of medical reports. The proposal combines the explainability and user-focused approach of case-based reasoning models with the deep learning techniques performance. Moreover, the framework is fully modular to fit a wide variety of tasks and data, such as real-time sensor captured data, images, or text, to name a few. An implementation of the proposed framework focusing on radiology report generation assistance is provided. This implementation is used to evaluate the proposal, showing that it can provide meaningful and accurate corrections, even when the amount of information available is minimal. Additional tests on the optimization degree of the case base are also performed, evidencing how the proposed framework can optimize this base to achieve optimal performance.* Corresponding author.E-mail addresses: eamador@fi.upm.es (E. Amador-Domínguez), emilioserra@fi.upm.es (E. Serrano), daniel.manrique@upm.es (D. Manrique), jbajo@fi.upm.es (J. Bajo).DOI:  10.9781/ijimai.2021.08.011I.\t IntroductionDeep Learning is currently a fundamental approach in Artificial Intelligence applied to the medical domain. Their applications include image segmentation [1]–[3], 3D image reconstruction [4], [5], and disease diagnosis [6], [7]. While these approaches offer outstanding results, they suffer from a considerable flaw: lack of explainability. This issue is particularly concerning in the medical domain, where it is crucial to understand the inference procedure carried by a model to perform a task. Moreover, deep learning-based approaches require a considerable amount of labelled data to be truly accurate, which may not always be available.Opposite to this approach, the Case-Based Reasoning (CBR) methodology [8], [9] provides computational models closely related to human reasoning. In CBR, the resolution of problems provides knowledge that permits to solve new, similar ones. A CBR model discovers the closest situation to the current one to solve and adapt its solution to fit the present scenario. One of CBR’s essential advantages is that it is easy to follow and understand the inference process they conduct, which has prompted its use in, for example, the medical domain [10], [11]. This paper proposes a hybrid CBR-deep learning model to tackle the problem of radiology report writing assistance. The main efforts in the radiology domain reside within image-related tasks, such as diagnosis or X-ray image segmentation. In this image-dominated field, medical reports play a secondary role, mostly used to support the aforementioned tasks. Thus, high quality labelled textual data in this domain may not always be available, which hinders the use of deep learning techniques.The proposed approach uses a CBR model to work with a few cases that can scale up, assisted by deep learning models to improve its performance. Therefore, it is a blended solution between a knowledge-based system [12], where the knowledge must be elicited, and a deep ",
        "publication_date": "2021-01-01",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Javier Bajo",
        "file_name": "10!9781%ijimai!2021!08!011.pdf",
        "file_path": "output/PDFs/10!9781%ijimai!2021!08!011.pdf",
        "pdf_link": null
    },
    {
        "title": "How to Validate Ontologies with Themis",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-32327-1_11",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Optimizing Automated Term Extraction for Terminological Saturation Measurement.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Assessing the completeness of a document collection, within a domain of interest, is a complicated task that requires substantial effort. Even if an auto-mated technique is used, for example, terminology saturation measurement based on automated term extraction, run times grow quite quickly with the size of the input text. In this paper, we address this issue and propose an optimized approach based on partitioning the collection of documents in disjoint constituents and computing the required term candidate ranks (using the c-value method) inde-pendently with subsequent merge of the partial bags of extracted terms. It is proven in the paper that such an approach is formally correct – the total c-values can be represented as the sums of the partial c-values. The approach is also vali-dated experimentally and yields encouraging results in terms of the decrease  of the necessary run time and straightforward parallelization without any loss  in quality.  Keywords: Automated term extraction, terminological saturation, partial  c-value, merged-partial c-value, optimization 1 Introduction Ontology learning from texts is a developing research field that aims to extract domain description theories from text corpora. It is increasingly acknowledged as a plausible alternative to ontology development based on the interviews of domain knowledge stakeholders. One shortcoming of learning an ontology from texts is that the input cor-pus has to be quite big for being representative for the subject domain. Another short-coming is that learning ontologies from text is expensive, in terms of taken time, as it involves the use of several algorithms, in a pipeline [1], that are computationally hard.   https://orcid.org/0000-0001-6157-8111https://orcid.org/0000-0002-5159-254Xmailto:gen.dobr@gmail.commailto:vadim@ermolayev.commailto:dchaves@fi.upm.esmailto:egorfedorencko@gmail.comAutomated term extraction (ATE) is an essential step at the beginning of the pipeline for ontology learning [1, 2], that is known to be bulky in terms of the increase of the run time with the growth of the input text corpus. Therefore, finding a way to reduce: (i) either the size of the processed text; or (ii) the time spent for term extraction; or (iii) both is of importance.  In our prior work [2, 3, 4, 5], we developed the ATE-based approach (OntoElect) that helps circumscribe the minimal possible representative part of a documents collec-tion, which forms the corpus for further ontology learning. This technique is based on measuring terminological saturation in the collection of documents, which is computa-tionally quite expensive in the terms of the run time.  In this paper, we present the approach, based on the partitioning of a document col-lection, which allows substantially reducing ATE run time in the OntoElect processing pipeline.  The remainder of the paper is structured as follows. In Sect. 2, we outline our Onto-Elect approach to detect terminological saturation in document collections describing a subject domain. In Sect. 3, we review the related work in ATE and argue for the choice of the c-value method as the best appropriate for measuring terminological saturation. In Sect. 4, we explain our motives to optimize the c-value method based on partitioning a document collection and present a formal framework for that. Section 5 reports on the setup and results of our experimental evaluation of the proposed optimization approach. Finally, we draw the conclusions and outline our plans for the future work in Sect. 6.  ",
        "publication_date": "2019-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Hennadii Dobrovolskyi, Egor Fedorenko, Vadim Ermolayev",
        "file_name": "20250514093145.pdf",
        "file_path": "output/PDFs/20250514093145.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2387/20190001.pdf"
    },
    {
        "title": "Automating ontology engineering support activities with OnToology",
        "implementation_urls": [
            {
                "identifier": "https://github.com/GeorgFerdinandSchneider/bot",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1016%j!websem!2018!09!003.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Electronic copy available at: https://ssrn.com/abstract=3260516 Pr ep rin t n ot p ee r r ev ie w ed supported by the integration of WIDOCO [12], a standalone application for generating HTML documentation for an individual ontol-ogy."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.09.003",
        "arxiv": null,
        "abstract": "AbstractDue to the increasing uptake of semantic technologies, ontologies are now part of a good number of informa-tion systems. As a result, software development teams that have to combine ontology engineering activitieswith software development practices are facing several challenges, since these two areas have evolved, in gen-eral, separately. In this paper we present OnToology, an approach to manage ontology engineering supportactivities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based applicationthat builds on top of Git-based environments and integrates existing semantic web technologies. We havevalidated OnToology against a set of representative requirements for ontology development support activitiesin distributed environments, and report on a survey of the system to assess its usefulness and usability.Keywords: ontology engineering, ontology evaluation, ontology documentation, ontology publication1. IntroductionSince the late 1990s, several ontology engineeringmethodologies have been proposed to transform theart of developing ontologies into an engineering ac-tivity. Methodologies such as METHONTOLOGY[1], On-To-Knowledge [2] and the NeOn Methodol-ogy [3] define clear guidelines, processes, activitiesand life cycles to guide ontology development.Now that ontologies are being increasinglyadopted in information systems, it is clear that on-tology development tasks may also benefit from theapplication of common software engineering prac-tices. Most of the ontology development supportactivities, such as documentation, visualization andevaluation, are usually performed individually, exe-cuting heterogeneous tools that make these activi-ties cumbersome and time consuming. In addition,maintaining and keeping track of the generated re-Email addresses: aalobaid@fi.upm.es (AhmadAlobaid), dgarijo@isi.edu (Daniel Garijo),mpoveda@fi.upm.es (Maŕıa Poveda-Villalón),isantana@fi.upm.es (Idafen Santana-Pérez),albafernandez@fi.upm.es (Alba Fernández-Izquierdo),ocorcho@fi.upm.es (Oscar Corcho)sources for each version of an ontology has becomea challenge for ontology developers.The ontology engineering community has alreadyshown progress towards adapting ontology develop-ment to agile software development methodologies[4, 5, 6]; as well as supporting collaborative ontol-ogy development throughout the use of common-practice software engineering tools [7, 8]. In fact,it is now common among ontology developers touse Git-based environments [9] such as GitHub1(usual in software development) for keeping trackof ontology revisions. However, existing approachespresent either partial solutions; require specializedskills that complicate their adoption (e.g., complexinstallation setup); or produce their outcome usingidiosyncratic formats that are difficult to integrate",
        "publication_date": "2018-10-09",
        "authors": "Ahmad Alobaid, Daniel Garijo, María Poveda‐Villalón, Idafen Santana-Pérez, Alba Fernández-Izquierdo, Óscar Corcho",
        "file_name": "10!1016%j!websem!2018!09!003.pdf",
        "file_path": "output/PDFs/10!1016%j!websem!2018!09!003.pdf",
        "pdf_link": null
    },
    {
        "title": "Calculating Heavy-Duty Truck Energy and Fuel Consumption Using Correlation Formulas Derived From VECTO Simulations",
        "implementation_urls": [],
        "doi": "10.4271/2019-01-1278",
        "arxiv": null,
        "abstract": "Abstract The Vehicle Energy Consumption calculation Tool (VECTO) is used in Europe for calculating standardised energy consumption and CO2 emissions from Heavy-Duty Trucks (HDTs) for certification purposes. The tool requires detailed vehicle technical specifications and a series of component efficiency maps, which are difficult to retrieve for those that are outside of the manufacturing industry. In the context of quantifying HDT CO2 emissions, the Joint Research Centre (JRC) of the European Commission received VECTO simulation data of the 2016 vehicle fleet from the vehicle manufacturers. In previous work, this simulation data has been normalised to compensate for differences and issues in the quality of the input data used to run the simulations. This work, which is a continuation of the previous exercise, focuses on the deeper meaning of the data received to understand the factors contributing to energy and fuel consumption. Fuel efficiency distributions and energy breakdown figures were derived from the data and are presented in this work. Correlation formulas were produced to calculate the energy loss contributions of individual components and resistances (air drag, rolling resistance, axle losses, gearbox losses, etc.) over the Regional Delivery and Long Haul cycles, given a limited number of input parameters such as vehicle characteristics and average component efficiencies. Default values and meaningful ranges of variation of these parameters obtained from the data of the fleet are also reported in this work. The importance of air drag and rolling resistance losses are highlighted since these losses account for about 70% of the energy consumed downstream the engine. Finally, based on the correlation formulas to calculate the individual energy losses, a method is presented that calculates the final energy consumption and CO2 emissions for all the regulated HDTs classes and that does not rely on the use of VECTO. Introduction The CO2 certification for Heavy-Duty Trucks (HDTs) was set to start from January 2019 [1] and the European Commission is focused on determining the current levels of CO2 emissions of these vehicles under different operating conditions [2]. Within the context of regulating emissions, attention is also given on the primary sources of energy losses during operation and the identification of the margins for their improvement [3]. Regulators, manufacturers and other stakeholders are interested in quantifying the reduction potentials and creating realistic scenarios for technology diffusion that would promote CO2 emissions reduction from road transport. In order to investigate the CO2 emissions reduction potential, it was required to define a reliable reference basis for certification and monitoring, which was achieved with the development of the Vehicle Energy Consumption calculation Tool (VECTO) [4–6]. VECTO is a vehicle simulation software that calculates energy consumption (EC), fuel consumption (FC) and CO2 emissions from HDTs. To comply with this task, and with the accuracy requested, VECTO adopts a sophisticated simulation approach that is based on certified ",
        "publication_date": "2019-04-02",
        "authors": "Alessandro Tansini, Georgios Fontaras, Biagio Ciuffo, Federico Millo, Ignacio-Iker Prado-Rujas, Nikiforos Zacharof",
        "file_name": "10!4271%2019-01-1278.pdf",
        "file_path": "output/PDFs/10!4271%2019-01-1278.pdf",
        "pdf_link": null
    },
    {
        "title": "Encoding of Media Value Chain Processes Through Blockchains and MPEG-21 Smart Contracts for Media",
        "implementation_urls": [],
        "doi": "10.1109/mmul.2023.3303393",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-08-11",
        "authors": "Mirko Zichichi, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An Ontology-Based Deep Learning Approach for Knowledge Graph Completion with Fresh Entities",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-23887-2_15",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-06-21",
        "authors": "Elvira Amador-Domínguez, Patrick Hohenecker, Thomas Lukasiewicz, Daniel Manrique, Emilio Serrano",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Construcción de una red de ontologías sobre eventos meteorológicos a partir de periódicos históricos",
        "implementation_urls": [],
        "doi": "10.1590/1678-9865202032e180077",
        "arxiv": null,
        "abstract": "AbstractCurrently, only a few people would deny the value of newspaper content for understanding issues associated with politics, culture, and society. Therefore, the digitalization of the newspapers’ archives allows retrieving outstanding historical and cultural articles. However, there is a large amount of “minor data” hidden within these newspapers. This paper addresses the challenge for accessing and dealing with the resources of National newspaper and periodicals libraries from Colombia, Ecuador, México and Uruguay, which TransInformação, Campinas, 32:e180077, 2020 http://dx.doi.org/10.1590/1678-9865202032e180077L. M. VILCHES-BLÁZQUEZ  et al.2collect newspapers where news about meteorological events from the XIX-XX centuries were posted. A news corpus is developed on these newspapers, which through technical readings and a bibliomining process using different tools allow building an ontology network. This network is composed of different modules (technical, general and news), which are built using different approaches (top-down and bottom-up) and methodologies (Methontology and NeOn), for providing a common and sharing understanding of the historical meteorological events in Latin America. Hence, this work entails an approach to take the newspaper and periodicals libraries to Semantic Web.Keywords: Bibliomining. Meteorology. Ontology. Newspaper.IntroducciónLos periódicos se encuentran entre las fuentes más valiosas para los estudiosos interesados en investigar la opinión pública y su configuración en el transcurso del tiempo. No obstante, entre algunos investigadores hubo cierto escepticismo sobre la utilización de periódicos históricos como fuente de información, debido a su dudosa precisión y naturaleza efímera (Bingham, 2010). Sin embargo, en la actualidad, pocos niegan el valor del contenido del periódico para comprender cuestiones relacionadas con política, cultura y sociedad, así como para conocer cómo los desarrollos e ideas se percibieron y se extendieron por diferentes países (Bingham, 2010; Neudecker; Antonacopoulos, 2016). Por todo ello, los periódicos facilitan un nuevo punto de acceso a la información que se almacena en recursos históricos (Smits, 2014), proporcionando acceso a una amplia gama de fuentes de información.Uno de los desarrollos más útiles para los historiadores modernos es la digitalización de los archivos de periódicos (Bingham, 2010), ya que permite rescatar artículos históricos y culturales relevantes y centrar el esfuerzo de análisis y descripción en ciertas publicaciones. Además, existen infinidad de “datos menores” ocultos en estos periódicos, por lo que se convierte en un desafío acceder a ellos puesto que pueden suministrar valiosa información no recogida en otros documentos.La creciente digitalización proporciona “nuevos” desafíos técnicos, ya que frecuentemente no se aplican tecnologías de reconocimiento de texto sobre estos recursos de información (Neudecker; Antonacopoulos, 2016). Esto obstaculiza la posibilidad de hacerlos más accesibles a otras instituciones y dificulta la aplicación de tecnologías para la extracción de eventos en formas narrativas o reconstrucción y análisis de patrones (Wijfjes, 2017) que aporten mayor claridad al contexto histórico recogido en los periódicos. No obstante, refleja que el acceso a la información ha cambiado y que existen nuevos modelos de producción, distribución y consumo, pues la información digital presenta características distintas a las tradicionales (Rodríguez García, 2016).Hoy resulta común hablar sobre la World Wide Web, Internet, bibliotecas digitales, repositorios digitales, XML, Web 2.0, Web Semántica, entre otros tópicos. Técnicamente, muestran cómo las tecnologías de la información protagonizan el escenario con relación al acceso a la información en la biblioteca (Rodríguez García, 2016). En este contexto de la Web y las bibliotecas digitales, Chowdhury y Chowdhury (2007) aclaran que las ontologías juegan un papel significativo porque sus mecanismos permiten el análisis del significado de los recursos, favoreciendo su desempeño en el proceso de acceso a la información al organizar y reunir la información heterogénea contenida en los recursos digitales.El término ontología ha sido objeto de estudio en diferentes áreas de investigación y en varios dominios ",
        "publication_date": "2020-01-01",
        "authors": "Luis M. Vilches‐Blázquez, Diana Comesaña, Lorena de Jesús Arrieta Moreno",
        "file_name": "10!1590%1678-9865202032e180077.pdf",
        "file_path": "output/PDFs/10!1590%1678-9865202032e180077.pdf",
        "pdf_link": null
    },
    {
        "title": "Participation of women in doctorate, research, innovation, and management activities at Universidad Politécnica de Madrid: analysis of the decade 2006–2016",
        "implementation_urls": [],
        "doi": "10.1007/s11192-019-03179-9",
        "arxiv": null,
        "abstract": "AbstractThis article studies the participation of women in doctorate, lecturing and research, innova-tion, and management activities at Universidad Politécnica de Madrid (UPM), the most important and largest university in Spain devoted to engineering and architecture. The analyses revealed significant differences in the ratio of male (76%) and female (24%) lec-turing and research staff. This unequal ratio conducted to women underrepresentation in other actions such as coordination of international projects, decision-making designations, patenting and software licensing, collaboration with companies, and PhD supervision. PhD enrolment and PhD defence data, disaggregated by gender and by technological area, were also analysed as they are the starting point of the academic career, and showed a wide-spread male prevalence over women (ca. 70% men vs. 30% women). The aim of this paper is to present actual, accurate, objective, and gender-segregated information extracted from UPM databases, to carry out a qualitative study drawing on an opinion survey and a “gap” analysis, and to undertake a critical examination of the historic, political, sociocultural and personal factors affecting gender inequalities in academia. Policy recommendations to improve the situation of women and to achieve gender balance in the disciplines of engi-neering and architecture are also provided.Keywords  Research · Innovation · Doctorate · Management · Women · STEAMMathematics Subject Classification  62-07 · 62-09JEL Classification  I23 · I24 · J24 · J71 · N34 · O30Electronic supplementary material  The online version of this article (https​://doi.org/10.1007/s1119​2-019-03179​-9) contains supplementary material, which is available to authorized users. *\t Asunción Gómez‑Pérez \t vicerrector.investigacion@upm.es1\t Vice‑Rectorate for Research, Innovation, and Doctoral Studies, Universidad Politécnica de Madrid, C/Ramiro de Maeztu 7, 28040 Madrid, Spainhttp://orcid.org/0000-0001-7869-6704http://orcid.org/0000-0001-9689-4798http://orcid.org/0000-0001-9299-1371http://orcid.org/0000-0002-3037-0331http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-019-03179-9&domain=pdfhttps://doi.org/10.1007/s11192-019-03179-9https://doi.org/10.1007/s11192-019-03179-91060\t Scientometrics (2019) 120:1059–10891 3IntroductionThe participation of women in scientific fields has attracted significant attention in the past years. Different reports have analysed the equality between men and women in the EU, studying the situation of women in different scenarios, such as tertiary education and exec-utive jobs. Moreover, the adoption of anti-discriminatory measures, the discussion of equal treatment policies and women’s rights was also well-documented in these papers (Lipinsky 2013; EC 2015, 2017).In the particular case of Spain, former governments have published reports analysing the participation of the women in science and technology (MINECO 2011; Puy 2015). In the same line, the enrolment and the contribution of women in the engineering and archi-tecture areas have been also widely documented (Pérez-Artieda et al. 2014), showing the prevalence of men over women in all the scenarios studied: undergraduates, PhD gradu-ates, workforce, and decision-making positions. Nevertheless the gender gap in the field of engineering and architecture is especially significant and, although extensively literature has described the gender imbalance in science, has not been completely explored.",
        "publication_date": "2019-07-11",
        "authors": "Estela Hernández‐Martín, F. Calle, Juan C. Dueñas, Miguel Holgado, Asunción Gómez‐Pérez",
        "file_name": "10!1007%s11192-019-03179-9.pdf",
        "file_path": "output/PDFs/10!1007%s11192-019-03179-9.pdf",
        "pdf_link": null
    },
    {
        "title": "An Analysis of Existing Production Frameworks for Statistical and Geographic Information: Synergies, Gaps and Integration",
        "implementation_urls": [],
        "doi": "10.3390/ijgi10060374",
        "arxiv": null,
        "abstract": "Abstract: The production of official statistical and geospatial data is often in the hands of highlyspecialized public agencies that have traditionally followed their own paths and established their ownproduction frameworks. In this article, we present the main frameworks of these two areas and focuson the possibility and need to achieve a better integration between them through the interoperabilityof systems, processes, and data. The statistical area is well led and has well-defined frameworks.The geospatial area does not have clear leadership and the large number of standards establish aframework that is not always obvious. On the other hand, the lack of a general and common legalframework is also highlighted. Additionally, three examples are offered: the first is the applicationof the spatial data quality model to the case of statistical data, the second of the application of thestatistical process model to the geospatial case, and the third is the use of linked geospatial andstatistical data. These examples demonstrate the possibility of transferring experiences/advancesfrom one area to another. In this way, we emphasize the conceptual proximity of these two areas,highlighting synergies, gaps, and potential integration.Keywords: geospatial information; statistical data; framework; interoperability1. IntroductionThe production of statistical data is nowadays more and more “geo”, and the produc-tion of geospatial data is more and more “statistical”; therefore, it is logical to envision agreater integration of these two areas. In this way, and according to the United NationsCommittee of Experts on Global Geospatial Information Management [1], the integrationof statistical and geospatial information and the resulting geospatially enabled statistics aresignificant components in meeting the data demands that inform decision-making needsat either the local, national, regional, or global level. Thereby, linking data about people,businesses, or the environment to a geographic location and their integration with othergeospatial information through their location can promote a much better understanding ofeconomic, social, and environmental perspectives.ISPRS Int. J. Geo-Inf. 2021, 10, 374. https://doi.org/10.3390/ijgi10060374 https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-6491-7430https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0002-8415-9531https://orcid.org/0000-0002-2983-4629https://orcid.org/0000-0001-6521-0760https://orcid.org/0000-0002-6373-4410https://doi.org/10.3390/ijgi10060374https://doi.org/10.3390/ijgi10060374https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/ijgi10060374https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/article/10.3390/ijgi10060374?type=check_update&version=1: International Journal ofiSpES Geo-InformationArticleAn Analysis of Existing Production Frameworks for Statisticaland Geographic Information: Synergies, Gaps and IntegrationFrancisco Javier Ariza-Lépez 1*, Antonio Rodriguez-Pascual ”, Francisco J. Lopez-Pellicer >,Luis M. Vilches-BlazquezManuel Antonio Urefia-Camara",
        "publication_date": "2021-06-02",
        "authors": "Francisco Javier Ariza López, Antonio Rodríguez-Pascual, Francisco J. López-Pellicer, Luis M. Vilches‐Blázquez, Agustín Villar-Iglesias, Joan Masó, Efrén Díaz-Díaz, Manuel A. Ureña-Cámara, Alberto González-Yanes",
        "file_name": "20250514093305.pdf",
        "file_path": "output/PDFs/20250514093305.pdf",
        "pdf_link": "https://www.mdpi.com/2220-9964/10/6/374/pdf?version=1622623685"
    },
    {
        "title": "A combination of supervised dimensionality reduction and learning methods to forecast solar radiation",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.6856079",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "DOI",
                        "source": "RSEF",
                        "source_paragraph": ""
                    }
                ]
            }
        ],
        "doi": "10.1007/s10489-022-04175-y",
        "arxiv": null,
        "abstract": "AbstractMachine learning is routinely used to forecast solar radiation from inputs, which are forecasts of meteorological variablesprovided by numerical weather prediction (NWP) models, on a spatially distributed grid. However, the number of featuresresulting from these grids is usually large, especially if several vertical levels are included. Principal Components Analysis(PCA) is one of the simplest and most widely-used methods to extract features and reduce dimensionality in renewableenergy forecasting, although this method has some limitations. First, it performs a global linear analysis, and second it isan unsupervised method. Locality Preserving Projection (LPP) overcomes the locality problem, and recently the LinearOptimal Low-Rank (LOL) method has extended Linear Discriminant Analysis (LDA) to be applicable when the numberof features is larger than the number of samples. Supervised Nonnegative Matrix Factorization (SNMF) also achieves thisgoal extending the Nonnegative Matrix Factorization (NMF) framework to integrate the logistic regression loss function.In this article we try to overcome all these issues together by proposing a Supervised Local Maximum Variance Preserving(SLMVP) method, a supervised non-linear method for feature extraction and dimensionality reduction. PCA, LPP, LOL,SNMF and SLMVP have been compared on Global Horizontal Irradiance (GHI) and Direct Normal Irradiance (DNI)radiation data at two different Iberian locations: Seville and Lisbon. Results show that for both kinds of radiation (GHI andDNI) and the two locations, SLMVP produces smaller MAE errors than PCA, LPP, LOL, and SNMF, around 4.92% betterfor Seville and 3.12% for Lisbon. It has also been shown that, although SLMVP, PCA, and LPP benefit from using a non-linear regression method (Gradient Boosting in this work), this benefit is larger for PCA and LPP because SMLVP is ableto perform non-linear transformations of inputs.Keywords Dimensionality reduction · Hybrid learning · Solar radiation forecast · Data mining1 IntroductionConsiderable efforts have been made in the past decadesto make solar energy a real alternative to the conventionalenergy generation system. There are two main technologies,solar thermal electricity (STE) and solar photovoltaic (PV)energy, and many countries have already reached a notablesolar share in their energy mixes. Moreover, importantgrowth is expected in the near future (International EnergyAgency, 2018).Contrary to conventional generation, solar electricitygeneration is conditioned by weather, and thus it is highly� Esteban Garcı́a-Cuestaesteban.garcia@fi.upm.esExtended author information available on the last page of the article.intermittent. Transient clouds and aerosol intermittencylead to considerable variability in the solar power plantsyield on a wide range of temporal scales, particularlyin minutes to hours time scales. This presents seriousissues regarding solar power plant management and theiryield integration into the electricity grid [1]. Currently, inaddition to expensive storage-based solutions, the use ofsolar radiation forecasts is the only plausible way to mitigatethe intermittency. Therefore, the development of accuratesolar radiation forecasting methods has become an essentialresearch topic [2].Solar forecasting methods can be classified dependingon the forecasting horizon. Nowcasting methods are mostlyrelated to one-hour ahead forecasts, short-term forecastingwith up to 6 hours ahead forecasts and forecasting methodsare aimed at producing days ahead forecasts. The techniquesassociated with these methods are essentially different [3–",
        "publication_date": "2022-10-06",
        "authors": "Esteban García-Cuesta, Ricardo Aler, David Pozo‐Vázquez, Inés M. Galván",
        "file_name": "10!1007%s10489-022-04175-y.pdf",
        "file_path": "output/PDFs/10!1007%s10489-022-04175-y.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s10489-022-04175-y.pdf"
    },
    {
        "title": "A Visual SHACL Shapes Editor Based On OntoPad.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractOn the Semantic Web, vocabularies and ontologies play a fundamental role to express the terminologyand rules of certain domains. New technologies like SHACL provide the possibility to express dataschemata specific to certain data sets, applications, and domains. However, the domain modeling pro-cess is collaborative and when using RDF, it requires technical knowledge. In this paper, we presenta tool to support a two-step-process to model a terminology and a schema with a combined graphicalRDF Schema editor and visual SHACL editor. This tool allows domain experts to create a terminologyand schema without the need for a deep understanding of RDF Schema or SHACL.Demo URL: https://ontopad.aksw.org/1. IntroductionThe W3C has promoted the Shapes Constraint Language (SHACL) [6] as recommendation toconstruct schematic blueprints as shapes of RDF data. These shapes can be used to validatedata, to construct input forms to author new RDF data, and to express a domain model. Inthis way, they provide a pragmatic and flexible way to express how the individual terms ina vocabulary (classes and properties) relate to each other and how the instance data shouldlook like. To model a domain means to understand and express its language and rules. Toformally express this domain model with means of the Semantic Web is a technical process.The overall process of domainmodeling is a collaborative process that requires the involvementof domain experts. Providing a graphical tool that allows to interact with SHACL shapes byusing a visual diagram component would allow to make the RDF layer transparent to its usersand provide a visual language to interact with the data model. To support the collaborativedomain modeling process, visual editors could help to increase the involvement of domainSEMANTiCS 2021, September 6–9, 2021, Amsterdam, NLarndt@informatik.uni-leipzig.de (N. Arndt); valdestilhas@informatik.uni-leipzig.de (A. Valdestilhas);gustavo.publio@informatik.uni-leipzig.de (G. Publio); cimmino@fi.upm.es (A. Cimmino);konrad.hoeffner@uni-leipzig.de (K. Höffner); thomas.riechert@htwk-leipzig.de (T. Riechert)https://aksw.org/NatanaelArndt (N. Arndt); https://aksw.org/AndreValdestilhas (A. Valdestilhas); https://aksw.org/GustavoPublio (G. Publio); https://aksw.org/KonradHoeffner (K. Höffner); https://aksw.org/ThomasRiechert(T. Riechert)0000-0002-8130-8677 (N. Arndt); 0000-0002-0079-2533 (A. Valdestilhas); 0000-0002-3853-3588 (G. Publio);0000-0002-1823-4484 (A. Cimmino); 0000-0001-7358-3217 (K. Höffner); 0000-0003-2053-5347 (T. Riechert)© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)https://ontopad.aksw.org/mailto:arndt@informatik.uni-leipzig.demailto:valdestilhas@informatik.uni-leipzig.demailto:gustavo.publio@informatik.uni-leipzig.demailto:cimmino@fi.upm.esmailto:konrad.hoeffner@uni-leipzig.demailto:thomas.riechert@htwk-leipzig.dehttps://aksw.org/NatanaelArndthttps://aksw.org/AndreValdestilhashttps://aksw.org/GustavoPubliohttps://aksw.org/GustavoPubliohttps://aksw.org/KonradHoeffnerhttps://aksw.org/ThomasRiechert",
        "publication_date": "2021-01-01",
        "authors": "Natanael Arndt, André Valdestilhas, Gustavo Publio, Andrea Cimmino Arriaga, Konrad Höffner, Thomas Riechert",
        "file_name": "20250514093407.pdf",
        "file_path": "output/PDFs/20250514093407.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2941/paper16.pdf"
    },
    {
        "title": "The modular SSN ontology: A joint W3C and OGC standard specifying the semantics of sensors, observations, sampling, and actuation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/w3c/sdw",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!3233%sw-180320.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "The SSN ontology is available at http://www.w3.org/ns/ssn/."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-180320",
        "arxiv": null,
        "abstract": "Abstract. The joint W3C (World Wide Web Consortium) and OGC (Open Geospatial Consortium) Spatial Data on the Web(SDW) Working Group developed a set of ontologies to describe sensors, actuators, samplers as well as their observations,actuation, and sampling activities. The ontologies have been published both as a W3C recommendation and as an OGC im-plementation standard. The set includes a lightweight core module called SOSA (Sensor, Observation, Sampler, and Actuator)available at: http://www.w3.org/ns/sosa/, and a more expressive extension module called SSN (Semantic Sensor Network) avail-able at: http://www.w3.org/ns/ssn/. Together they describe systems of sensors and actuators, observations, the used procedures,the subjects and their properties being observed or acted upon, samples and the process of sampling, and so forth. The set ofontologies adopts a modular architecture with SOSA as a self-contained core that is extended by SSN and other modules toadd expressivity and breadth. The SOSA/SSN ontologies are able to support a wide range of applications and use cases, includ-ing satellite imagery, large-scale scientific monitoring, industrial and household infrastructures, social sensing, citizen science,observation-driven ontology engineering, and the Internet of Things. In this paper we give an overview of the ontologies anddiscuss the rationale behind key design decisions, reporting on the differences between the new SSN ontology presented here andits predecessor [9] developed by the W3C Semantic Sensor Network Incubator group (the SSN-XG). We present usage examplesand describe alignment modules that foster interoperability with other ontologies.Keywords: Ontology, Sensor, Actuator, Observation, Actuation, Sampling, Linked Data, Web of Things, Internet of Things1570-0844/18/$35.00 c© 2018 – IOS Press and the authors. All rights reservedmailto:armin.haller@anu.edu.aumailto:kerry.taylor@anu.edu.aumailto:jano@geog.ucsb.edumailto:simon.cox@csiro.aumailto:maxime.lefrancois@emse.frmailto:danh.lephuoc@tu-berlin.demailto:jlieberman@fas.harvard.edumailto:rgarcia@fi.upm.esmailto:rob@metalinkage.com.aumailto:cstadler@informatik.uni-leipzig.dehttp://www.w3.org/ns/sosa/http://www.w3.org/ns/ssn/A. Haller et al. / The Modular SSN Ontology: A Joint W3C and OGC Standard 11 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 1516 1617 1718 1819 1920 2021 21",
        "publication_date": "2018-08-31",
        "authors": "Armin Haller, Krzysztof Janowicz, Simón Cox, Maxime Lefrançois, Kerry Taylor, Danh Le-Phuoc, Joshua Lieberman, Raúl García‐Castro, Rob Atkinson, Claus Stadler",
        "file_name": "10!3233%sw-180320.pdf",
        "file_path": "output/PDFs/10!3233%sw-180320.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic technologies and interoperability in the built environment",
        "implementation_urls": [],
        "doi": "10.3233/sw-180321",
        "arxiv": null,
        "abstract": "Abstract. The built environment consists of plenty of physical assets with which we interact on a daily basis. In order toimprove not only our built environment, but also our interaction with that environment, we would benefit a lot from semanticrepresentations of this environment. This not only includes buildings, but also large infrastructure (bridges, tunnels, waterways,underground systems), and geospatial data. With this special issue, an insight is given into the current state of the art in termsof semantic technologies and interoperability in this built environment. This editorial not only summarizes the content of theSpecial Issue on Semantic Technologies and interoperability in the Built Environment, it also provides a brief overview of thecurrent state of the art in general in terms of standardisation and community efforts.Keywords: Semantics, built environment, building information model, linked data, architecture, construction1. Semantics in the built environmentThe built environment encompasses a variety of ar-tifacts ranging from buildings to infrastructures. Theseartifacts are linked at different scales and in variousways. Namely, relations can be found between ele-ments in a singular building, furniture, street lights,and so forth to elements and concepts covering theentire city, such as infrastructure, traffic, and peopleflows. Apart from this multiplicity of artifacts, the builtenvironment also gathers multiple stakeholders whocollaborate in various ways. Collaboration and inter-action not only happens in the built environment as itexists on a daily basis, but even more so throughout*Corresponding author. E-mail: pipauwel.pauwels@ugent.be.all the design, construction and operation phases tak-ing place within the built environment. This includesspecialists (architects, engineers, and contractors), butalso local administrators, facility managers, and citi-zens.A large part of this environment is governed by theArchitecture, Engineering, and Construction (AEC)industry. An effective collaboration and interoperabil-ity between these different actors throughout the life-cycle of the built environment has always been akey challenge to this industry. Data from stakehold-ers is modeled and published in various languagesand scales, in particular using Building InformationModelling (BIM) tools [2], and data evolves consid-erably over time. Hence, maintaining data consistencythroughout the whole life-cycle of a building, espe-1570-0844/18/$35.00 © 2018 – IOS Press and the authors. All rights reservedmailto:pipauwel.pauwels@ugent.bemailto:mpoveda@fi.upm.esmailto:alvaro.sicilia@salle.url.edumailto:Jerome.Euzenat@inria.frmailto:pipauwel.pauwels@ugent.be732 P. Pauwels et al. / Semantic technologies and interoperability in the built environmentcially during the design and construction phases, is afundamental challenge to this industry.Beyond the design and construction phases, impor-tant other amounts of data are present as well in thebuilt environment in general. This data is used to make",
        "publication_date": "2018-08-24",
        "authors": "Pieter Pauwels, María Poveda‐Villalón, Álvaro Sicilia, Jérôme Euzenat",
        "file_name": "10!3233%sw-180321.pdf",
        "file_path": "output/PDFs/10!3233%sw-180321.pdf",
        "pdf_link": null
    },
    {
        "title": "Pody: A Solid-Based Approach to Embody Agents in Web-Based Multi-Agent-Systems",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-48539-8_15",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-01-01",
        "authors": "Antoine Zimmermann, Andrei Ciortea, Catherine Faron Zucker, Eoin O’Neill, María Poveda‐Villalón",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "InDaMul: Incentivized Data Mules for Opportunistic Networking Through Smart Contracts and Decentralized Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/umbral-rs",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1145%3587696.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Retrieved from https://github.com/miker83z/umbral-rs."
                    }
                ]
            }
        ],
        "doi": "10.1145/3587696",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2023-03-18",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1145%3587696.pdf",
        "file_path": "output/PDFs/10!1145%3587696.pdf",
        "pdf_link": null
    },
    {
        "title": "A Distributed Ledger Based Infrastructure for Smart Transportation System and Social Good",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%ccnc46108!2020!9045640.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [4] S."
                    }
                ]
            }
        ],
        "doi": "10.1109/ccnc46108.2020.9045640",
        "arxiv": "1910.03280",
        "abstract": "AbstractThis paper presents a system architecture to promote the development of smart transportation systems. Thanks to the use ofdistributed ledgers and related technologies, it is possible to create, store and share data generated by users through their sensors,while moving. In particular, IOTA and IPFS are used to store and certify data (and their related metadata) coming from sensors orby the users themselves. Ethereum is exploited as the smart contract platform that coordinates the data sharing and provisioning.The necessary privacy guarantees are provided by the usage of Zero Knowledge Proof. We show some results obtained from someuse case scenarios that demonstrate how such technologies can be integrated to build novel smart services and to promote socialgood in user mobility.I. INTRODUCTIONIn the last decades, smart transportation systems have emerged as a way to improve transportation efficiency, travel safety,vehicle security and better choices for drivers and passengers. Today, intelligent vehicles and transportation systems representfundamental technologies, that improve drivers comfort and security. A variety of applications and protocols can be enforcedaltogether to obtain advanced and improved transportation systems. However, to fully exploit their potential and promotethe development of smart mobility applications and services for social good, several novel challenges must be faced, thatrequire substantial changes in transportation system models. The mentioned goals can be accomplished only through the useof procedures, systems and devices that allow data gathering, communication, analysis and distribution among individualsvehicles, infrastructures and services.A reduced presence of (human) intermediaries can lead to the creation of smart services that take advantage of fasterprocessing and better performances to provide the basis for smart moving and peer-to-peer services. Moreover, in the case ofdata sharing, service automation enables users to completely maintain control over the data they produce, making possible anintelligent sharing for social good.In this scenario, another prominent technology that can play a main role is the decentralized management of crowd-sourceddata, i.e. the blockchain [1]. The blockchain, made famous by Bitcoin [2], enabled a new vision for both finance and trustin distributed systems. Since their inception, the growth of blockchain technologies renewed the concepts of contracts anddigital democracy, especially after the introduction of Ethereum [3]. The decentralized computation enabled by the Ethereumblockchain allows to create self-managed structures that do not rely on a central control, thus eliminating the presence ofsingle point of failures [4]. Moreover, this blockchain allows using smart contracts in order to build Decentralized Applications(dApps) and Decentralized Autonomous Organizations (DAOs) that can realize novel important applications for social good [5].Under the technical viewpoint, a blockchain is a specific type of Distributed Ledger Technology (DLT) with the scope tomove trust from a human intermediary, that manages a transaction between two parties, to a protocol that allows the twoparties to transact directly, i.e. without the need of such third party. There are different implementations of DLTs, each onewith its pros and cons. For example, Ethereum [3] provides a distributed virtual machine that is able to process any kind ofcomputation but with constraints in scalability. Conversely, IOTA ledger [6] is thought to provide better scalability but it doesnot support distributed computation. Thus, if one wants to build a sophisticated software architecture, acting as the middlewarefor secure and certified smart transportation system applications, multiple DLTs can be utilized and combined, so as to takethe best of multiple worlds. This is the philosophy we followed in our approach.The aim of this work is to use DLTs to propose an infrastructure for smart transportation systems. Two main features areat the basis on this infrastructure: data sharing and smart services. We claim that the combination of data sharing and smartservices allows creating a framework that promotes social good in user mobility. In particular, data sharing services are definedto let users and sensors to share their data. These services allow defining how the data can be shared but also how (from whoand through which technology) they are acquired. The proposed infrastructure is based on DLTs, in combination with other0 The publisher version of this paper is available at https://doi.org/10.1109/CCNC46108.2020.9045640. This is the pre-peer reviewed version of thearticle: “Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo. A Distributed Ledger Based Infrastructure for Smart Transportation System andSocial Good. Proceedings of the IEEE Consumer Communications and Networking Conference 2020 (CCNC 2020).”.arXiv:1910.",
        "publication_date": "2020-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%ccnc46108!2020!9045640.pdf",
        "file_path": "output/PDFs/10!1109%ccnc46108!2020!9045640.pdf",
        "pdf_link": null
    },
    {
        "title": "Injecting data into ODRL privacy policies dynamically with RDF mappings",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3587358",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2023-04-28",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Satellite Earth Observation for Essential Climate Variables Supporting Sustainable Development Goals: A Review on Applications",
        "implementation_urls": [],
        "doi": "10.3390/rs15112716",
        "arxiv": null,
        "abstract": "Abstract: Essential climate variables (ECVs) have been recognized as crucial information for achievingSustainable Development Goals (SDGs). There is an agreement on 54 ECVs to understand climateevolution, and multiple rely on satellite Earth observation (abbreviated as s-ECVs). Despite the effortsto encourage s-ECV use for SDGs, there is still a need to further integrate them into the indicatorcalculations. Therefore, we conducted a systematic literature review to identify s-ECVs used inSDG monitoring. Results showed the use of 14 s-ECVs, the most frequent being land cover, ozone,precursors for aerosols and ozone, precipitation, land surface temperature, soil moisture, soil carbon,lakes, and leaf area index. They were related to 16 SDGs (mainly SDGs 3, 6, 11, 14, and 15), 33 targets,and 23 indicators. However, only 10 indicators (belonging to SDGs 6, 11, and 15) were calculatedusing s-ECVs. This review raises research opportunities by identifying s-ECVs yet to be used in theindicator calculations. Therefore, indicators supporting SDGs must be updated to use this valuablesource of information which, in turn, allows a worldwide indicator comparison. Additionally, thisreview is relevant for scientists and policymakers for future actions and policies to better integrates-ECVs into the Agenda 2030.Keywords: SDG; sustainable development; satellite; Earth observation; review; essential variables;climate1. IntroductionThe Agenda 2030 for Sustainable Development and its 17 goals (SDGs) are connectedwith the environment, economy, and society dimensions of sustainable development [1].The 17 goals, their 169 associated targets, and 231 indicators are based on the first data-driven policy development framework, following the principle of “If you don’t measureit, you can’t manage it” [2] (p. 2). Despite the recognized importance of measuring theprogress towards the SDGs, two-thirds of the indicators remain unreported, especially inlow-income countries [3]. Moreover, less than 44% of the SDG indicators can be easilymeasured [4]. Therefore, it is a priority to boost the measuring and monitoring of theprogress towards the SDGs. In our work, we focus on two key approaches to pursue thisRemote Sens. 2023, 15, 2716. https://doi.org/10.3390/rs15112716 https://www.mdpi.com/journal/remotesensinghttps://doi.org/10.3390/rs15112716https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.comhttps://orcid.org/0000-0002-6926-4827https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0001-7380-0701https://orcid.org/0000-0001-5145-9223https://orcid.org/0000-0001-6559-2033https://doi.org/10.3390/rs15112716https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.com/article/10.3390/rs15112716?type=check_update&version=1Remote Sens. 2023, 15, 2716 2 of 25aim: (1) essential variables (EVs) that have been defined as an intermediate layer betweenobservations and indicators [5] and (2) satellite Earth observation (sEO) data that gainedparticular attention as worldwide feasible, cost-effective, and analysis-ready data acrossscales in remote, non-accessible, and poorly monitored regions [6].The EVs emerged in various social and environmental scientific communities relatedto specific domains such as climate, biodiversity, agriculture, and society [5,7–9]. Referto [10,11] for detailed EV compendiums. These kinds of variables are “a minimal setof variables that determine the system’s state and developments, [which] are crucial for",
        "publication_date": "2023-05-24",
        "authors": "Daniela Ballari, Luis M. Vilches‐Blázquez, María Lorena Orellana-Samaniego, Francisco Salgado, Ana Ochoa‐Sánchez, Valerie Graw, Nazli Turini, Jörg Bendix",
        "file_name": "20250514093652.pdf",
        "file_path": "output/PDFs/20250514093652.pdf",
        "pdf_link": "https://www.mdpi.com/2072-4292/15/11/2716/pdf?version=1684894645"
    },
    {
        "title": "Conformance testing of ontologies through ontology requirements",
        "implementation_urls": [],
        "doi": "10.1016/j.engappai.2020.104026",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-10-23",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A multi-agent architecture for mobile sensing systems",
        "implementation_urls": [],
        "doi": "10.1007/s12652-019-01608-4",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-12-07",
        "authors": "Francisco Laport, Emilio Serrano, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "An information sharing strategy based on linked data for net zero energy buildings and clusters",
        "implementation_urls": [],
        "doi": "10.1016/j.autcon.2021.103592",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-02-01",
        "authors": "Yehong Li, Shushan Hu, Cathal Hoare, James O’Donnell, Raúl García‐Castro, Sergio Vega Sánchez, Xiangyang Jiang",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The NeOn Methodology for Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_2",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez, Mariano Fernández‐López",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "DockerPedia: A Knowledge Graph of Software Images and Their Metadata",
        "implementation_urls": [],
        "doi": "10.1142/s0218194022500036",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Maximiliano Osorio, Carlos Buil-Aranda, Idafen Santana-Pérez, Daniel Garijo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "T2WML: A Cell-Based Language to Map Tables into Wikidata Records.",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. The web contains millions of useful spreadsheets and CSVfiles, but these files are difficult to use in applications because they usea wide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes it easyto map and link arbitrary spreadsheets and CSV files to the Wikidatadata model. The output of T2WML consists of Wikidata statementsthat can be loaded in the public Wikidata, or loaded in a Wikidataclone, creating an augmented Wikidata knowledge graph that applicationdevelopers can query using SPARQL.1Keywords: Knowledge Graphs, RDF, Entity Linking, Wikidata1 IntroductionThe web contains millions of useful spreadsheets and CSV files, including datafrom many government and international organizations. Organizations that offerdata often have web sites where users can search, browse and download dataon a large number of topics. Most institutions offer their data in Excel andCSV formats. The downloaded data is seldom directly usable because, unlikedatabases, which use one column per variable, spreadsheets often arrange thedata in different layouts.Fig. 1 illustrates the problem using data about homicide rates in differentcountries, downloaded from the United Nations web site2. We truncated andcolored the files for ease of presentation. The cells with the homicide numbersare highlighted in green, the cells that provide contextual information for thevalue are highlighted in blue, and header cells are highlighted in dark blue.Fig. 1a shows the layout of the data provided in the UN website; Fig. 1b showsa more compact representation using multi-level headers; Fig. 1c shows a layout1 Copyright (c) 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0). This material is basedupon work supported by United States Air Force under Contract No. FA8650-17-C-7715.2 https://dataunodc.un.org/crime/intentional-homicide-victimsFig. 1. Intentional Homicide Data (Excel file downloaded from dataunodc.un.org)that could be used to store the data in a database, and that can be used directlyin tools such as Pandas; Fig. 1d illustrates a common convention for arrangingdata by topic, by creating stacked tables that share common headings. All tablespresent the same homicide data. The interpretation of each value is defined byfour cells (country, year, population and source) that identify the context for avalue. In each table, the context cells are located in different parts of the data.Only in Fig. 1c (Database) the context cells are in the same row as the value; inthe other tables, context cells appear in different rows, in header rows (examplesa and b), or in visually distinct rows within the table (example d).Existing languages for mapping structured data to RDF, including R2RML3,RML [1], Karma [3] and CSV2RDF [2] process tabular data row by row, requiringtabular data to be in database format (Fig. 1c). RML supports non-tabularformats (JSON and XML) and Karma provides folding and unfolding operatorsto rearrange data for row-based processing. None support complex layouts suchas those in examples b or d.T2WML is a mapping language designed to meet three objectives: 1) Identifyand map data and their context qualifiers in arbitrary data layouts found in Exceland CSV files without the need of complex preprocessing steps to transformtables into a canonical \"Database\" representation; 2) Enable users who are not",
        "publication_date": "2019-01-01",
        "authors": "Pedro Szekely, Daniel Garijo, Jay Pujara, Divij Bhatia, Jiasheng Wu",
        "file_name": "20250514093800.pdf",
        "file_path": "output/PDFs/20250514093800.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper12.pdf"
    },
    {
        "title": "Themis: a tool for validating ontologies through requirements",
        "implementation_urls": [],
        "doi": "10.18293/seke2019-117",
        "arxiv": null,
        "abstract": "Abstract— The validation of ontologies, whose aim is to checkwhether an ontology matches the conceptualization it is meantto specify, is a key activity for guaranteeing the quality ofontologies. This work is focused on the validation throughrequirements, with the aim of assuring, both the domainexperts and ontology developers, that the ontologies they arebuilding or using are complete regarding their needs. Inspiredby software engineering testing processes, this work proposesa web-based tool called Themis, independent of any ontologydevelopment environment, for validating ontologies by meansof the application of test expressions which, following lexico-syntactic patterns, represent the desired behaviour that willpresent an ontology if a requirement is satisfied.I. INTRODUCTIONIn software engineering it is inconceivable to deliver asoftware product without its pertinent tests which guaranteethat it fulfills all its requirements. Besides, there are severalapproaches integrated into the software development processwhose aim is to test the software. Unit testing [1], whichvalidates that each unit of the software performs as designed,and behaviour-driven development [2], which focuses onthe behaviour the software product is implementing, areexamples of these approaches. Moreover, there are specificsyntaxes, such as Gherkin,1 which generate unambiguousspecifications of software to automate the testing process.However, in ontology engineering there is a lack of clearlydefined testing processes in order to be able to ascertainwhether an ontology satisfies its functional requirements [3],which state the particular knowledge that should be repre-sented. Such ontological requirements used to be writtenin form of competency questions [4] or natural languagesentences. The main issue when performing testing processesin the ontology engineering field is the ambiguity of theontological requirements, which sometimes are difficult toformalize into tests and to translate into axioms. Therefore,inspired by software engineering and its specific syntax forthe definition of tests, we propose Themis,2 a tool whichprovides a set of test expressions based on lexico-syntacticpatterns (LSPs) related to ontological requirements. TheseLSPs allows to relate different types of requirements withthe axioms needed to implement them in an ontology, andsuch implementations are used by Themis to identify whethera requirement is satisfied.DOI reference number: 10.18293/SEKE2019-1711https://docs.cucumber.io/gherkin2http://themis.linkeddata.esThemis can be used by both domain experts and ontologydevelopers to validate ontologies regarding their functionalrequirements. Other type of requirements, such as non-functional ones (e.g., “the ontology URIs must be in En-",
        "publication_date": "2019-07-10",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "10!18293%seke2019-117.pdf",
        "file_path": "output/PDFs/10!18293%seke2019-117.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic Web-Enabled Protocol Mediation for the Logistics Domain",
        "implementation_urls": [],
        "doi": "10.4018/978-1-60566-066-0.ch004",
        "arxiv": null,
        "abstract": "Abstract Among the problems that arise when trying to make different applications interoperate with each other, protocol mediation is one of the most difficult ones and for which less relevant literature can be found. Protocol mediation is concerned with non-matching message interaction patterns in application interaction. In this paper we describe the design and implementation of a protocol mediation component that has been applied in the interoperation between two heterogeneous logistic provider systems (using two different standards: RosettaNet and EDIFACT), for a specific freight forwarding task. 1 Current Situation Logistics management is a typical business problem where the use of a Service Oriented Architecture is clearly suited. As pointed out in (Evans-Greenwood and Stason, 2006) the current trend in logistics is to divide support between planning applications, which compute production plans overnight, and execution applications, which manage the flow of events in an operational environment. This disconnection forces users to deal with business exceptions (lost shipments, for example), manually resolving the problems by directly updating the execution and planning applications. However, this human-dependency problem can be ameliorated by using Web technology to create a heterogeneous composite application involving all participants in the process, providing a complete Third-Party Logistics solution, and giving users a single unified view into the logistics pipeline. This consolidated logistics solution greatly simplifies the task of identifying and correcting business exceptions (e.g., missing shipments or stock shortages) as they occur. Furthermore, (Evans-Greenwood and Stason, 2006) also talk about the possibility of combining multiple Third-Party Logistics solutions into a single heterogeneous virtual logistics network. With such a virtual network, each shipment is assigned a route dynamically assembled from one or more individual logistics providers, using dynamically created virtual supply chains. Most of these business functions are still manual and offline, but most of them can be automated with the use of Service Oriented Architectures, as will be presented in this chapter. Obviously, the main advantages of using such solutions are the decreases in cost and speed in transactions, which influence in a better quality of the service provided to customers. The main barrier to set up a business relationship with a company in the logistics domain is that it usually requires an initial large investment of time and money. This is ameliorated by the emergence of some industry standards like EDIFACT (EDIFACT), AnsiX12 (AnsiX12) or RosettaNet (RosettaNet), which ease the integration tasks between information systems that comply with them. However, given that these standards have some flexibility in what respects the content and sequencing of the messages that can be exchanged, the integration of systems is still time and effort consuming. Besides, there is sometimes a need to integrate systems that use different standards, what makes the integration task even more time and effort consuming. This is the focus of one of the four case studies developed in the context of the EU project SWWS1 (Semantic-Web enabled Web Services), a demonstrator of business-to-business integration in the logistics domain using Semantic Web Service technology. All the features of this                                                            demonstrator are described in detail in (Preist et al., 2005), including aspects related to the discovery and selection of relevant services, their execution and the mediation between services following different protocols.  In this chapter we will focus on the last aspect (mediation) and more specifically on protocol mediation, which is concerned with the problem of non-matching message interaction patterns. We will describe the design and implementation of the protocol mediation component applied in this case study to show how to make logistic provider systems using two different standards (RosettaNet and EDIFACT) interoperate for a specific freight forwarding task.  The chapter is structured as follows. The rest of this section introduces a motivating example, focusing on the needs for protocol mediation, and gives some background on how the problem of mediation can be characterised in general and on the approaches for mediation proposed in the ",
        "publication_date": "2009-01-01",
        "authors": "Óscar Corcho, Silvestre Losada, Richard Benjamins",
        "file_name": "10!4018%978-1-60566-066-0!ch004.pdf",
        "file_path": "output/PDFs/10!4018%978-1-60566-066-0!ch004.pdf",
        "pdf_link": null
    },
    {
        "title": "Ontology Requirements Specification",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_5",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Spanish corpora for sentiment analysis: a survey",
        "implementation_urls": [],
        "doi": "10.1007/s10579-019-09470-8",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-05-31",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1016%j!websem!2020!100596.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "All the resources described in this section are available online.10 3.1."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2020.100596",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2020-08-08",
        "authors": "David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, Óscar Corcho",
        "file_name": "10!1016%j!websem!2020!100596.pdf",
        "file_path": "output/PDFs/10!1016%j!websem!2020!100596.pdf",
        "pdf_link": null
    },
    {
        "title": "Introduction to the Special Issue “Artificial Intelligence Knowledge Representation”",
        "implementation_urls": [],
        "doi": "10.3390/systems7030035",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2019-07-22",
        "authors": "Paola Di Maio, Mari Carmen Suárez-Figueroa",
        "file_name": "20250514093929.pdf",
        "file_path": "output/PDFs/20250514093929.pdf",
        "pdf_link": "https://www.mdpi.com/2079-8954/7/3/35/pdf?version=1563965423"
    },
    {
        "title": "Machine Learning-based Query Augmentation for SPARQL Endpoints",
        "implementation_urls": [],
        "doi": "10.5220/0006925300570067",
        "arxiv": null,
        "abstract": "Abstract:Linked Data repositories have become a popular source of publicly-available data. Users accessingthis data through SPARQL endpoints usually launch several restrictive yet similar consecutivequeries, either to find the information they need through trial-and-error or to query related re-sources. However, instead of executing each individual query separately, query augmentation aimsat modifying the incoming queries to retrieve more data that is potentially relevant to subsequentrequests. In this paper, we propose a novel approach to query augmentation for SPARQL end-points based on machine learning. Our approach separates the structure of the query from itscontents and measures two types of similarity, which are then used to predict the structure andcontents of the augmented query. We test the approach on the real-world query logs of the Spanishand English DBpedia and show that our approach yields high-accuracy prediction. We also showthat, by caching the results of the predicted augmented queries, we can retrieve data relevant toseveral subsequent queries at once, achieving a higher cache hit rate than previous approaches.1 INTRODUCTIONLinked Data repositories have grown to providea wealth of publicly-available data, with somerepositories containing millions of concepts de-scribed by RDF triples (e.g. DBpedia1, FOAF2,GeoNames3). Users access the data in theserepositories through public SPARQL endpointsthat allow them to issue SPARQL queries, thestandard query language for RDF stores. Consec-utive queries received from the same client usuallyexhibit some patterns, such as querying identicalor similar resources than previous queries.Caching query results was first proposed tokeep recently retrieved data in a memory cachefor use with later queries (Dar et al., 1996; Mar-tin et al., 2010; Yang and Wu, 2011). However,caching only works if the exact same data is ac-cessed multiple times. In reality, it is more com-1DBpedia: https://wiki.dbpedia.org/2FOAF: http://www.foaf-project.org/3GeoNames: http://www.geonames.org/mon to have similar consecutive queries that re-trieve related resources from the repository (Boni-fati et al., 2017; Mario et al., 2011). Queryaugmentation takes advantage of this fact, re-trieving data that will potentially be used by fu-ture queries before the queries are received bythe SPARQL endopint. Previous approaches toquery augmentation are divided into two maincategories, (1) techniques based on informationfound in the data source, and (2) techniquesbased on analysis of previous (historic) queries,as discussed in section 2.In this paper, we present an approach to queryaugmentation for SPARQL endpoints based ondetecting recurring patterns in historic querylogs. The novelty of our approach is that we",
        "publication_date": "2018-01-01",
        "authors": "Mariano Rico, Rizkallah Touma, Anna Queralt, Marı́a S. Pérez",
        "file_name": "10!5220%0006925300570067.pdf",
        "file_path": "output/PDFs/10!5220%0006925300570067.pdf",
        "pdf_link": null
    },
    {
        "title": "Semantic conflation in GIScience: a systematic review",
        "implementation_urls": [],
        "doi": "10.1080/15230406.2021.1952109",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-08-24",
        "authors": "Luis M. Vilches‐Blázquez, José Ángel Ramos",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Type Prediction of RDF Knowledge Graphs Using Binary Classifiers with Structural Data",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_27",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-01-01",
        "authors": "Nandana Mihindukulasooriya, Mariano Rico",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Classification of retinal vessels using a collaborative agent-based architecture",
        "implementation_urls": [],
        "doi": "10.3233/aic-180772",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2018-09-18",
        "authors": "Pablo Chamoso, Sara Rodrı́guez, Fernando De la Prieta, Javier Bajo",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "The impact of NFT profile pictures within social network communities",
        "implementation_urls": [],
        "doi": "10.1145/3524458.3547230",
        "arxiv": "2206.06443",
        "abstract": "",
        "publication_date": "2022-08-23",
        "authors": "Simone Casale-Brunet, Mirko Zichichi, Lee Hutchinson, Marco Mattavelli, Stefano Ferretti",
        "file_name": "10!1145%3524458!3547230.pdf",
        "file_path": "output/PDFs/10!1145%3524458!3547230.pdf",
        "pdf_link": null
    },
    {
        "title": "Multilayered Linked Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_3",
        "arxiv": null,
        "abstract": "Abstract Although confidence in democracy to tackle societal problems is falling,new civic participation tools are appearing supported by modern ICT technologies.These tools implicitly assume different views on democracy and citizenship whichhave not been fully analysed, but their main fault is their isolated operation innon-communicated silos. We can conceive public knowledge, like in Karl Popper’sWorld 3, as distributed and connected in different layers and by different connec-tors, much as it happens with the information in the web or the data in the linkeddata cloud. The interaction between people, technology and data is still to bedefined before alternative institutions are founded, but the so called linkeddemocracy should rest on different layers of interaction: linked data, linked plat-forms and linked ecosystems; a robust connectivity between democratic institutionsis fundamental in order to enhance the way knowledge circulates and collectivedecisions are made.Keywords Linked democracy � Multilayered linked democracy � Linked data �Linked platforms � Linked ecosystems � World 3 � Institutions3.1 IntroductionContemporary democracies face growing scepticism about their capacity to managecomplex societal problems. Financial crises, inequality and poverty, climate changeand armed conflicts routinely test the resilience of our democratic systems.Researchers are predominantly expressing concern about the developments of thelast decade. Larry Diamond draws from Freedom House data to argue that we are ina ‘mild but protracted democratic recession’ since 2006 (Diamond 2015, 144).Roberto Foa and Yascha Mounk analyse World Values Surveys to conclude thatcitizens in Western democracies have ‘become more cynical about the value of© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_351http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_3democracy as a political system, less hopeful that anything they do might influencepublic policy, and more willing to express support for authoritarian alternatives’(Foa and Mounk 2016, 7). John Boik et al. warn that traditional democratic insti-tutions are failing and that ‘the versions of democracy attempted by newlydemocratizing nations have been even less effective’ (Boik et al. 2015). Globally,voter turnout—a standard proxy to measure citizens’ satisfaction with democraticinstitutions—has been steadily but consistently declining since the 1960s (IDEAInternational 2016).This sceptical outlook coexists with some unprecedented technology trends: by2020, about 1.7 megabytes of new information will be created every second, forevery human being (Forbes 2015); there will be more mobile phone subscriptionsthan people on the planet and more than 6 billion of these devices will be smart-phones (ITU 2015). Digital technologies not only disrupt business models, theynow shape the way we access information, knowledge, and increasingly, the waywe exercise our rights. In doing so, they also transform civic action and enable newforms of citizenship.Political science, media and culture studies, and ICT disciplines have alreadyproduced a vast literature on civic participation online (e.g., see meta-analysis by",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "20250514093954.pdf",
        "file_path": "output/PDFs/20250514093954.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_3.pdf"
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "output/PDFs/10!1109%nca51143!2020!9306721.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [5] M."
                    }
                ]
            }
        ],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "abstract": "Abstract—This paper presents an architecture of a PersonalInformation Management System, in which individuals can definethe access to their personal data by means of smart contracts.These smart contracts, running on the Ethereum blockchain,implement access control lists and grant immutability, traceabil-ity and verifiability of the references to personal data, which isstored itself in a (possibly distributed) file system. A distributedauthorization mechanism is devised, where trust from multiplenetwork nodes is necessary to grant the access to the data.To this aim, two possible alternatives are described: a SecretSharing scheme and Threshold Proxy Re-Encryption scheme. Theperformance of these alternatives is experimentally comparedin terms of execution time. Threshold Proxy Re-Encryptionappears to be faster in different scenarios, in particular whenincreasing message size, number of nodes and the threshold value,i.e. number of nodes needed to grant the data disclosure.I. INTRODUCTIONThe transformation introduced by digital technologies hashad (and is having) a significant impact on economy and soci-ety. Data is at the heart of this transformation and individualsare the main sources generating more and more of it. There isan urgent need to place (again) individuals at the center and torelieve the absence of technical instruments and standards thatmake the exercise of one’s rights simple and not excessivelyburdensome [1], [2]. The EU’s GDPR 1 helps to promote thisvision and at the same time seeks to pave the way for opendata spaces for the social and economic good 2.Our aim is to seek such a technology by enabling userswith the sovereignty over their data, while guaranteeing itsconfidentiality. In our view, the data owner can define accessby limiting the scope of data utility, delegating these privilegesor giving up ownership completely, without the need to relyon (un)trusted entities to facilitate this task. The developmentof a Personal Information Management System (PIMS) 3 that∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - RIoE.1Council of European Union, Regulation 2016/679 - directive 95/462European Commission, COM(2020) 66, “A European strategy for data”3European Data Protection Supervisors, Opinion 9/2016, “EDPS Opinionon Personal Information Management Systems”fulfils these goals can be based on a distributed softwarearchitecture, where each individual is associated to a digitalspace containing personal data. This space will be used toattend the data access requests coming from data providersand data consumers. Distributed Ledger Technologies (DLT)and Decentralized File Storages (DFS) combination providesa range of features suitable for data management and sharing,such as transparency, immutability and reliability [2], [3].",
        "publication_date": "2020-11-24",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": "output/PDFs/10!1109%nca51143!2020!9306721.pdf",
        "pdf_link": null
    },
    {
        "title": null,
        "implementation_urls": [],
        "doi": "10.26342/2022-69-6",
        "arxiv": null,
        "abstract": null,
        "publication_date": null,
        "authors": null,
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A scoping review on the use, processing and fusion of geographic data in virtual assistants",
        "implementation_urls": [],
        "doi": "10.1111/tgis.12720",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2021-01-05",
        "authors": "Carlos Granell, Paola Pesántez-Cabrera, Luis M. Vilches‐Blázquez, Rosario Achig, Miguel R. Luaces, Alejandro Cortiñas‐Álvarez, Carolina Chayle, Villie Morocho",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Semantic Interoperability for DR Schemes Employing the SGAM Framework",
        "implementation_urls": [],
        "doi": "10.1109/sest48500.2020.9203338",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2020-09-01",
        "authors": "Andrea Cimmino, Nikoleta Andreadou, Alba Fernández-Izquierdo, Christos Patsonakis, Apostolos C. Tsolakis, Alexandre Lucas, Dimosthenis Ioannidis, Evangelos Kotsakis, Dimitrios Tzovaras, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "A SAREF Extension for Semantic Interoperability in the Industry and Manufacturing Domain",
        "implementation_urls": [],
        "doi": "10.1002/9781119564034.ch25",
        "arxiv": null,
        "abstract": "",
        "publication_date": "2018-10-26",
        "authors": "Laura Daniele, Matthijs Punter, Christopher Brewster, Raúl García Castro, María Poveda, Alba Fernández",
        "file_name": "10!1002%9781119564034!ch25.pdf",
        "file_path": "output/PDFs/10!1002%9781119564034!ch25.pdf",
        "pdf_link": null
    },
    {
        "title": "First Attempt to an Easy-to-Read Adaptation of Repetitions in Captions",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-08648-9_48",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2022-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Isam Diab, Álvaro González, Jesica Rivero-Espinosa",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Towards a new generation of ontology based data access",
        "implementation_urls": [],
        "doi": "10.3233/sw-190384",
        "arxiv": null,
        "abstract": "Abstract. Ontology Based Data Access (OBDA) refers to a range of techniques, algorithms and systems that can be used todeal with the heterogeneity of data that is common inside many organisations as well as in inter-organisational settings andmore openly on the Web. In OBDA, ontologies are used to provide a global view over multiple local datasets; and mappings arecommonly used to describe the relationships between such global and local schemas. Since its inception, this area has evolvedin several directions. Initially, the focus was on the translation of original sources into a global schema, and its materialisation,including non-OBDA approaches such as the use of Extract Transform Load (ETL) workflows in data warehouses and, morerecently, in data lakes. Then OBDA-based query translation techniques, relying on mappings, were proposed, with the aim ofremoving the need for materialisation, something especially useful for very dynamic data sources. We think that we are nowwitnessing the emergence of a new generation of OBDA approaches. It is driven by the fact that a new set of declarative mappinglanguages, most of which stem from the W3C Recommendation R2RML for Relational Databases (RDB), are being created.In this vision paper, we enumerate the reasons why new mapping languages are being introduced. We discuss why it may berelevant to work on translations among them, so as to benefit from the engines associated to each of them whenever one languageand/or engine is more suitable than another. We discuss the emerging concept of “mapping translation”, the basis for this newgeneration of OBDA, together with some of its desirable properties: information preservation and query result preservation. Weshow several scenarios where mapping translation can be or is being already applied, even though this term has not necessarilybeen used in existing literature.Keywords: OBDA, Data Translation, Query Translation, Mapping Translation1. IntroductionDatabase technologies play a vital role in the devel-opment of information systems for all sorts of organi-sations. So far, relational databases (RDB) are still thedominating type of structure and technology used fordata management inside organisations, although otherformats (e.g. JSON, spreadsheets, XML) and typesof databases (e.g. noSQL, graph databases) have alsoemerged as alternatives for data representation andmanagement in the last decades.*Corresponding author. E-mail: ocorcho@fi.upm.es.In the early days of information system devel-opment, it was natural for organisations to developtheir own data models, which were strongly alignedwith their activities. This led to a large heterogene-ity across organisations, and even across different de-partments inside the same organisation. Such hetero-geneity was especially evident in the case of organ-isational changes, merges, etc. Similarly, data ware-houses were also used in order to align and materialisedata from different sources, normally from the sameorganisation, so as to provide support for analyticalqueries and for the generation of reports. These situ-ations made researchers and professionals start work-1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:ocorcho@fi.upm.esmailto:fpriyatna@fi.upm.esmailto:dchaves@fi.upm.esmailto:ocorcho@fi.upm.es2 O. Corcho et al. / Towards a New Generation of Ontology Based Data Access1 12 23 3",
        "publication_date": "2019-12-31",
        "authors": "Óscar Corcho, Freddy Priyatna, David Chaves-Fraga",
        "file_name": "10!3233%sw-190384.pdf",
        "file_path": "output/PDFs/10!3233%sw-190384.pdf",
        "pdf_link": null
    },
    {
        "title": "Towards Blockchain and Semantic Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-36691-9_19",
        "arxiv": null,
        "abstract": null,
        "publication_date": "2019-01-01",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": null,
        "file_path": null,
        "pdf_link": null
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "arxiv": null,
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "publication_date": "2021-05-07",
        "authors": "",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": "output/PDFs/10!3233%ssw51.pdf",
        "pdf_link": null
    }
]