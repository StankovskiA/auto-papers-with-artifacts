[
    {
        "title": "Gender imbalance in doctoral education: an analysis of the Spanish university system (1977–2021)",
        "implementation_urls": [],
        "doi": "10.1007/s11192-023-04648-y",
        "abstract": "AbstractDoctoral education is a key feature of university systems, as well as a basic foundation of scientific practice. That period culminates in a dissertation and examination of the candi‑date that has been studied from several points of view. This paper reports the results of an analysis on the evolution and characteristics of gender imbalance of a complete doctoral system for a wide period of time. Data from the database Teseo was used in order to iden‑tify the individuals involved in the process, the scientific fields in which the dissertations where classified, and the institutions in which the examination took place. Results: the Spanish system shows a clear evolution towards gender balance, but also some concern‑ing trends that are worth tracking. Seemingly, STEM disciplines look to be evolving more slowly than other branches of science in several aspects. A leaky pipeline is characterized in this system around the roles of supervisors, candidates, members and chairs of the dis‑sertation committees. Gender assortativity is also studied and described, and its possible effects discussed around the academic relations that surround doctoral examination.Keywords  Gender imbalance · STEM · Dissertations · Teseo · Leaky pipeline · Gender assortativity *\t Rodrigo Sánchez‑Jiménez \t rodsanch@ucm.es1\t Library and Information Science Department, SCImago Group, Universidad Complutense de Madrid, Madrid, Spain2\t Library and Information Science Department (Internet Medialab Research Group), Universidad Complutense de Madrid, Madrid, Spain3\t Sales Engineering EMEA, Neo4j, London, UK4\t Ontology Engineering Group (Artificial Intelligence Department), Universidad Politécnica de Madrid, Madrid, Spain5\t Library and Information Science Department, Universidad Complutense de Madrid, Madrid, Spainhttp://orcid.org/0000-0002-3685-7060http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-023-04648-y&domain=pdf2578\t Scientometrics (2023) 128:2577–25991 3IntroductionGender imbalance and gender bias in science have been studied and described for a long time. Zuckerman and Cole (1975) already described this issue in quantitative terms and advanced the “principle of triple penalty” (cultural inappropriateness/perceived incompe‑tence/direct discrimination). Shiebinger (1987) went over the very abundant literature on the history of women in science and described how at least the number of female scientists was growing faster (low numbers having been associated with that cultural inappropriate‑ness) but the perception of a lesser competence by women (women were systematically employed in less prestigious jobs in the academia) and blatant discrimination (unjustified salary gaps were huge) was ever persistent. Etzkowitz, et al (1992) moved on to put the focus on the de-genderization of science and society, and on the existence of “different gender styles of scientific work”, an idea that has been one way or another behind many studies comparing output, collaboration, and impact of men and women. Bordons et  al. (2003) acknowledge this factor as a warning to interpret their SCI-based results but take it a step further. They also explained the cumulative advantage of achieving high ranks in academia over productivity, which in turn accounts for the gender differences in pro‑ductivity. Several years later, Lariviere et al., (2011) reached a somewhat different conclu‑sion, finding again that gender differences were present in terms of production and funding, although the nature of these differences was complex. The subject is therefore very much open to debate, and the focus on its study has varied significantly over time (Tomassini, ",
        "publication_date": "2023-02-11",
        "authors": "Rodrigo Sánchez Jiménez, Iuliana Botezán, Jesús Barrasa-Rodríguez, Mari Carmen Suárez-Figueroa, Manuel Blázquez Ochando",
        "file_name": "10!1007%s11192-023-04648-y.pdf",
        "file_path": "./PDFs/10!1007%s11192-023-04648-y.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s11192-023-04648-y.pdf"
    },
    {
        "title": "Using the SPAR Ontology Network to Represent the Scientific Production of a University: A Case Study",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-16187-3_20",
        "abstract": "Abstract. Research is commonly used to measure the prestige of universities. Currently, many universities register some of their scientific production (such as thesis, articles) in open access repositories using technologies like DSpace or ePrints. Likewise, scientific production is available in different and overlapping databases such as Scopus, Web of Science, Google Scholar, Crossref. Connecting these datasets, with the application of ontologies, will potentially increase their value and help discover interesting relationships amongst them. The present study aims to use the SPAR Ontology Network, which allows representing schol-arly publishing, in order to check whether it is possible to represent the scientific production of universities. For that, we propose competency questions with the purpose of measuring scientific production. Likewise, we obtained data from Scopus regarding an Ecuadorian university (University of Guayaquil) and trans-formed it to RDF using the SPAR Ontology Network and other well-known on-tologies to build it semantically. We used SPARQL queries to answer the com-petency questions. We concluded that the SPAR Ontology Network is a wide-ranging solution for scholarly publishing. Nevertheless, it is necessary to build an extension to one of their ontologies to provide a complete representation of a university’s scientific production.  Keywords: RDF, Ontology, Scientific Production, Scholarly Publishing, SPAR Ontology Network, SPARQL. 1 Introduction According to the Oxford dictionary,1 a university is a high-level educational institution in which students study for degrees, and academic research is done. Research is an essential activity inside universities, hence the scientific production of a university is a crucial piece in the scoring of universities. Some internationally recognized organiza-tions that measure and score universities, such as Times Higher Education-QS World University Rankings (www.timeshighereducation.com), Academic Ranking of World Universities (www.shanghairanking.com), Webometrics (www.webometrics.info) in-clude research as one of the evaluation criteria. Universities are now generally using open access repositories technologies like DSpace (duraspace.org) or ePrints (www.eprints.org) to register the scientific produc-tion (thesis or papers), which has helped to visualize and centralize the information in institutional repositories. Besides, they provide support for publishing stored contents in the form of Linked (Open) Data [1] and establish arbitrary relations between objects or provide additional metadata [2]. Likewise, information about the scientific produc-tion is available in different and overlapping databases (such as Scopus, Web of                                                          1 https://en.oxforddictionaries.com/definition/university http://www.webometrics.info/2 Science, Google Scholar, Crossref). Connecting these datasets, with the application of ontologies, will potentially increase their value and help discover interesting relation-ships amongst them [3].  The Semantic Web presents a new perspective on the association of the information contained in different databases [4] through the use of ontologies. An ontology is an engineering artifact constituted by a specific vocabulary to describe a particular reality [5]. Among their benefits, ontologies provide machine-readable metadata for data sources, using agreed standards that permit computers to assist in the tasks of infor-mation discovery and integration [6]. The Semantic Publishing and Referencing Ontol-ogies, also known as the SPAR Ontology Network, is a suite of orthogonal and com-",
        "publication_date": "2019-01-01",
        "authors": "Mariela Tapia-León, Janneth Chicaiza, Paola Espinoza-Arias, Idafen Santana-Pérez, Óscar Corcho",
        "file_name": "Tapia_SparOntologyScientificProduction.pdf",
        "file_path": "./PDFs/Tapia_SparOntologyScientificProduction.pdf"
    },
    {
        "title": "Caracterización de riesgos urbanos en prensa aplicando minería de texto para el enriquecimiento de datos abiertos",
        "implementation_urls": [],
        "doi": "10.22201/iibi.24488321xe.2022.91.58538",
        "abstract": "AbstractNews is freely spread and widely available to Internet users much more easily than traditional media. In the news, we can find an infinite number of hidden “minor data,” that can provide valuable information not col-lected in other sources of information. In this context, we have been interested in analyzing and characteriz-ing the urban risks contained in the Uruguayan open newspapers using text mining techniques. This pro-posal makes it possible to create a news corpus based on risk events included in open data. The corpus cov-ers 2003-2019 and is built from the digital open news-papers El Eco Digital, Montevideo Portal, and La Red 21. Various text mining techniques are applied to this corpus using the QDA-MinerLite software and the Python language (concretely, through the Scattertext library) to identify, characterize, and discover insights on these events. The corpus processing results help en-rich the existing open data on risks in Uruguay, incor-porating information on their effects, actors, and asso-ciated interventions.Keywords: Urban Risk; Text Mining; Open Digi-tal Newspapers; Open DataCARACTERIZACIÓN DE RIESGOS URBANOS EN PRENSA APLICANDO MINERÍA...87DOI: http://dx.doi.org/10.22201/iibi.24488321xe.2022.9",
        "publication_date": "2022-05-09",
        "authors": "Luis M. Vilches‐Blázquez, Diana Comesaña",
        "file_name": "10!22201%iibi!24488321xe!2022!91!58538.pdf",
        "file_path": "./PDFs/10!22201%iibi!24488321xe!2022!91!58538.pdf"
    },
    {
        "title": "Towards the Definition of a Language-Independent Mapping Template for Knowledge Graph Creation (short paper).",
        "implementation_urls": [],
        "abstract": "ABSTRACTThe use of knowledge graphs is spreading in the scientific commu-nity across different domains, from social sciences to biomedicine.The creation of knowledge graphs usually needs the integrationof multiple heterogeneous data sources in different formats andschemas. One common way to achieve this process is using declara-tive mappings, which establish the relationships between the sourcedata and the ontology, improving relevant aspects such as main-tainability, readability and understandability. Learning how to useand create mappings is not an easy task, hindering the use of thistechnology to anyone outside the area. As a result, this task is usu-ally carried out by experts. To ease the mapping creation, severalmapping editors have been developed, but their success is limited.In this paper, we devise the use of a well-known tool commonlyused in the scientific community, the spreadsheets, to specify themapping rules in a language-independent way. Our aim is to easethe mapping creation and make it more accessible for the commu-nity. We also show a real use case, in which using spreadsheetshelps in the mapping creation process and enables a handy way forediting and visualizing mapping rules.CCS CONCEPTS•Computingmethodologies→Artificial intelligence; Knowl-edge representation and reasoning.KEYWORDSKnowledge graph, spreadsheet, declarative mapping1 INTRODUCTIONThe expansion of the Semantic Web technologies has reached usersacross several domains, such as legal and biomedical. An increasingnumber of knowledge graphs from these areas are being created,restructuring knowledge in a machine-readable way [4]. For theirconstruction it is necessary to integrate different data sources; thenthey allow search optimization and the possibility of applying ma-chine learning techniques to obtain new knowledge, among otherpossibilities. Some examples are DBpedia [1] and Wikidata [18].There are multiple approaches to create knowledge graphs, fromusing ad-hoc tools to declarative mappings. The later defines rulesCopyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).to establish relationships between the global schema and the datasources. Examples of mappings languages are the W3C recommen-dation R2RML [7] and its extension RML [9].The use of declarative mappings for semantic web non-expertsis often complicated. That is one of the reasons why the mappingcreation is usually carried out by knowledge engineers. This posesa barrier for potential users from other domains. To face this issue,several mapping editors have been proposed. They aim at makingthe mapping creation and editing easier and more intuitive [11, 16].Despite these efforts, users prefer to use tools like OpenRefine1,which is non-declarative, thus hindering the reproducibility andmaintainability of the transformations performed.",
        "publication_date": "2019-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho",
        "file_name": "no_doi_20250624160336.pdf",
        "file_path": "./PDFs/no_doi_20250624160336.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short3.pdf"
    },
    {
        "title": "Characterizing water quality datasets through multi-dimensional knowledge graphs: a case study of the Bogota river basin",
        "implementation_urls": [],
        "doi": "10.2166/hydro.2022.070",
        "abstract": "ABSTRACTThe world is transforming into a predominantly urban space, meaning that cities have to be ready to provide services, for instance, to ensureavailability and sustainable management of water and sanitation for all. In this scenario, the water quality evaluation has a crucial role andoften needs multiple sources segregated. Our purpose is to build bridges between these data silos to provide an integrated and interoperableview, where different datasets can be provided and combined through knowledge graphs in order to characterize water quality. This workshows the quality of the Bogota river basin’s water bodies by analyzing physicochemical and biological properties using spatio-temporal andlegal elements. So, our knowledge graphs allow us to discover what, when, and where infractions happened on water quality in a river basinof the most populated cities of Latin America during a critical period (2007–2013), highlighting the presence of high values of suspendedsolids and nitrites, lower amounts of dissolved oxygen, and the worst water quality during the driest periods (appearing until a maximumof 63 infractions in a year).Key words: Bogota river basin, knowledge graph, ontology, spatio-temporal characteristics, water qualityHIGHLIGHTS• A new water quality ontology with three modules composed of diverse international standards.• Multi-dimensional knowledge graphs about the water quality of the Bogota river basin were developed.• The water quality characterization using spatio-temporal distribution and legal framework from an integrated and interoperable scenario.GRAPHICAL ABSTRACTThis is an Open Access article distributed under the terms of the Creative Commons Attribution Licence (CC BY 4.0), which permits copying, adaptation andredistribution, provided the original work is properly cited (http://creativecommons.org/licenses/by/4.0/).://iwaponline.com/jh/article-pdf/24/2/295/1030721/jh0240295.pdfhttps://orcid.org/0000-0001-5799-469Xmailto:lmvilches@cic.ipn.mxhttp://orcid.org/http://orcid.org/0000-0001-5799-469Xhttp://creativecommons.org/licenses/by/4.0/http://creativecommons.org/licenses/by/4.0/https://crossmark.crossref.org/dialog/?doi=10.2166/hydro.2022.070&domain=pdf&date_stamp=2022-02-23Journal of Hydroinformatics Vol 24 No 2, 296Downloaded frby gueston 24 June 20INTRODUCTIONThe world is transforming into a predominantly urban space, and in this sense, cities around the globe have experiencedunprecedented growth during the last four decades. According to the United Nations (United Nations 2018a), 55% of theworld’s population resides in urban places, a proportion that is expected to increase to 70% by 2050. In Latin Americaand the Caribbean, the tendency toward urbanization has been even more dramatic, going from 50.5% in 1961 to 81% in2018 (United Nations 2018b).This growth of urban areas around the world makes cities and their populations more vulnerable to global warming andclimate change (Valipour et al. 2021) since these spaces are often located in hazardous locations where economic assetsand residents increasingly find themselves at elevated risk of climate-related events (Gasper et al. 2011). Therefore, thisurban context entails that cities need to be ready to confront these new scenarios and, among other issues, to ensure avail-ability and sustainable water and sanitation management for all. It is part of the Sustainable Development Goals; concretely,goal 6 is related to the following target: improving water quality by reducing pollution, eliminating dumping, and minimizingrelease of hazardous chemicals and materials, halving the proportion of untreated wastewater and substantially increasingrecycling and safe reuse globally.This scenario points out the well-known vital importance of water because it is essential for life and, consequently, itsaccess to all people must be ensured through an adequate, sufficient, accessible, and innocuous supply concerning what isreferred to as its quality (WHO 2017; Pahl-Wostl 2020). In addition, the effects of global warming and climate changeaffect available water and impact its quality due to factors such as the low dilution of the pollutant load (Radhapyari et al.2021).The water quality monitoring and analysis is a fertile research area (Resh & Unzicker 1975; Ward et al. 1986; Ouyang 2005;",
        "publication_date": "2022-02-23",
        "authors": "Juan D. Rondón Díaz, Luis M. Vilches‐Blázquez",
        "file_name": "10!2166%hydro!2022!070.pdf",
        "file_path": "./PDFs/10!2166%hydro!2022!070.pdf"
    },
    {
        "title": "RML-star : a declarative mapping language for RDF-star generation",
        "implementation_urls": [],
        "abstract": "Abstract. RDF-star was recently proposed as a convenient represen-tation to annotate statements in RDF with metadata by introducingthe so-called RDF-star triples, bridging the gap between RDF and prop-erty graphs. However, even though there are many solutions to generateRDF graphs, there is no systematic approach so far to generate RDF-star graphs from heterogeneous data sources. In this paper, we proposeRML-star, an extension of the RML mapping language to generate RDF-star. We introduce the extension of the RML ontology and the associatedspecification with representative examples.URL: https://w3id.org/kg-construct/rml-starKeywords: RML · R2RML · RDF-star · Knowledge Graphs.1 IntroductionRDF-star was proposed as a compact representation to annotate statements inRDF with metadata [4]. For instance, the following declares that Bob claims Al-ice was born in 1996: :bob :claims <<:alice :birthYear 1996>>. Followingthe uptake of the proposed solution, a W3C Community Group was formed3 anda W3C Draft Report [5] was recently released with improvements over the orig-inal proposal. By now, several RDF-related programming libraries, e.g., EclipseRDF4J, Apache Jena, RDF.rb, and N3.js, and RDF graph database systems,e.g., Blazegraph, AnzoGraph, Stardog and GraphDB, have adopted RDF-star4.However, no mapping language supports the generation of RDF-star graphsso far. Most data are still heterogeneous, represented in different formats (e.g.,relational databases, CSV, JSON, or XML). One of the most common approachesnowadays to integrate them into RDF graphs is the use of declarative mappingCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.w3.org/community/rdf-dev/4 https://blog.liu.se/olafhartig/https://orcid.org/0000-0001-9521-2185https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-2138-7972https://w3id.org/kg-construct/rml-starhttps://www.w3.org/community/rdf-dev/https://blog.liu.se/olafhartig/Delva et al.Fig. 1: The RML-star extension (Chowlk notation [3]). Orange classes and darkorange object properties show the additions to the RML ontology, light orangeobject properties represent extensions (i.e., change in domain and/or range).languages such as R2RML [1] and RML [2]. R2RML is the W3C Recommenda-tion mapping language to generate RDF graphs from relational databases. RMLis a superset of R2RML that generates RDF graphs from data formats beyondrelational databases, such as CSV, JSON, or XML. Extending a mapping lan-guage to specify how RDF-star datasets can be generated from heterogeneousdata sources can potentially increase the amount of available RDF-star datasetsand, thus, foster the adoption of the RDF-star proposal.In this paper, we propose RML-star, an extension of RML to generate RDF-star graphs from heterogeneous data sources. We introduce a set of new classes",
        "publication_date": "2021-01-01",
        "authors": "Thomas Delva, Julián Arenas-Guerrero, Ana Iglesias-Molina, Óscar Corcho, David Chaves-Fraga, Anastasia Dimou",
        "file_name": "no_doi_20250624160402.pdf",
        "file_path": "./PDFs/no_doi_20250624160402.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper374.pdf"
    },
    {
        "title": "Supervising Attention in an E-Learning System",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-01746-0_46",
        "abstract": "Abstract. Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. First, the worst aspect about this approach is that it only points out a potential decrease of attention after a productivity loss. An approach that could point out, in advance, upcoming breaks in attention could allow active/preventive interventions rather than reactive ones. In this paper we present a distributed system for monitoring attention in teams (of people). It is especially suit-ed for people working with computers and it can be interesting for domains such as the workplace or the classroom. It constantly analyz-es the behavior of the user while interacting with the computer and together with knowledge about the task, is able to temporally classify attention. Keywords: Distributed Intelligent System, Attention, Behavioral Biometrics. 1 Introduction In the field of computer science, an intelligent environment is a digitally augmented physical world where sensor-enabled and networked devices work continuously and collaboratively to make the lives of the inhabitants more comfortable. With this techno-logical evolution, job offers have changed, bringing along many significant and broad changes. Some of the most notorious ones can be pointed out by the emergence of indicators such as attentiveness which, in extreme cases, can compromise the life and well-being of the workers. In more moderate cases it will impair attention, general cognitive skills and productivity. In addition to these factors, many of these jobs are the so-called desk-jobs, in which people frequently sit for more than 8 hours [1]. Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. While the true nature of this relationship is yet to be thoroughly studied (properly contextualized in each work domain), there are other issues that need to be addressed. First, the worst aspect about this approach is that it only points out a potential decrease of attention after a productivity loss. This means that the “damage” is already done and that it is most  likely too late for the worker to cope with whatever caused the attention loss. An approach that could point out, in advance, upcoming breaks in attention (e.g. through the observation of behavioral patterns) could allow active/preventive interventions rather than reactive ones [2]. In this paper we present a distributed system for monitoring attention in teams (of people), in line with the vision of intelligent environments [3]. It is especially suited for people working with computers and it can be interesting for domains such as the workplace or the classroom. It constantly analyzes the behavior of the user while interacting with the computer and, together with knowledge about the task, is able to temporally classify attention. This work may be very interesting for team managers to assess the level of attention of their teams, identifying potentially dis-tracting events, hours or individuals. Moreover, distraction often appears when the individual is fatigued, bored or not motivated. This tool can thus be an important indicator of the team, allowing the manager to act accordingly at an individual or group level. In the overall, this tool will support the implementation of better human resources management strategies. 1.1 Previous Work Part of this framework was implemented in previous work. The first version focused on the analysis of the individuals’ interaction patterns with the computer, including features such as mouse velocity or acceleration, click duration, typing speed or rhythm, among others. For a complete list of features as well as the process of their acquisition and extraction, please see [4]. However, a limitation was also identified in this previous work. In fact, a user that opened a no work-related application and did not interact anymore with the computer until the end of the task had 0% of attention. On the other hand, if the user opens a work-related appli-cation and does not interact with the computer after that, the user's attention will be classified as 100% when he is most likely not even at the computer. The present work adds a new feature to this previously existing framework, by providing a precise measure of attention based not on the tasks work-related patterns but also on the key typing or mouse movement pattern. It thus constitutes a much more precise and reliable mechanism for attention monitoring, while maintaining all the advantages of the existing system: nonintru-sive, lightweight and transparent. mailto:d.alves@alumnos.upm.esmailto:pjon@di.uminho.pt2 2 Architecture From the architecture of the developed environment described in Figure 1 it is possible to collect data that describe the interaction with both the mouse and the keyboard in the devices in which students work. Three parts compose the module’s architecture.  In the devices operating by the students, it’s installed software that generates raw data, which store locally until it is synchro-nized with the web server in the cloud. In this step it’s encodes each event with the corresponding necessary information (e.g. ",
        "publication_date": "2018-11-04",
        "authors": "Dalila Durães, Javier Bajo, Paulo Nováis",
        "file_name": "10!1007%978-3-030-01746-0_46.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-01746-0_46.pdf"
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "publication_date": "2021-07-19",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": "./PDFs/10!23919%annsim52504!2021!9552126.pdf"
    },
    {
        "title": "Depicting Vocabulary Summaries with Devos",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/DeVoS",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/3543873.3587359.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3https://eswc-conferences.org/4https://iswc20XX.semanticweb.org/(where the XX is the year) 5Ontologies available at https://github.com/oeg-upm/DeVoS/tree/main/data/ieswc 251 https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize https://eswc-conferences.org/https://github.com/oeg-upm/DeVoS/tree/main/data/ieswc (b) Visual summary example using the ClaFreq technique."
                    }
                ]
            }
        ],
        "doi": "10.1145/3543873.3587359",
        "abstract": "ABSTRACT • License: Apache 2.0 • Demo: https://devos.linkeddata.es • GitHub: https://github.com/oeg-upm/Devos Communicating ontologies to potential users is still a difcult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams to-gether with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on diferent strategies for summarizing the ontology. CCS CONCEPTS • Computing methodologies → Knowledge representation and reasoning. KEYWORDS ontology diagrams, ontology summarization, ontology engineering ACM Reference Format: Ahmad Alobaid, Jhon Toledo, Oscar Corcho, and María Poveda-Villalón. 2023. Depicting Vocabulary Summaries with Devos. In Companion Pro-ceedings of the ACM Web Conference 2023 (WWW ’23 Companion), April 30–May 04, 2023, Austin, TX, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3543873.3587359 1 INTRODUCTION Together with data-sharing initiatives at a web-scale, as Linked Open Data [1], the adoption of ontologies1 to semantically describe the data shared has risen accordingly. Reusing ontologies is highly encouraged to maximize interoperability among datasets or appli-cations and reduce development eforts [9]. 1The terms “vocabulary” and “ontology” are used interchangeably in this paper. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWW ’23 Companion, April 30–May 04, 2023, Austin, TX, USA © 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9419-2/23/04. https://doi.org/10.1145/3543873.3587359 To facilitate the understanding and, therefore, the reuse of exist-ing vocabularies by others, vocabulary publishers should make their vocabularies available together with the documentation, metadata, and examples of use [4]. A key aspect of ontology documentation is the generation of diagrams or graphical descriptions to help po-tential users understand the conceptual model behind the code. However, very often, vocabularies are published without providing graphical visualizations, leading to more difculties in understand-ing the model as the vocabularies get bigger. ",
        "publication_date": "2023-04-28",
        "authors": "Ahmad Alobaid, Jhon Toledo, Óscar Corcho, María Poveda‐Villalón",
        "file_name": "3543873.3587359.pdf",
        "file_path": "./PDFs/3543873.3587359.pdf",
        "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3543873.3587359"
    },
    {
        "title": "Re-Construction Impact on Metadata Representation Models",
        "implementation_urls": [],
        "doi": "10.1145/3587259.3627554",
        "publication_date": "2023-11-29",
        "authors": "Ana Iglesias-Molina, Jhon Toledo, Óscar Corcho, David Chaves-Fraga",
        "file_name": "10!1145%3587259!3627554.pdf",
        "file_path": "./PDFs/10!1145%3587259!3627554.pdf"
    },
    {
        "title": "Declarative generation of RDF-star graphs from heterogeneous data",
        "implementation_urls": [],
        "doi": "10.3233/sw-243602",
        "abstract": "ABSTRACTReification in knowledge graphs has been present since the incep-tion of RDF to allow capturing additional information in triples,usually metadata. The need of adopting or changing a metadatarepresentation in a pre-existing graph to enhance the knowledgecapture and access can lead to inducing complex structural changesin the graph, according the target representation’s schema. In thesesituations, it is necessary to decide whether to construct the knowl-edge graph again from its original sources, or to re-construct itusing the current version of the graph. In this paper we conduct anempirical study to analyze which re-construction approach is moresuitable for switching the representation approach from the createdgraph ensuring that the additional represented knowledge is pre-served. We study four well-known metadata representations, usingmapping languages to construct the graph, and SPARQL CONSTRUCTqueries to re-construct it. With this work we aim to provide insightsabout the impact of re-construction on metadata representationsinteroperability and the implications of different approaches.CCS CONCEPTS• Information systems→ResourceDescription Framework (RDF);Graph-based database models; Data exchange; Data model exten-sions;KEYWORDSKnowledge Graphs, Metadata, SPARQL, Declarative Mappings.ACM Reference Format:Ana Iglesias-Molina, Jhon Toledo, Oscar Corcho, and David Chaves-Fraga.2023. Re-Construction Impact onMetadata RepresentationModels. InKnowl-edge Capture Conference 2023 (K-CAP ’23), December 5–7, 2023, Pensacola, FL,USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3587259.3627554Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.K-CAP ’23, December 5–7, 2023, Pensacola, FL, USA© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0141-2/23/12. . . $15.00https://doi.org/10.1145/3587259.36275541 INTRODUCTIONKnowledge graphs (KG) have gained popularity in recent years forintegrating and publishing knowledge on the web [22]. KGs aremodelled according to schemes that are not immutable, as theyare subject to modifications triggered by changes in the domain ofknowledge or in the consumption needs of downstream tasks. Anexample is the need to incorporate additional knowledge about atriple, which is known as statement reification. Reification is usuallyused to include metadata and provenance to existing triples [15].",
        "publication_date": "2024-03-20",
        "authors": "Julián Arenas-Guerrero, Ana Iglesias-Molina, David Chaves-Fraga, Daniel Garijo, Óscar Corcho, Anastasia Dimou",
        "file_name": "10!3233%sw-243602.pdf",
        "file_path": "./PDFs/10!3233%sw-243602.pdf"
    },
    {
        "title": "DLT-Based Personal Data Access Control with Key-Redistribution",
        "implementation_urls": [
            {
                "identifier": "https://github.com/disnocen/umbral-rs",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/bcca58897.2023.10338895.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3The implementation can be found at https://github.com/disnocen/umbral-rs 2023 Fifth International Conference on Blockchain Computing and Applications (BCCA) 172 Authorized licensed use limited to: Univ Politecnica de Madrid."
                    }
                ]
            }
        ],
        "doi": "10.1109/bcca58897.2023.10338895",
        "abstract": "Abstract—Data management services present a challenge interms of trust, as service managers can access the data on theirservers easily. Decentralized data services and smart contractscan solve problems related to the presence of centralized trustedauthorities, but in turn they can introduce other issues relatedto compliance with data protection and regulations (e.g., GDPR).Historically, encryption has been used to address some of theseconcerns, but it restricts data sharing. To facilitate encrypteddecentralized file storage while enabling data sharing, we proposea Key-Redistribution Proxy Re-Encryption (KeRePRE) system.KeRePRE is a decentralized and encrypted data-service, whereauthorization servers are part of a threshold proxy re-encryptionscheme. A key-redistribution mechanism (that extends the Um-bral scheme) allows for the addition and removal of managersin a decentralized and trustless manner. Additionally, we offera proof of concept implementation, where data access control isbased on an access control list, implemented as a smart contractin a DLT, and can be read-only accessed by the authorizationservers.Index Terms—Proxy re-encryption, Threshold scheme, GDPR,Data Sharing, Decentralized File SystemI. INTRODUCTIONNowadays, data are of high value to individuals, businesses,and governments. The increasing amount of data generated byvarious sources like mobile devices, sensors, and social mediahas given rise to big data and data-driven decision-making.However, the volume and complexity of data have createdsignificant challenges in managing, processing, analyzing, andprotecting them, especially regarding the new directives relatedto data protection such as GDPR [1]. To overcome thesechallenges, data intermediation has become crucial. A dataintermediator acts as an intermediary between data holdersand recipients, facilitating the flow of data while ensuring itsquality and security.Decentralized systems such as blockchains or DistributedLedger Technologies (DLTs) have gained considerable at-tention in data intermediation. These systems allow datarecipients and processors to be limited to the data holder’sinstructions through smart contracts, enabling intermediationwith a higher level of control over data, see e.g. [2]. De-centralization, however, necessitates cryptography to securedecentralized systems and protect data while sharing it. In atrustless or semi-trusted decentralized system, comprehensivesecurity is essential to maintain user trust. To solve theseproblems, approaches such as (t, n)-threshold cryptosystemsinvolve multiple parties performing a cryptographic operationtogether, using a share of a secret in a secret-sharing scheme,in order to transform one central party into a committee. Inparticular, a Threshold Proxy Re-Encryption (TPRE) schemedelegates some data intermediaries (proxies) to re-encrypt the",
        "publication_date": "2023-10-24",
        "authors": "Fadi Barbàra, Mirko Zichichi, Stefano Ferretti, Claudio Schifanella",
        "file_name": "bcca58897.2023.10338895.pdf",
        "file_path": "./PDFs/bcca58897.2023.10338895.pdf",
        "pdf_link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10338895"
    },
    {
        "title": "Decentralized Health Data Distribution: A DLT-based Architecture for Data Protection",
        "implementation_urls": [],
        "doi": "10.1109/blockchain55522.2022.00023",
        "abstract": "Abstract—The management, protection and sharing of sensitivedata such as those associated with the health domain are crucialin enabling personal care and contributing to worldwide medicaladvancements. Distributed Ledger Technologies (DLTs) allow fordata protection compliant solutions in untrusted contexts thatguarantee data immutability, protection and transparency whenneeded. This paper proposes an architecture based on DLTs,Smart Contracts and Distributed File Storage (DFS), enablinguser data sovereignty, confidentiality and secure access control.A use case on health data is presented, where we apply acombination of DLT, DFS and an access control mechanism toallow users to distribute their data. Finally, we show an ex-perimental evaluation of the overall architecture to demonstratethe feasibility of implementing practical DLT-based healthcaresolutions. The results are collected through independent tests,available opensource, that verify the system’s response time ineach of its functions and as the load increases. The results arepromising and show that the system is feasible and can scale asthe load increases.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed StorageI. INTRODUCTIONDigital technologies are continuously transforming society.Personal devices are foundational to this transformation, whereindividuals are the primary sources of information generation.Storing data in inaccessible and disconnected data lakes makesthem inaccessible to the public for innovation [1]. Followingthis, the interest in data ownership arises first and foremostfrom the lack of transparency in how data is collected, storedand used by different services and companies. In this regard, alow effort has been spent on easing the data management foran individual to understand and manage the risks associatedwith exploiting his private data.Healthcare could enormously benefit from the ability toshare information, transitioning from centralized to decentral-ized system architectures. Individuals can make a substantialcontribution through personal devices to science, specificallyin personalized medicine [2], [3]. Unfortunately, there arestrong barriers represented by privacy and security for thehealth sector: sharing information without the individual’sexplicit consent constitutes a substantial violation of an in-dividual’s rights. Regulations such as the European Union’sThis work has received funding from Regione Marche with DDPF n. 1189and from the EU’s Horizon 2020 research and innovation programme underthe MSCA ITN grant agreement No 814177 LAST-JD-RIoE.General Data Protection Regulation (GDPR) [4] help promotea pro-individual view. Specifically, these regulations imposemany accountability measures on actors responsible for pro-cessing personal data and assign several rights to individuals.However, these do not always address the lack of transparency",
        "publication_date": "2022-08-01",
        "authors": "Gioele Bigini, Mirko Zichichi, Emanuele Lattanzi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "blockchain55522.2022.00023.pdf",
        "file_path": "./PDFs/blockchain55522.2022.00023.pdf",
        "pdf_link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9881818"
    },
    {
        "title": "EBOCA: Evidences for BiOmedical Concepts Association Ontology",
        "implementation_urls": [
            {
                "identifier": "https://github.com/drugs4covid/EBOCA-Resources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-031-17105-5_11.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The result-ing KGs and related resources (mappings, queries) are available in GitHub20."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-17105-5_11",
        "arxiv": "2208.01093",
        "abstract": "Abstract. There is a large number of online documents data sourcesavailable nowadays. The lack of structure and the differences betweenformats are the main difficulties to automatically extract informationfrom them, which also has a negative impact on its use and reuse. Inthe biomedical domain, the DISNET platform emerged to provide re-searchers with a resource to obtain information in the scope of humandisease networks by means of large-scale heterogeneous sources. Specif-ically in this domain, it is critical to offer not only the information ex-tracted from different sources, but also the evidence that supports it.This paper proposes EBOCA, an ontology that describes (i) biomedi-cal domain concepts and associations between them, and (ii) evidencessupporting these associations; with the objective of providing an schemato improve the publication and description of evidences and biomedicalassociations in this domain. The ontology has been successfully evalu-ated to ensure there are no errors, modelling pitfalls and that it meetsthe previously defined functional requirements. Test data coming froma subset of DISNET and automatic association extractions from textshas been transformed according to the proposed ontology to create aKnowledge Graph that can be used in real scenarios, and which has alsobeen used for the evaluation of the presented ontology.Keywords: Ontology · Biomedicine · Evidences · Provenance · Semantic1 IntroductionThe availability of biomedical data has increased in recent decades [6]. This typeof content, whether structured (i.e. relational databases) or unstructured (i.e.text), is usually organized in separate isles owned by companies or institutions,sometimes with proprietary formats. This heterogeneity makes it difficult to ex-tract knowledge through them. The search for drugs, for instance, that couldarXiv:2208.01093v1  [cs.AI]  1 Aug 20222 A. Álvarez Pérez et al.interact with a certain drug, e.g. Plaquenil, during the treatment of COVID-19based on the experiments published in scientific publications becomes a chal-",
        "publication_date": "2022-01-01",
        "authors": "Andrea Álvarez-Pérez, Ana Iglesias-Molina, Lucía Prieto Santamaría, María Poveda‐Villalón, Carlos Badenes-Olmedo, Alejandro Rodríguez‐González",
        "file_name": "10!1007%978-3-031-17105-5_11.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-17105-5_11.pdf"
    },
    {
        "title": "VocEditor - An Integrated Environment to Visually Edit, Validate and Versioning RDF Vocabularies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AKSW/shaclEditor",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/icsc50631.2021.9475916.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "SHACLEditor code and releases are open available at https://github.com/AKSW/shaclEditor."
                    }
                ]
            }
        ],
        "doi": "10.1109/icsc50631.2021.9475916",
        "abstract": "Abstract—In this paper, we present an early stage of an inte-grated environment allowing the user to edit an RDF vocabularyvisualizing the relations among classes, properties, and SHACLshapes. It also includes automatic generation of SHACL shapesbased on the SHACL classes and properties as well as thevalidation of the instances according to the SHACL constraints.The proposed environment is integrated with a versioned triplestore which allows to have multiple versions of the vocabularywhile tracking provenance information. To the best of ourknowledge, this is the first integrated environment for visualizing,Validating, and Versioning RDF vocabularies.I. INTRODUCTIONThe RDF1 visualization represents an important character-istic of the Linked Open Data community to make RDF dataunderstandable for users with little or no knowledge about theSemantic Web [1], also strong enforcement of research in RDFvisualization has been done [2], [3], [4] which justify the needfor visualize RDF vocabularies.Besides the need for visualization of RDF vocabularies,since 20072, the public Knowledge Graphs (KGs) are growingsignificantly, to deal with the quality and assessment of thislarge data cloud, the W3C has promoted a recommendationcalled Shapes Constraint Language (SHACL). Since then,SHACL has become a relevant research topic [5].Given the decentralized and collaborative nature of RDFvocabulary creation, there is a need for maintaining andmanaging track of the different versions. To deal with thisversioning need, we use the Quit Store [6], which was in-spired by and built upon the successful Git system. Duringthe collaborative curation process, the system automaticallyversions the RDF dataset and tracks provenance information.It also provides support to branch, merge, and synchronizedistributed RDF datasets. The merging process is guarded byspecific merge strategies for RDF data.1http://www.w3.org/RDF/2Official website measuring the growth of LOD Cloud: https://lod-cloud.net/A. Use casesAs a motivation of our work, we describe two distinctscenarios where the proposed resource may be useful:a) Scenario 1:: Given an RDF class called person,assuming that a person cannot die before his birthday andthis person can only have one social security number. Thefollowing questions are raised:• How to validate if the birth date and death date arecorrect?• How to validate if a person has only one social securitynumber?• How to do all this validation without know RDF orSHACL constraints?",
        "publication_date": "2021-01-01",
        "authors": "André Valdestilhas, Gustavo Publio, Andrea Cimmino Arriaga, Thomas Riechert",
        "file_name": "icsc50631.2021.9475916.pdf",
        "file_path": "./PDFs/icsc50631.2021.9475916.pdf",
        "pdf_link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9475916"
    },
    {
        "title": "Defying Wikidata: Validation of terminological relations in the web of data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/sinaahmadi/LDTerm",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!13025%rev8-z474.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This work is openly available at https://github.com/sinaahmadi/LDTerm."
                    }
                ]
            }
        ],
        "doi": "10.13025/rev8-z474",
        "abstract": "AbstractIn this paper we present an approach to validate terminological data retrieved from open encyclopaedic knowledge bases. This needarises from the enrichment of automatically extracted terms with information from existing resources in the Linguistic Linked OpenData cloud. Specifically, the resource employed for this enrichment is WIKIDATA, since it is one of the biggest knowledge bases freelyavailable within the Semantic Web. During the experiment, we noticed that certain RDF properties in the Knowledge Base did notcontain the data they are intended to represent, but a different type of information. In this paper we propose an approach to validate theretrieved data based on four axioms that rely on two linguistic theories: the x-bar theory and the multidimensional theory of terminology.The validation process is supported by a second knowledge base specialised in linguistic data; in this case, CONCEPTNET. In ourexperiment, we validate terms from the legal domain in four languages: Dutch, English, German and Spanish. The final aim is to gen-erate a set of sound and reliable terminological resources in RDF to contribute to the population of the Linguistic Linked Open Data cloud.Keywords: linguistic linked open data, knowledge representation, terminological relations, resource population, multilingualism1. IntroductionWe are living what many people call the Fourth IndustrialRevolution that is distinguished by the strengthening of In-formation and Communications Technology, and specif-ically the emergence of the Knowledge Society (Bindé,2005). In this society, data and knowledge have becomevery valuable assets: generating, sharing and reusing dataare common transactions. However, not every area ofknowledge is undergoing such transformation at the samespeed; this is the case of the legal domain.In the last decade, law publishers, such as the EuropeanUnion, joined the effort of publishing documents in XML-based structured formats (Caterina Lupo et al., 2007). Sim-ilarly, there have also been several non-official initiatives(Chalkidis et al., 2017) (Frosterus et al., 2013) (Rodríguez-Doncel et al., 2018) to publish legislation as per the LinkedData Principles (Bizer et al., 2009). However, what seemsto be the definitive push in the exposure of legal resourcesas data in Europe is the European Legislation Identifier(ELI) 1, an initiative to harmonise the manner in which leg-islation is published. Every piece of legislation is identi-fied by an HTTP URI and homogeneously described witha common minimum set of metadata elements supportedby the ELI Ontology. Still, legal affairs have always beendifficult to understand by non-experts due to its intricateand complex jargon. Scarce documentation available on-line hardly helps improve this situation as many linguisticresources from the legal domain are still published in phys-ical formats and those that are online usually present a non-machine-readable structure (published as PDF).If we take the LLOD cloud2 as reference of structured lan-guage resources, we find domain specific assets (termi-nologies, thesauri, vocabularies and knowledge bases) such1https://publications.europa.eu/en/web/eu-vocabularies/eli2http://linguistic-lod.org/as the UMTHES3 (environmental thesaurus), GBA4 (geo-logical thesaurus, SentiWS5) (sentimental analysis vocabu-lary), amongst others. Nonetheless, very few resources in",
        "publication_date": "2020-05-16",
        "authors": "Patricia Martín-Chozas, Sina Ahmadi, Elena Montiel-Ponsoda",
        "file_name": "10!13025%rev8-z474.pdf",
        "file_path": "./PDFs/10!13025%rev8-z474.pdf",
        "pdf_link": "https://aclanthology.org/2020.lrec-1.694.pdf"
    },
    {
        "title": "CORAL: A Corpus of Ontological Requirements Annotated with Lexico-Syntactic Patterns",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/ontology-BTN100",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-21348-0_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "CORAL is openly available in three different open formats, namely, HTML, CSV and RDF under “Creative Commons Attribution 4.0 International” license."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.1967306",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-21348-0_29",
        "abstract": "Abstract. Ontological requirements play a key role in ontology devel-opment as they determine the knowledge that needs to be modelled. Inaddition, the analysis of such requirements can be used (a) to improveontology testing by easing the automation of requirements into tests; (b)to improve the requirements specification activity; or (c) to ease ontol-ogy reuse by facilitating the identification of patterns. However, thereis a lack of openly available ontological requirements published togetherwith their associated ontologies, which hinders such analysis. Therefore,in this work we present CORAL (Corpus of Ontological RequirementsAnnotated with Lexico-syntactic patterns), an openly available corpus of834 ontological requirements annotated and 29 lexico-syntactic patterns,from which 12 are proposed in this work. CORAL is openly availablein three different open formats, namely, HTML, CSV and RDF under“Creative Commons Attribution 4.0 International” license.Keywords: Corpus · Linked data · Ontological requirements ·Lexico-syntactic patternsResource type: DatasetDOI: https://doi.org/10.5281/zenodo.1967306URL: http://coralcorpus.linkeddata.es1 IntroductionIn recent years, the definition of functional ontological requirements [3,17,18],which represent the needs that the ontology to be built should cover, and theirautomatic formalization into axioms or tests (e.g., [5,13,19]) have been studied.This work is partially supported by the H2020 project VICINITY: Open virtual neigh-bourhood network to connect intelligent buildings and smart objects (H2020-688467),by ETSI Specialist Task Force 534, and by a Predoctoral grant from the I+D+i pro-gram of the Universidad Politécnica de Madrid. The authors want to thank Agnieszka�Lawrynowicz and her team for helping in the collection of ontological requirements.c© Springer Nature Switzerland AG 2019P. Hitzler et al. (Eds.): ESWC 2019, LNCS 11503, pp. 443–458, 2019.https://doi.org/10.1007/978-3-030-21348-0_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-21348-0_29&domain=pdfhttps://doi.org/10.5281/zenodo.1967306http://coralcorpus.linkeddata.eshttps://doi.org/10.1007/978-3-030-21348-0_29444 A. Fernández-Izquierdo et al.The aim of these studies is to reduce the time consumed by ontology develop-ers during the ontology verification activity, in which the ontology is comparedagainst the ontological requirements, thus ensuring that the ontology is builtcorrectly [16]. Functional ontological requirements can be written in the form ofcompetency questions, which are natural language questions that the ontologyto be modelled should be able to answer, or as statements, which are sentencesthat determine what should be built in the ontology.However, to accurately define ontological requirements is not a trivial taskand, therefore, neither is their automatic translation into a formal language. Dueto the fact that some requirements are ambiguous [9] or vague, their transfor-mation into axioms or tests is not usually direct and, consequently, it is verydifficult to automate such translation. The analysis of these functional onto-logical requirements can be used in several domains, e.g., to improve ontologytesting by easing the automation of requirements into tests, to improve the",
        "publication_date": "2019-01-01",
        "authors": "Alba Fernández-Izquierdo, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "10!1007%978-3-030-21348-0_29.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-21348-0_29.pdf"
    },
    {
        "title": "LACLICHEV: Exploring the History of Climate Change in Latin America Within Newspapers Digital Collections",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-85082-1_11",
        "arxiv": "2105.00792",
        "abstract": "Abstract. This paper introduces LACLICHEV (Latin American Cli-mate Change Evolution platform ), a data collections exploration en-vironment for exploring historical newspapers searching for articles re-porting meteorological events. LACLICHEV is based on data collections’exploration techniques combined with information retrieval, data analyt-ics, and geographic querying and visualization. This environment pro-vides tools for curating, exploring and analyzing historical newspapersarticles, their description and location, and the vocabularies used for re-ferring to meteorological events. The objective being to understand thecontent of newspapers and identifying possible patterns and models thatcan build a view of the history of climate change in the Latin Americanregion.Keywords: Data curation · metadata extraction · data collections ex-ploration · data analytics1 IntroductionHistorical analysis of climate behaviour can provide conclusions about clima-tologic phenomena and Earth climate behaviour. There exist several scientificefforts to study the history of climate change. The Climate of the Past [1],for example, is an international scientific journal dedicated to the publicationand discussion of research articles, short communications, and review papers onEarth’s climate history. The journal covers all temporal scales of climate change? We thank the master student Santiago Ruiz Angulo of the Universidad Autónoma deGuadalajara who implemented the first version of LACLICHEV during his internshipat the Barcelona Super Computer Centre, Spain funded by the CONACYT “becamixta” fellowship program of the Mexican government.arXiv:2105.00792v1  [cs.DL]  3 May 20212 G. Vargas-Solar et al.and variability, from geological time through to multidecade studies of the lastcentury. The Government of Canada provides access to historical observationson climate in Canada starting from 1840 [2] . However, these data collections are",
        "publication_date": "2021-01-01",
        "authors": "Genoveva Vargas‐Solar, José-Luis Zechinelli-Martini, Javier A. Espinosa-Oviedo, Luis M. Vilches‐Blázquez",
        "file_name": "10!1007%978-3-030-85082-1_11.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-85082-1_11.pdf",
        "pdf_link": "https://arxiv.org/pdf/2105.00792"
    },
    {
        "title": "SAD Generator: Eating Our Own Dog Food to Generate KGs and Websites for Academic Events",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-32327-1_19",
        "publication_date": "2019-01-01",
        "authors": "Pieter Heyvaert, David Chaves-Fraga, Freddy Priyatna, Juan Sequeda, Anastasia Dimou",
        "file_name": "10!1007%978-3-030-32327-1_19.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-32327-1_19.pdf"
    },
    {
        "title": "Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2009.10263",
        "publication_date": "2020-01-01",
        "authors": "Juan A. Cabrera, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda",
        "file_name": "arxiv.2009.10263",
        "file_path": "10.48550/arxiv.2009.10263"
    },
    {
        "title": "Interpretable machine learning models for predicting and explaining vehicle fuel consumption anomalies",
        "implementation_urls": [],
        "doi": "10.1016/j.engappai.2022.105222",
        "arxiv": "2010.16051",
        "abstract": "AbstractIdentifying anomalies in the fuel consumption of the vehiclesof a fleet is a crucial aspect for optimizing consumption andreduce costs. However, this information alone is insufficient,since fleet operators need to know the causes behind anoma-lous fuel consumption.We combine unsupervised anomaly detection techniques, do-main knowledge and interpretable Machine Learning modelsfor explaining potential causes of abnormal fuel consumptionin terms of feature relevance. The explanations are used forgenerating recommendations about fuel optimization, that areadjusted according to two different user profiles: fleet man-agers and fleet operators.Results are evaluated over real-world data from telematics de-vices connected to diesel and petrol vehicles from differenttypes of industrial fleets. We measure the proposal regardingmodel performance, and using Explainable AI metrics thatcompare the explanations in terms of representativeness, fi-delity, stability, contrastiveness and consistency with aprioribeliefs. The potential fuel reductions that can be achieved isround 35%.Keywords Explainable Artificial Intelligence. Fuel Con-sumption. Anomaly Detection. Explainable Boosting Ma-chine. Feature Relevance. Generalized Additive Models.1. IntroductionCombining Advanced Analytics techniques together withIoT (Internet of Things) data offers many possibilities tofind and extract relevant insights for business decisions. AtTelefónica, for instance, we see how the union of MachineLearning (ML) with IoT data helps to create new use casesfor the Fleet Management Industry. An example of it is theusage of ML for anomaly detection of the fuel consumptionof vehicles. For a fleet manager, it is very useful to be able tofind which vehicles are having an abnormal fuel consump-tion, since it is crucial for optimizing costs.*Corresponding author at Telefónica, 28050 Madrid, Spain. E-mail address: alberto.barbadogonzalez@telefonica.com (A. Bar-bado)Copyright © 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.However, detecting which vehicles have an anomalousfuel consumption alone is not enough. Only providing thatinformation leads to more questions than answers. Why arethe vehicles consuming that extra amount of fuel? Howcould it be reduced?. These questions are not answered by abinary output that indicates which consumption are anoma-lous and which ones are not.This is one of the reasons why Explainable AI (XAI) isrelevant: it enhances that initial information with differenttypes of explanations, helping to answer those questions that",
        "publication_date": "2022-07-30",
        "authors": "Alberto Barbado, Óscar Corcho",
        "file_name": "10!1016%j!engappai!2022!105222.pdf",
        "file_path": "./PDFs/10!1016%j!engappai!2022!105222.pdf"
    },
    {
        "title": "MAS: A Corpus of Tweets for Marketing in Spanish",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-98192-5_53",
        "abstract": "Abstract. This paper presents a corpus of manually tagged tweets inSpanish language, of interest for marketing purposes. For every Twitterpost, tags are provided to describe three different aspects of the text: theemotions, whether it makes a mention to an element of the marketing mixand the position of the tweet author with respect to the purchase funnel.The tags of every Twitter post are related to one single brand, whichis also specified for every tweet. The corpus is published as a collectionof RDF documents with links to external entities. Details on the usedvocabulary and classification criteria are provided, as well as details onthe annotation process.Keywords: corpus, marketing, marketing mix, sentiment analysis, NLP,purchase funnel, emotion analysis1 IntroductionTwitter is a source of valuable feedback for companies to probe the public per-ception of their brands. Whereas sentiment analysis has been extensively appliedto social media messages (see [16] among many), other dimensions of brand per-ception are still of interest and have received less attention [12], specially thoserelated to marketing. In particular, marketing specialists are highly interested in:(a) knowing the position of a tweet author in the purchase funnel (this is, wherein the different stages of the customer journey is the author in); (b) knowingto which element or elements of the marketing mix3 the text refers to and (c)knowing the author’s affective situation with respect to a brand in the tweet.This paper presents the MAS Corpus, a Spanish corpus of tweets of interestfor marketing specialists, labeling messages in the three dimensions aforemen-tioned. The corpus is freely available at http://mascorpus.linkeddata.es/and has been developed in the context of the Spanish research project LPS BIG-GER4, which analyzed different dimensions of tweets in order to extract relevantinformation on marketing purposes. A first version of the corpus containing onlythe sentiment analysis annotations was released as the Corpus for Sentiment3 http://economictimes.indiatimes.com/definition/marketing-mix4 http://www.cienlpsbigger.es2 M. Navas-Loro et al.Analysis towards Brands (SAB) and was described in [15]. Following this work,we have expanded the corpus tagging the messages in the two remaining di-mensions described before: the purchase funnel and the marketing mix. Tweetsthat were almost identical to others have been removed. Categories of each ofthe three aspects tagged in the corpus (Sentiment Analysis, Marketing Mix andPurchase Funnel) can be found in Table 1.Table 1. Tags for each category.Category TagsPurchase funnel awareness, evaluation, purchase, postpurchase, ambiguous, NC2Marketing Mix product, price, promotion, place, NC2SentimentAnalysislove, hate, satisfaction, dissatisfaction, happiness, sadness,trust, fear, NC22 Related Work2.1 Sentiment AnalysisEven when Sentiment Analysis is a major field in Natural Language Processing,most of works in Spanish tend to focus on polarity [10, 5], being the efforts",
        "publication_date": "2018-01-01",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel, Idafen Santana-Pérez, Alba Fernández-Izquierdo, Alberto Sánchez",
        "file_name": "10!1007%978-3-319-98192-5_53.pdf",
        "file_path": "./PDFs/10!1007%978-3-319-98192-5_53.pdf",
        "pdf_link": "https://ceur-ws.org/Vol-2111/paper1.pdf"
    },
    {
        "title": "Linked Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4",
        "abstract": "Abstract This chapter presents Linked Data, a new form of distributed data on theweb which is especially suitable to be manipulated by machines and to shareknowledge. By adopting the linked data publication paradigm, anybody can publishdata on the web, relate it to data resources published by others and run artificialintelligence algorithms in a smooth manner. Open linked data resources maydemocratize the future access to knowledge by the mass of internet users, eitherdirectly or mediated through algorithms. Governments have enthusiasticallyadopted these ideas, which is in harmony with the broader open data movement.Keywords Linked data � Semantic web � Democracy � Ontologies � Knowledgerepresentation � eDemocracy1.1 IntroductionMore than half of the world’s population has access to the Internet. Vast amounts ofknowledge accumulated in roughly 2 billion websites are available to anyone whois able to read and can afford an internet connection.Entertainment habits, interpersonal human relations and almost any conceivableaspect of human life have been profoundly transformed with the arrival of theinternet. Yet modern democracies have remained relatively unaffected. It is true thatpropaganda techniques have undergone changes, political parties organize theircampaign strategies differently and the idea of eDemocracy is perhaps about tohatch; but the public institutions, the habits of citizens and the overall political gameare all apparently the same.We have to indulge—Internet is a new thing. But a careful observation of theevolution of technologies and the new organizational forms they enable revealdiscrete signs of change, now with little effect but potentially of much impact.This chapter introduces some new technologies and ideas which may seemirrelevant today, but which will probably exert a powerful influence on the forth-coming transformations of the concept of democracy.© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_11.2 The World Wide Web as a Source of Dataand Knowledge1.2.1 Data, Information and KnowledgeMarshall McLuhan described technology as extensions of man (McLuhan 1964),whereby our bodies and our senses are extended beyond their natural limits.Certainly, a shovel is an improvement of our hands when we dig a trench andtelescopes are augmented eyes when we look at the stars. In top level chess tour-naments, chess players prepare their games and study their opponents with a jointteam of humans and machine—machines also extend human’s capabilities forthinking.In order to make a value judgement, we need data—this is a truism. But todaywe also need machines which need data. Whenever we take an important decision,we usually google for some related information. Our decisions are mediated byinformation provided by a company, or a handful of companies, whose interestsmay not match our interests. Maybe in the future we will have a wider range of",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%978-3-030-13363-4.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-13363-4.pdf"
    },
    {
        "title": "KerA: Scalable Data Ingestion for Stream Processing",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00152",
        "abstract": "Abstract—Big Data applications are increasingly moving frombatch-oriented execution models to stream-based models thatenable them to extract valuable insights close to real-time. Tosupport this model, an essential part of the streaming processingpipeline is data ingestion, i.e., the collection of data from vari-ous sources (sensors, NoSQL stores, filesystems, etc.) and theirdelivery for processing. Data ingestion needs to support highthroughput, low latency and must scale to a large number of bothdata producers and consumers. Since the overall performanceof the whole stream processing pipeline is limited by that ofthe ingestion phase, it is critical to satisfy these performancegoals. However, state-of-art data ingestion systems such as ApacheKafka build on static stream partitioning and offset-based recordaccess, trading performance for design simplicity. In this paperwe propose KerA, a data ingestion framework that alleviate thelimitations of state-of-art thanks to a dynamic partitioning schemeand to lightweight indexing, thereby improving throughput,latency and scalability. Experimental evaluations show that KerAoutperforms Kafka up to 4x for ingestion throughput and up to5x for the overall stream processing throughput. Furthermore,they show that KerA is capable of delivering data fast enough tosaturate the big data engine acting as the consumer.Keywords—Stream processing, dynamic partitioning, ingestion.I. INTRODUCTIONBig Data real-time stream processing typically relies onmessage broker solutions that decouple data sources from ap-plications. This translates into a three-stage pipeline describedin Figure 1. First, in the production phase, event sources (e.g.,smart devices, sensors, etc.) continuously generate streams ofrecords. Second, in the ingestion phase, these records are ac-quired, partitioned and pre-processed to facilitate consumption.Finally, in the processing phase, Big Data engines consume thestream records using a pull-based model.Since users are interested in obtaining results as soon aspossible, there is a need to minimize the end-to-end latencyof the three stage pipeline. This is a non-trivial challengewhen records arrive at a fast rate and create the need tosupport a high throughput at the same time. To this purpose,Big Data engines are typically designed to scale to a largenumber of simultaneous consumers, which enables processingfor millions of records per second [1], [2]. Thus, the weaklink of the three stage pipeline is the ingestion phase: it needsto acquire records with a high throughput from the producers,serve the consumers with a high throughput, scale to a largenumber of producers and consumers, and minimize the writelatency of the producers and, respectively, the read latency ofthe consumers to facilitate low end-to-end latency.Fig. 1. Stream processing pipeline: records are collected at event timeand made available to consumers earliest at ingestion time, after the eventsare acknowledged by producers; processing engines continuously pull these",
        "publication_date": "2018-07-01",
        "authors": "Ovidiu-Cristian Marcu, Alexandru Costan, Gabriel Antoniu, Marı́a S. Pérez, Bogdan Nicolae, Radu Tudoran, Stefano Bortoli",
        "file_name": "10!1109%icdcs!2018!00152.pdf",
        "file_path": "./PDFs/10!1109%icdcs!2018!00152.pdf"
    },
    {
        "title": "Context aware ontology‐based hybrid intelligent framework for vehicle driver categorization",
        "implementation_urls": [],
        "doi": "10.1002/ett.3729",
        "abstract": "AbstractIn public vehicles, one of the major concerns is driver's level of expertise for itsdirect proportionality to safety of passengers. Hence, before a driver is subjectedto certain type of vehicle, he should be thoroughly evaluated and categorizedwith respect to certain parameters instead of only one-time metric of havingdriving license. These aspects may be driver's expertise, vigilance, aptitude, expe-rience years, cognition, driving style, formal education, terrain, region, minorviolations, major accidents, and age group. The purpose of this categorizationis to ascertain suitability of a driver for certain vehicle type(s) to ensure passen-gers' safety. Currently, no driver categorization technique fully comprehends theimplicit as well as explicit characteristics of drivers dynamically. In this paper,machine learning–based dynamic and adaptive technique named D-CHAITs(driver categorization through hybrid of artificial intelligence techniques) is pro-posed for driver categorization with an objective focus on driver's attributesmodeled in DriverOntology. A supervised mode of learning has been employedon a labeled dataset, having diverse profiles of drivers with attributes pertinentto drivers' perspectives of demographics, behaviors, expertise, and inclinations.A comparative analysis of D-CHAIT with three other machine learning tech-niques (fuzzy logic, case-based reasoning, and artificial neural networks) isalso presented. The efficacy of all techniques was empirically measured whilecategorizing the drivers based on their profiles through metrics of accuracy, pre-cision, recall, F-measure performance, and associated costs. These empiricalquantifications assert D-CHAIT as a better technique than contemporary ones.The novelty of proposed technique is signified through preprocessing of featureattributes, quality of data, training of machine learning model on more relevantdata, and adaptivity.1 INTRODUCTIONThe foremost concern of any transportation authority would be assuring the safety of passengers using their facility.Adoption of modern technologies in transportation has not only enhanced the degree of comfort for travelers but caus-ing serious threats to passengers. Such threats are evident from 43 000 causalities just in year 2005.1 Out of 1.2 millionaccidents, 20% to 30% were caused by negligence of drivers. This gives rise to the need of thoroughly assessing driver'sexpertise in certain terrains, abilities to drive on longer routes with certain vehicles, violation history, and regions. OnceTrans Emerging Tel Tech. 2019;e3729. wileyonlinelibrary.com/journal/ett © 2019 John Wiley & Sons, Ltd. 1 of 14https://doi.org/10.1002/ett.3729Trans Emerging Tel Tech. 2022;33:e3729. wileyonlinelibrary.com/journal/ett © 2019 John Wiley & Sons, Ltd. 1 of 14https://doi.org/10.1002/ett.3729http://wileyonlinelibrary.com/journal/etthttps://doi.org/10.1002/ett.3729https://orcid.org/0000-0002-8438-6726http://crossmark.crossref.org/dialog/?doi=10.1002%2Fett.3729&domain=pdf&date_stamp=2019-08-292 of 14 SARWAR ET AL.the picture of the driver's ability is clear by rightly categorizing the drivers, he can be delegated to certain vehicle, region,and route.Different techniques have been proposed for driver categorization1-5 that categorize driver profiles by exploiting the“unsupervised mode” of learning for classification. Some techniques take into account only driver's behavior for catego-rizing the drivers as given in the works of Angkititrakul and Hansen1 and Barón and Green.2 Others consider only theenvironmental factors that distract the drivers (mobile phones, conversing with passengers, turning patterns, and lanechanging) using hidden Markov models (HMMs) for better future measures.4 Some techniques evaluate the drivers overa specific temporal scale that does not fully depict the category of driver in normal circumstances. Moreover, few tech-niques consider certain age groups5 for their studies. The driving patterns of vehicle are used to classify drivers based on",
        "publication_date": "2019-08-29",
        "authors": "Sohail Sarwar, Saad Zia, Zia Ul Qayyum, Muddesar Iqbal, Muhammad Safyan, Shahid Mumtaz, Raúl García‐Castro, Kostromitin Konstantin Igorevich",
        "file_name": "10!1002%ett!3729.pdf",
        "file_path": "./PDFs/10!1002%ett!3729.pdf",
        "pdf_link": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ett.3729"
    },
    {
        "title": "Government plans in the 2016 and 2021 Peruvian presidential elections: A natural language processing analysis of the health chapters",
        "implementation_urls": [],
        "doi": "10.12688/wellcomeopenres.16867.5",
        "abstract": "AbstractWe describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections ofdiscrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which eachitem of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, inturn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context oftext modeling, the topic probabilities provide an explicit representation of a document. We presentefficient approximate inference techniques based on variational methods and an EM algorithm forempirical Bayes parameter estimation. We report results in document modeling, text classification,and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSImodel.1. IntroductionIn this paper we consider the problem of modeling text corpora and other collections of discretedata. The goal is to find short descriptions of the members of a collection that enable efficientprocessing of large collections while preserving the essential statistical relationships that are usefulfor basic tasks such as classification, novelty detection, summarization, and similarity and relevancejudgments.Significant progress has been made on this problem by researchers in the field of informa-tion retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999). The basic methodology proposed byIR researchers for text corpora—a methodology successfully deployed in modern Internet searchengines—reduces each document in the corpus to a vector of real numbers, each of which repre-sents ratios of counts. In the popular tf-idf scheme (Salton and McGill, 1983), a basic vocabularyof “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of thenumber of occurrences of each word. After suitable normalization, this term frequency count iscompared to an inverse document frequency count, which measures the number of occurrences of ac©2003 David M. Blei, Andrew Y. Ng and Michael I. Jordan.BLEI, NG, AND JORDANword in the entire corpus (generally on a log scale, and again suitably normalized). The end resultis a term-by-document matrix X whose columns contain the tf-idf values for each of the documentsin the corpus. Thus the tf-idf scheme reduces documents of arbitrary length to fixed-length lists ofnumbers.While the tf-idf reduction has some appealing features—notably in its basic identification of setsof words that are discriminative for documents in the collection—the approach also provides a rela-tively small amount of reduction in description length and reveals little in the way of inter- or intra-document statistical structure. To address these shortcomings, IR researchers have proposed severalother dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwesteret al., 1990). LSI uses a singular value decomposition of the X matrix to identify a linear subspacein the space of tf-idf features that captures most of the variance in the collection. This approach canachieve significant compression in large collections. Furthermore, Deerwester et al. argue that thederived features of LSI, which are linear combinations of the original tf-idf features, can capturesome aspects of basic linguistic notions such as synonymy and polysemy.To substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it isuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI torecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generativemodel of text, however, it is not clear why one should adopt the LSI methodology—one can attemptto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.A significant step forward in this regard was made by Hofmann (1999), who presented theprobabilistic LSI (pLSI) model, also known as the aspect model, as an alternative to LSI. The pLSIapproach, which we describe in detail in Section 4.3, models each word in a document as a samplefrom a mixture model, where the mixture components are multinomial random variables that can beviewed as representations of “topics.” Thus each word is generated from a single topic, and different",
        "publication_date": "2022-10-25",
        "authors": "Rodrigo M. Carrillo‐Larco, Manuel Castillo‐Cara, Jesús Lovón-Melgarejo",
        "file_name": "10!12688%wellcomeopenres!16867!5.pdf",
        "file_path": "./PDFs/10!12688%wellcomeopenres!16867!5.pdf"
    },
    {
        "title": "Supporting Demand-Response strategies with the DELTA ontology",
        "implementation_urls": [],
        "doi": "10.1109/aiccsa53542.2021.9686935",
        "abstract": "Abstract—The increasing interest of demand response (DR)strategies has fostered the funding of new research projectsin Europe. The development of DR in these projects requiresa bi-directional data exchange, which revolves around diverseoverlapping topics, between infrastructures and providers. As aresult, ontologies are required to enable a transparent exchangeand consumption of data, ensuring its semantic correctness.Nevertheless, currently, there is no ontology that can be usedfor such data exchange covering all these topics. In the contextof the EU project DELTA, which focuses on DR, an ontologyhas been developed with the goal of filling the aforementionedlack. This paper describes the approach followed to develop thisontology, specifies its requirements, and also includes a practicalexample of how to use it.Index Terms—ontology development, demand-response, energyI. INTRODUCTIONDemand Response (DR) is a strategy that consists of con-trolling the power consumption from a utility by optimising thecustomer demand for power with the supply (response) of theenergy provider [1]. Europe (EU) is fostering the development,adoption, and implementation of DR since it brings severalbenefits [23], such as the reduction of energy consumption,and thus energy cost, and a cutback of CO2 emissions. As acommitment with DR, the EU has fostered and funded a widecorpora of research projects related to DR [27].The adoption of DR requires a bidirectional data exchangebetween infrastructures and providers; for the former to sendpower consumption data to the latter and the latter sendingback power consumption orders to the former. These data mustbe expressed according to an energy data model that needs tocover a wide range of topics, which are: location, equipment,measurements, events, stakeholders, and DR. Although theprevious list of topics could be extended, the aforementionedones are the most critical according to some of the EUprojects (e.g., DELTA,1 SHAR-Q,2 or eDREAM3). There is alarge corpus of existing data models from the energy domaincovering some of these topics, namely grouped into UML-based (e.g., SGAM [19]), XML-based (e.g., USEF [8]), orontologies (e.g., SAREF4ENER).There is a set of benefits on using ontologies instead ofXML or UML based models for expressing the data thatneeds to be exchanged in the energy domain, particularly in1https://www.delta-h2020.eu/2http://www.sharqproject.eu/home3https://edream-h2020.eu/DR, which are [5]: A) Semantic interoperability, which allowstransparently exchanging and consuming data among systemsthat rely on heterogeneous data models; B) Semantic valida-tion, which allows verifying not only the syntax of data butalso that data is compliant with ontology constraints increasing",
        "publication_date": "2021-11-01",
        "authors": "Alba Fernández-Izquierdo, Andrea Cimmino, Raúl García‐Castro",
        "file_name": "10!1109%aiccsa53542!2021!9686935.pdf",
        "file_path": "./PDFs/10!1109%aiccsa53542!2021!9686935.pdf",
        "pdf_link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9686935"
    },
    {
        "title": "A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine",
        "implementation_urls": [],
        "doi": "10.3390/app131910778",
        "abstract": "Abstract: Due to the success of artificial intelligence (AI) applications in the medical field overthe past decade, concerns about the explainability of these systems have increased. The reliabilityrequirements of black-box algorithms for making decisions affecting patients pose a challenge evenbeyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integratingexplainability into these systems. While most traditional AI methods and expert systems are in-herently interpretable, the recent literature has focused primarily on explainability techniques formore complex models such as deep learning. This scoping review critically analyzes the existingliterature regarding the explainability and interpretability of AI methods within the clinical do-main. It offers a comprehensive overview of past and current research trends with the objective ofidentifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) inthe field of medicine. Such constraints encompass the diverse requirements of key stakeholders,including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition,the absence of standardised evaluation criteria, the potential for mistaking explanations for causalrelationships, and the apparent trade-off between model accuracy and interpretability. Furthermore,this review discusses possible research directions aimed at surmounting these challenges. Theseinclude alternative approaches to leveraging medical expertise to enhance interpretability withinclinical settings, such as data fusion techniques and interdisciplinary assessments throughout thedevelopment process, emphasizing the relevance of taking into account the needs of final users todesign trustable explainability methods.Keywords: artificial intelligence; medicine; explainable AI; interpretable AI1. Introduction1.1. AI in Medicine: Opportunities and ChallengesToday’s Artificial Intelligence (AI), with its capability to automate and ease almost anykind of task, frequently appearing to surpass human performance, has become a popularand widespread technology for many applications, especially over the last decade, thanksto advances in deep learning (DL), with clinical healthcare being no exception.Medicine has been one of the most challenging, but also most attention-getting appli-cation fields for AI for the past five decades, with diagnostic decision support, the interpre-tation of medical images and clinical lab tests, drug development, patient management,and others all demonstrating the broad and diverse scope of AI techniques applied tomedical issues.AI methods have promised a range of potential advantages for medical informaticssystems. Automating burdensome tasks can be of great help, alleviating clinicians fromunnecessary efforts and allowing them to focus on more important issues surroundingAppl. Sci. 2023, 13, 10778. https://doi.org/10.3390/app131910778 https://www.mdpi.com/journal/applscihttps://doi.org/10.3390/app131910778https://doi.org/10.3390/app131910778https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0002-1215-3333https://doi.org/10.3390/app131910778https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app131910778?type=check_update&version=1Appl. Sci. 2023, 13, 10778 2 of 23patient care. AI systems can perform these tasks with high precision and efficiency, and also,they can assist the extraction of relevant information from the large quantities of databeing produced by modern medicine [1]. AI systems might be particularly beneficial in",
        "publication_date": "2023-09-28",
        "authors": "Raquel González-Alday, Esteban García-Cuesta, Casimir A. Kulikowski, Víctor Maojo",
        "file_name": "no_doi_20250624160510.pdf",
        "file_path": "./PDFs/no_doi_20250624160510.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/13/19/10778/pdf?version=1695880594"
    },
    {
        "title": "A Context-Aware Indoor Air Quality System for Sudden Infant Death Syndrome Prevention",
        "implementation_urls": [],
        "doi": "10.3390/s18030757",
        "abstract": "Abstract: Context-aware monitoring systems designed for e-Health solutions and ambient assistedliving (AAL) play an important role in today’s personalized health-care services. The majority of thesesystems are intended for the monitoring of patients’ vital signs by means of bio-sensors. At present,there are very few systems that monitor environmental conditions and air quality in the homes ofusers. A home’s environmental conditions can have a significant influence on the state of the healthof its residents. Monitoring the environment is the key to preventing possible diseases caused byconditions that do not favor health. This paper presents a context-aware system that monitors airquality to prevent a specific health problem at home. The aim of this system is to reduce the incidenceof the Sudden Infant Death Syndrome, which is triggered mainly by environmental factors. In theconducted case study, the system monitored the state of the neonate and the quality of air while it wasasleep. The designed proposal is characterized by its low cost and non-intrusive nature. The resultsare promising.Keywords: context-aware; SIDS; non-intrusive; e-health; pediatric1. IntroductionDue to the recent advances in sensor systems, the Internet-of-Things, and medical devices, it ispossible to provide personalized and continuous health care at home [1]. People with chronic diseasesor elderly people are the ones that most benefit from these home care systems [2]. Thanks to thesedevelopments, patients can be more independent and reduce their visits to the doctor. This contributesgreatly to the quality of their life [3]. The evolution of monitoring systems entails sensors collectinggreater amounts of data; this means that we have to be prepared to process more medical data [4].It is therefore essential to find solutions that will allow for the management of large amountsof data in a fast, efficient, and accurate way [5]. To achieve efficient management, it is essential toapply a context layer to the collected data. Context-aware systems play a notable role in the processingand analyzing of data [6]. The correct contextualization of the collected information is essential forunderstanding it and generating knowledge that can be used in decision making. To contextualize data,it is necessary to have additional information on the context that will make the collected data coherentand reliable [7]. In many cases, additional context information comes from heterogeneous data sources,and it is necessary that they are reliable and accurate context providers. Poorly contextualized datasetscan lead intelligent applications and systems to make the wrong decisions. This is a very seriousissue for medical systems, in which a wrong decision caused by a contextualization error can put thehealth of the patient at risk [8]. Hence, when designing homecare systems, it is important to pay closeattention to the performance of the systems that add and manage contexts in context-aware systems.Sensors 2018, 18, 757; doi:10.3390/s18030757 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18030757http://www.mdpi.com/journal/sensorsSensors 2018, 18, 757 2 of 22Another important issue in today’s homecare systems is environmental factors such as air quality.It has been demonstrated that acting over these factors is a determinant in the prevention of differentdiseases [9]. The problem is that much of current sensor systems employ only biomedical measurementdevices that users must carry with them or with which they must interact in some way (such as a pulsesensor, body temperature meter, or glucometer). Just a few projects implement systems that monitorenvironmental factors in the home.The lack of research in this area creates a necessity for new proposals in the field of air qualitymonitoring systems in homecare. These systems must measure air quality in domestic environmentswith the aim of improving user’s health. Current systems are compatible with traditional health",
        "publication_date": "2018-03-02",
        "authors": "Daniel H. de la Iglesia, Juan F. De Paz, Gabriel Villarrubia González, Alberto López Barriuso, Javier Bajo",
        "file_name": "no_doi_20250624160533.pdf",
        "file_path": "./PDFs/no_doi_20250624160533.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/3/757/pdf?version=1519986905"
    },
    {
        "title": "R4R: Template-based REST API Framework for RDF Knowledge Graphs.",
        "implementation_urls": [],
        "abstract": "Abstract. Knowledge graphs (KGs) are increasingly being used to makestructured information available on the Web, by means of REST APIsand/or SPARQL endpoints. In many cases, these REST APIs are gen-erated on top of the SPARQL endpoints, using existing technology ap-proaches that are based on proprietary configuration files or ontologies tocreate the APIs. These approaches may impose content-based or struc-tural constraints when composing Web resources. To relax these con-straints we propose R4R, a more flexible solution based on Web stan-dards and REST principles that creates and publishes customizable APIsexposing Web resources from SPARQL queries organized in file systemdirectories. R4R features include individual and nested resources, pagi-nated queries, optional fields, web authentication, query parameters andsorting lists.Resource type: SoftwareLicense: Apache License 2.0DOI: https://doi.org/10.5281/zenodo.3543320Keywords: API · Knowledge Graph · REST · SPARQL1 IntroductionKnowledge graphs (KGs) are drawing increasing attention from both academiaand industry for representing, sharing and using knowledge in applications [7, 3].They may be made available as RDF-based datasets, including a SPARQL end-point (e.g., DBpedia), and/or via REST APIs (e.g., Google Knowledge Graph).In both cases, KGs share many commonalities from the data representation pointof view (both use triples to represent facts), but they are radically different interms of query capabilities: SPARQL provides a more expressive query languagethan what can be normally done with a REST API, but it can be a barrier fornon-expert users. Web APIs usually present data according to REpresentativeState Transfer (REST) architecture principles mapping HTTP verbs (POST,GET, PUT, DELETE) to CRUD operations (Create, Read, Update, Delete).Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0)Fig. 1. Step-by-step processing of a request in R4RHowever, API resources do not necessarily match to KG resources. A publicprocurement-focused API and a technology-focused API may present, in a dif-ferent way, the information retrieved from the same KG about companies. Onemore focused on merit and the other on innovations.In this demo we present R4R, an open source framework that facilitates thepublication of a KG via a REST API over HTTP. Our approach proposes afully customizable definition of resources, both naming and content (and evennesting), through a hierarchical organization of data (Figure 1). It deploys a webservice based on SPARQL queries to retrieve the information and provides tem-plates to compose resources that are organized in folders in a system directory.Finally, we describe a motivating example where R4R is used to enhance KGaccess.2 KG data consumption via Web APISeveral approaches are available to provide Web developers with mechanismsto ease KG data consumption without dealing with the complexity of SemanticWeb standards and technologies, namely SPARQL. Some of these approacheshave been focused on the provision of Web APIs that allow developers to in-teract with KG data. There are tools [1][5] that generate Web APIs from set of",
        "publication_date": "2021-01-01",
        "authors": "Carlos Badenes-Olmedo, Paola Espinoza-Arias, Óscar Corcho",
        "file_name": "no_doi_20250624160555.pdf",
        "file_path": "./PDFs/no_doi_20250624160555.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper339.pdf"
    },
    {
        "title": "Predicting the risk of suffering chronic social exclusion with machine learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-62410-5_16",
        "abstract": "Abstract. The fight against social exclusion is at the heart of the Eu-rope 2020 strategy: 120 million people are at risk of suffering this con-dition in the EU. Risk prediction models are widely used in insurancecompanies and health services. However, the use of these models to allowan early detection of social exclusion by social workers is not a commonpractice. This paper describes a data analysis of over 16K cases withover 60 predictors from the Spanish region of Castilla y León. The useof machine learning paradigms such as logistic regression and randomforest makes possible a high precision in predicting chronic social exclu-sion. The paper is complemented with a responsive web available onlinethat allows social workers to calculate the risk of a social exclusion caseto become chronic through a smartphone.Keywords: Social exclusion, Social services, Data analysis, Machinelearning, Data mining.1 IntroductionSocial exclusion is a complex and multi-dimensional process involving the lack ofresources, rights, goods and services, and the inability to participate in the nor-mal relationships and activities, available to most people in a society, whether ineconomic, social, cultural or political scopes [8]. Social exclusion affects not onlythe quality of life of individuals, but also the equity and cohesion of society as awhole. The economic crisis is undermining the sustainability of social protectionsystems in the EU [1]: 24% of all the EU population (over 120 million people)are at risk of poverty or social exclusion [1]. The fight against poverty and socialexclusion is at the heart of the Europe 2020 strategy for smart, sustainable andinclusive growth.In chronic medical diseases, there is strong evidence supporting that earlydetection results in less severe outcomes. This paper intends to provide socialworkers with methods and tools to bring this early detection, which is so bene-ficial in the medical field, to the challenging problem of chronic social exclusion.To this purpose, the paper contributes with (1) an analysis of the social servicesdata of Castilla y León (CyL), which is the largest region in Spain and countswith around two and a half million inhabitants. This analysis allows getting2 Emilio Serrano et al.insights into why social exclusion can become chronic. Furthermore, a (2) ma-chine learning model capable of quantifying the risk of chronic social exclusionis build. Finally, a (3) a responsive web is deployed to allow queries by socialworkers through a number of devices such as smartphones, tablets, or laptops.A RESTful web service is also provided to integrate the predictive capabilitiesinto other software applications.The paper outline is as follows. After revising some of the most relevantrelated works in section 2, the process used to analyze the data is explained insection 3. Section 4 reports the outcomes of the experiments conducted. Section5 explains, analyzes, and compares the results. Section 6 introduces the webservice implemented. Finally, section 7 concludes and offers future works.2 Related worksRisk prediction models are widely used in insurance companies to allow cus-tomers to estimate their policies cost. Manulife Philippines [2] offers a numberof online tools to calculate the likelihood of disability, critical illness, or deathbefore the age of 65; based on age, gender, and smoking status. Health is anotherapplication field where risk estimations are undertaken for preventive purposes.",
        "publication_date": "2017-06-20",
        "authors": "Emilio Serrano, Pedro del Pozo-Jiménez, Mari Carmen Suárez-Figueroa, Jacinto González‐Pachón, Javier Bajo, Asunción Gómez‐Pérez",
        "file_name": "10!1007%978-3-319-62410-5_16.pdf",
        "file_path": "./PDFs/10!1007%978-3-319-62410-5_16.pdf"
    },
    {
        "title": "Relationship recommender system in a business and employment-oriented social network",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2017.12.050",
        "publication_date": "2017-12-26",
        "authors": "Pablo Chamoso, Alberto Rivas, Sara Rodrı́guez, Javier Bajo",
        "file_name": "10!1016%j!ins!2017!12!050.pdf",
        "file_path": "./PDFs/10!1016%j!ins!2017!12!050.pdf"
    },
    {
        "title": "Prediction and failure analysis of composite resin restorations in the posterior sector applied in teaching dental students",
        "implementation_urls": [],
        "doi": "10.1007/s12652-020-01804-7",
        "abstract": "AbstractThe failure of composite resin restorations in the posterior region is an ongoing concern in current clinical practice This study assesses possible factors and causes of the failure of restoration 1 year after their placement by fourth year dental students (on a 5-year degree course). While the systematic assessment of dental students does not appear to have received much attention in the field, this study asserts the need and benefit of an assessment methodology that can (1) predict the success or failure of restorations placed by dental students and (2) assist the clinical instructor in identifying the performance profile of each student. Eighty-one patients aged 26–77 years were treated by 81 undergraduates in a prospective cohort study from Novem-ber 2013 to December 2015. One year after treatment, restorations were assessed by the same staff member who acted as the supervisor during the restoration placement. A CBR system was applied to make predictions about restorations. The CBR includes different machine learning techniques and statistical tests in the CBR cycle. The system calculates the relevant vari-ables, which are used to predict failures. The accuracy of the system is measured with the AUC and the accuracy. The AUC obtained is 0.935 while the Kappa index and the accuracy are 91.36 and 0.75, respectively. In conclusion, factors related to the patient and to the treatment are associated to the failure of the restorative treatment. Of particular interest, the CBR was useful for the performance of a predictive model to estimate the probability of failure of resin restorations placed by students.Keywords  Dental caries · Failure · Competency based education · Assessment · CBR · Classifiers · Statistical test1  IntroductionComposite resins are today the first choice material for den-tal restorations (Ferracane 2013). While their use for ante-rior restorations has been standard practice for many years, composites of sufficient strength and durability for posterior placement have only more recently been introduced (Ferra-cane 2013; D’Arcangelo et al. 2014; Hamburger et al. 2011; Manhart et al. 2010) and, despite numerous improvements, both short- and long term failure rates are still high with reported estimates of 3–9% (Ferracane 2013). Moreover, the treatment of failed restorations currently accounts for 50–78% of a dentist’s routine task (Manhart et al. 2010). One of the consequences of retreatment tends to be an increased cavity size (Ferracane 2013; Hamburger et al. 2011; Manhart et al. 2010).Restoration failure is defined as an amount of wear that prevents the effectiveness of the treatment in terms of its function, esthetics or prevention of further disease (Ferracane 2013). The causes of restoration failure can be related to the material used, the technique (controlled by  *\t Juan F. De Paz \t fcofds@usal.es\t Ignacio J. Aliaga \t i.aliaga@pdi.ucm.es\t Vicente Vera \t viventevera@odon.ucm.es\t Álvaro E. García \t aegarcia@odon.ucm.es\t Javier Bajo \t jbajo@fi.upm.es1\t Department of Estomatology II, Complutense University of Madrid, Plaza Ramón y Cajal, s/n, 28040 Madrid, Spain2\t Department of Computer Science, University of Salamanca, Plaza de los Caídos s/n. Facultad de Ciencias, 37008 Salamanca, Spain",
        "publication_date": "2020-02-28",
        "authors": "Ignacio Aliaga, Juan F. De Paz, Vicente Vera, Álvaro Enrique García Barbero, Javier Bajo",
        "file_name": "10!1007%s12652-020-01804-7.pdf",
        "file_path": "./PDFs/10!1007%s12652-020-01804-7.pdf"
    },
    {
        "title": "Conformance Test Cases for the RDF Mapping Language (RML)",
        "implementation_urls": [
            {
                "identifier": "https://github.com/rmlio/rml-test-cases",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-21395-4_12.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The test cases are described at http://rml.io/test-cases/and the corresponding files are available at https://github.com/rmlio/rml-test-cases."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-21395-4_12",
        "abstract": "Abstract. Knowledge graphs are often generated using rules that applysemantic annotations to data sources. Software tools then execute theserules and generate or virtualize the corresponding RDF-based knowl-edge graph. RML is an extension of the W3C-recommended R2RMLlanguage, extending support from relational databases to other datasources, such as data in CSV, XML, and JSON format. As part of theR2RML standardization process, a set of test cases was created to assesstool conformance the specification. In this work, we generated an initialset of reusable test cases to assess RML conformance. These test casesare based on R2RML test cases and can be used by any tool, regard-less of the programming language. We tested the conformance of twoRML processors: the RMLMapper and CARML. The results show thatthe RMLMapper passes all CSV, XML, and JSON test cases, and mosttest cases for relational databases. CARML passes most CSV, XML,and JSON test cases regarding. Developers can determine the degree ofconformance of their tools, and users determine based on conformanceresults to determine the most suitable tool for their use cases.Keywords: RML · R2RML · test case1 IntroductionKnowledge graphs are often generated based on rules that apply semantic an-notations to raw or semi-structured data. For example, the DBpedia knowledgegraph is generated by applying classes and predicates of the DBpedia ontologyto Wikipedia [1]. Software tools execute these rules and generate correspondingRDF triples and quads [2], which materialize knowledge graphs. In the past,custom scripts prevailed, but lately, rule-driven tools emerged. Such tools distin-guish the rules that define how RDF terms and triples are generated from thetool that executes those rules. R2RML [3] is the W3C-recommended languageto define such rules for generating knowledge graphs from data in relationaldatabases (RDBs). An R2RML processor is a tool that, given a set of R2RMLrules and a relational database, generates an RDF dataset. Examples of R2RMLprocessors include Ultrawrap [4], Morph-RDB [5], Ontop [6], and XSPARQL [7].A subset of them was included in the RDB2RDF Implementation Report [8]which lists their conformance to the R2RML specification. Conformance is as-sessed based on whether the correct knowledge graph is generated for a set ofrules and certain relational database.Given that R2RML is focused on relational databases only, extensions andadaptations were applied to account for other types of data sources. These in-clude RML [9], XSPARQL [7], and xR2RML [10]. RML provides an extension ofR2RML to support heterogeneous data sources, including different formats suchas CSV, XML, JSON, and access interfaces, such as files and Web APIs. VariousRML processors emerged, such as the RMLMapper3, CARML4, GeoTriples [11],and Ontario5. Unlike R2RML, there are no test cases available to determine theconformance to the RML specification. As a result, processors are either nottested or only tested with custom test cases, which do not necessarily assess everyaspect of the specification. Consequently, no implementation report is availablethat allows comparing the different processors that generate knowledge graphsfrom heterogeneous data sources based on the conformance to the specification.This way, it is hard to determine the most suitable processor for a certain usecase.In this work, we introduce an initial set of RML test cases, which contains 297",
        "publication_date": "2019-01-01",
        "authors": "Pieter Heyvaert, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho, Erik Mannens, Ruben Verborgh, Anastasia Dimou",
        "file_name": "10!1007%978-3-030-21395-4_12.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-21395-4_12.pdf",
        "pdf_link": "https://pieterheyvaert.com/publications/heyvaert2019testcases/paper.pdf"
    },
    {
        "title": "A framework for creating knowledge graphs of scientific software metadata",
        "implementation_urls": [
            {
                "identifier": "github.com/features/wikis",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1162%qss_a_00167.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "DATA AVAILABILITY Datasets related to this article can be found at https://doi.org/10.6084/m9.figshare.14916684.v1: The SOSEN-KG (Turtle format), hosted at FigShare."
                    }
                ]
            }
        ],
        "doi": "10.1162/qss_a_00167",
        "abstract": "ABSTRACTAn increasing number of researchers rely on computational methods to generate ormanipulate the results described in their scientific publications. Software created to thisend—scientific software—is key to understanding, reproducing, and reusing existing work inmany disciplines, ranging from Geosciences to Astronomy or Artificial Intelligence. However,scientific software is usually challenging to find, set up, and compare to similar software due toits disconnected documentation (dispersed in manuals, readme files, websites, and codecomments) and the lack of structured metadata to describe it. As a result, researchers have tomanually inspect existing tools to understand their differences and incorporate them into theirwork. This approach scales poorly with the number of publications and tools made availableevery year. In this paper we address these issues by introducing a framework for automaticallyextracting scientific software metadata from its documentation (in particular, their readmefiles); a methodology for structuring the extracted metadata in a Knowledge Graph (KG) ofscientific software; and an exploitation framework for browsing and comparing the contents ofthe generated KG. We demonstrate our approach by creating a KG with metadata from over10,000 scientific software entries from public code repositories.1. INTRODUCTIONComputational methods have become crucial for making scientific discoveries in areas rang-ing from Astronomy (LIGO-VIRGO, n.d.) or High Energy Physics (Albrecht, Alves et al., 2019)to Geosciences (USGS, n.d.) and Biology (Prlić & Lapp, 2012). Software developed for thispurpose is known as scientific software, and refers to the code, tools, frameworks, and scriptscreated in the context of a research project to process, analyze or generate a result in an aca-demic publication. Examples of scientific software involve novel algorithm implementations,simulation models, data processing work flows, and data visualization scripts.Scientific software is key for the reproducibility of scientific results as it helps others under-stand how a data product has been created or modified as part of a computational experimentor simulation and avoids replicating effort. Therefore, scientific software should adapt the prin-ciples for Finding, Accessing, Interoperating and Reusing scientific data (FAIR) (Wilkinsonet al., 2016) to help scientists find, compare, understand, and reuse the software developedby other researchers (Lamprecht, Garcia et al., 2020).Fortunately, the scientific community, academic publishers, and public stakeholders havestarted taking measures towards making scientific software a first-class citizen for academican open a c ce s s j o u r na lCitation: Kelley, A., & Garijo, D. (2021).A framework for creating knowledgegraphs of scientific software metadata.Quantitative Science Studies, 2(4),1423–1446. https://doi.org/10.1162/qss_a_00167DOI:https://doi.org/10.1162/qss_a_00167Corresponding Authors:Aidan Kelleyaidankelley@wustl.eduDaniel Garijodgarijo@isi.eduCopyright: © 2021 Aidan Kelley andDaniel Garijo. Published under aCreative Commons Attribution 4.0International (CC BY 4.0) license.",
        "publication_date": "2021-01-01",
        "authors": "Aidan Kelley, Daniel Garijo",
        "file_name": "10!1162%qss_a_00167.pdf",
        "file_path": "./PDFs/10!1162%qss_a_00167.pdf"
    },
    {
        "title": "SoMEF: A Framework for Capturing Scientific Software Metadata from its Documentation",
        "implementation_urls": [],
        "doi": "10.1109/bigdata47090.2019.9006447",
        "abstract": "Abstract—Scientific software has become a key assetto reproduce and understand the products of scientificresearch in many disciplines. However, scientific software isbecoming increasingly complex and, as a result, researchersneed to spend a significant amount of time finding, readingand understanding software documentation to set it up.In this paper we describe SoMEF, a Software MetadataExtraction Framework designed to help highlighting themost important parts of scientific software documentation.SoMEF processes the README files in GitHub repositoriesto automatically extract which parts of their text referto the description, installation, invocation, or citation ofa software component. Despite its simple features, SoMEFsuccessfully categorizes README excerpts with a mini-mum 0.92 precision and 0.90 ROC AUC. These results,tested on a corpus of over 70 scientific software repositories,are a promising start towards automatically generatingknowledge graphs of scientific software metadata.I. INTRODUCTIONWithin the past few decades, computational sciencehas increasingly become recognized as a fundamentalapproach to answer scientific questions alongside theoryand experimentation [1]. However, the continuous devel-opment of new software makes it difficult for scientiststo keep track of different method implementations andto evaluate whether a certain piece of software suitstheir needs [2]. Scientists are required to spend timeporing through available software documentation andsource code in order to understand the software used ina project [3] and how to properly cite it. This process istime consuming and unappealing to scientists due to theheterogeneity and lack of unified structure in softwaredocumentation.Existing efforts have attempted to simplify this prob-lem by avoiding “wordy, unstructured, introductory de-scriptions” in favor of a specialized language just fordocumentation [3]. However, text documentation contin-ues to grow at an exponential rate [4].In this paper we aim to ease the process of under-standing, reusing and attributing scientific software bypresenting SoMEF [5], a Software Metadata ExtractionFramework that automatically extracts relevant softwaremetadata from its documentation. SoMEF takes as inputa README file from a GitHub repository and identifiesits description, installation instructions, invocation setupand citation. Our approach uses binary classificationmethods and organizes the results into a structuredformat that is comprehensible to both humans and ma-chines. In addition, SoMEF extracts additional metadataabout a piece of software beyond its documentation by",
        "publication_date": "2019-12-01",
        "authors": "Allen Mao, Daniel Garijo, Shobeir Fakhraei",
        "file_name": "10!1109%bigdata47090!2019!9006447.pdf",
        "file_path": "./PDFs/10!1109%bigdata47090!2019!9006447.pdf"
    },
    {
        "title": "A Semantically Enhanced UPnP Control Point for Sharing Multimedia Content",
        "implementation_urls": [],
        "doi": "10.1109/mic.2011.83",
        "abstract": "Abstracts on Human Factors in Computing Systems (CHI 97), ACM Press, pp. 168–169. 11.\t J.P. Chin, V.A. Diehl, and K.L. Norman, “Development of an Instrument Measuring User Satisfaction of the Human-Computer Interface,” Proc. ACM Conf. Human Factors in Computing Systems (CHI 88), ACM Press, 1988, pp. 213–218.Mariano Rico is a teaching assistant in the computer sci-ence department at the Universidad Autónoma de Madrid and a collaborator in the Ontology Engineering Group at the Universidad Politécnica de Madrid. His research interests include Semantic Web technologies, virtual worlds, user interfaces, and natural language processing. Rico has a PhD in computer science from the Universidad Autónoma de Madrid. Contact him at mariano.rico@uam.es.Oscar Corcho is an associate professor in the artificial intelligence department at the Universidad Politéc-nica de Madrid, where he’s a member of the Ontology Engineering Group. His research activities focus on semantic e-Science and real-world Internet, although he also conducts research in the more general areas of Semantic Web and ontological engineering. Corcho has a PhD in artificial intelligence from the Universi-dad Politécnica de Madrid. Contact him at ocorcho@fi.upm.es.Víctor Méndez is a research fellow at Intelligent Software Components (iSOCO) in Madrid. His research interests include Semantic Web applied to multimedia, opinion mining, reputation, and provenance. Méndez has a BSc in computer engineering from the Carlos III University of Madrid. Contact him at vmendez@isoco.com.Jose Manuel Gomez-Perez is the director of R&D at Intel-ligent Software Components (iSOCO) in Madrid. His research aims to support users in creating, shar-ing, and accessing knowledge and spans knowledge acquisition, provenance analysis, intelligent infor-mation access, and their applications. Gomez-Perez has a PhD in computer science from the Universidad Politécnica de Madrid. Contact him at jmgomez@isoco.com.IC-15-06-Rico.indd   64 10/8/11   5:46 PM",
        "publication_date": "2011-07-06",
        "authors": "Mariano Rico, Óscar Corcho, Víctor Méndez Muñoz, Asunción Gómez‐Pérez",
        "file_name": "10!1109%mic!2011!83.pdf",
        "file_path": "./PDFs/10!1109%mic!2011!83.pdf"
    },
    {
        "title": "Knowledge Engineering of PhD Stories: A Preliminary Study",
        "implementation_urls": [
            {
                "identifier": "https://github.com/nvbach91/phd-odyssey",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3460210!3493579.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The results of this preliminary study are published on GitHub5 including analyzed stories, coding results, and modeled stories."
                    }
                ]
            }
        ],
        "doi": "10.1145/3460210.3493579",
        "abstract": "ABSTRACTSupport for PhD students and their advisors in decision-makingbefore and along their PhD journeys requires providing them witha deep understanding and knowledge of the life-cycle of a PhD. Thismeans giving them access to a thorough understanding of causalrelations between events, decisions, and the possible outcome. Thisknowledge can be attained primarily from insider stories, studyreports, communications threads with advisors and colleagues, in-terviews, and scholarly databases. However, it is unclear how to givethis knowledge a reasonable structure (due to the heterogeneity ofconcepts and data sources) so that we can use it for decision-makingduring the PhD journey. In this paper, we explore how to analyzeand model PhD stories to uncover and extract causal relationshipsfound within each story to get insights into the co-occurrencesand causalities. We analyze these stories with thematic analysis tounderstand their main points and we use concept maps to createsemi-formal graphs of connected events and objects where the rela-tionships are being emphasized from the perspective of cause andeffect. Our results at this point are a collection of PhD stories inthe form of concept maps, thematic codes, a proposed approach forgoal-directed PhD story modeling which we describe in this paper.CCS CONCEPTS• General and reference → Biographies; • Theory of compu-tation → Semantics and reasoning.KEYWORDScausal relation, causality, concept map, knowledge engineering,PhD story modeling, thematic analysisACM Reference Format:Viet Bach Nguyen, Vojtech Svatek, Marek Dudas, and Oscar Corcho. 2021.Knowledge Engineering of PhD Stories: A Preliminary Study. In Proceedingsof the 11th Knowledge Capture Conference (K-CAP ’21), December 2–3, 2021,Virtual Event, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3460210.3493579Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’21, December 2–3, 2021, Virtual Event, USA.© 2021 Association for Computing Machinery.ACM ISBN 978-1-4503-8457-5/21/12. . . $15.00https://doi.org/10.1145/3460210.34935791 INTRODUCTIONA PhD journey is a long-term process of a scholar learning how toconduct research and advance in a research area. Each PhD journeyis a unique experience consisting of several stages depending on thestudy program. Throughout each PhD journey, many decisions aremade and many events happen that lead to a (un)favorable outcome.",
        "publication_date": "2021-11-24",
        "authors": "Viet Bach Nguyen, Vojtěch Svátek, Marek Dudáš, Óscar Corcho",
        "file_name": "10!1145%3460210!3493579.pdf",
        "file_path": "./PDFs/10!1145%3460210!3493579.pdf"
    },
    {
        "title": "TermInteract: An Online Tool for Terminologists Aimed at Providing Terminology Quality Metrics",
        "implementation_urls": [],
        "doi": "10.1109/cist49399.2021.9357197",
        "abstract": "Abstract—This position paper describes the results of ourmeetings with terminologists aimed at capturing missing func-tionalities concerning term extraction. These experts lack anonline tool to allow them the evaluation, modification and cre-ation of terminologies from corpora in different languages. Theresult is an open-access online website where terminologists canextract automatically terminologies and can upload terminologiescreated by third parties. The tool also allows them to polishcollaboratively the terminologies. Additionally, this tool providesa quality metrics based on the changes made to the terminology,as well as a way to compare two terminologies for the samecorpus but created with different methods. We have tested thetool with terminologies in English and Spanish, with corporafrom several domains: bio (covid19), legal and scientific papers.Our preliminary results point out the utility of this tool forterminologists.Index Terms—terminologies, pattern-based part of speech,neural networks, corporaI. INTRODUCTIONTerminology extraction is increasingly becoming a funda-mental sub-task in the information extraction process of Nat-ural Language Processing (NLP). In recent years, numeroustechniques, based on supervised approaches [1]–[3] as wellas unsupervised approaches [4]–[6], have been developed andimproved, providing increasingly better tools for experts andresearchers, but some problems and shortcomings remain.Despite all the tools and systems that have been developedin the literature, experts in the field still need functionalitiesthat have not been effectively addressed. The most completesystems, such as sketchengine.eu1 has limitations like thelack of an effective terminology comparator and the lack ofterminology quality metrics.We contacted with Spanish terminologists belonging to theSpanish Association of Terminology2 and had several meetingsaimed at identifying these missing functionalities. From thesemeetings we obtained a requirements specification that wehave implemented in the online tool that we describe in thisposition paper.We would like to thank Accenture for his partial support under the projectKeyQ, hosted at the AInnovation center in UPM Montegacedo campus(http://ainnovation.upm.es).1See https://www.sketchengine.eu/2See Asociación Española de Terminologı́a (Spanish Association of Termi-nology, https://aeter.org/terminesp/).This tool has been created to cope with tasks like the gen-eration of terminologies, their edition and most importantly,their comparison. Here we also present some of the results ofthe comparison of terminologies generated from very differentmethods such as methods based on regular expressions (unsu-pervised) and methods based on neural networks (supervised).",
        "publication_date": "2020-06-05",
        "authors": "Pedro Hernandez-Vegas, Lucía Guasp, Mariano Rico",
        "file_name": "10!1109%cist49399!2021!9357197.pdf",
        "file_path": "./PDFs/10!1109%cist49399!2021!9357197.pdf"
    },
    {
        "title": "Toward proactive social inclusion powered by machine learning",
        "implementation_urls": [],
        "doi": "10.1007/s10115-018-1230-x",
        "abstract": "Abstract The ñght against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a c o m m o n practice. This paper describes a data analysis of over 16 K cases with over 60 predictors from the Spanish region of Castilla y León. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion: around 9 0 % in the most conservative predictions. This prediction models offer a quick rule of thumb that can detect citizens who are in danger of been excluded from the society beyond a temporary situation, allowing social workers to further study these cases. Keywords Social exclusion - Social services - Data analysis - Machine learning - Data mining 1 Introduction S b c W gxcZwjWM is a complex and multidimensional process involving the lack of resources, rights, goods and services, and the inability to participate in the normal relationships and activities, available to most people in a society, whether in economic, social, cultural or political scopes [14]. Social exclusion affects not only the quality of life of individuals, but also the equity and cohesion of society as a whole. El Emilio Serrano emilioserra @ fi.upm.es Mari Carmen Suárez-Figueroa mcsuarez @ ñ.upm. es Jacinto González-Pachón jgpachon@fi.upm.es Asunción Gómez-Pérez asun@fi.upm.es t Ontology Engineering Group, Artificial Intelligence Department, Universidad Politécnica de Madrid, Madrid, Spain http://fi.upm.esmailto:jgpachon@fi.upm.esmailto:asun@fi.upm.esThe economic crisis is undermining the sustainability of social protection systems in the E U [6]: 2 4 % of all the E U population (over 120 million people) are at risk of poverty or social exclusion [6]. The fight against poverty and social exclusion is at the heart of the Europe 2020 strategy for smart, sustainable and inclusive growth. In chronic medical diseases, there is strong evidence supporting that early detection results in less severe outcomes. This paper intends to provide social workers with methods and tools to bring this early detection, which is so beneficial in the medical field, to the challenging problem of chronic social exclusion. Note that although poverty has a significant effect on some dimensions of social exclusion, there are other important causes such as age, ethnicity, disability, gender, and employment status. Therefore, it is considerably more challenging to analyze, detect, treat, and predict social exclusion than poverty. This paper contributes with an (1) analysis of the social services data of Castilla y León (CyL), which is the largest region in Spain and counts with around two and a half million inhabitants. This analysis allows getting insights into why social exclusion can become chronic. Furthermore, a(2) machine learning model capable ofquantifying the risk ofchronic social exclusion is build. Finally, a (3) responsive web application is deployed to allow queries by social workers through a number of devices such as smartphones, tablets, or laptops. A RESTful web service is also provided to integrate the predictive capabilities into other ",
        "publication_date": "2018-06-28",
        "authors": "Emilio Serrano, Mari Carmen Suárez-Figueroa, Jacinto González‐Pachón, Asunción Gómez‐Pérez",
        "file_name": "10!1007%s10115-018-1230-x.pdf",
        "file_path": "./PDFs/10!1007%s10115-018-1230-x.pdf"
    },
    {
        "title": "OBA: An Ontology-Based Framework for Creating REST APIs for Knowledge Graphs",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KnowledgeCaptureAndDiscovery/OBA",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-62466-8_4.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Resource type: Software License: Apache 2.0 DOI: https://doi.org/10.5281/zenodo.3686266 Repository: https://github.com/KnowledgeCaptureAndDiscovery/OBA/Keywords: Ontology · API · REST · JSON · data accessibility · knowl-edge graph."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_4",
        "arxiv": "2007.09206",
        "abstract": "Abstract. In recent years, Semantic Web technologies have been in-creasingly adopted by researchers, industry and public institutions todescribe and link data on the Web, create web annotations and consumelarge knowledge graphs like Wikidata and DBPedia. However, there isstill a knowledge gap between ontology engineers, who design, popu-late and create knowledge graphs; and web developers, who need tounderstand, access and query these knowledge graphs but are not fa-miliar with ontologies, RDF or SPARQL. In this paper we describe theOntology-Based APIs framework (OBA), our approach to automaticallycreate REST APIs from ontologies while following RESTful API bestpractices. Given an ontology (or ontology network) OBA uses standardtechnologies familiar to web developers (OpenAPI Specification, JSON)and combines them with W3C standards (OWL, JSON-LD frames andSPARQL) to create maintainable APIs with documentation, units tests,automated validation of resources and clients (in Python, Javascript,etc.) for non Semantic Web experts to access the contents of a targetknowledge graph. We showcase OBA with three examples that illustratethe capabilities of the framework for different ontologies.Resource type: SoftwareLicense: Apache 2.0DOI: https://doi.org/10.5281/zenodo.3686266Repository: https://github.com/KnowledgeCaptureAndDiscovery/OBA/Keywords: Ontology · API · REST · JSON · data accessibility · knowl-edge graph.1 IntroductionKnowledge graphs have become a popular technology for representing structuredinformation on the Web. The Linked Open Data Cloud1 contains more than1200 linked knowledge graphs contributed by researchers and public institutions.Major companies like Google,2 Microsoft,3 or Amazon [16] use knowledge graphs1 https://lod-cloud.net/2 https://developers.google.com/knowledge-graph3 https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/arXiv:2007.09206v1  [cs.AI]  17 Ju",
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Maximiliano Osorio",
        "file_name": "10!1007%978-3-030-62466-8_4.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-62466-8_4.pdf"
    },
    {
        "title": "Predicting incorrect mappings",
        "implementation_urls": [],
        "doi": "10.1145/3167132.3167164",
        "abstract": "ABSTRACTDBpedia releases consist of more than 70multilingual datasets thatcover data extracted from different language-specific Wikipediainstances. The data extracted from those Wikipedia instances aretransformed into RDF usingmappings created by theDBpedia com-munity. Nevertheless, not all the mappings are correct and con-sistent across all the distinct language-specific DBpedia datasets.As these incorrect mappings are spread in a large number of map-pings, it is not feasible to inspect all such mappings manually toensure their correctness. Thus, the goal of this work is to pro-pose a data-driven method to detect incorrect mappings automati-cally by analyzing the information from both instance data as wellas ontological axioms. We propose a machine learning based ap-proach to building a predictive model which can detect incorrectmappings. We have evaluated different supervised classification al-gorithms for this task and our best model achieves 93% accuracy.These results help us to detect incorrect mappings and achieve ahigh-quality DBpedia.CCS CONCEPTS• Information systems→ Resource Description Framework(RDF); • Computing methodologies → Cross-validation; Se-mantic networks; • Social and professional topics→ Quality as-surance;KEYWORDSLinked Data, Data Quality, Mappings, DBpedia, Machine LearningPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full cita-tion on the first page. Copyrights for components of this work owned by others thanACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SAC 2018, April 9–13, 2018, Pau, France© 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-5191-1/18/04. . . $15.00https://doi.org/10.1145/3167132.3167164ACM Reference format:Mariano Rico, Nandana Mihindukulasooriya, Dimitris Kontokostas, HeikoPaulheim, Sebastian Hellmann, and Asunción Gómez-Pérez. 2018. Predict-ing Incorrect Mappings: A Data-Driven Approach Applied to DBpedia. InProceedings of SAC 2018: Symposium on Applied Computing , Pau, France,April 9–13, 2018 (SAC 2018), 8 pages.https://doi.org/10.1145/3167132.31671641 INTRODUCTIONA large number of RDF knowledge bases are created by transform-ing non-RDF data sources into RDF. Such non-RDF formats includerelational databases, CSV files, key-value pairs, etc. A key input tothis transformation process is a mapping that defines how to trans-form the non-RDF source data into RDF. Such mapping specifieshow tomap the source schema into RDF vocabularies, and possibly",
        "publication_date": "2018-04-09",
        "authors": "Mariano Rico, Nandana Mihindukulasooriya, Dimitris Kontokostas, Heiko Paulheim, Sebastian Hellmann, Asunción Gómez‐Pérez",
        "file_name": "10!1145%3167132!3167164.pdf",
        "file_path": "./PDFs/10!1145%3167132!3167164.pdf"
    },
    {
        "title": "Deliberative and Epistemic Approaches to Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_2",
        "abstract": "Abstract Deliberative and epistemic approaches to democracy are two importantdimensions of contemporary democratic theory. This chapter studies thesedimensions in the emerging ecosystem of civic and political participation tools, andappraises their collective value in a new distinct concept: linked democracy. Linkeddemocracy is the distributed, technology-supported collective decision-makingprocess, where data, information and knowledge are connected and shared bycitizens online. Innovation and learning are two key elements of Atheniandemocracies which can be facilitated by the new digital technologies, and across-disciplinary research involving computational scientists and democratic the-orists can lead to new theoretical insights of democracy.Keywords Deliberative democracy � Epistemic democracy � Semantic web �Institutions � Participatory ecosystems2.1 IntroductionSemantic Web engineers have often complained that building ontologies is hard. Tobuild an ontology for a given domain—for example, tort law—one needs to recruitexperts in that domain, elicit their legal knowledge, and then reach a shared, explicitconsensus of how such legal knowledge will be represented and formalised so thatcomputers can ‘understand it’. It is not an easy task, indeed, especially if ontologieshave to be designed from scratch and the subject matter is complex.If it is hard to build ontologies, mapping the conceptual domain of deliberativeand epistemic theories of democracy is not less harder. In fact, it is quite theopposite. In the last thirty years, political philosophers and scientists have producedan oceanic body of literature on the justification, mechanisms, and outcomes ofdemocracy based on a number of procedural and cognitive arguments. They havedone so at different levels: normative (discussing the foundational values), theo-retical (formulating hypothesis), and empirical (developing case studies and testingnew institutional arrangements). Successive generations of scholars have expanded,© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_227http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_2refined, or remixed their different approaches with extraordinary sophistication. Asa result, any attempt to represent the domain of contemporary models of democracywill necessarily be limited and selective. Like the making of the 19th centuryOxford English Dictionary, or the 21st century Wikipedia, the effort would requirethe involvement of hundreds if not thousands of dedicated volunteers.This chapter will take an oblique route by briefly considering the debates indemocracy theory over the last decades that have explored the meaning and practiceof democratic participation. The discussions about the role of citizen participationare sometimes structured into a binary between ‘procedural’ and ‘epistemic’accounts of democratic practice, or, with a different terminology, between ‘ma-joritarian’ and ‘populist’ approaches. Hélène Landemore has proposed a moreexpressive dichotomy: the ‘talkers’ and the ‘counters’ (Landemore 2013, 53).1 The‘talkers’ walk the path of ‘deliberation followed by majority rule as a fallible butoverall reliable way to make collective decisions’; the ‘counters’ explore ‘theepistemic properties of judgement aggregation when large groups of people are",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "no_doi_20250624160653.pdf",
        "file_path": "./PDFs/no_doi_20250624160653.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_2.pdf"
    },
    {
        "title": "Widaug. Data augmentation for named entity recognition using Wikidata",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/widaug",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!26342%202023-70-12.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The code and the experiments can be found in our public GitHub repository.2 The paper is structured as follows: Section 2 details the state-of-the-art of data augmen-tation and Section 3 presents the approach of the method, the use case and the experi-ments performed."
                    }
                ]
            }
        ],
        "doi": "10.26342/2023-70-12",
        "abstract": "Abstract: The current state of the art of Natural Language Processing modelsare based on the use of a big amount of data to be trained. The more, the better.However, this is quite a limitation in the creation of datasets for specific naturallanguage processing tasks such as Named Entity Recognition, which involves oneor more annotators to read, understand and annotate those required named enti-ties along a corpus. Currently, there are many good general domain corpora for theEnglish language. However, particular domains or scenarios and other non-Englishlanguages are still not so represented in the research community. Thus, data aug-mentation techniques are explored to create synthetic data similar to the originalsto enrich the training process of the models. On the other hand, knowledge graphscontain a lot of valuable information that is not being used to help in the data aug-mentation process. This work proposes a data augmentation method based on theWikidata knowledge graph which is tested in a Spanish corpus for a Named EntityRecognition challenge.Keywords: Named Entity Recognition, data augmentation, Wikidata.Resumen: El estado del arte actual de los modelos de Procesamiento de Lengua-je Natural se basa en el uso de una gran cantidad de datos para ser entrenados.Cuantos más, mejor. Sin embargo, esto es una gran limitación en la creación de con-juntos de datos para tareas espećıficas de procesamiento de lenguaje natural, comoel reconocimiento de entidades nombradas, que involucra a uno o más anotadorespara leer, comprender y anotar las entidades nombradas requeridas a lo largo deun corpus. Actualmente, hay bastantes corpus buenos de dominio general para elinglés. Sin embargo, los dominios o escenarios particulares y otros idiomas distintosdel inglés aún no están tan representados en la comunidad de investigación. Por ello,se exploran técnicas de aumento de datos para crear datos sintéticos similares a losoriginales para luego enriquecer el proceso de entrenamiento de los modelos. Porotro lado, los grafos de conocimiento contienen much́ısima información valiosa queno se está utilizando para ayudar en el proceso de aumento de datos. Este trabajopropone un método de aumento de datos basado en el grafo de conocimiento deWikidata que es evaluado en un corpus español para un desaf́ıo de reconocimientode entidades nombradas.Palabras clave: Reconocimiento de Entidades Nombradas, aumento de datos, Wi-kidata.1 IntroductionNamed Entity Recognition (NER) is a Na-tural Language Processing (NLP) task thatconsists of identifying named entities in thetext and classifying them. Traditionally, tho-se name entities are proper names of per-sons, locations, organizations or miscellaneo-us proper names (Grishman and Sundheim,1996; Tjong Kim Sang, 2002). Nowadays,the original classification groups are exten-ded and adapted to particular domains orto scenarios of interest. For instance, thebiomedical domain has defined its own re-levant classification groups such as disea-ses, chemical compounds, DNA, etc. (Perera,Procesamiento del Lenguaje Natural, Revista nº 70, marzo de 2023, pp. 145-155 recibido 16-12-2022 revisado 31-01-2023 aceptado 07-02-2023ISSN 1135-5948 DOI 10.26342/2023-70-12 ©2023 Sociedad Española para el Procesamiento del Lenguaje Natural",
        "publication_date": "2023-03-01",
        "authors": "Óscar Corcho, Alfredo Sánchez Alberca, Pablo Calleja Ibáñez",
        "file_name": "10!26342%202023-70-12.pdf",
        "file_path": "./PDFs/10!26342%202023-70-12.pdf"
    },
    {
        "title": "Five challenges for the Semantic Sensor Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-2010-0005",
        "abstract": "Abstract. The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet. Most efforts in this area focused on the provision of platforms that could be used to build sensor-based applications more efficiently, considering some of the most important challenges in sensor-based data management and sensor network configuration. The introduction of semantics into these platforms provides the opportunity of going a step forward into the understanding, management and use of sensor-based data sources, and this is a topic being ex-plored by ongoing initiatives. In this paper we go through some of the most relevant challenges of the current Sensor Web, and describe some ongoing work and open opportunities for the introduction of semantics in this context. Keywords: Sensor, ontology, query language  1. Introduction The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet [1,6,7,11,15]. Most of the work done on this topic, performed in some cases under the umbrella of the OGC Sensor Web Enablement Working Group1, focused on the creation of specifications for different functionalities related to the management of sensor-based data (observations, measurements, sensor net-work descriptions, transducers, data streaming, etc.), and for the different types of services that may han-dle these data sources (planning, alert, observation and measurement collection and management, etc.).  Some additional work has focused on the provi-sion of platforms that provide the services needed to develop sensor-based applications. These platforms include libraries for common domain-independent data management tasks, such as data cleaning, stor-age, aggregation, query processing, etc., and they are                                                            * Corresponding author. E-mail:  ocorcho@fi.upm.es. 1 http://www.opengeospatial.org/projects/groups/sensorweb used to provide domain-specific aggregated services (e.g., coastal imaging [6], patient care [15]).  Finally, centralized registries for sensor-based data have appeared (e.g., Pachube2, SensorMap3), focused on the registration of sensor-based data sources, and on the provision of access to them in multiple ways, by means of REST-based interfaces, web services, or ad-hoc query languages, to name a few. Figure 1 presents a general architecture of Sensor Web applications; which can be characterised by:   • variability and heterogeneity of data, devices and networks (including unreliable nodes and links, noise, uncertainty, etc.);  • use of rich data sources (sensors, images, GIS, etc.) in different settings (live, streaming, his-torical, and processed);  • existence of multiple administrative domains; and  ",
        "publication_date": "2010-01-01",
        "authors": "Óscar Corcho, Raúl García‐Castro",
        "file_name": "10!3233%sw-2010-0005.pdf",
        "file_path": "./PDFs/10!3233%sw-2010-0005.pdf"
    },
    {
        "title": "Architecting Data Science Education.",
        "implementation_urls": [],
        "abstract": "Abstract. Data scientists are currently among the most demanded professionals in many spheres, including industries, governments, public sector, among oth-ers. This is due to several good reasons. Probably an important one of those rea-sons is the growing demand to find proper ways to face the challenges of estab-lishing data-driven economies and societies. As academics and educationalists, but also Data Science professionals, we look at how to bring up this kind of specialists such that to meet the current shortages but also mid-term demands. In this position paper we deliberate about how to architect thematically, didacti-cally, and organizationally a university program under the thematic umbrella of Data Science. We focus on the selection of learning units or disciplines to be covered in order to produce the M.Sci. and Ph.D. graduates who will be ready to face the future challenges in the mid-term perspective. We outline our rec-ommendation on using learning tools and materials. We also concisely present the approach for stimulating competitive and cooperative atmosphere in the class that stimulates intensive collective and individual learning. We recom-mend to reinforce an academic program by involving industrial partners inten-sively in the process. We ground our deliberation on our experience in imple-menting relevant M.Sci. and Ph.D. programs in Data Science and Semantic Technologies. Keywords: Data Science education, topical scope, program structure, learning tools, didactics, collaboration with industries.  1 Introduction The boost in the abundance, complexity, and variety of data in all spheres of human activity is a phenomenon that leaves a rare information professional negligent these days. Industries are entering into data driven economy, which demands having and using data as a primary asset. On the other hand, the shift to more intensive use of data results in the increase of data generation and storage at unprecedented scales in terms of volumes and rates. A few topical examples are as follows (c.f. [1]): https://orcid.org/0000-0002-5159-254X“Exponential growth of data volumes is accelerated by the dramatic increase of social networking applications that allow non-specialist users create a huge amount of content easily and freely. Equipped with rapidly evolving mobile devices, a user is becoming a nomadic gateway boosting the generation of ad-ditional real-time sensor data. The emerging Internet of Things makes every-thing a data or content, adding billions of additional artificial and autonomic sources of data to the overall picture. Smart spaces, where people, devices, and their infrastructure are all loosely connected, also generate data of unprece-dented volumes and with velocities rarely observed before.” Hence, data generation is a phenomenon that fuels itself and so far we do not ob-serve any signs of saturation for this process. Straightforwardly, the societal demand for the professionals capable of efficient and effective processing of these data also increases at unprecedented rate. These gave rise to Data Science as a discipline and community. As denoted by Hoehndorf and Queralt-Rosinach [2]: “Data Science has as its subject matter the extraction of knowledge from data. While data has been analyzed and knowledge extracted for millennia, the rise of “Big” data has led to the emergence of Data Science as its own discipline that studies how to translate data through analytical algorithms typically taken from statistics, machine learning or data mining, and turning it into knowledge. Data Science also encompasses the study of principles and methods to store, process and communicate with data throughout its life cycle, and starts just af-",
        "publication_date": "2018-01-01",
        "authors": "Vadim Ermolayev, Mari Carmen Suárez-Figueroa, Oleksii Molchanovskyi",
        "file_name": "no_doi_20250624160721.pdf",
        "file_path": "./PDFs/no_doi_20250624160721.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2104/paper_266.pdf"
    },
    {
        "title": "Reference Ontology and (ONTO)2 Agent: The Ontology Yellow Pages",
        "implementation_urls": [],
        "doi": "10.1007/pl00011649",
        "abstract": "Abstract. Knowledge reuse by means of ontologies faces three important problems at present: (1) there are no standardized identifying features that characterize ontologies from the user point of view; (2) there are no web sites using the same logical organization, presenting relevant information about ontologies; and (3) the search for appropriate ontologies is hard, time-consuming and usually fruitless. To solve the above problems, we present: (1) a living set of features that allow us to characterize ontologies from the user point of view and have the same logical organization; (2) a living domain ontology about ontologies (called Reference Ontology) that gathers, describes and has links to existing ontologies; and (3) (ONTO)2Agent, the ontology-based WWW broker about ontologies that uses Reference Ontology as a source of its knowledge and retrieves descriptions of ontologies that satisfy a given set of constraints. 1. Introduction and Motivation During recent years, considerable progress has been made in developing the con­ceptual bases for building technology that allows knowledge component reuse and sharing. One of the main motivations underlying both ontologies and problem-solving methods (PSM) is to enable sharing and reuse of knowledge and reasoning behavior across domains and tasks. PSMs and ontologies can be seen as com­plementary reusable components to construct knowledge systems (Gómez-Pérez and Benjamins, 1998). Ontologies are concerned with static domain knowledge and PSMs with dynamic reasoning knowledge. The integration of ontologies and PSMs is a possible solution to the 'interaction problem' (Bylander and Chan-drasekaran, 1988), which states that representing knowledge for the purpose of solving some problem is strongly affected by the nature of the problem and the inference strategy to be applied to the problem. Ontologies are defined as a formal, explicit specification of a shared conceptu­alization (G. Gruber, 1993; Borst, 1997); that is, 'Conceptualization refers to an abstract model of some phenomenon in the world by having identified the relevant concepts of that phenomenon. Explicit means that the type of concepts used, and the constraints on their use are explicitly defined. Formal refers to the fact that the ontology should be machine-readable. Shared reflects the notion that an ontology captures consensual knowledge, that is, it is not private to some individual, but accepted by a group' (Studer et al, 1998). PSMs describe the reasoning process of a knowledge-based system in an implementation- and domain-independent man­ner (Benjamins and Fensel, 1998). There are also the notions of task ontologies (Mizoguchi et al, 1995) and PSM ontologies (Chandrasekaran et al, 1998). Nowadays, it is easy to get information from organizations that have ontolo­gies and PSMs on the web. There are even accessible points that gather informa­tion about ontologies and have links to other web pages containing more explicit information about such ontologies (see The Ontology Page, also known as TOP; http ://www.medg.lcs.mit.edu/doyle/top) and there are also ontology servers, like The Ontology Server (http://www-ksl.standford.edu:5915) (Farquhar et al, 1995, 1996), Cycorp's Upper CYC Ontology Server (http://www.cyc.com) (Lenat, 1990) or Ontosaurus (http://indra.isi.edu:8000/Loom) (Swartout et al, 1997), that col­lect a huge number of very well-known ontologies. In the PSM area, there are also many PSM repositories at different locations but they are not accessible for outsiders and they are not compatible (Benjamins et al, 1998). At present, the knowledge component reuse and sharing community has identified the need to provide intelligent agents or intelligent brokering services on the WWW that ease the search for such knowledge components. In the ontology field, the need for this kind of services was identified in Fikes and Farquhar (1997) ",
        "publication_date": "2000-11-01",
        "authors": "Julio César Arpírez, Asunción Gómez‐Pérez, Adolfo Lozano-Tello, H. Sofia Pinto",
        "file_name": "10!1007%pl00011649.pdf",
        "file_path": "./PDFs/10!1007%pl00011649.pdf"
    },
    {
        "title": "Exploiting Declarative Mapping Rules for Generating GraphQL Servers with Morph-GraphQL",
        "implementation_urls": [],
        "doi": "10.1142/s0218194020400070",
        "publication_date": "2020-06-01",
        "authors": "David Chaves-Fraga, Freddy Priyatna, Ahmad Alobaid, Óscar Corcho",
        "file_name": "10!1142%s0218194020400070.pdf",
        "file_path": "./PDFs/10!1142%s0218194020400070.pdf"
    },
    {
        "title": "FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/SDM-TIB/FunMap",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-62419-4_16.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "We aim to answer the following research questions: Q1) What is the impact of data duplication rate in the execution time of a knowledge graph creation approach? Q2) What is the impact of different types of complexity over transformation functions during a knowledge graph creation process? Q3) How does the repe-tition of a same function in different mappings affect the existing RML engines? Q4) What is the impact of relational data sources in the knowledge graph cre-ation process? All the resources used to perform this evaluation are available in our Github repository6."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62419-4_16",
        "arxiv": "2008.13482",
        "abstract": "Abstract. Data has exponentially grown in the last years, and knowl-edge graphs constitute powerful formalisms to integrate a myriad of ex-isting data sources. Transformation functions – specified with function-based mapping languages like FunUL and RML+FnO – can be appliedto overcome interoperability issues across heterogeneous data sources.However, the absence of engines to efficiently execute these mappinglanguages hinders their global adoption. We propose FunMap, an inter-preter of function-based mapping languages; it relies on a set of loss-less rewriting rules to push down and materialize the execution of func-tions in initial steps of knowledge graph creation. Although applicable toany function-based mapping language that supports joins between map-ping rules, FunMap feasibility is shown on RML+FnO. FunMap reducesdata redundancy, e.g., duplicates and unused attributes, and convertsRML+FnO mappings into a set of equivalent rules executable on RML-compliant engines. We evaluate FunMap performance over real-worldtestbeds from the biomedical domain. The results indicate that FunMapreduces the execution time of RML-compliant engines by up to a factorof 18, furnishing, thus, a scalable solution for knowledge graph creation.Keywords: Knowledge Graph Creation · Mapping Rules · Functions1 IntroductionKnowledge graphs (KGs) have gained momentum due to the explosion of avail-able data and the demand for expressive formalisms to integrate factual knowl-edge spread across various data sources [14]. KG creation requires the descriptionof schema alignments among data sources and an ontology, as well as the specifi-cation of methods to curate and transform data collected from the input sourcesinto a unified format. A rich spectrum of mapping languages has been proposedto specify schema-ontology alignments across data sources implemented in avariety of semi-structured and structured formats; exemplar approaches includearXiv:2008.13482v2  [cs.DB]  5 Oct 20202 Jozashoori et al.",
        "publication_date": "2020-01-01",
        "authors": "Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, María-Esther Vidal, Óscar Corcho",
        "file_name": "10!1007%978-3-030-62419-4_16.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-62419-4_16.pdf"
    },
    {
        "title": "Editorial: Special issue on Semantic eScience: Methods, tools and applications",
        "implementation_urls": [],
        "doi": "10.3233/sw-200380",
        "abstract": "Abstract. Openly shared, available, and accessible scientific resources facilitate tackling grand challenges that our society,organizations, communities and individuals are facing today. Pandemics, climate change, environmental modeling, genomics,or space exploration all create open research questions for which Artificial Intelligence – and Semantic Web in particular –have a unique potential to accelerate scientific discoveries. In this editorial, we introduce a special issue on Semantic eSciencemethods, tools and applications for the Semantic Web Journal and outline five challenges for Semantic eScience in the years tocome.1. IntroductionAn increasing number of datasets, software toolsand methods reported in scientific publications areshared today to support scientific results and ease theirreproducibility. Open initiatives such as Zenodo1 orthe European Open Science Cloud2 contribute signif-icantly to finding and accessing these scientific re-sources. However, reusing and understanding datasets,software and methods is still challenging given theeffort required to address interoperability issues; andcapture constraints, assumptions, limitations, exam-ples and internal variables associated with data andcode.Computational experiments are becoming morecomplex due to the heterogeneity of models involved,the amount and type of data required, computing re-sources needed, and interdisciplinary collaborations.1https://zenodo.org/2https://www.eosc-portal.eu/about/eoscArtificial Intelligence and Semantic Web technologiescan uniquely contribute to address these challengesby providing means to create meaningful and well-defined descriptions for scientific resources and en-abling the automation of tedious tasks that are cur-rently performed manually.Making scientific resources accessible, semanticallydescribed, and interlinked would create an invalu-able network for scientific research, increasing trans-parency of Science, enabling reproducibility and as-sisting researchers in communicating scientific out-comes to current and future generations. Researcherswould be able to build on previous scientific experi-ments and its results more efficiently; and focus theirefforts in tackling new challenges instead of manipu-lating data in the right format for their analysis. Edu-cators may leverage semantically annotated resourcesfor new students to easily access, understand and reuseexperiments openly available worldwide.The goal of this special issue is to emphasize thesebenefits by collecting the latest research solutions tobridge the gap between existing scientific communica-1570-0844/20/$35.00 © 2020 – IOS Press and the authors. All rights reservedmailto:dgarijo@isi.edumailto:nvillanuevarosales@utep.edu",
        "publication_date": "2020-08-18",
        "authors": "Daniel Garijo, Natalia Villanueva‐Rosales, Tomi Kauppinen",
        "file_name": "10!3233%sw-200380.pdf",
        "file_path": "./PDFs/10!3233%sw-200380.pdf"
    },
    {
        "title": "Author response: Development, validation, and application of a machine learning model to estimate salt consumption in 54 countries",
        "implementation_urls": [],
        "doi": "10.7554/elife.72930.sa2",
        "abstract": "Abstract Global targets to reduce salt intake have been proposed, but their monitoring is chal-lenged by the lack of population-based data on salt consumption. We developed a machine learning (ML) model to predict salt consumption at the population level based on simple predictors and applied this model to national surveys in 54 countries. We used 21 surveys with spot urine samples for the ML model derivation and validation; we developed a supervised ML regression model based on sex, age, weight, height, and systolic and diastolic blood pressure. We applied the ML model to 54 new surveys to quantify the mean salt consumption in the population. The pooled dataset in which we developed the ML model included 49,776 people. Overall, there were no substantial differences between the observed and ML-predicted mean salt intake (p<0.001). The pooled dataset where we applied the ML model included 166,677 people; the predicted mean salt consumption ranged from 6.8 g/day (95% CI: 6.8–6.8 g/day) in Eritrea to 10.0 g/day (95% CI: 9.9–10.0 g/day) in American Samoa. The countries with the highest predicted mean salt intake were in the Western Pacific. The lowest predicted intake was found in Africa. The country-specific predicted mean salt intake was within reasonable difference from the best available evidence. An ML model based on readily available predictors estimated daily salt consumption with good accuracy. This model could be used to predict mean salt consumption in the general population where urine samples are not available.Editor's evaluationSalt intake is a major determinant of volume status, blood pressure values, and congestion, but its estimation is challenging because of the need of measuring 24-h urinary sodium excretion over a number of days, which is unfeasible in most countries. The demonstration of the feasibility of esti-mating accurately salt intake at the population level using artificial intelligence starting from simple and widely available variable is therefore important for epidemiological and intervention studies in which salt intake is a major player, particularly, but not only, in countries experiencing economic hardships.Research Article*For correspondence: r.carrillo-larco@imperial.ac.ukCompeting interest: The authors declare that no competing interests exist.Funding: See page 10Received: 09 August 2021Preprinted: 02 September 2021Accepted: 15 December 2021Published: 25 January 2022Reviewing Editor: Gian Paolo Rossi, University of Padua, Padua, Italy‍ ‍ Copyright Guzman-Vilca et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.https://en.wikipedia.org/wiki/Open_accesshttps://creativecommons.org/https://elifesciences.org/",
        "publication_date": "2021-12-07",
        "authors": "Wilmer Cristobal Guzman‐Vilca, Manuel Castillo‐Cara, Rodrigo M. Carrillo‐Larco",
        "file_name": "10!7554%elife.72930.sa2.pdf",
        "file_path": "./PDFs/10!7554%elife.72930.sa2.pdf"
    },
    {
        "title": "A Secured and Trusted Demand Response system based on Blockchain technologies",
        "implementation_urls": [],
        "doi": "10.1109/inista.2018.8466303",
        "abstract": "Abstract—The aim of the proposed work is to introduce a secure and interoperable Demand Response (DR) management platform that will assist Aggregators (or other relevant Stakeholders involved in DR business scenarios) in their decision making mechanisms over their portfolios of prosumers. This novel architecture incorporates multiple strategies and policies provided from energy market stakeholders, establishing a more modular and future-proof DR solution. By employing an innovative multi-agent decision making system and self-learning algorithms to enable aggregation, segmentation and coordination of several diverse clusters, consisting of supply and demand assets, a fully autonomous design will be delivered. This DR framework is further fortified in terms of data security by not only implementing cutting-edge blockchain infrastructure, but also by making use of Smart Contracts and Decentralized Applications (dApps) which will further secure and facilitate Aggregators-to-Prosumers transactions. The blockchain technologies will be combined with well-known open protocols (i.e. OpenADR) towards also supporting interoperability in terms of information exchange.  Keywords—blockchain, smart contracts, smart grid, demand response. I. INTRODUCTION  Demand Side Resources have already infiltrated the EU energy market, playing a new active role in the electricity distribution grids, as flexible components responding to new grid fluctuations brought on by added levels of wind, solar and other intermittent and volatile distributed generation resources. Besides, recent EU targets aim in reaching a 20% share of renewables by 2020 [1] which increases to at least a 27% share by 2030 [2], with a simultaneous delivery of greenhouse gas emissions reduction by 40%, hence creating a new energy landscape is created. This new reality highlights a growing need for increased operational flexibility as more renewable capacity is added to the grid, with the application of Demand Response (DR) strategies presenting the most efficient answer to a reliable grid management. Either as a behavior-modifying or an automated mechanism, DR is able to change the net load shape and procurement of resources in response to the grid needs. DR being a relatively new commercial mechanism (in 2013 Europe was almost entirely shut to DR) offers vast margins of improvement to a rather unique energy market, unwrapping opportunities for new solutions always in line with the decarbonisation agenda. Taking also into consideration the recent launch of the European Commission’s Clean Energy Package in 2016 [3], the start of the large-scale unlocking of Demand Response potential in Europe has been marked.  Nevertheless, despite the numerous benefits by the DR mechanisms introduced over the past decade, a lot of space ",
        "publication_date": "2018-07-01",
        "authors": "Apostolos C. Tsolakis, Ioannis Moschos, Konstantinos Votis, Dimosthenis Ioannidis, Dimitrios Tzovaras, Pankai Pandey, Sokratis Katsikas, Evangelos Kotsakis, Raúl García‐Castro",
        "file_name": "10!1109%inista.2018.8466303.pdf",
        "file_path": "./PDFs/10!1109%inista.2018.8466303.pdf"
    },
    {
        "title": "Distributed training and inference of deep learning solar energy forecasting models",
        "implementation_urls": [],
        "doi": "10.1109/pdp59025.2023.00035",
        "abstract": "Abstract—Different accurate predictive models have been de-veloped to forecast the amount of solar energy produced ina given area. These models are usually run in a centralizedmanner, considering irradiance inputs taken from a set of sensorsthat are deployed in that area. CAIDE is a framework thatsupports the deployment and analysis of solar plants followingModel Based System Engineering (MBSE) and Internet of Things(IoT) methodologies. However, the current solution performs thetraining and inference phases of the solar energy forecastingmodels in a central way, not taking advantage of the distributedenvironment modeled by means of CAIDE. This work presentsan extension of CAIDE that allows us to distribute the trainingand inference phases, obtaining performance improvements, andachieving a greater adaptation to the inherently distributedtopology of the deployment of the sensors.Index Terms—Complex Systems, Discrete Event System Spec-ification, Deep Learning, Solar Irradiance, Parallel and Dis-tributed Simulation.I. INTRODUCTION AND RELATED WORKThe increasing number of climatic hazards and extremeweather events highlight the need to switch to renewableenergy sources and achieve net zero emissions by 2050 [1].Despite their associated variability, some renewable energiesare growing very fast, and such is the case for solar energy [2].However, effective solar plant management requires deployingsolar irradiance sensors that monitor the amount of energythat can be produced and the support of forecasting modelsto estimate its values over time. In addition, these models andmonitoring systems are usually run in a centralized way, whichlimits their usability and neglects the distributed topology ofthe overall system.This article builds on our previous work [3], where theCAIDE framework was introduced. As in that case, weconsider a complex scenario in which multiple independentsolar irradiance sensor farms are deployed to analyze thepossibilities of Photovoltaic (PV) solar production of a givenregion. Following an architecture based on the Internet ofThings (IoT), the edge layer consists of a set of sensorsthat continuously send data to the fog layer, which alsoThis work has been supported by the Autonomous Region of Madridthrough the program CABAHLA-CM (GA No. P2018/TCS-4423).communicates with a cloud layer periodically (see Figure 1).The framework provides several services for domain expertsand high-level authorities, such as data analysis, inference,or outlier detection. However, it did not initially consider thepossibility of retraining the Deep Learning (DL) model oncethe forecast performance deteriorates. The main contributionof this work is the enlargement of CAIDE’s capabilities byenabling it to perform periodic model training at the cloudlayer. Three different retraining techniques are studied, and",
        "publication_date": "2023-03-01",
        "authors": "Javier Campoy, Ignacio-Iker Prado-Rujas, José L. Risco‐Martín, Katzalin Olcoz, Marı́a S. Pérez",
        "file_name": "10!1109%pdp59025.2023.00035.pdf",
        "file_path": "./PDFs/10!1109%pdp59025.2023.00035.pdf"
    },
    {
        "title": "Best Practices for Implementing FAIR Vocabularies and Ontologies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/ssw200034",
        "arxiv": "2003.13084",
        "abstract": "Abstract. With the adoption of Semantic Web technologies, an increas-ing number of vocabularies and ontologies have been developed in differ-ent domains, ranging from Biology to Agronomy or Geosciences. How-ever, many of these ontologies are still difficult to find, access and un-derstand by researchers due to a lack of documentation, URI resolvingissues, versioning problems, etc. In this chapter we describe guidelinesand best practices for creating accessible, understandable and reusableontologies on the Web, using standard practices and pointing to exist-ing tools and frameworks developed by the Semantic Web community.We illustrate our guidelines with concrete examples, in order to helpresearchers implement these practices in their future vocabularies.Keywords: Ontology metadata · Ontology publication · Ontology ac-cess · FAIR principles · Linked Data principles.1 IntroductionIn the last decade, a series of initiatives for open data, transparency and openscience have led to the development of a myriad of datasets and linked Knowl-edge Graphs on the Web.3 Ontologies and vocabularies have been developedaccordingly to represent the contents of these datasets and Knowledge Graphsand help in their integration and linking. However, while significant effort hasbeen spent on making data Findable, Accessible, Interoperable and Reusable(FAIR) [18], ontologies and vocabularies are often difficult to access, understandand reuse. This may be due to several reasons, including a lack of definitionsof ontology classes and properties; deprecated or unavailable imported ontolo-gies, non-resolvable ontology URIs, lack of examples and diagrams in the docu-mentation, or having scientific publications describing an ontology without anyreference to its implementation.The scientific community has started to acknowledge the need for ontologiesto be properly documented, versioned, published and maintained following the3 https://lod-cloud.net/http://arxiv.org/abs/2003.13084v1https://lod-cloud.net/2 Garijo and Poveda-VillalónLinked Data Principles [6] and adapting the FAIR principles for data [8]. Butthese recommendations do not include guidelines on how to implement them for atarget vocabulary. In this chapter we address this issue by describing how to makean ontology or vocabulary comply with the FAIR principles, including examplessummarizing best practices from the community and our own experience; andpointing to popular existing tools and frameworks.Our guidelines are aimed at ontology engineers, and therefore the paper isstructured according to their ontology development processes: Section 2 describesseveral design decisions for an ontology URI (naming conventions, versioning,permanent URIs); Section 3 describes how to create a documentation that is easyto reuse and understand by others (minimum metadata, diagrams to include);Section 4 illustrates how to make an ontology accessible and findable in the Web;Section 5 points to existing end-to-end frameworks that support the ontologypublication process; and Section 6 concludes our paper. We consider the designand development of an ontology out of the scope of this paper, as it has beencovered by methodologies for ontology development (e.g., LOT4 or NeOn [14]).2 Accessible Ontology URI DesignOntologies are digital artifacts, and therefore their URIs should follow the Linked",
        "publication_date": "2020-11-12",
        "authors": "Daniel Garijo, María Poveda‐Villalón",
        "file_name": "10!3233%ssw200034.pdf",
        "file_path": "./PDFs/10!3233%ssw200034.pdf"
    },
    {
        "title": "Similar Terms Grouping Yields Faster Terminological Saturation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OntoElect/Code",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-13929-2_3.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Absolute (thd) and relative (thdr) terminological differences are computed for fur-ther assessing if Ti+1 differs from Ti more than the individual term significance threshold 7 The baseline THD algorithm is implemented in Python and is publicly available at https://github.com/OntoElect/Code/tree/master/THD https://github.com/OntoElect/Code/tree/master/THD eps."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-13929-2_3",
        "abstract": "Abstract: This paper reports on the refinement of the algorithm for measuring termi-nological difference between text datasets (THD). This baseline THD algorithm, de-veloped in the OntoElect project, used exact string matches for term comparison. In this work, it has been refined by the use of appropriately selected string similarity measures (SSM) for grouping the terms, which look similar as text strings and pre-sumably have similar meanings. To determine rational term similarity thresholds for several chosen SSMs, the measures have been implemented as software functions and evaluated on the developed test set of term pairs in English. Further, the refined al-gorithm implementation has been evaluated against the baseline THD algorithm. For this evaluation, the bags of terms have been used that had been extracted from the three different document collections of scientific papers, belonging to different sub-ject domains. The experiment revealed that the use of the refined THD algorithm, compared to the baseline, resulted in quicker terminological saturation on more com-pact sets of source documents, though at an expense of a noticeably higher computa-tion time. Keywords: Automated Term Extraction, OntoElect, Terminological Difference, String Similarity Measure, Bag of Terms, Terminological Saturation. DOI: https://doi.org/10.1007/978-3-030-13929-2_3    https://doi.org/10.1007/978-3-030-13929-2_3Similar Terms Grouping Yields Faster Terminological Saturation Victoria Kosa1 [0000-0002-7300-8818], David Chaves-Fraga2 [0000-0003-3236-2789],  Natalya Keberle1 [0000-0001-7398-3464] and Aliaksandr Birukou3, 4 [0000-0002-4925-9131] 1 Department of Computer Science, Zaporizhzhia National University,  Zaporizhzhia, Ukraine {victoriya1402.kosa, nkeberle}@gmail.com,  2 Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid, Spain dchaves@fi.upm.es 3 Springer-Verlag GmbH, Heidelberg, Germany aliaksandr.birukou@springer.com 4 Peoples’ Friendship University of Russia (RUDN University), Moscow, Russia Abstract. This paper reports on the refinement of the algorithm for measuring terminological difference between text datasets (THD). This baseline THD algo-rithm, developed in the OntoElect project, used exact string matches for term comparison. In this work, it has been refined by the use of appropriately selected string similarity measures (SSM) for grouping the terms, which look similar as text strings and presumably have similar meanings. To determine rational term similarity thresholds for several chosen SSMs, the measures have been imple-mented as software functions and evaluated on the developed test set of term pairs in English. Further, the refined algorithm implementation has been evalu-ated against the baseline THD algorithm. For this evaluation, the bags of terms have been used that had been extracted from the three different document collec-tions of scientific papers, belonging to different subject domains. The experiment revealed that the use of the refined THD algorithm, compared to the baseline, resulted in quicker terminological saturation on more compact sets of source doc-uments, though at an expense of a noticeably higher computation time. Keywords: Automated Term Extraction, OntoElect, Terminological Difference, String Similarity Measure, Bag of Terms, Terminological Saturation. ",
        "publication_date": "2019-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Natalya Keberle, Aliaksandr Birukou",
        "file_name": "10!1007%978-3-030-13929-2_3.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-13929-2_3.pdf"
    },
    {
        "title": "Events Matter: Extraction of Events from Court Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia200847",
        "abstract": "Abstract. The analysis of court decisions and associated events is part of the dailylife of many legal practitioners. Unfortunately, since court decision texts can oftenbe long and complex, bringing all events relating to a case in order, to understandtheir connections and durations is a time-consuming task. Automated court decisiontimeline generation could provide a visual overview of what happened throughouta case by representing the main legal events, together with relevant temporal infor-mation. Tools and technologies to extract events from court decisions however arestill underdeveloped. To this end, in the current paper we compare the effectivenessof three different extraction mechanisms, namely deep learning, conditional randomfields, and rule-based method, to facilitate automated extraction of events and theircomponents (i.e., the event type, who was involved, and when it happened). In addi-tion, we provide a corpus of manually annotated decisions of the European Court ofHuman Rights, which shall serve as a gold standard not only for our own evaluation,but also for the research community for comparison and further experiments.Keywords. event extraction, named entity recognition, court decisions1. IntroductionCourt decisions are an important source of law information for legal practitioners: theyelaborate on the facts of a case, involved parties, interpretations of the circumstances,the applicable law and legal principles, and finally the legal assessment leading to thedecision. Legal professionals constantly extract, interpret and reason with and about priorcases whilst arguing for a decision in a current, undecided case. However, court decisionstexts can be long and complex and thus time-consuming to read. Therefore it would bebeneficial to find a means to provide a quick overview of a case, thereby helping to turndecisions into operational, consumable and actionable legal knowledge.In this work we focus specifically on using Natural Language Processing (NLP) tech-niques to automatically extract the essence of a court case. Besides extracting generallegal rules from individual cases, we aim at providing a quick overview of what hap-pened, who was involved and when the event took place. In the terminology of NLP, eventextraction can be treated as a text classification task aiming at assigning text fragments(typically, paragraphs, sentences or smaller parts of documents) to predefined (event)Legal Knowledge and Information SystemsS. Villata et al. (Eds.)© 2020 The Authors, Faculty of Law, Masaryk University and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA20084733classes [1]. Another, related NLP task is Named Entity Recognition (NER) which ex-tracts entities referred to in texts and classifies them into categories [2], for instance peo-ple, places and organizations; moreover, named entities can also be domain-specific, forinstance, courts or laws. Event extraction can benefit from NER, since it can be used toenrich events with relevant information, such as the parties involved. This paper focuseson the extraction of events and their components from court decisions of the EuropeanCourt of Human Rights (ECHR)1 based on a sample thereof.Summarizing our contributions, we: (i) provide a corpus of manually annotatedECHR decisions; (ii) perform a comparison of different approaches to automatically ex-tract events and their components – implementations as well as our evaluation results aremade available on GitHub; and (iii) introduce a prototypical web interface that can beused to display court decisions along with their extracted timelines.The remainder of this paper is structured as follows. We outline related works in Sec-",
        "publication_date": "2020-12-01",
        "authors": "Erwin Filtz, María Navas-Loro, Cristiana Santos, Axel Polleres, Sabrina Kirrane",
        "file_name": "10!3233%faia200847.pdf",
        "file_path": "./PDFs/10!3233%faia200847.pdf"
    },
    {
        "title": "Semantic Discovery in the Web of Things",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-74433-9_2",
        "abstract": "Abstract. While the number of things present in the Web grows, theability of discovering such things in order to successfully interact withthem becomes a challenge, mainly due to heterogeneity. The contributionof this paper is two-fold. First, an ontology-based approach to leverageweb things discovery that is transparent to the syntax, protocols andformats used in things interfaces is described. Second, a semantic modelfor describing web things and how to extract and understand the relevantinformation for discovery is proposed.Keywords: discovery, interoperability, web of things, ontologies1 IntroductionThe Internet of Things is characterized by its inherent heterogeneity [1], which isevident when thinking about the diversity of things that can be accessed throughthe Internet. Such diversity does not apply only to the type of things, e.g., ther-mostat, traffic light; but also to many other aspects like their communicationprotocols, data formats and even the IoT standards [2] they implement. Further-more, the range of possibilities for the aforementioned aspects are not expectedto stop growing, so one can say that heterogeneity can only evolve.The number of various things that are being made available through theInternet is growing steadily1. Therefore, IoT consumers cannot be asked to beaware of every possible aspect, platforms and individual things out there, so thatit is necessary to rely on mechanisms and services that enable them to searchfor and discover what they want to consume. In other words, discovery is meantto cope and take advantage of the heterogeneity and large population of thingsin the IoT. For example, discovery is one of the Common Service Functions ofthe architecture proposed by the oneM2M standarization organization2.A detailed description of the open issues in discovery for the so-called Webof Things (WoT) is provided in Section 2, including a characterization of web? This research is partially supported by the European project VICINITY: Openvirtual neighbourhood network to connect intelligent buildings and smart objects(H2020-688467) http://vicinity2020.eu/vicinity/1 http://www.businessinsider.com/there-will-be-34-billion-iot-devices-installed-on-earth-by-2020-2016-52 http://www.onem2m.org/technical/published-documents2 Semantic Discovery in the Web of Thingsthings based on the current discussion of the corresponding W3C Working Group(WG)3, and outlining a desirable common data model for thing descriptions.A particular approach for semantic discovery of web things, their interactionpatterns and attributes regardless of specific communication protocols, syntaxand data formats used in their web interfaces is provided as first contributionof this paper and it is described in Section 3. A use case that implements theproposed approach is also provided.Since such approach is ontology-based, an overview of the developed on-tologies for supporting the proposed solution is presented in Section 4. Thisdescription framework for the Web of Things in the form of an ontology whichanswers to what a thing is, where in the Web are its interfaces and how to ex-tract and understand the discovery-relevant information from it represents thesecond contribution of the present work.Existing approaches for WoT discoverability and related semantic models areoutlined in Section 5 before concluding and discussing future work in Section 6.2 Discovery in the Web of Things",
        "publication_date": "2018-01-01",
        "authors": "Fernando Serena, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "10!1007%978-3-319-74433-9_2.pdf",
        "file_path": "./PDFs/10!1007%978-3-319-74433-9_2.pdf"
    },
    {
        "title": "morph-GraphQL: GraphQL Servers Generation from R2RML Mappings (S)",
        "implementation_urls": [],
        "doi": "10.18293/seke2019-055",
        "publication_date": "2019-07-10",
        "authors": "Freddy Priyatna, David Chaves-Fraga, Ahmad Alobaid, Óscar Corcho",
        "file_name": "10!18293%seke2019-055.pdf",
        "file_path": "./PDFs/10!18293%seke2019-055.pdf"
    },
    {
        "title": "IoT Approaches for Distributed Computing",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/morph-graphql",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1155%2018%9741053.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All the information about the dataset, mapping and queries and their results is available online6."
                    }
                ]
            }
        ],
        "doi": "10.1155/2018/9741053",
        "abstract": "Abstract—REST has become in the last decade the mostcommon manner to provide web services, yet it was not originallydesigned to handle typical modern applications (e.g., mobileapps). GraphQL was released publicly in 2015 and since thenhas gained momentum as an alternative approach to REST.However, generating and maintaining GraphQL resolvers is noteasy. First, a domain expert has to analyse a dataset, designthe corresponding GraphQL schema and map the dataset to theschema. Then, a software engineer (e.g., GraphQL developer)implements the corresponding GraphQL resolvers in a specificprogramming language. In this paper we present an approachthat generates GraphQL resolvers from declarative mappingsspecification in the W3C Recommendation R2RML, hence, canbe used both by a domain expert as without the need toinvolve software developers to implement the resolvers, and bysoftware developers as the initial version of the resolvers to beimplemented. Our approach is implemented in morph-GraphQL.Index Terms—GraphQL, R2RML, OBDAI. INTRODUCTIONIntroduced in 2000, Representational State Transfer (REST)has become the most common manner to provide web servicesin the last few years. Those web services that conform tothe REST principles, known as RESTful web services, useHTTP/S and its operations to make requests to the underlyingserver, such as GET to retrieve objects, POST to add objects,PUT to modify objects and DELETE to remove objects,among others.Over the years, the complexity of modern software concepthas evolved since the inception of REST. For example, typicalmobile applications have to take into account aspects thatreceive little attention in traditional applications, such as thesize of data being exchanged/transmitted and the number ofAPI calls being made. These aspects are relevant to the prob-lem known as over-fetching and under-fetching. Over-fetchingrefers to the situation in which a REST endpoint returns moreDOI reference number: 10.18293/SEKE2019-055data than what is required by the developer. For example, adeveloper may need some information about the name of a userso she hits the corresponding endpoint (/user). However,the endpoint may return information that is not needed by theclient, such as birth date and address. The opposite also raises aproblem, which is having the REST endpoint provide less datathan required. Such a case is called under-fetching. It refers tothe situation in which a single REST endpoint does not providesufficient information requested by the client. For example, inorder to obtain the names of all friends of a particular user,typically two endpoints may be needed: the first is the endpointthat returns the identifiers of all the friends (/friends), andthe second is the one that returns the details of each of thefriends based on the identifier (/user).",
        "publication_date": "2018-01-01",
        "authors": "Javier Prieto, Abbes Amira, Javier Bajo, Santiago Mazuelas, Fernando De la Prieta",
        "file_name": "10!1155%2018%9741053.pdf",
        "file_path": "./PDFs/10!1155%2018%9741053.pdf"
    },
    {
        "title": "A Dialogical Approach to Readiness for Change towards Sustainability in Higher Education Institutions: The Case of the SDGs Seminars at the Universidad Polit é cnica de Madrid",
        "implementation_urls": [],
        "abstract": "Abstract: The transformation for sustainability requires a paradigm shift towards systems thinkingand interdisciplinary collaboration, which entails, above all, a process of cultural change affectingindividual mindsets, organizations and society as a whole. Sustainability in higher educationinstitutions (HEIs) has been a recurrent research field in the past decades. However, little attention hasbeen paid to the processes of internal and cultural change and, in particular, to the first steps to prepareacademic communities for change. Understanding “readiness for change” as a core organizationalcompetency to overcome continuous environmental changes and considering the diluted hierarchyat HEIs, this article proposes the adoption of dialogical and developmental approaches in a singleaction case, the SDGs Seminars at the Universidad Politécnica de Madrid. This methodology wasused to diagnose organizational and individual readiness for change considering cognitive, affectiveand behavioural components, and to identify consequences in organizational structures and culture.Our findings reveal that reframing dialogical spaces in HEIs to experience a collaborative andsustainability culture can unlock change, breaking down organizational silos, reducing resistancesand engaging academic communities in the cocreation of institutional strategies. Furthermore, thecase suggests that acting at the group level has impacts both on the individual and institutional levels.Keywords: readiness for change; higher education; academic culture; collaboration; dialogical;developmental; conversations; transformation; sustainable development goals (SDGs)1. IntroductionThe 2030 Agenda for Sustainable Development [1] requires a paradigm shift towardssystems thinking, collaboration and interdisciplinarity [2]. This transformation entails,above all, a process of cultural change which affects individual mindsets, organizationsand society as a whole [3–5].Higher education institutions (HEIs) are not an exception and should transform theirorganizational structure, culture and communication practices in order to overcome disci-plinary and sectoral boundaries [2,6]. In fact, their highly fragmented and monodisciplinarystructures inherited from the 19th century, as well as a conservative and competitive culturewhich encourages individualism, hierarchy, incrementalism, bureaucracy and market-oriented strategies [7–11], hinder internal and external collaborative potential [12,13]. As aconsequence, their response to societal needs is slowed down [14].Sustainability 2021, 13, 9168. https://doi.org/10.3390/su13169168 https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.comhttps://orcid.org/0000-0003-1077-5611https://orcid.org/0000-0003-2728-5731https://orcid.org/0000-0002-7441-3241https://orcid.org/0000-0003-3444-1501https://doi.org/10.3390/su13169168https://doi.org/10.3390/su13169168https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/su13169168https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/article/10.3390/su13169168?type=check_update&version=2ze sustainabilityArticleA Dialogical Approach to Readiness for Change towardsSustainability in Higher Education Institutions: The Case of theSDGs Seminars at the Universidad Politécnica de Madrid1",
        "file_name": "no_doi_20250624160831.pdf",
        "file_path": "./PDFs/no_doi_20250624160831.pdf",
        "pdf_link": "https://www.mdpi.com/2071-1050/13/16/9168/pdf?version=1629191461"
    },
    {
        "title": "A Template-Based Approach for Annotating Long-Tail Datasets.",
        "implementation_urls": [],
        "abstract": "Abstract. An increasing amount of data is shared on the Web throughheterogeneous spreadsheets and CSV files. In order to homogenize andquery these data, the scientific community has developed Extract, Trans-form and Load (ETL) tools and services that help making these files ma-chine readable in Knowledge Graphs (KGs). However, tabular data maybe complex; and the level of expertise required by existing ETL toolsmakes it difficult for users to describe their own data. In this paper wepropose a simple annotation schema to guide users when transformingcomplex tables into KGs. We have implemented our approach by extend-ing T2WML, a table annotation tool designed to help users annotatetheir data and upload the results to a public KG. We have evaluated oureffort with six non-expert users, obtaining promising preliminary results.Keywords: Dataset annotation · Metadata · Knowledge Graph.1 IntroductionAn increasing amount of data is shared on the Web by multiple organizationsusing Excel and CSV formats. Content creators usually prefer to use tabulardata because it is simple to generate, manipulate and visualize by humans; andthere is a significant number of tools to help explore and edit the contents ofspreadsheets. These data need to be properly understood by others, and hencedocumentation (e.g., variables captured, provenance, usage notes, etc.) is usuallyincluded in auxiliary files or the spreadsheets themselves. As a result, many ofthese spreadsheets have comments, clarifications, notes and references to otherfiles explaining how to interpret the information contained in them.In order to convert tabular data to a machine readable format, the SemanticWeb community has created Extract, Transform and Load (ETL) tools (e.g.,[4]) and mapping languages (e.g., [1, 5]) that help translating spreadsheets intoKnowledge Graphs. However, these tools and languages require significant exper-tise when transforming heterogeneous tabular data with comments, incompletevalues or columns that are interrelated to each other, making it difficult fordomain experts to integrate their own datasets with existing KGs.? Copyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 Garijo et al.In this paper we describe an approach to help non-experts transform theirdata into a structured representation through dataset annotations. Our contri-butions include 1) a dataset annotation schema that helps generating templatesfor translating datasets into KGs; 2) an extension of the T2WML dataset an-notation tool [6] to accommodate the proposed schema; and 3) an approach toupload annotated datasets to a registry once the dataset annotation is complete.In order to assess our approach, we conducted a preliminary evaluation with 6users unfamiliar with Knowledge Representation or Semantic Web technologies,who were able to describe and integrate their annotated datasets as a KG.2 Challenges in Long-Tail Dataset AnnotationWe focus on those datasets that are not straightforward to map into a structuredrepresentation. Consider for example Table 1, which depicts the food prices indifferent regions of Ethiopia at different points in time. The table has a time seriesfor the price value of different items at different dates, a repeated column withthe item being described (ignore), the item category and different informationabout the region where that item was produced. The dataset has also somemissing values and labels marked as ”unknown”, which we may want to skip.",
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Ke-Thia Yao, Amandeep Singh, Pedro Szekely",
        "file_name": "no_doi_20250624160853.pdf",
        "file_path": "./PDFs/no_doi_20250624160853.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2722/profiles2020-paper-3.pdf"
    },
    {
        "title": "Enhancing Trust in Trust Services: Towards an Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [],
        "doi": "10.24251/hicss.2022.712",
        "publication_date": "2022-01-01",
        "authors": "Liuwen Yu, Mirko Zichichi, Réka Markovich, Amro Najjar",
        "file_name": "10!24251%hicss!2022!712.pdf",
        "file_path": "./PDFs/10!24251%hicss!2022!712.pdf"
    },
    {
        "title": "Annotating OGC web feature services automatically for generating geospatial knowledge graphs",
        "implementation_urls": [],
        "doi": "10.1111/tgis.12863",
        "abstract": "AbstractAs their name suggests, trust is of crucialimportance in “trust service”. Nevertheless, in manycases, these services suffer from a lack transparency,documentation, traceability, and inclusive multi-lateraldecision-making mechanisms. To overcome thesechallenges, in this paper we propose an integratedframework which incorporates formal argumentationand negotiation within a blockchain environmentto make the decision-making processes of fundmanagement transparent and traceable. We introducethree possible architectures and we evaluate andcompare them considering different technical, financial,and legal aspects.1. IntroductionIt is a commonplace that trust has a specialimportance in entering into contractual relations. Thereis a domain in which the importance of trust is socrucial that the whole type of service is named afterit, i.e. trust services, where the fund managers arein the position of a fiduciary acting on behalf of theprincipals. The whole service the fiduciary providesis subject to the overall duty to act in the best interestof the client. The legislator can (and does1) declarethe principal’s right to check the fiduciary’s relevantactivities in order to give some weight to this duty byits intended controlability. On one hand, though, mostprobably there is a difference between the principal’sand the fund managers’ expertise and overview giving∗∗This work has received funding from the EU H2020 researchand innovation programme under the MSCA ITN European JointDoctorate grant agreement No 814177 LAST-JD-RIoE.1For instance, the 6:315. § of the Hungarian Civil Code (Act V of2013) says: The principal and the beneficiary shall have the right tocheck the fiduciary’s activities relating to asset management.the very reason to enter in such a relationship, and onthe other hand, the lack of the decision making processesbeing documented might limit the transparency one cangain by practicing this right.While trust companies might rely on smart contractswhen engaging in their core activities in the securitiesmarket—as suggested by scholars [1, 2] and provenby the surge of Decentralized Finance (DeFi) [3]—theinvolvement of Distributed Ledger Technologies (DLTs)for the securities transactions does not address thepossible trust issues between the principal and thefiduciary: the former does not have access to the reasonwhy the transaction took place and whether it was reallyin his interest. To this regard, trust can be understoodas a relational attribute between a social actor and",
        "publication_date": "2021-12-07",
        "authors": "Víctor Saquicela, Luis M. Vilches‐Blázquez, Renán Freire, Óscar Corcho",
        "file_name": "10!1111%tgis!12863.pdf",
        "file_path": "./PDFs/10!1111%tgis!12863.pdf"
    },
    {
        "title": "Converting UML-Based Ontology Conceptualizations to OWL with Chowlk",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Chowlk",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-80418-3_8.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The service is available online1 (See Figure 1) and the source code is shared in a GitHub repository2 under the Apache 2.0 license."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-80418-3_8",
        "abstract": "Abstract. During the ontology conceptualization activity, developersusually generate preliminary models of the ontology in the form of dia-grams. Such models drive the ontology implementation activity, wherethe models are encoded using an implementation language, typically bymeans of ontology editors. The goal of this demo is to take advantageof the developed ontology conceptualizations in order to accelerate theontology implementation activity. For doing so we present Chowlk, aconverter to transform digital UML-based ontology diagrams into OWL.This system aims at supporting users in the generation of the first ver-sions of ontologies by reusing the ontology conceptualization output.Keywords: Ontology engineering · ontology conceptualization · Ontol-ogy implementation1 IntroductionOne important step in ontology development is the conceptualization one, duringwhich the ontology development team defines a set of concepts and propertiesto represent the knowledge of an specific domain. Often, this conceptualizationis materialized in a diagram that displays the ontology elements and their con-nections. From this model, the ontology implementation is carried out normallyusing an ontology editor, encoding the model in OWL. In this process the di-agram is in most of the cases only used as a guideline to manually implementthe ontology. To address this issue, some tools have been proposed to allow thegraphical creation or modification of ontologies[5, 2, 4, 3].The presented work aims at leveraging the above-mentioned diagrams inorder to allow for a smoother transition from the conceptualization activityto the actual implementation, that is transforming XML diagrams followinga UML-based ontology visual notation into OWL. For doing so, rather thanbuilding a graphical ontology editor, the process builds on top of well-adoptedsystem as diagrams.net which allows collaborative and synchronous edition ofthe conceptualization models.? This work has been supported by the BIMERR project funded from the EuropeanUnion’s Horizon 2020 research and innovation programme under grant agreementno. 820621.2 Serge Chávez-Feria, Raúl Garćıa-Castro, and Maŕıa Poveda-Villalón2 Chowlk featuresChowlk is a web application that takes as input an ontology conceptualizationcreated with diagrams.net and generates the OWL implementation. The serviceis available online1 (See Figure 1) and the source code is shared in a GitHubrepository2 under the Apache 2.0 license.The conceptualization should follow the Chowlk visual notation3 which isalso provided as a diagrams.net library4 to allow users to easily reuse the correctshapes to avoid problems during the transformation and save to time during theconceptualization.The converter is able to identify concepts, object properties, datatype prop-erties, and restrictions between those elements. Also, the converter identifiesontology metadata, namespaces and the prefixes being used in the model, dueto specific blocks dedicated to this type of information. Labels to each ontol-ogy element are added during the detection process. Once the XML diagram isloaded into the system, Chowlk starts searching for all the ontology elementsin the conceptualization ignoring shapes not included in the specification. Afterthe detection and creation of the corresponding associations between the onto-",
        "publication_date": "2021-01-01",
        "authors": "Serge Chávez-Feria, Raúl García‐Castro, María Poveda‐Villalón",
        "file_name": "10!1007%978-3-030-80418-3_8.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-80418-3_8.pdf"
    },
    {
        "title": "KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk",
                "type": "git",
                "paper_frequency": 5,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-62466-8_18.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Resource type: Software License: MIT DOI: https://doi.org/10.5281/zenodo.3828068 Repository: https://github.com/usc-isi-i2/kgtk/Keywords: knowledge graph · knowledge graph embedding · knowl-edge graph filtering · knowledge graph manipulation 1 Introduction Knowledge graphs (KGs) have become the preferred technology for representing, sharing and using knowledge in applications."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_18",
        "arxiv": "2006.00088",
        "abstract": "Abstract. Knowledge graphs (KGs) have become the preferred tech-nology for representing, sharing and adding knowledge to modern AIapplications. While KGs have become a mainstream technology, theRDF/SPARQL-centric toolset for operating with them at scale is hetero-geneous, difficult to integrate and only covers a subset of the operationsthat are commonly needed in data science applications. In this paper wepresent KGTK, a data science-centric toolkit designed to represent, cre-ate, transform, enhance and analyze KGs. KGTK represents graphs intables and leverages popular libraries developed for data science applica-tions, enabling a wide audience of developers to easily construct knowl-edge graph pipelines for their applications. We illustrate the frameworkwith real-world scenarios where we have used KGTK to integrate andmanipulate large KGs, such as Wikidata, DBpedia and ConceptNet.Resource type: SoftwareLicense: MITDOI: https://doi.org/10.5281/zenodo.3828068Repository: https://github.com/usc-isi-i2/kgtk/Keywords: knowledge graph · knowledge graph embedding · knowl-edge graph filtering · knowledge graph manipulation1 IntroductionKnowledge graphs (KGs) have become the preferred technology for representing,sharing and using knowledge in applications. A typical use case is building a newknowledge graph for a domain or application by extracting subsets of several ex-isting knowledge graphs, combining these subsets in application-specific ways,augmenting them with information from structured or unstructured sources, andcomputing analytics or inferred representations to support downstream applica-tions. For example, during the COVID-19 pandemic, several efforts focused onarXiv:2006.00088v3  [cs.AI]  26 May 20212 Filip Ilievski, Daniel Garijo et. al.building KGs about scholarly articles related to the pandemic starting from the",
        "publication_date": "2020-01-01",
        "authors": "Filip Ilievski, Daniel Garijo, Hans Chalupsky, Naren Teja Divvala, Yixiang Yao, Craig Milo Rogers, Rongpeng Li, Jun Liu, Amandeep Singh, Daniel Schwabe, Pedro Szekely",
        "file_name": "10!1007%978-3-030-62466-8_18.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-62466-8_18.pdf"
    },
    {
        "title": "Neural networks in distributed computing and artificial intelligence",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2017.06.022",
        "publication_date": "2017-06-17",
        "authors": "Javier Bajo, Juan M. Corchado",
        "file_name": "10!1016%j!neucom!2017!06!022.pdf",
        "file_path": "./PDFs/10!1016%j!neucom!2017!06!022.pdf"
    },
    {
        "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
        "implementation_urls": [],
        "doi": "10.1145/3453172",
        "abstract": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requiresprior specific permission and/or a fee. Request permissions from permissions@acm.org.© 2021 Association for Computing Machinery.2160-6455/2021/06-ART11 $15.00https://doi.org/10.1145/3453172ACM Transactions on Interactive Intelligent Systems, Vol. 11, No. 2, Article 11. Publication date: June 2021.mailto:permissions@acm.orghttps://doi.org/10.1145/3453172http://crossmark.crossref.org/dialog/?doi=10.1145%2F3453172&domain=pdf&date_stamp=2021-07-2111:2 Y. Gil et al.KELLY COBOURN and ZEYA ZHANG, Department of Forest Resources and EnvironmentalConservation, Virginia Tech, Blacksburg, VA 24061CHRISTOPHER DUFFY, Department of Civil Engineering, The Pennsylvania State University,University Park, PA 16802LELE SHU, Department of Land, Air and Water Resources, University of California Davis, Davis, CA95616Major societal and environmental challenges involve complex systems that have diverse multi-scale inter-acting processes. Consider, for example, how droughts and water reserves affect crop production and howagriculture and industrial needs affect water quality and availability. Preventive measures, such as delay-ing planting dates and adopting new agricultural practices in response to changing weather patterns, canreduce the damage caused by natural processes. Understanding how these natural and human processesaffect one another allows forecasting the effects of undesirable situations and study interventions to takepreventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system’s response to a diversity of conditions. A major chal-lenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide vari-ety of data sources available only in formats that require complex conversions. Using expert models forparticular problems requires integration of models with third-party data as well as integration of modelsacross disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotempo-ral, and execution mismatches, which are largely done by hand today and may take more than 2 years ofeffort.We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce mod-eling effort while ensuring utility for decision making. Our work to date makes several innovative contri-butions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assiststhem by suggesting relevant choices and automating steps along the way; (2) semantic metadata for mod-els, including their modeling variables and constraints, that ensures model relevance and proper use for agiven decision-making problem; and (3) semantic representations of datasets in terms of modeling variablesthat enable automated data selection and data transformations. This framework is implemented in the MINT(Model INTegration) framework, and currently includes data and models to analyze the interactions betweennatural and human systems involving climate, water availability, agricultural production, and markets. Ourwork to date demonstrates the utility of AI techniques to accelerate modeling to support decision-makingand uncovers several challenging directions for future work.CCS Concepts: • Computing methodologies → Artificial intelligence; Ontology engineering; Planning andscheduling; Neural networks; Visual analytics; Modeling methodologies; • Applied computing → Decisionanalysis; Agriculture;Additional Key Words and Phrases: Intelligent user interfaces, integrated modeling, model metadata, regional-level decision-making, remote sensing dataACM Reference format:Yolanda Gil, Daniel Garijo, Deborah Khider, Craig A. Knoblock, Varun Ratnakar, Maximiliano Osorio, HernánVargas, Minh Pham, Jay Pujara, Basel Shbita, Binh Vu, Yao-Yi Chiang, Dan Feldman, Yijun Lin, Hayley Song,Vipin Kumar, Ankush Khandelwal, Michael Steinbach, Kshitij Tayal, Shaoming Xu, Suzanne A. Pierce, Lissa",
        "publication_date": "2021-06-30",
        "authors": "Yolanda Gil, Daniel Garijo, Deborah Khider, Craig A. Knoblock, Varun Ratnakar, Maximiliano Osorio, Hernán Vargas, Tam Minh Pham, Jay Pujara, Basel Shbita, Bình Dương Vũ, Yao‐Yi Chiang, Dan Feldman, Yijun Lin, Hae Jin Song, Vipin Kumar, Ankush Khandelwal, Michael Steinbach, Kshitij Tayal, Shaoming Xu, Suzanne A. Pierce, Lissa Pearson, Daniel Hardesty-Lewis, Ewa Deelman, Rafael Ferreira da Silva, Rajiv Mayani, Armen R. Kemanian, Yuning Shi, Lorne Leonard, S. D. Peckham, Maria Stoica, Kelly M. Cobourn, Zeya Zhang, Christopher Duffy, Lele Shu",
        "file_name": "10!1145%3453172.pdf",
        "file_path": "./PDFs/10!1145%3453172.pdf"
    },
    {
        "title": "Towards Integrating Public Procurement Data into a Semantic Knowledge Graph",
        "implementation_urls": [],
        "abstract": "Abstract. Public procurement accounts for a substantial part of thepublic investment and global economy. Therefore, improving effectiveness,efficiency, transparency and accountability of government procurement isof broad interest. To this end, in this poster paper, we present our approachfor integrating procurement data, including public spending and corporatedata, from multiple sources across the EU into a semantic knowledge graph.We are aiming to improve procurement processes through supportingmultiple stake holders, such as government agencies, companies, controlauthorities, journalists, researchers, and individual citizens.Keywords: Knowledge graph · Public procurement · Ontology.1 IntroductionPublic procurement accounts for a substantial part of the public investment andglobal economy. Every year, over 250 000 public authorities in the EU spendaround 14% of GDP on the purchase of services, works and supplies1. Therefore,improving effectiveness, efficiency, transparency and accountability of governmentprocurement is of broad interest [1]. To this end, European Commission hasput several relevant directives forward, i.e., for public sector information (e.g.,Directive 2003/98/EC) and public procurement (e.g., Directive 2014/24/EU8),to improve public procurement practices. As a result of these, national publicprocurement portals have been created, which live together with regional, localas well as EU-wide public procurement portals. However, there is no commonagreement across the EU (not even, in many cases, inside the same country)on the data formats to be used for exposing such data sources and on the datamodels that need to be used for exposing such data, which leads to a largeheterogeneity in the data that is being exposed.? This work is funded by EU H2020 TheyBuyForYou project (780247).?? Corresponding author. Email: ahmet.soylu@sintef.no1 https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enIn Europe, contracting portals like Tenders Electronic Daily2 (TED) may beseen as a way to homogenise the data that is being provided, but unfortunatelythis portal is only used for those contracts that are larger than a predefinedbudget threshold, and hence this does not cover the whole richness of types ofpublic contracts nor does it force the usage of this format for those contracts thatdo not need to be published there. The only relevant data model that is gettingsome important traction worldwide is the Open Contracting Data Standard3(OCDS). However, it has been mostly developed with a focus on transparency inthe public contracting procedures. Though, several ontologies, such as LOTED2[2], PPROC [3], PCO[4] and upcoming eProcurement ontology4, are developedwith different levels of detail and focus for representing procurement data, there isno solution integrating supplier and procurement data enabling such as matchingof suppliers and buyers and advanced analytics and procurement intelligence.In this poster paper, we present our approach, in the context of TheyBuy-ForYou5 project, for integrating procurement data, including public spending andcorporate data, from multiple sources across the EU into a knowledge graph. Weare aiming to improve procurement processes through supporting multiple stakeholders, such as government agencies, companies, control authorities, journalists,researchers, and individual citizens. The proposed solution enables developersto create fully functional, robust, and scalable data integration pipelines, fromincluding sourcing the data, to pre-processing, augmenting, and interlinking it.",
        "publication_date": "2018-01-01",
        "authors": "Ahmet Soylu, Óscar Corcho, Elena Simperl, Dumitru Roman, Francisco Yedro Martínez, Chris Taggart, Ian Makgill, Brian Elvesæter, Ben Symonds, Helen McNally, George Konstantinidis, Yuchen Zhao, Till Christopher Lech",
        "file_name": "no_doi_20250624161028.pdf",
        "file_path": "./PDFs/no_doi_20250624161028.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2262/ekaw-poster-01.pdf"
    },
    {
        "title": "Thesaurus Enhanced Extraction of Hohfeld’s Relations from Spanish Labour Law",
        "implementation_urls": [],
        "abstract": "Abstract. In this paper we describe the design of an experiment to extract Ho-hfeld’s deontic relations from legal texts. Our approach intends to minimise themanual effort in the annotation process by expanding a set of initial annotationswith the legal domain knowledge contained in thesauri represented in SemanticWeb formats. With such annotations, we perform a set of iterations to train a deeplearning relation extraction model. After analysing the results, we will adapt theprocess to work on the extraction of Hohfeld’s potestative relations. We also planto use that model to recognise relations in unseen legal sub-domains.Keywords: Relation Extraction · Thesaurus · Terminology · Semantic Web1 IntroductionNew legal documentation is being generated daily, which implies new regulations andlaws that need to be processed and, most importantly, understood. Several works havealready tackled the difficulties in legal information processing, such as [5], which identi-fies five major aggravating factors: multijurisdictionality, volume, accessibility, updatesand consolidation and vagueness of legal document classification.Natural language processing tools help solving such challenges, and they can reachgreat performance on many language understanding tasks [25]. Yet, these models re-quire significantly large annotated datasets and language resources to train. We found,however, that legal language resources are scarce, mostly monolingual, and sometimespublished in close and proprietary formats. This may be one of the reasons why most In-formation Extraction systems, and Relation Extraction tools specifically, do not handlelegal texts properly and, if they do, they tend to return very general results (see Sec-tion 2). Therefore, with the aim of making legal information understandable and easieraccessible, in this paper we describe the design of an experiment to extract relationsamongst terms in legal texts. We further represent them as part of rich domain-specificmulti-lingual resources, that can be ultimately exploited for different use cases.This work is framed within Lynx3 project, an Innovation Action funded by the Eu-ropean Union’s Horizon 2020, whose goal is to create a Knowledge Graph of legal and3 http://lynx-project.eu/2 P. Martı́n-Chozas et al.regulatory data to ease the access to information from different jurisdictions, languagesand domains. Such a Legal Knowledge Graph (LKG) could be of a great help to complywith current regulations, specially for non-legal-expert users.Amongst all legal relations the Hohfeld’s fundamental legal relations are the mostgeneral ones [10]. The Hohfeld’s relations, being the highest abstraction of all possi-ble legal relations, may serve the basis for more detailed domain-specific legal rela-tions. In other words, the legal relations appearing in legal sub-domains may be seenas sub-relation of Hohfeld’s relations. They are divided in two sets of relations: deon-tic relations (Right, Duty, No-Right and Priviledge) and potestative relations (Power,Liability, Disability and Immunity). The term “deontic” refers to a branch of the logicthat is responsible for studying the inferential relationships between normative formulasthat include the operators of permission (P), obligation (O) and prohibition (F), amongstothers [24]. While deontic relations (Figure 1) are those that modify (ordinary) actions,potestative relations modify deontic relations. In this preliminary experiment we willput the focus on the deontic relations, leaving potestative relations for future work.Right DutyNo-right Privilegeoppositeoppositecorrelativecorrelative",
        "file_name": "no_doi_20250624161029.pdf",
        "file_path": "./PDFs/no_doi_20250624161029.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2918/paper4.pdf"
    },
    {
        "title": "Extracting and Understanding Call-to-actions of Push-Notifications",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-08473-7_14",
        "abstract": "Abstract. Push-notifications are a communication tool leveraged bymany apps to disseminate information, engage with their user base andprovide a means of encouraging users to take particular actions. The nu-anced intent behind a push is not always distinguishable to the end-userat moments of delivery. This work explores the text content of notifica-tions pushed by a number of prominent apps in the marketplace overthe period of 463 days. We present a new ontology that defines notifi-cation Call-to-action (CTA) labels in use today. This facilitates greaterunderstanding behind a push and is a step towards standardisation formarketing teams. Subsequently, we then present results of a notificationdataset annotated with our CTA labels and propose and evaluate a CTAtext classification task, which could facilitate improved solutions for bothusers subscribed to, and marketers creating, push-notifications.Keywords: Push-notification · Call-to-action · Ontology Engineering· Multi-label Classification · MarketingThis preprint has not undergone any post-submission improvements or cor-rections. The work was accepted and is going to be presented at 27th Interna-tional Conference on Applications of Natural Language to Information Systems,NLDB 2022, Valencia, Spain, June 15–17, 2022. The Version of Record of thiscontribution will be published in the Natural Language Processing and Infor-mation Systems volume (NLDB 2022, LNCS 13286 proceedings), and will beavailable online at https://link.springer.com/book/9783031084737.1 IntroductionIn recent years, push-notifications have grown from a mechanism alerting usersto new information to being a key persuasive weapon in a brand’s arsenal forenticing engagement, increasing purchases and achieving many other Key Perfor-mance Indicators (KPIs), on which business success is measured. Users of tech-nology subscribe to receiving communication from brands when they download2 B. Esteves et al.their app and/or opt-in via their platform (e.g. mobile vs web push). However,many of these subscribers, at the point of opt-in, are unaware of the actions thesebrands will subsequently try and nudge them to make through the medium ofpush-notification. Consent dialogue boxes have been predominantly used sincethe introduction of the GDPR, however, it has been shown that these do notfacilitate a sufficient understanding of usage to end-users and actually tend tomanipulate actions [17]. Persuasive Technology research [5,6] has uncovered tech-niques leveraged in mobile app messaging and how effective they are at nudgingusers into a particular pattern of behaviour or taking a particular action.With advances in Natural Language Processing research and its deploymentin commercial applications, a number of platforms have appeared which aidmarketing teams create text content which is personalized and tailored for max-imizing engagement. Phrasee3 and Copy.ai4 are such examples of this. Whilstthe text generated from these platforms is widely distributed, there is no consen-sus on, or taxonomy for, the types of Call-to-action (CTA) that are embeddedin the subsequent copy pushed at subscribers, particularly with respect to push-notifications, which have limited character space and therefore can be less trans-parent with respect to the action they are encouraging. This paper addressesthis by examining push-notification copy, forming a taxonomy of CTA labelsand facilitating CTA prediction.2 Background",
        "publication_date": "2022-01-01",
        "authors": "Beatriz Esteves, Kieran Fraser, Shridhar Kulkarni, Owen Conlan, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%978-3-031-08473-7_14.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-08473-7_14.pdf"
    },
    {
        "title": "Linked-Fiestas: A Knowledge Graph to Promote Cultural Tourism in Spain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/fpriyatna/linked-fiestas",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-03056-8_18.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3 its DCAT description at https://github.com/fpriyatna/linked-fiestas/blob/master/linked-fiestas-dcat.ttl 4 Cimmino et al."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-03056-8_18",
        "abstract": "Abstract. Spain is a hot-spot for the European tourism that conformsan important part of its economy. Large cities tend to monopolize thissector unbalancing the outcome of the tourism in Spain. Promoting festi-vals from less-known regions that belong to the Spanish cultural heritagehas been proposed as a solution to balance the economy of this sector.Unfortunately there is a lack of visibility of such festivals that hindersthe feasibility of such approach. In this paper we introduce the Linked-Fiestas dataset that aims at providing data of festivals and events fromnot so well-known regions, so spreading their cultural heritage, bringingvisibility to them, and thus, increasing tourists interest. Linked-Fiestasgathers data from well-known datasets, such as DBpedia and Wikidata,and from other datasets outside the Web of Data community.Keywords: festivals, linked-data, cultural-heritage1 IntroductionTourism is an emerging industry in Europe that has witness a significant growthduring the last decade [1]. Southern countries as Spain are specially suitable to bevisited namely due to their weather, their rich cultural heritage of national andinternational interest, and other bespoke country characteristics [2]. However theeconomy related to this sector is not balanced at all, less known regions have tocompete with big well-known cities in order to attract tourists [3].Promoting festivals that are part of the Spanish cultural heritage is onestrategy to increase the tourists interest on those less-known Spanish regions [3].Unfortunately, although many of these festivals have been designated as nationaland international interests, their lack of visibility keeps them unknown even forthe locals.In this paper we aim at publishing a dataset containing namely not so well-known Spanish festivals according to the linked data principles, so producing5-star quality data1. In addition, we aim at showing that there is a lack of thiskind of data in the Web of Data. To accomplish our former goal we have exploreddifferent available datasets of both structure and semi-structured nature, then1 https://www.w3.org/DesignIssues/LinkedData.html2 Cimmino et al.we transformed all the data into RDF format using Schema [4] vocabulary.Following, we linked by means of owl:sameAs all instances of the datasets, thelinks were later validated by a domain expert ensuring their quality. As a resultwe have produced one dataset containing festivals. Finally, we show that datasetsthat have been included from the Web of Data do not contain relevant data forthis scenario.This paper is organized as follows: in section 2 we present the relevant exist-ing datasets containing festivals in Spain. In section 3 we describe the followedmethodology to generate Linked-Fiestas. In section 4 we report the existingdatasets’ coverage regarding the festivals categorized by the size of the citieswhere they are held. Finally in section 5 we present our conclusions and ourfuture work. For sake of reproductibility, all the datasets and/or source code isavailable at http://tourismkg.linkeddata.es2 DatasetsTo generate Linked-Fiestas we linked two datasets from two tourism-orientedwebpages (fiestas.net and spain.info) with two datasets from the Web ofData.DBpedia and Wikidata are two examples of cross-domain knowledge graphs",
        "publication_date": "2018-01-01",
        "authors": "Andrea Cimmino, Nandana Mihindukulasooriya, Freddy Priyatna, Mariano Rico",
        "file_name": "10!1007%978-3-030-03056-8_18.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-03056-8_18.pdf"
    },
    {
        "title": "A framework and computer system for knowledge-level acquisition, representation, and reasoning with process knowledge",
        "implementation_urls": [],
        "doi": "10.1016/j.ijhcs.2010.05.004",
        "abstract": "ABSTRACT The development of knowledge-based systems is usually approached through the combined skills of software and knowledge engineers (SEs and KEs, respectively) and of subject matter experts (SMEs). One of the most critical steps in this task aims at transferring knowledge from SMEs’ expertise to formal, machine-readable representations, which allow systems to reason with such knowledge. However, this process is costly and error prone. Alleviating such knowledge acquisition bottleneck requires enabling SMEs with the means to produce the target knowledge representations, minimizing the intervention of KEs. This is especially difficult in the case of complex knowledge types like processes. The analysis of scientific domains like Biology, Chemistry, and Physics uncovers: i) that process knowledge is the single most frequent type of knowledge occurring in such domains and ii) specific solutions need to be devised in order to allow SMEs to represent it in a computational form. We present a framework and computer system for the acquisition and representation of process knowledge in scientific domains by SMEs. We propose methods and techniques to enable SMEs to acquire process knowledge from the domains, to formally represent it, and to reason about it. We have developed an abstract process metamodel and a library of Problem Solving Methods (PSMs), which support these tasks, respectively providing the terminology for SME-tailored process diagrams and an abstract formalization of the strategies needed for reasoning about processes. We have implemented this approach as part of the DarkMatter system and formally evaluated it in the context of the intermediate evaluation of Project Halo, an initiative aiming at the creation of question answering systems by SMEs.   1. INTRODUCTION Building knowledge-based systems is an activity that has been traditionally carried out by a combination of software and knowledge engineers and of subject matter experts (SMEs), also known as domain experts. Software engineers (SEs) are focused on architectural and user interface issues related to the development of software. Knowledge engineers (KEs) are focused on knowledge acquisition and representation tasks, with the aim of building the required knowledge bases. For these tasks, they normally work in collaboration with SMEs, who act as repositories of domain knowledge to a large extent. The combination of KEs and SMEs is feasible for a number of domains. However, it has two main drawbacks, first characterized as the knowledge acquisition bottleneck by Feigenbaum in 1977: i) it is costly and ii) it can be error prone, especially in complex domains.   A large amount of work in knowledge-based systems in the past three decades has concentrated on providing frameworks and tools that support the collaboration of KEs and SMEs with the goal of alleviating the knowledge acquisition bottleneck. Despite such work, existing knowledge acquisition tools are still not effective and intuitive enough to allow SMEs to capture the knowledge from a domain by themselves.   Among the different types of knowledge that can be used in knowledge-based systems, in our work we focus on the particular case of process knowledge. Process knowledge is one of the most widely used but also complex types of knowledge across domains, posing important challenges for knowledge acquisition. A process can be considered as a special concept which encapsulates such things as preconditions, results, contents, ",
        "publication_date": "2010-06-05",
        "authors": "Asunción Gómez‐Pérez, Michael Erdmann, Mark Greaves, Óscar Corcho, Richard Benjamins",
        "file_name": "10!1016%j!ijhcs!2010!05!004.pdf",
        "file_path": "./PDFs/10!1016%j!ijhcs!2010!05!004.pdf"
    },
    {
        "title": "Development, validation, and application of a machine learning model to estimate salt consumption in 54 countries",
        "implementation_urls": [],
        "doi": "10.7554/elife.72930",
        "abstract": "Abstract Global targets to reduce salt intake have been proposed, but their monitoring is chal-lenged by the lack of population-based data on salt consumption. We developed a machine learning (ML) model to predict salt consumption at the population level based on simple predictors and applied this model to national surveys in 54 countries. We used 21 surveys with spot urine samples for the ML model derivation and validation; we developed a supervised ML regression model based on sex, age, weight, height, and systolic and diastolic blood pressure. We applied the ML model to 54 new surveys to quantify the mean salt consumption in the population. The pooled dataset in which we developed the ML model included 49,776 people. Overall, there were no substantial differences between the observed and ML-predicted mean salt intake (p<0.001). The pooled dataset where we applied the ML model included 166,677 people; the predicted mean salt consumption ranged from 6.8 g/day (95% CI: 6.8–6.8 g/day) in Eritrea to 10.0 g/day (95% CI: 9.9–10.0 g/day) in American Samoa. The countries with the highest predicted mean salt intake were in the Western Pacific. The lowest predicted intake was found in Africa. The country-specific predicted mean salt intake was within reasonable difference from the best available evidence. An ML model based on readily available predictors estimated daily salt consumption with good accuracy. This model could be used to predict mean salt consumption in the general population where urine samples are not available.Editor's evaluationSalt intake is a major determinant of volume status, blood pressure values, and congestion, but its estimation is challenging because of the need of measuring 24-h urinary sodium excretion over a number of days, which is unfeasible in most countries. The demonstration of the feasibility of esti-mating accurately salt intake at the population level using artificial intelligence starting from simple and widely available variable is therefore important for epidemiological and intervention studies in which salt intake is a major player, particularly, but not only, in countries experiencing economic hardships.Research Article*For correspondence: r.carrillo-larco@imperial.ac.ukCompeting interest: The authors declare that no competing interests exist.Funding: See page 10Received: 09 August 2021Preprinted: 02 September 2021Accepted: 15 December 2021Published: 25 January 2022Reviewing Editor: Gian Paolo Rossi, University of Padua, Padua, Italy‍ ‍ Copyright Guzman-Vilca et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.https://en.wikipedia.org/wiki/Open_accesshttps://creativecommons.org/https://elifesciences.org/",
        "publication_date": "2022-01-05",
        "authors": "Wilmer Cristobal Guzman‐Vilca, Manuel Castillo‐Cara, Rodrigo M. Carrillo‐Larco",
        "file_name": "10!7554%elife!72930.pdf",
        "file_path": "./PDFs/10!7554%elife!72930.pdf"
    },
    {
        "title": "LUBM4OBDA: Benchmarking OBDA Systems with Inference and Meta Knowledge",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/lubm4obda",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!13052%jwe1540-9589!2284.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are available online at GitHub6 and archived at Zenodo.7 In the following we introduce these elements."
                    }
                ]
            }
        ],
        "doi": "10.13052/jwe1540-9589.2284",
        "abstract": "AbstractOntology-based data access focuses on enabling query evaluation over het-erogeneous relational databases according to the model represented by anontology. The relationships between the ontology and the data sources arecommonly defined with declarative mappings, which are used by systemsto perform SPARQL-to-SQL query translation or to generate RDF dumpsfrom the relational databases. Besides the potential homogenization of databecause of using an ontology, some additional advantages of this paradigmare that it may allow applying reasoning thanks to the ontology, as wellas querying for meta knowledge, which describes statements with informa-tion such as provenance or certainty. In this paper, (i) we adapt a widelyused RDF graph store benchmark, namely LUBM, for ontology-based dataaccess, (ii) extend the benchmark for the evaluation of queries that exploitmeta knowledge, and (iii) apply it for performance evaluation of state-of-the-art declarative mapping systems. Our proposal, the LUBM4OBDABenchmark, considers inference capabilities that are not covered by previousJournal of Web Engineering, Vol. 22_8, 1163–1186.doi: 10.13052/jwe1540-9589.2284© 2024 River Publishers1164 J. Arenas-Guerrero et al.ontology-based data access benchmarks, and it is the first one for the eval-uation of meta knowledge and the RDF-star data model. The experimentalevaluation shows that current virtualization systems cannot handle someadvanced inference tasks, and that optimizations are needed to scale RDF-starmaterialization.Keywords: OBDA, semantic web, ontology, data integration.1 IntroductionRelational databases (RDBs) are widely used by organizations to managetheir data. Some application scenarios can benefit from exploiting this dataas knowledge graphs [22], to potentially homogenize data using ontologies,applying reasoning over ontologies, get new insights, or for semantic dataintegration use cases [39, 41]. Ontology-based data access [40] (OBDA)is a paradigm for making data sources such as RDBs (or local schemas)available through a standardized and common view (or global schema). Thisview is given in the form of an ontology, which also enriches the databasewith context by providing background knowledge, reasoning capabilities,and interlinking with other knowledge bases. The relationships between theglobal and local schemas are usually defined through mappings [27,39] whichpopulate the ontology with instances generated from the RDB. As a result, itis possible to perform semantic queries independently of the structure of theoriginal RDB.In the Semantic Web community, a set of standards has been pro-posed: OWL [21] and RDFS [8] are the languages used to encode ontolo-gies, RDF [13] allows representing data, and queries are expressed usingSPARQL [18]. The mappings are specified using R2RML [14], the W3Crecommendation to map RDBs to RDF that follows the global as view [27]approach, in which the mappings define, for each element in the globalschema, a query over the local schemas. In addition, RML [24] is a well-known superset of R2RML to map not only RDBs, but also other types ofdata sources such as CSV, JSON or XML.",
        "publication_date": "2024-02-22",
        "authors": "Julián Arenas-Guerrero, Marı́a S. Pérez, Óscar Corcho",
        "file_name": "10!13052%jwe1540-9589!2284.pdf",
        "file_path": "./PDFs/10!13052%jwe1540-9589!2284.pdf"
    },
    {
        "title": "FAIR Computational Workflows",
        "implementation_urls": [],
        "doi": "10.1162/dint_a_00033",
        "abstract": "ABSTRACTComputational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.†\t Corresponding author: Carole Goble (E-mail: carole.goble@manchester.ac.uk, ORCID: 0000-0003-1219-2137).© 2019 Chinese Academy of Sciences Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) licenseDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/dint_a_00033 by guest on 24 June 2025http://dx.doi.org/10.1162/dint_a_00051http://dx.doi.org/10.1162/dint_a_00030https://w3id.org/fair/principles/terms/F1https://w3id.org/fair/principles/terms/F2https://w3id.org/fair/principles/terms/F3https://w3id.org/fair/principles/terms/F4https://w3id.org/fair/principles/terms/A1https://w3id.org/fair/principles/terms/A1.1https://w3id.org/fair/principles/terms/A1.2https://w3id.org/fair/principles/terms/A2https://w3id.org/fair/principles/terms/I1https://w3id.org/fair/principles/terms/I2https://w3id.org/fair/principles/terms/I3https://w3id.org/fair/principles/terms/R1https://w3id.org/fair/principles/terms/R1.1https://w3id.org/fair/principles/terms/R1.2https://w3id.org/fair/principles/terms/R1.3http://crossmark.crossref.org/dialog/?doi=10.1162/dint_a_00033&domain=pdf&date_stamp=2020-01-31Data Intelligence\t 109FAIR Computational Workflows1.  INTRODUCTIONIn data intensive science, e-infrastructures and software tool-chains are heavily used to help scientists manage, analyze, and share increasing volumes of complex data [1]. Data processing tasks like data cleansing, normalisation and knowledge extraction need to be automated stepwise in order to foster performance, standardisation and re-usability. Increasingly complex data computations and parameter-driven simulations need reliable e-infrastructures and consistent reporting to enable systematic comparisons of alternative setups [2, 3]. As a response to these needs, the practice of performing computational processes using workflows has taken hold in different domains such as the life sciences [4, 5, 6], biodiversity [7], astronomy [8], geosciences [9], and social sciences [10]. Workflows also support the adoption of novel computational approaches, notably machine learning methods [11], due to the ease with which single components in a processing pipeline can be exchanged or updated.Generally speaking, a workflow is a precise description of a procedure – a multi-step process to coordinate multiple tasks and their data dependencies. In computational workflows each task represents the execution of a computational process, such as: running a code, the invocation of a service, the calling of a command line tool, access to a database, submission of a job to a compute cloud, or the execution of data processing script or workflow. Figure 1 gives an example of a real workflow for variant detection in genomics, represented using the Common Workflow Language open standard [12].Computational workflows promise support for automation that scale across computational infrastructures ",
        "publication_date": "2019-11-01",
        "authors": "Carole Goble, Sarah Cohen‐Boulakia, Stian Soiland‐Reyes, Daniel Garijo, Yolanda Gil, Michael R. Crusoe, Kristian Peters, Daniel Schober",
        "file_name": "10!1162%dint_a_00033.pdf",
        "file_path": "./PDFs/10!1162%dint_a_00033.pdf"
    },
    {
        "title": "An Overview of the TBFY Knowledge Graph for Public Procurement.",
        "implementation_urls": [],
        "abstract": "Abstract. A growing amount of public procurement data is being madeavailable in the EU for the purpose of improving the effectiveness, effi-ciency, transparency, and accountability of government spending. However,there is a large heterogeneity, due to the lack of common data formats andmodels. To this end, we developed an ontology network for representingand linking tender and company data and ingested relevant data fromtwo prominent data providers into a knowledge graph, called TBFY. Inthis poster paper, we present an overview of our knowledge graph.Keywords: Public procurement · Knowledge graph · Ontology.1 IntroductionIn the EU, public authorities spend around 14% of GDP on the purchase ofservices, works, and supplies every year7. Therefore, a growing amount of publicprocurement data is being made available in the EU through public portals for thepurpose of improving the effectiveness, efficiency, transparency, and accountabilityof government spending. However, there is a large heterogeneity, due to the lackof common data formats and models for exposing such data.There are various standardization initiatives for electronic procurement, suchas Open Contracting Data Standard (OCDS)8 and TED eSenders 9. However,these are mostly oriented to achieve interoperability, document-oriented, andprovide no standardised practices to refer to third parties, companies participatingin the process, etc. This again generates a lot of heterogeneity. The Semantic Web? Copyright c© 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).7 https://ec.europa.eu/growth/single-market/public-procurement_en8 http://standard.open-contracting.org/latest/en/9 http://simap.ted.europa.eu/https://ec.europa.eu/growth/single-market/public-procurement_enhttp://standard.open-contracting.org/latest/en/http://simap.ted.europa.eu/approach has been proposed as a response [1]. For example, several ontologies havebeen developed, such as PPROC ontology [3] for describing public processes andcontracts, LOTED2 ontology [2] for public procurement notices, PCO ontology [4]for contracts in public domain, and MOLDEAS ontology [5] for announcementsabout public tenders. Each of these was developed with different concerns inmind (legal, process-oriented, etc.) without significant adoption so far.To this end, we developed an ontology network for representing and linkingtender and company data and ingested relevant data from two prominent dataproviders into a knowledge graph, called TBFY. In this poster paper, we presentan overview of our knowledge graph for public procurement.2 Knowledge GraphWe integrated two datasets according to an ontology network: tender dataprovided by OpenOpps10 in the OCDS format and company data provided byOpenCorporates11. OpenOpps has gathered over 2M tender documents frommore than 300 publishers through Web scrapping and by using open APIs, whileOpenCorporates currently has 140M entities collected from national registers.2.1 Ontology NetworkWe are currently using two main ontologies. First, an ontology for tender data(see Figure 1) that we developed using the OCDS’ data model12.Fig. 1. A fragment of OCDS ontology depicting some of the key classes.Second, we reused the euBG ontology for company data13. Both ontologies",
        "publication_date": "2019-01-01",
        "authors": "Ahmet Soylu, Brian Elvesæter, Philip Turk, Dumitru Roman, Óscar Corcho, Elena Simperl, Ian Makgill, Chris Taggart, Marko Grobelnik, Till Christopher Lech",
        "file_name": "no_doi_20250624161044.pdf",
        "file_path": "./PDFs/no_doi_20250624161044.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper14.pdf"
    },
    {
        "title": "Boosting Knowledge Graph Generation from Tabular Data with RML Views",
        "implementation_urls": [
            {
                "identifier": "https://github.com/morph-kgc/morph-kgc",
                "type": "git",
                "paper_frequency": 12,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-031-33455-9_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Materials are publicly available in Zenodo [4]."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-33455-9_29",
        "abstract": "Abstract. A large amount of data is available in tabular form. RMLis commonly used to declare how such data can be transformed intoRDF. However, RML presents limitations that lead, in many cases, tothe need for additional preprocessing using scripting. Although someproposed extensions (e.g., FnO or RML fields) address some of theselimitations, they are verbose, unfamiliar to most data engineers, andimplemented in systems that do not scale up when large volumes of dataneed to be processed. In this work, we expand RML views to tabularsources so as to address the limitations of this mapping language. In thisway, transformation functions, complex joins, or mixed syntax can bedefined directly in SQL queries. We present our extension of Morph-KGCto efficiently support RML views for tabular sources. We validate ourimplementation adapting R2RML test cases with views and compareit against state-of-the-art RML+FnO systems showing that our systemis significantly more scalable. Moreover, we present specific examplesof a real use case in the public procurement domain where basic RMLmappings could not be used without additional preprocessing.Resource type: Software frameworkLicense: Apache 2.0DOI: 10.5281/zenodo.7385488URL: https://github.com/morph-kgc/morph-kgcKeywords: Knowledge Graph · RML · CSV · Data Integration1 IntroductionAn extensive amount of data is stored as CSV, Microsoft Excel spreadsheets,and other tabular formats such as Apache Parquet [3] or Apache ORC [2].Many organizations are also transforming these data sources into RDF knowl-edge graphs [30] (KGs), given their potential to integrate, represent, and publishheterogeneous data according to the model given by one or several ontologies.Data transformations from tabular sources into RDF are typically definedin a systematic manner using mapping languages [43]. These languages increasethe maintainability of the data integration pipelines and prevent the use of ex-ternal scripting [13]. In addition, mappings leverage specialized data integrationhttps://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0001-8637-6313https://orcid.org/0000-0003-1011-5023https://orcid.org/0000-0003-2949-3307https://orcid.org/0000-0002-9260-0753https://github.com/morph-kgc/morph-kgc2 Arenas-Guerrero et al.systems that come with rich functionality and are optimized for large-scale usecases.The RDF Mapping Language [23] (RML) is a popular language [10] that ex-tends theW3C Recommendation RDB to RDFMapping Language [17] (R2RML)to data formats beyond relational databases (RDBs). In real-world data integra-tion scenarios, some computations, such as transformation functions, complexjoins, or extraction of embedded values, need to be applied to the input data.R2RML enables these computations by wrangling the data using SQL queriesin the mappings that are executed over RDBs. However, RML does not allowthis for tabular sources, which limits the capabilities of the mapping languagefor these common scenarios.",
        "publication_date": "2023-01-01",
        "authors": "Julián Arenas-Guerrero, Ahmad Alobaid, María Navas-Loro, Marı́a S. Pérez, Óscar Corcho",
        "file_name": "10!1007%978-3-031-33455-9_29.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-33455-9_29.pdf"
    },
    {
        "title": "OpenADR Ontology: Semantic Enrichment of Demand Response Strategies in Smart Grids",
        "implementation_urls": [
            {
                "identifier": "https://github.com/albaizq/OpenADRontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%sest48500!2020!9203093.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "SHACL shapes for the proposed OpenADR ontology can be found in our Github repository23."
                    }
                ]
            }
        ],
        "doi": "10.1109/sest48500.2020.9203093",
        "abstract": "Abstract—Demand Response (DR) gains increasing attention asa core building block of smart grids. Advanced ICT systems havebeen made available in the last decades and have been employedalready in commercial energy markets. As more and morehardware and software solutions are flooding the market, theneed for interoperability among systems has become a necessity.Building upon OpenADR, a well-known standard for DR, thiswork presents its semantic enrichment towards transforming itinto an ontology (publicly available), which ultimately enablessemantic interoperability among various DR stakeholders andsystems and other semantic-related features like data validation,reusing terms and integration with other standard ontologies.Following the Linked Open Terms methodology, a detaileddescription of the main OpenADR services is presented, encodedin OWL, along with needed extensions that derive from otherwell-known ontologies. By introducing an OpenADR ontology,the adoption and deployment of OpenADR in both researchand industrial implementations is expected to expand, ultimatelypromoting significantly semantic interoperability in DR systems.Keywords—OpenADR, ontology, demand response, smart grid,semantic interoperabilityI. INTRODUCTIONA. MotivationDemand Response (DR) is already part of the energy marketin multiple European countries and in many more there are fu-ture plans drafted to accommodate this relatively new businessmodel [1]. An increasing number of equipment, services, roles,as well as information exchange have made their appearancein every day energy and financial transactions. Thus, althoughenabling the participation of customers in DR schemes isbecoming a commodity, besides opportunities, it also givesbirth to new challenges, one of which is interoperability [2].The EU Energy Efficiency Directive establishes the require-ments and technical modalities for the respective stakeholdersamong member states towards facilitating the uptake of DRservices [3]. Nevertheless, actual deployment of DR has anincreasing number of variations in response to different energymarket products and regulations amongst member states. Thisvariety of DR programs, where each has its own requirements,highlights the need for clearly defined standards to communi-cate DR program and market information [4]. Thus, multiplestandards have been proposed to describe this domain.The Energy Market Information Exchange (EMIX1) is astandard for price and product definition. The Universal SmartEnergy Framework (USEF [5]) describes the market for flexi-bility, enabling commoditization and market trading of flexibleenergy use. The Smart Grid Architecture Model (SGAM [6])aims to present the design of smart grid use cases from anarchitectural viewpoint. It consists of five layers represent-ing business objectives and processes, functions, information",
        "publication_date": "2020-09-01",
        "authors": "Alba Fernández-Izquierdo, Andrea Cimmino, Christos Patsonakis, Apostolos C. Tsolakis, Raúl García‐Castro, Dimosthenis Ioannidis, Dimitrios Tzovaras",
        "file_name": "10!1109%sest48500!2020!9203093.pdf",
        "file_path": "./PDFs/10!1109%sest48500!2020!9203093.pdf"
    },
    {
        "title": "Efficient Clustering from Distributions over Topics",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.931305",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1145/3148011.3148019",
        "arxiv": "2012.08206",
        "abstract": "ABSTRACT�ere are many scenarios where we may want to �nd pairs of tex-tually similar documents in a large corpus (e.g. a researcher doingliterature review, or an R&D project manager analyzing projectproposals). To programmatically discover those connections canhelp experts to achieve those goals, but brute-force pairwise com-parisons are not computationally adequate when the size of thedocument corpus is too large. Some algorithms in the literaturedivide the search space into regions containing potentially simi-lar documents, which are later processed separately from the restin order to reduce the number of pairs compared. However, thiskind of unsupervised methods still incur in high temporal costs. Inthis paper, we present an approach that relies on the results of atopic modeling algorithm over the documents in a collection, asa means to identify smaller subsets of documents where the simi-larity function can then be computed. �is approach has provedto obtain promising results when identifying similar documentsin the domain of scienti�c publications. We have compared ourapproach against state of the art clustering techniques and withdi�erent con�gurations for the topic modeling algorithm. Resultssuggest that our approach outperforms (> 0.5) the other analyzedtechniques in terms of e�ciency.CCS CONCEPTS•Mathematics of computing ! Probability and statistics; •Information systems!Document topic models; •Appliedcomputing ! Document management and text processing;KEYWORDStopic models; semantic similarity; large-scale text analysis; schol-arly data1 INTRODUCTIONGiven the huge amount of information about any domain that isbeing produced or captured daily, it becomes crucial to providemechanisms for automatically identifying the elements that canbring value for the involved agents (general consumers, experts,�is work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R,�nanced by the Spanish Ministry MINECO and co-�nanced by FEDER..Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro�t or commercial advantage and that copies bear this notice and the full citationon the �rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi�ed. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci�c permission and/or afee. Request permissions from permissions@acm.org.K-CAP 2017, Austin, TX, USA© 2017 ACM. 978-1-4503-5553-7/17/12. . .$15.00DOI: 10.1145/3148011.3148019companies, investors…) and discard the noisy, non-relevant infor-mation. Much of the information is presented in the form of textualdocuments, making necessary for experts to browse through manyof these texts to �nd relevant data. A way to explore the knowledge",
        "publication_date": "2017-12-04",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!1145%3148011!3148019.pdf",
        "file_path": "./PDFs/10!1145%3148011!3148019.pdf"
    },
    {
        "title": "Ontology based document annotation: trends and open research problems",
        "implementation_urls": [],
        "doi": "10.1504/ijmso.2006.008769",
        "publication_date": "2006-01-01",
        "authors": "Óscar Corcho",
        "file_name": "ijmso.2006.008769",
        "file_path": "10.1504/ijmso.2006.008769"
    },
    {
        "title": "Integration of building material databases for IFC-based building performance analysis",
        "implementation_urls": [],
        "doi": "10.22260/isarc2021/0027",
        "publication_date": "2021-11-02",
        "authors": "Stefan Fenz, Julia Bergmayr, Nico Plattner, Serge Chávez-Feria, María Poveda-Villalón, Georgios B. Giannakis",
        "file_name": "10!22260%isarc2021%0027.pdf",
        "file_path": "./PDFs/10!22260%isarc2021%0027.pdf"
    },
    {
        "title": "Clasificación temática automática de documentos basada en vocabularios y frecuencias de uso. El caso de artículos de divulgación científica",
        "implementation_urls": [],
        "doi": "10.3989/redc.2023.3.1996",
        "abstract": "Abstract: It is often necessary to classify documents by assigning them a theme or topic from a series of predefined options. This work is usually done manually, by reading the document by a specialist. This manual process is tedious, requires time and resources, and is prone to bias and preferences of each specialist.As an alternative, this article presents an automatic thematic classification system, capable of classifying hundreds of documents in a few seconds, highly parameterized, and that does not require the specialists intervention. The system is based on predefined thematic vocabularies and frequencies of use of lexical forms, and assigns one or more priority top-ics to each document. The suggested approach has been developed and tested in the context of scientific dissemination articles in the Spanish language.Using this approach, it is possible to systematically classify large amounts of documents by topic, using fewer resources than doing it manually, and avoiding unknown biases. The approach has shown to be as effective as other proposals, but requires less computational resources.Keywords: Document classification; thematic classification; algorithm; vocabularies; lexical frequencies; science dis-semination.Copyright: © 2023 CSIC. Este es un artículo de acceso abierto distribuido bajo los términos de la licencia de uso y distribución Creative Commons Reconocimiento 4.0 Internacional (CC BY 4.0).https://doi.org/10.3989/redc.2023.3.1996mailto:cesar.gonzalez-perez@incipit.csic.eshttps://orcid.org/0000-0002-3976-7589mailto:nacho.vidal@cchs.csic.eshttps://orcid.org/0000-0001-6169-784Xmailto:ana.garcia.g@cchs.csic.eshttps://orcid.org/0000-0002-5952-4971mailto:pcalleja%40fi.upm.es?subject=https://orcid.org/0000-0001-8423-8240https://doi.org/10.3989/redc.2023.3.1996César González-Pérez, José Ignacio Vidal Liy, Ana García García, Pablo Calleja Ibáñez2 Rev. Esp. Doc. Cient., 46(3), julio-septiembre 2023, e362. ISSN-L: 0210-0614. https://doi.org/10.3989/redc.2023.3.19961. INTRODUCCIÓNCualquier institución que trate con un gran nú-mero de documentos, sobre todo si son de pro-cedencia externa, debe clasificarlos temáticamente para su adecuada gestión. Este es el caso de bi-bliotecas o archivos, por ejemplo. La clasificación temática consiste en asignar uno o más temas a cada documento, de un repertorio de temas que puede ser fijo o bien cambiante. En cualquier caso, esta clasificación suele realizarse de forma manual mediante un proceso de análisis de contenido, le-yendo el documento o un resumen de este, si exis-te, por parte de un especialista humano, y asig-nando después uno o más temas. Para decidir qué temas se asignan a un documento, el especialista hace uso de su conocimiento tácito y experiencia, y, a veces, también de criterios previamente espe-cificados. Sea como sea, este proceso es tedioso, requiere mucho tiempo y recursos humanos, y es propenso a los sesgos y preferencias de cada es-pecialista.Hoy en día, en un momento en el que muchos de los documentos que manejamos existen en for-",
        "publication_date": "2023-07-06",
        "authors": "César González-Pérez, José Ignacio Vidal Liy, Ana García‐García, Pablo Calleja Ibáñez",
        "file_name": "10!3989%redc!2023!3!1996.pdf",
        "file_path": "./PDFs/10!3989%redc!2023!3!1996.pdf"
    },
    {
        "title": "Experiential Observations: An Ontology Pattern-Based Study on Capturing the Potential Content within Evidences of Experiences",
        "implementation_urls": [
            {
                "identifier": "https://github.com/eureadit/crowdsourcing-ontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3586078.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "25OWL 2 profiles: https://www.w3.org/TR/owl2-profiles."
                    }
                ]
            }
        ],
        "doi": "10.1145/3586078",
        "abstract": "can we perform an abstraction over the cultural domain to render these data interoperable through areusable ontological framework?RQ2: If such an abstraction is possible, what will its key concepts be, so that the phenomenological coordi-nates of the experience can be extracted from the informational content, while still preserving theirdiversity?A hypothetical upper ontology of observation would abstract from the phenomenon by providing a middle-ware between phenomenological ontologies and ontologies of sources (Figure 1). On the one hand, ontologiesof sources like LRMoo/FRBRoo [4] and SPAR [5] describe objective features of sources but not their informationcontent. On the other hand, experiential ontologies describe the phenomenon in object but not the relationsbetween the content structures and features of the sources. In other words, the need for the E&O is grounded onthe need for a language to express general features of sources with a direct relation to their phenomenologicalcontent. For example, a listener may recount that hearing a particularly “groovy” riff in a rock song, or a peculiar1Submission to the ODP portal: http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation.ACM Journal on Computing and Cultural Heritage, Vol. 16, No. 3, Article 58. Publication date: August 2023.http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation58:4 • A. Antonini et al.Fig. 1. Observations are the result of a combination of activity, experience and the opportunity to express the observation(evidence).rendition of it in a live performance, inspired them to conduct research on the musicians who wrote, played orwere influenced by it. This latter example demonstrates that an abstraction over this reality into a knowledgepattern should not make assumptions on what is labelled as sources—that is, those whose consumption promptsthe observation. They could belong anywhere in the information layering of FRBR, such as works, expressionsor manifestations (which musical performances generate),2 and which ones should be treated as sources dependson the user’s ability to discern among those layers when recording their evidence.Indeed, each different experiential study adopts different models for the description of data, grounded on thenature of the phenomenon in object and relying on a set of assumptions concerning the homogeneous nature ofsources. In this scenario, the E&O pattern supports data interoperability between research use cases, clarifyingthe relation between activity, reflection and the evidences used in experiential studies.1.2 Application ScenariosThe E&O pattern is intended to support extending schemas and ontologies to encode research data from expe-riential studies to express key facts concerning sources, which would otherwise be lost with the backgroundknowledge of the experts on those sources.Interoperable research data do introduce new issues. Indeed, data reuse enables a detachment between theanalysis of sources and the use of the generated data: the in-depth understanding of sources (e.g., provenanceand limitations) is therefore no longer a requirement for using the data. For instance, a scholar in ClassicalStudies can mix and match data from heterogeneous studies, such as online book reviews or marginalia froman author’s library, without being an expert in either type of source or method. In other words, the technicalfeasibility granted by having a common schema of the research subject does remove the need for informationnecessary to the correct framing of a study. E&O provides a way to reinstate the context of datasets, which isnecessary to interpret the data correctly. For instance, is the reading experience spontaneous or guided by aquestionnaire? Is it a mature recollection years after reading or an immediate response? Is reading a free choiceor part of a school or work assignment?The rest of the article is structured as follows. Section 2 provides a brief background on experiential studiesand the issues concerning observing experience. Section 3 presents a summary of the state of the art, includingrelevant patterns and experiential ontologies. Section 4 describes the E&O pattern in detail, whereas Section 5is devoted to its evaluation. In Section 6, we discuss the application of the pattern and of the retrospective casestudies. Section 7 provides a discussion of the case studies in terms of common patterns emerging from applyingE&O to the encoding of the different types of sources. Finally, Section 8 concludes the article and includes futurelines of work.2https://www.ifla.org/publications/functional-requirements-for-bibliographic-records.",
        "publication_date": "2023-08-09",
        "authors": "Alessio Antonini, Alessandro Adamou, Mari Carmen Suárez-Figueroa, Francesca Benatti",
        "file_name": "10!1145%3586078.pdf",
        "file_path": "./PDFs/10!1145%3586078.pdf"
    },
    {
        "title": "Genetic correlations and genome-wide associations of cortical structure in general population samples of 22,824 adults",
        "implementation_urls": [],
        "file_name": "no_doi_20250624161125.pdf",
        "file_path": "./PDFs/no_doi_20250624161125.pdf",
        "pdf_link": "https://www.nature.com/articles/s41467-020-18367-y.pdf"
    },
    {
        "title": "Design and implementation of a low-cost Universal Control for intelligent electric wheelchairs",
        "implementation_urls": [],
        "doi": "10.1109/tla.2018.8408424",
        "abstract": "Abstract— Electric wheelchairs are a fundamental tool for millions of people all over the world. Thanks to this type of wheelchairs persons who suffer from reduced mobility can go about their normal lives. Numerous research works had been carried out with the aim of improving the quality of life of people who are bound to a wheelchair, focusing on the creation of novel control interfaces. Through these interfaces, wheelchair users can have control of it using their voice, simple gestures or eye movement. However, if we look objectively at these interfaces, they are all developed on the basis of a different control system, making it difficult to embed these solutions in real environments. Currently, there is no universal control device which would act as a link between the user’s wheelchair and the developed control interfaces. In this work, we propose the design of a low-cost, universal control board for the control of the wheelchairs’ trajectory, so that any systems integrator of control interface can use it for both research and the development of prototypes that will improve the quality of life in our society.  Index Terms— Intelligent, low cost, Universal Control Device, Wheelchair. I. INTRODUCCIÓN egún estudios realizados por la Organización de Naciones Unidas [1], en torno al 15% de la población mundial sufre algún tipo de discapacidad, ya sea física o psíquica, que les dificulta la realización de rutinas diarias en todos los ámbitos (trabajo, educación, personal, etc.). Dado que estos colectivos, en mayor o menor medida, se enfrentan a desventajas claras, se hace necesario realizar todos los esfuerzos posibles para la minimización de estas dificultades.  Las sillas de ruedas son, para millones de personas, el único método para moverse con autonomía en su día a día. No todas las personas con problemas de movilidad pueden manejar estas sillas de ruedas, ya sean sillas de ruedas convencionales impulsadas por los propios usuario o sillas de ruedas eléctricas.   D. H. De La Iglesia, Department of Computer Science and Automation, University of Salamanca, Salamanca, Spain, danihiglesias@usal.es G. Villarubia, Department of Computer Science and Automation, University of Salamanca, Salamanca, Spain, gvg@usal.es  J.F. De Paz, Department of Computer Science and Automation, University of Salamanca, Salamanca, Spain, fcofds@usal.es J. Bajo, Artificial Intelligence Department. Polytechnic University of Madrid. Spain, jbajope@ieee.org  Existe un colectivo de personas discapacitadas que no pueden operar una silla de ruedas convencional o eléctrica ya que sus limitaciones motrices se lo impiden. Para las personas incluidas en estos colectivos, el gesto de accionar un joystick o ",
        "publication_date": "2018-05-01",
        "authors": "Daniel H. de la Iglesia, Gabriel Villarrubia González, Juan F. De Paz, Javier Bajo",
        "file_name": "10!1109%tla!2018!8408424.pdf",
        "file_path": "./PDFs/10!1109%tla!2018!8408424.pdf"
    },
    {
        "title": "Linked Open Vocabularies (LOV): A gateway to reusable semantic vocabularies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-160213",
        "abstract": "Abstract. One of the major barriers to the deployment of Linked Data is the difficulty that data publishers have in determiningwhich vocabularies to use to describe the semantics of data. This systematic report describes Linked Open Vocabularies (LOV),a high-quality catalogue of reusable vocabularies for the description of data on the Web. The LOV initiative gathers and makesvisible indicators such as the interconnections between vocabularies and each vocabulary’s version history, along with past andcurrent editor (individual or organization). The report details the various components of the system along with some innovations,such as the introduction of a property-level boost in the vocabulary search scoring that takes into account the property’s type(e.g., dc:comment) associated with a matching literal value. By providing an extensive range of data access methods (full-textsearch, SPARQL endpoint, API, data dump or UI), the project aims at facilitating the reuse of well-documented vocabulariesin the Linked Data ecosystem. The adoption of LOV by many applications and methods shows the importance of such a set ofvocabularies and related features for ontology design and the publication of data on the Web.Keywords: LOV, Linked Open Vocabularies, ontology search, Linked Data, vocabulary catalogue1. IntroductionThe last two decades have seen the emergence ofa “Semantic Web” enabling humans and computersystems to exchange data with unambiguous, sharedmeaning. This vision has been supported by WorldWide Web Consortium (W3C) Recommendations suchas the Resource Description Framework (RDF), RDF-*Corresponding author.E-mail: pierre-yves.vandenbussche@ie.fujitsu.com.**Thanks to Amélie Gyrard, Thomas Francart, Thérèze Rogez,Laurent Irles and Anthony McCauley for their help on the project.Schema and the Web Ontology Language (OWL).Thanks to a major effort in publishing data followingSemantic Web and Linked Data principles [6], thereare now tens of billions of facts spanning hundreds oflinked datasets on the Web covering a wide range oftopics. Access to the data is facilitated by portals (suchas Datahub1 or UK Government Data2) or direct pub-lication by organisations (e.g. New York Times3).1http://datahub.io/.2http://data.gov.uk/.3http://data.nytimes.com/.1570-0844/17/$35.00 © 2017 – IOS Press and the authors. All rights reservedmailto:ghislain.atemezing@mondeca.commailto:mpoveda@fi.upm.esmailto:bernard.vatant@mondeca.commailto:pierre-yves.vandenbussche@ie.fujitsu.comhttp://datahub.io/http://data.gov.uk/http://data.nytimes.com/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-160213&domain=pdf&date_stamp=2016-01-14438 P.-Y. Vandenbussche et al. / LOV: A gateway to reusable semantic vocabularies on the WebDespite the enormous volume of data now avail-able on the Web, the Linked Data community has rel-atively little interest in vocabulary4 management, fo-cusing rather on the data itself. A vocabulary con-sists of classes, properties and datatypes that define themeaning of data. RDF vocabularies are themselves ex-pressed and published following the Linked Data prin-",
        "publication_date": "2016-12-06",
        "authors": "Pierre-Yves Vandenbussche, Ghislain Auguste Atemezing, María Poveda‐Villalón, Bernard Vatant",
        "file_name": "10!3233%sw-160213.pdf",
        "file_path": "./PDFs/10!3233%sw-160213.pdf"
    },
    {
        "title": "NLP4Types: Predicting Types Using NLP.",
        "implementation_urls": [],
        "abstract": "Abstract. Type inference for resources in Knowledge Graphs is a widelystudied problem, for which different approaches have been proposed, in-cluding reasoning, statistical analysis, and the usage of the textual in-formation related to the resources. We focus on the latter, exploitingtext classification techniques for predicting semantic types from textualdescriptions. In this paper we introduce NLP4Types, an online tool thatcombines different standard NLP techniques and classifiers for predicttypes based on DBpedia abstracts, as well as to collect feedback fromthe users for those predictions.Keywords: DBpedia, Natural Language Processing, Data Quality, Linked Data1 IntroductionType statements, that is, assertions of types for entities, are the most basic andfundamental piece of information for semantic resources. This information can begenerated by different means, including manual, automated and semi-automatedapproaches. In this paper we cover DBpedia, which is generated automaticallyfrom the information contained in Wikipedia, using a set of translation map-pings, from the entries in tabular format contained the infoboxes of each page.As not all pages contain infoboxes it is not always possible to generate type infor-mation. According to our calculation, around a 16% of resources from Wikipediado not have any type mapped to DBpedia. We have also to take into accountthat, even in those cases in which this information can be generated, it is not al-ways complete or correct, as mappings are defined manually and collaborativelyby users.In this paper we explore how textual abstracts can be exploited, using NLPtechniques, to classify entries into the DBpedia ontology. We combine document-to-term matrix and Named Entity Recognition to train a model that we lateruse to predict types from free text on our tool. We have evaluated our modelusing K-fold evaluation and a well-known gold standard, obtaining high results.The final result of this process is NLP4Types1, an online tool that allows userto explore type predictions and to collect feedback from them.∗This work was partially funded by projects RTC-2016-4952-7, TIN2013-46238-C4-2-R and TIN2016-78011-C4-4-R, from the Spanish State Investigation Agency of theMINECO and FEDER Funds.1http://nlp4types.linkeddata.es2 Related WorkTyping resources on large datasets is a widely studied problem that has beenaddressed during last decade, being SDType [7] the most prominent system.SDType exploits the statistical information of property distribution to infer newtyping statements. Other approaches have been introduced, exploiting differentNLP-based techniques for type assignment based on text [3, 2]. In [5] a hier-archy of Support Vector Machines (hSVM) is introduced for applying lexico-syntactic patterns using a bag-of-words model, extracted from short abstractsand Wikipedia categories. This work extends the Linked Hypernym DatasetFramework [4], by the same authors, for extracting these pattern-based struc-tures. These works introduce also the LHD Gold Standard dataset, which we usein this paper, to measure the performance of our system and compare it to otherexisting tools. This gold standard has been produced, as reported by authors,using experts to assign types to a subset of the English DBpedia resources. Wehave used it to evaluate our system, as it provides means for comparing ourcontribution to both, hSVM and SDType.",
        "publication_date": "2018-01-01",
        "authors": "Idafen Santana-Pérez, Mariano Rico",
        "file_name": "no_doi_20250624161153.pdf",
        "file_path": "./PDFs/no_doi_20250624161153.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2262/ekaw-demo-19-poster.pdf"
    },
    {
        "title": "Completeness and consistency analysis for evolving knowledge bases",
        "implementation_urls": [
            {
                "identifier": "http://github.com/AKSW/RDFUnit",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1016%j!websem!2018!11!004.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "@prefix sh: <http ://www.w3.org/ns/shacl# > ."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.11.004",
        "arxiv": "1811.12721",
        "abstract": "AbstractAssessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessmentprocedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curatethe data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas ofquality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolutionanalysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality,and range constraints. For completeness analysis, we use data profiling information from consecutive knowledge base releases toestimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the resultsof the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively andqualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach isevaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpediaKB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learningmodels over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the bestperforming model in our experimental setup is the Random Forest, reaching an F1 score greater than 90% for minimum andmaximum cardinality and 84% for range constraints.Keywords: Quality Assessment, Evolution Analysis, Validation, Knowledge Base, RDF Shape, Machine Learning1. IntroductionIn recent years, numerous efforts have been put towards shar-ing Knowledge Bases (KBs) in the Linked Open Data (LOD)cloud1. This has led to the creation of large corpora, makingbillions of RDF2 triples available from different domains suchas Geography, Government, Life Sciences, Media, Publication,Social Networking, and User generated data. These KBs evolveover time: their data instances and schemas are updated, ex-tended, revised and refactored [1]. Unlike in more controlledtypes of knowledge bases, the evolution of KBs exposed in theLOD cloud is usually unrestrained [2], which may cause datato suffer from a variety of quality issues, at both schema leveland data instance level. Considering the aggregated measure ofconformance, the empirical study carried out by Debattista etal. [2] shows that datasets published in the LOD cloud havereasonable overall quality, but significant issues remain con-cerning different quality metrics, such as data provenance andlicensing. Therefore, by looking at individual metrics, we canexplore certain aspects, for example data quality issues in thedata collection or integration processes.Data quality relates to the perception of the “fitness for use”in a given context [3]. One of the common tasks for data qual-1http://lod-cloud.net2https://www.w3.org/RDFity assessment is to perform a detailed data analysis with dataprofiling [4]. Data profiling is usually defined as the process ofexamining data to collect statistics and provide relevant meta-data about the data [5]. Based on the information we gatherfrom data profiling, we can thoroughly examine and understanda KB, its structure, and its properties before using the KB. Vari-ous approaches have been developed for KB quality assessmentbased on manual, semi-automatic, and automated approaches.For example, Flemming’s [6] data quality assessment approachevaluate data quality scores based on manual user input for data",
        "publication_date": "2018-11-22",
        "authors": "Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Marco Torchiano, Nandana Mihindukulasooriya, Óscar Corcho, Raúl García‐Castro",
        "file_name": "10!1016%j!websem!2018!11!004.pdf",
        "file_path": "./PDFs/10!1016%j!websem!2018!11!004.pdf"
    },
    {
        "title": "A contribution-based framework for the creation of semantically-enabled web applications",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2009.07.004",
        "abstract": "example, the F-plugin contains one instance of the class Vpoet. Abstract classes and interfaces are written in italics. Fig. 5 shows the layers involved in the development of F-plugins. The upper layer is the API provided by JSPWiki, which provides the abstract class WikiPiugin. Below this layer, the Fortunata API provides a set of generic classes that can be exploited by specific application classes. This is the case of the semantic web application VPOET, which uses the class Vpoet, shown in the layer named \"Semantic Web application\". The last layer, named in the figure \"F-plugin\", is for specific appli­cation plugins. The class Adcivisuai izat ionis an example of the kind of classes that exist in this layer, and howit is related to the other layers. A semantically-enabled web application is represented by a class derived from the abstract class FortunataSWAppii-cat ion, which provides developers with useful methods as well as forcé developers to implement the methods c r éa t e i n ­d iv idua l () and f i i iDataModei( ) concerning semantics persistence. All the plugins in a Fortunata-based application share a semantic web application. In this example, the figure shows the class AddVisual izat ion. This class is an F-plugin, and consequently it inherits the methods implemented in the base class For tuna taP lug in and it is forced to implement three methods (one from the interface WikiPiugin and two from the class For tunataPlugin) . This plugin contains an instance of the class Vpoet, which implements two methods from the class FortunataSWApplication concerning seman­tic data management. The process to créate and contribute an F-plugin is detailed in the upper part of Fig. 6, and follows the usual procedure in any plugin-based architecture. First, the developer must créate the F-plugin locally (steps 1-3) and perform an ade-quate number of tests to check that it is working correctly (step 4). Then she must proceed to the publication (step 5) of the plugin source code and of the documentation about its usage. The bottom part of the figure depicts the process to créate new functionality by reusing the initial functionality following a \"contributively collaboration\" schema. It com-prises the following steps: installation of an existing F-plugin (step 6), reading and understanding of its associated ontol-ogies (either by manually reading the OWL files, using any off-the-shelf ontology editor, or by means of OMEMO) in order to find the elements that must be added, removed or modified, or in order to decide whether a new set of ontologies has to be imported and used (step 7), local creation of the extended plugin (steps 8-10), local tests (step 11) and publication (step 12). The purpose of this detailed explanation is to show the low complexity of the plugin reuse and contribution process. Table 1 summarizes the development tasks that are normally associated to the development of a typical semantic web application, and compares the skills that are required to perform these tasks when using a traditional development approach and a Fortunata-based approach. Traditional development requires more competencies (more development tools and roles) than Fortunata-based development. This is one of the main results of the comparison performed with real developers, which is described in Section 4. 3. OMEMO and VPOET: examples of Fortunata-based semantic web applications This section illustrates how the Fortunata framework can be used to créate two prototypical semantically-enabled web applications. These applications are not intended to be original or innovative, since similar types of applications are available in the current state of the art, but we aim at demonstrating that they are easy to implement and extend using our approach. OMEMO is focused on the HTML publication of ontologies (in a similar fashion to systems like OWLDoc (See http://www.co-ode.org/downloads/owldoc/)), and it is interesting as a case study since it exploits many features of the wiki infrastructure, such as orphan links or the simplicity of the wiki syntax. VPOET is focused on semantic data visualization, and especially http://www.coode.org/downloads/owldoc/http://www.coode.org/downloads/owldoc/http://ode.org/downloads/owldoc/)),r r OÍ Q. 3 ai o O CD ' ü i ",
        "publication_date": "2009-07-12",
        "authors": "Mariano Rico, David Camacho, Óscar Corcho",
        "file_name": "10!1016%j!ins!2009!07!004.pdf",
        "file_path": "./PDFs/10!1016%j!ins!2009!07!004.pdf"
    },
    {
        "title": "Is Automated Consent in Solid GDPR-Compliant? An Approach for Obtaining Valid Consent with the Solid Protocol",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/duo-odrl-dpv",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624161205.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://www.dataprotectioncontrol.org/spec/(accessed on 25 October 2023)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info14120631",
        "abstract": "Abstract: Personal Information Management Systems (PIMS) are acquiring a prominent role in thedata economy by promoting services that help individuals to have more control over the processing oftheir personal data, in line with the European data protection laws. One of the highlighted solutionsin this area is Solid, a new protocol that is decentralizing the storage of data, through the usage ofinteroperable web standards and semantic vocabularies, to empower its users to have more controlover the processing of data by agents and applications. However, to fulfill this vision and gatherwidespread adoption, Solid needs to be aligned with the law governing the processing of personaldata in Europe, the main piece of legislation being the General Data Protection Regulation (GDPR).To assist with this process, we analyze the current efforts to introduce a policy layer in the Solidecosystem, in particular, related to the challenge of obtaining consent for processing personal data,focusing on the GDPR. Furthermore, we investigate if, in the context of using personal data forbiomedical research, consent can be expressed in advance, and discuss the conditions for validconsent and how it can be obtained in this decentralized setting, namely through the matching ofprivacy preferences, set by the user, with requests for data and whether this can signify informedconsent. Finally, we discuss the technical challenges of an implementation that caters to the previouslyidentified legal requirements.Keywords: personal information management systems; Solid; semantic web; data protection; consent1. IntroductionThe General Data Protection Regulation (GDPR) [1] has become the gold standard inthe European Union (EU) and its effects are being globally felt in Asia, Latin America andAfrica [2].The purpose of the GDPR is twofold: on the one hand, it protects individuals in whatconcerns their human rights and, on the other hand, it enables the free flow of personaldata (Article 1 GDPR). The EU expressed a vision that encompasses the creation of a singleEuropean market for data, where access to personal and non-personal data from acrossthe world is secure and can be used by an ecosystem of companies, governments, andindividuals to provide high-quality data-driven products and services for its citizens whileensuring that “EU law can be enforced effectively” and data subjects are still in control ofwhat happens to their personal data [3].In addition to the GDPR, novel data-related legislation with new data governanceschemes, such as the Data Governance Act (DGA) [4], is being brought forward by the EUto build an infrastructure for data-sharing and to improve citizens’ trust. In particular, trusthas been proven as an important factor that positively influences the perceived usefulnessInformation 2023, 14, 631. https://doi.org/10.3390/info14120631 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14120631https://doi.org/10.3390/info14120631https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-0319-8935https://orcid.org/0000-0003-0259-7560https://doi.org/10.3390/info14120631https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14120631?type=check_update&version=1Information 2023, 14, 631 2 of 33and ease of use of digital personal datastores [5] in data-handling services and allow themto share their sensitive data for the ‘public good’.However, this has not come without challenges in its interpretation and enforcement.",
        "publication_date": "2023-11-24",
        "authors": "Marcu Florea, Beatriz Esteves",
        "file_name": "no_doi_20250624161205.pdf",
        "file_path": "./PDFs/no_doi_20250624161205.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/14/12/631/pdf?version=1700836284"
    },
    {
        "title": "SLoG: Large-Scale Logging Middleware for HPC and Big Data Convergence",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00156",
        "abstract": "Abstract—Cloud developers traditionally rely on purpose-specific services to provide the storage model they need for anapplication. In contrast, HPC developers have a much morelimited choice, typically restricted to a centralized parallel filesystem for persistent storage. Unfortunately, these systems oftenoffer very low performance when subject to highly-concurrent,conflicting I/O patterns. This makes difficult the implementationof inherently concurrent data structures such as distributedshared logs. Yet, this data structure is key to applications suchas computational steering, data collection from physical sensorgrids or discrete event generators. In this paper we tackle thisissue. We present SLoG, a shared log middleware providing ashared log abstraction over a parallel file system, designed tocircumvent the aforementioned limitations. We evaluate SLoGdesign on up to 100,000 cores of the Theta supercomputer: itdemonstrates high append velocity at scale while also providingsubstantial benefits for other persistent backend storage systems.I. INTRODUCTIONTraditionally, cloud system designers have used purpose-specific storage services for their data storage needs. Theseinclude key-value stores [1]–[3], wide-column databases [4]–[6] or streaming message brokers [7], [8]. On the contrary,High-Performance Computing (HPC) platforms rest upon ona much more constrained set of storage primitives, typicallylimited to parallel file systems [9], [10] or transient burstbuffers [11], [12]. The low availability of local storage on thecompute nodes of most supercomputers today and the lack ofadministrative access give little opportunity for users to deploythe storage system they need.As the boundaries between HPC and Big Data Analytics(BDA) continue to blur [13], new challenges arise. A criticalobjective set in this convergence context is to foster applicationportability across platforms [14]. Let us consider an appli-cation running in a cloud context, which uses a specializedstorage service such as a distributed shared log [15]. Portingthis application to HPC (e.g., to leverage specific hardwarecapabilities) is challenging. Indeed, deploying cloud-orientedshared log services on HPC is often not possible because ofthe unique specificities of these platforms. While one couldconsider a shared log abstraction over the available parallelfile system, providing the illusion of a storage paradigm atopanother is extremely difficult considering conflicting set ofconstraints and APIs between the two models [16], [17].Shared log storage is indeed one of these storage modelsthat are both unavailable and very difficult to implement onHPC platforms using the available storage primitives. Yet,in scientific applications, distributed logs could play manyroles, e.g. for in-situ visualization of large data streams,collection of telemetry events for computational steering, ordata aggregation from arrays of physical sensors. A shared",
        "publication_date": "2018-07-01",
        "authors": "Pierre Matri, Philip Carns, Robert Ross, Alexandru Costan, Marı́a S. Pérez, Gabriel Antoniu",
        "file_name": "10!1109%icdcs!2018!00156.pdf",
        "file_path": "./PDFs/10!1109%icdcs!2018!00156.pdf"
    },
    {
        "title": "ContractFrames: Bridging the Gap Between Natural Language and Logics in Contract Law",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mnavasloro/ContractFrames",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-31605-1_9.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "6 Results The code of ContractFrames10 is publicly available in a GitHub repository11."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-31605-1_9",
        "abstract": "Abstract. This paper introduces ContractFrames, a framework able totranslate natural language texts referring to the different events relatedto the status of a purchase contract to logic clauses from a legal reasoningsystem called PROLEG. Diverse frames and rules have been developedfor the extraction and storage of this information before its conversionto logic clauses. Our framework uses natural language tools and rules toextract relevant information, store it in the form of frames, and returnthe PROLEG version of the input text. Also an ontology, called theContract Workflow Ontology, has been developed to represent all therelevant information of the events related to a contract. The frameworkhas been tested in a syntethic dataset, and shows promising results.Keywords: Legal NLP · PROLEG · Contract Life-cycle · Legal Ontol-ogy.1 IntroductionMaking machines to understand commercial contracts is a challenging and mul-tidisciplinary task. Natural Language Processing (NLP) techniques are requiredto analyze different documents to extract relevant information, which can beexpressed in very different formats and records. Once obtained, this informationneeds to be properly represented, via some knowledge representation system. Fi-nally, reasoning methods are also required to extract new knowledge consideringcollected information.Focusing on the legal domain, there exist different proposals for formalizinglegal information in the form of logical predicates. We find among them PROLEG[25], a legal reasoning system able to represent and reason about contract status? This work was partially supported by JSPS KAKENHI Grant Number 17H06103and by a project with funding from the European Union’s Horizon 2020 researchand innovation programme under grant agreement No 780602. It has been also par-tially supported by a Predoctoral grant from the I+D+i program of the UniversidadPolitécnica de Madrid. This work has been done during an internship funded by theNational Institute of Informatics.M. Navas-Loro et al.and derive information such as its validity or the right or reason of a rescission,among others. Nevertheless, in spite of having all the contract law logic neededalready coded, how to automatically transform the input text into logic factsremains still as an open and important challenge. Currently, the translation oftexts describing contract events must be manually coded, being therefore veryinefficient in terms of cost and time, and remaining unsolved the fact that anyulterior change would need manual curation. A bridge between NLP and thislogical system is therefore needed to automatically retrieve all relevant facts fromtext to populate the PROLEG fact knowledge base.In this paper, we propose a framework, called ContractFrames, able to trans-late natural language texts referring to the different status of a purchase contractinto PROLEG clauses. These texts are not normative texts nor regular texts(being both types extensively studied in previous literature), but some naturallanguage text at a mid point between regular language and pure legal language;an example of one of these texts can be found in Fig. 1, along with its transla-tion into PROLEG. To the aim of expressing these texts in a full logical legallanguage, we have developed different frames3 and rules for representing andextracting the relevant information that will feed the PROLEG reasoner. Theseresources are integrated into a natural language processing pipeline able to take",
        "publication_date": "2019-01-01",
        "authors": "María Navas-Loro, Ken Satoh, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%978-3-030-31605-1_9.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-31605-1_9.pdf"
    },
    {
        "title": "Technologies and Applications for Big Data Value",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_1",
        "abstract": "Abstract The continuous and significant growth of data, together with improvedaccess to data and the availability of powerful computing infrastructure, has ledto intensified activities around Big Data Value (BDV) and data-driven ArtificialIntelligence (AI). Powerful data techniques and tools allow collecting, storing,analysing, processing and visualising vast amounts of data, enabling data-drivendisruptive innovation within our work, business, life, industry and society.The adoption of big data technology within industrial sectors facilitates organisa-tions to gain a competitive advantage. Driving adoption is a two-sided coin. On oneside, organisations need to master the technology necessary to extract value frombig data. On the other side, they need to use the insights extracted to drive theirdigital transformation with new applications and processes that deliver real value.This book has been structured to help you understand both sides of this coin andbring together technologies and applications for Big Data Value.This chapter defines the notion of big data value, introduces the Big Data ValuePublic-Private Partnership (PPP) and gives some background on the Big Data ValueAssociation (BDVA)—the private side of the PPP. It then moves on to structure theE. Curry (�)Insight SFI Research Centre for Data Analytics, NUI Galway, Irelande-mail: edward.curry@nuigalway.ieS. AuerLeibniz Universität Hannover, Hanover, GermanyA. J. BerreSINTEF Digital, Oslo, NorwayA. MetzgerPaluno, University of Duisburg-Essen, Essen, GermanyM. S. PerezUniversidad Politécnica de Madrid, Boadilla del Monte, Madrid, SpainS. ZillnerSiemens AG, München, Germany© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_1&domain=pdfmailto:edward.curry@nuigalway.iehttps://doi.org/10.1007/978-3-030-78307-5_12 E. Curry et al.contributions of the book in terms of three key lenses: the BDV Reference Model,the Big Data and AI Pipeline, and the AI, Data and Robotics Framework.Keywords Data ecosystem · Big data value · Data-driven innovation · Big Data1 IntroductionThe continuous and significant growth of data, together with improved access todata and the availability of powerful computing infrastructure, has led to intensifiedactivities around Big Data Value (BDV) and data-driven Artificial Intelligence (AI).Powerful data techniques and tools allow collecting, storing, analysing, processingand visualising vast amounts of data, enabling data-driven disruptive innovationwithin our work, business, life, industry and society. The rapidly increasing volumesof diverse data from distributed sources create significant technical challenges forextracting valuable knowledge. Many fundamental, technological and deploymentchallenges exist in the development and application of big data and data-driven AI",
        "publication_date": "2022-01-01",
        "authors": "Edward Curry, Sören Auer, Arne J. Berre, Andreas Metzger, Marı́a S. Pérez, Sonja Zillner",
        "file_name": "10!1007%978-3-030-78307-5_1.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-78307-5_1.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-78307-5_1.pdf"
    },
    {
        "title": "PHDD: Corpus of Physical Health Data Disclosure on Twitter During COVID-19 Pandemic",
        "implementation_urls": [
            {
                "identifier": "https://github.com/echen102/COVID-19-TweetIDs",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%s42979-022-01097-x.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Data and Material Availability The data that support the findings of this study are openly available in zenodo at https://​doi.​org/​10.​5281/​ zenodo.​45383​59."
                    }
                ]
            }
        ],
        "doi": "10.1007/s42979-022-01097-x",
        "abstract": "AbstractHealth-related information is considered as ‘highly sensitive’ by the European General Data Protection Regulations (GDPR) and determining whether a text document contains health-related information or not is of interest for both individuals and companies in a number of different scenarios. Although some efforts have been made to detect different categories of per-sonal data in texts, including health information, the classification task by machines is still challenging. In this work, we aim to contribute to solving this challenge by building a corpus of tweets being shared in the current COVID-19 pandemic context. The corpus is called PHDD(Corpus of Physical Health Data Disclosure on Twitter During COVID-19 Pandemic) and contains 1,494 tweets which have been manually tagged by three taggers in three dimensions: health-sensitivity status, categories of health information, and subject of health history. Furthermore, a lightweight ontology called PTHI(Privacy Tags for Health Information), which reuses two other vocabularies, namely hl7 and dpv, is built to represent the corpus in a machine-readable format. The corpus is publicly available and can be used by NLP experts for implementation of techniques to detect sensitive health information in textual documents.Keywords  Corpus · NLP · Personal data detection · Health-related data · General data protection regulationIntroductionPeople frequently share their personal information on social media networks without considering the potential consequences and threats to their privacy. Simultaneously, the amount of textual data, of which many are sensitive or personal, collected by various organizations is overgrow-ing [7]. Whenever sensitive or personal data is involved, privacy concerns exist as well: in case of online personal data disclosure, users are exposed to a number of threats, such as identity theft, cyber fraud [9], harassment, bullying [16] discrimination in job, credit and visa applications,1 and maybe to other unknown long term consequences. When sensitive categories of personal data, such as health-related information are disclosed, the results can be even more serious: insurance companies may increase costs by finding users’ individual and family health history [3], for example. Other results such as stigma, discrimination, and prejudice are also expected, especially when highly sensitive diseases such as mental illnesses, sexually transmitted diseases, or physical disabilities are involved [1, 15].Personal data resources collected by data controllers are also at risk. This is because of two main reasons: they could be copied and disseminated by unauthorized parties, or they could be published by companies for different purposes such as research or advertisement, without conducting any saniti-zation or de-identification process. In these situations, data controllers are not protecting personal data, thus failing to comply with regulations such as the European General Data Protection Regulation (GDPR). In that case, the cost of vio-lation is high: up to 20 million euros or 4% of the total inter-national turnover, whichever is higher, would be considered This work has been supported by the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant agreement No. 813497 (PROTECT), and the European Union’s Horizon 2020 research and innovation programme under Grant agreement No. 825182 (Prêt-à-LLOD). *\t Rana Saniei ",
        "publication_date": "2022-04-06",
        "authors": "Rana Saniei, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%s42979-022-01097-x.pdf",
        "file_path": "./PDFs/10!1007%s42979-022-01097-x.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s42979-022-01097-x.pdf"
    },
    {
        "title": "Rule extraction in unsupervised anomaly detection for model explainability: Application to OneClass SVM",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2021.116100",
        "arxiv": "1911.09315",
        "abstract": "AbstractOneClass SVM is a popular method for unsupervisedanomaly detection. As many other methods, it suffers fromthe black box problem: it is difficult to justify, in an intu-itive and simple manner, why the decision frontier is iden-tifying data points as anomalous or non anomalous. Suchtype of problem is being widely addressed for supervisedmodels. However, it is still an uncharted area for unsuper-vised learning. In this paper, we evaluate several rule ex-traction techniques over OneClass SVM models, as well aspresent alternative designs for some of those algorithms. To-gether with that, we propose algorithms to compute metricsrelated with eXplainable Artificial Intelligence (XAI) regard-ing the \"comprehensibility\", \"representativeness\", \"stability\"and \"diversity\" of the extracted rules. We evaluate our propos-als with different datasets, including real-world data comingfrom industry. With this, our proposal contributes to extendXAI techniques to unsupervised machine learning models.Keywords XAI, OneClass SVM, unsupervised learning,rule extraction, anomaly detection, metrics1. IntroductionResponsible Artificial Intelligence (RAI) is defined as thedifferent AI principles that should be considered when de-veloping and deploying real applications based on AI [1].RAI serves as a methodological framework to both iden-tify core aspects (or AI principles) that should be consideredwhen developing AI solutions while also proposing how toimplement them. These AI principles include aspects suchas Fairness, Explainability, Security, Privacy and Human-centric design [2].The AI principle of Explainability is addressed throughthe use of Explainable AI (XAI) techniques, which can beapplied to black-box models in order to obtain post-hoc ex-planations based on the information that they provide. In theliterature, there are many XAI proposals for supervised MLmodels. However, some of the most recent and thorough re-views on XAI [3, 4, 1, 5] do not mention many applicationsof those techniques to unsupervised learning.Copyright © 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.Outlier detection is one of the tasks where unsupervisedlearning is applied. It is defined as the process of detect-ing anomalous observations within a dataset, and sometimesremove it as a first step within data-mining applications[6]. There is often no prior information about outliers in adataset, hence unsupervised ML algorithms offer the chanceto infer patterns and detect anomalies. However, not only isit important to detect outliers, but also to explain them. Ex-planations can help to understand why a particular datapointhas been labelled anomalous (and what changes in the fea-",
        "publication_date": "2021-10-25",
        "authors": "Alberto Barbado, Óscar Corcho, Richard Benjamins",
        "file_name": "10!1016%j!eswa!2021!116100.pdf",
        "file_path": "./PDFs/10!1016%j!eswa!2021!116100.pdf"
    },
    {
        "title": "Description of Postdata Poetry Ontology V1.0",
        "implementation_urls": [
            {
                "identifier": "https://github.com/bncolorado/CorpusSonetosSigloDeOro",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624161248.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "url: https://www.w3.org/OWL/(visited on 03/08/2021)."
                    }
                ]
            }
        ],
        "doi": "10.51305/icl.cz.9788076580336.02",
        "abstract": "AbstractOne stream of work in the digital humanities focuses on interoperability processesand the description of traditional concepts using computer-readable languages. Inthe case of literary studies, there has been some research into these topics, but thecomplexity of the knowledge domain remains an issue. This complexity is based onthe different interpretations of concepts in different traditions, the use of isolated andprivate databases, unique applications of language and, thus, the richness of poeticinformation. All of this suggests the need to explore new options to represent thecomplexity in computer-readable language. This paper presents an ontology networkdesigned to capture poetry domain knowledge. The ontologies in question relate topoetic works and their structural and prosodic components.1 IntroductionThe Poetry Standardization and Linked Open Data Project, POSTDATA, aimsto provide a means for European poetry researchers to publish and accesssemantically enriched data. To achieve this goal, it was necessary to developa poetry ontology. This ontology attempts to enhance interoperability in theEuropean poetry research community and capture the concepts and propertiesthat define the domain of European poetry knowledge. The development of theontology began with an attempt to define a domain model of poetry based on ananalysis of 23 poetry repertories (i.e. poetry research databases) (Curado Malta,González-Blanco, et al. 2016; Postdata ERC project 2021). These repertorieswere selected because of their relevance, availability in digital format (i.e. allare implemented in databases) and the rich sample they provided of multi-lingual poetry. They ranged from the classical period (e.g. Pedecerto1) to themodern era (e.g. Corpus of Spanish Golden-Age Sonnets2) to the Middle Ages(e.g. Cantigas de Santa Maria for singers.3) This first step allowed us to identifythe most significant identities and properties that define a poetic work (i.e. apoem), taking into account the different traits related to this literary genre. Theresult was a domain model that reflected poetry’s complexity and heterogene-ity. We then transformed this domain model into an ontology network, whichwould allow for its effective and extensive use in computational frameworksas a Post-data computer-aided annotation tool. In this paper, we present thefirst version of our network’s four most significant ontologies (i.e. version 1.0).4These ontologies relate to poetic works, their structural and prosodic compo-nents and information about relevant dates. To populate the ontologies, weincorporated the ontology definitions into OMEKA, a framework that facilitatesthe use of these ontologies in research tasks. This article is structured as follows:In Section 2, we present some previous results related to ontologies of literature,especially of the poetry domain. Section 3 describes the methodology that weused to develop our ontologies. Section 4 presents a detailed description ofthe most relevant ontologies that we created. Finally, Section 5 outlines ourconclusions and directions for future work.1 http://www.pedecerto.eu2 https://github.com/bncolorado/CorpusSonetosSigloDeOro3 http://www.cantigasdesantamaria.com/4 http://postdata.linhd.uned.es/results/network-of-ontologies/http://www.pedecerto.euhttps://github.com/bncolorado/CorpusSonetosSigloDeOrohttp://www.cantigasdesantamaria.com/http://postdata.linhd.uned.es/results/network-of-ontologies/",
        "publication_date": "2022-01-01",
        "authors": "María Luisa Díez Platas, Helena Bermúdez Sabel, Salvador Ros, Elena González‐Blanco, Óscar Corcho, Omar Khalil Gómez, Laura Hernández‐Lorenzo, Mirella De Sisto, Javier de la Rosa, Álvaro Pérez Pozo, Aitor Diez, Rodríguez Gómez",
        "file_name": "no_doi_20250624161248.pdf",
        "file_path": "./PDFs/no_doi_20250624161248.pdf",
        "pdf_link": "https://www.plottingpoetry.org/books/tackling_toolkit/pdf/02_diezplatas.pdf"
    },
    {
        "title": "Knowledge Graph Construction with R2RML and RML: An ETL System-based Overview",
        "implementation_urls": [],
        "abstract": "Abstract. Knowledge graphs have proven to be a powerful technologyto integrate and structure the myriad of data available nowadays. Thesemantic web community has actively worked on data integration sys-tems, providing an important set of engines and mapping languages tofacilitate the construction of knowledge graphs. Despite these impor-tant efforts, there is a lack of objective evaluations of the capabilities ofthese engines in terms of performance, scalability, and conformance withmapping specifications. In this work, we conduct such evaluation consid-ering several R2RML and RML processors to identify their strengths andweaknesses. We (i) perform a qualitative analysis of the distinctive fea-tures of each engine, (ii) examine their conformance with the mappinglanguage specification they support, and (iii) assess their performanceand scalability using the GTFS-Madrid-Bench benchmark.Keywords: Knowledge Graphs · RML · R2RML · GTFS-Madrid-Bench1 IntroductionIn recent years, knowledge graphs (KGs) have become one of the most widelyused technologies in data integration reaching the top positions of the GartnerHype Cycle for Artificial intelligence in 20203. This popularity has resulted inopen KGs like Wikidata [25] or YAGO [12], and in the adoption of this tech-nology by major technology companies such as Facebook, Google, or eBay [19].To construct KGs from non-RDF data sources, mapping languages allow practi-tioners to define the relationships between input data sources and ontologies inCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0002-8235-7331https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-2924-7272https://orcid.org/0000-0002-5603-6390https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/Arenas-Guerrero et al.a declarative and maintainable manner [17]. Although there are several mappinglanguages in the state of the art (e.g., SPARQL-Generate [16] or ShExML [9]),there are two of them that stand out: R2RML [5], which is the W3C standardlanguage for RDB2RDF mapping, and RML [8], which is a well-known extensionof R2RML for data formats beyond relational databases (RDBs).KGs can be constructed with [R2]RML-compliant engines that can imple-ment two strategies: materialization or virtualization [20]. The former is theETL approach that generates the entire KG (i.e., all the triples), while the lat-ter generates results for SPARQL queries by translating them to the native querylanguage of the input data source (e.g., SQL queries in the case of RDBs) [3,21].Given the high number of engines available [7,13,3,23,22,10], it is easy for anypractitioner to get lost in deciding which one best fits their use case. Whilethere are comprehensive and structured evaluations for the virtualization ap-proach [4,15] that ease the user’s choice, there is a lack of such an evaluation for",
        "publication_date": "2021-03-11",
        "authors": "Julián Arenas-Guerrero, Mario Scrocca, Ana Iglesias-Molina, Jhon Toledo, Luis Pozo-Gilo, Daniel Doña, Óscar Corcho, David Chaves-Fraga",
        "file_name": "no_doi_20250624161251.pdf",
        "file_path": "./PDFs/no_doi_20250624161251.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2873/paper11.pdf"
    },
    {
        "title": "Extending Ontology Engineering Practices to Facilitate Application Development",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/oatapi",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-031-17105-5_2.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "9 An API specification template we have developed is available in https://github.com/oeg-upm/oatapi/tree/main/Additional%20Resources 10 Espinoza-Arias et al."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-17105-5_2",
        "abstract": "Abstract. Ontologies define data organization and meaning in Knowl-edge Graphs (KGs). However, ontologies have generally not been takeninto account when designing and generating Application ProgrammingInterfaces (APIs) to allow developers to consume KG data in a developer-friendly way. To fill this gap, this work proposes a method for API gener-ation based on the artefacts generated during the ontology developmentprocess. This method is described as part of a new phase, called ontologyexploitation, that may be included in the last stages of the traditionalontology development methodologies. Moreover, to support some of thetasks of the proposed method, we developed OATAPI, a tool that gen-erates APIs from two ontology artefacts: the competency questions andthe ontology serialization. The conclusions of this work reflect that thelimitations found in the state-of-the-art have been addressed both atthe methodological and tooling levels for the generation of APIs basedon ontology artefacts. Finally, the lines of future work present severalchallenges that need to be addressed so that the potential of KGs andontologies can be more easily exploited by application developers.Keywords: Ontology Engineering · Application Development · Appli-cation Programming Interface · Ontology Artefacts.1 IntroductionOver recent years, Knowledge Graphs (KGs) have been generated and adoptedby many organizations to integrate data, facilitate interoperability, and generatenew insights and recommendations. KGs are commonly structured according toontologies, which allow data to be unambiguously defined with a shared andagreed meaning, as well as to infer new knowledge. However, despite their adop-tion, KGs are still challenging to consume by application developers.On the one hand, developers face a production-consumption challenge: thereis a gap between the ontology engineers who design an ontology and may in-tervene in KG creation and the application developers who want to consumeits contents [7]. Ontologies may be complex, and the resources generated duringtheir development (use cases, requirements, etc.) are often not made available totheir users (e.g. application developers). As a result, developers usually need to2 Espinoza-Arias et al.duplicate some of the effort already done by ontology engineers when they wereunderstanding the domain, interacting with domain experts, taking modeling de-cisions, etc. On the other hand, application developers face technical challenges:many of them are not familiar with Semantic Web standards such as OWL andSPARQL, and hence those KGs that are exclusively based on Semantic Webtechnologies remain hardly accessible to them [18]. Developers (and in particu-lar application developers) are mostly used to data representation formats likeJSON and Application Programming Interfaces (APIs) for accessing data.In order to address both production-consumption and technical challenges,multiple approaches have been proposed by the Semantic Web community, rang-ing from Semantic RESTful APIs [15] which are compatible with Semantic Weband REST; to tools to create Web APIs on top of SPARQL endpoints [3, 12, 1,4]. Outside the Semantic Web community, approaches like GraphQL1 are gain-ing traction among developers due to their flexibility to query and retrieve datafrom public endpoints. However, generating APIs based on ontology artefactshas received less attention so far. These artefacts are any intermediate or finalresources generated during the ontology development process (e.g. competency",
        "publication_date": "2022-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "10!1007%978-3-031-17105-5_2.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-17105-5_2.pdf"
    },
    {
        "title": "A Systematic Review of Ontologies for the Water Domain",
        "implementation_urls": [],
        "doi": "10.1002/9781394171460.ch2",
        "abstract": "ABSTRACT. Big data generated by remote sensing, ground-based measurements, models and simulations, social media and crowd- sourcing, and a wide range of structured and unstructured sources necessitates significant data and knowledge management efforts. In- novations and developments in information technology over the last couple of decades have made data and knowledge management pos- sible for an insurmountable amount of data collected and generated over the last decades. This enabled open knowledge networks to be built that led to new ideas in scientific research and the business world. To design and develop open knowledge networks, ontologies are essential since they form the backbone of conceptualization of a given knowledge domain. A systematic literature review was conducted to examine research involving ontologies related to hydrological processes and water resource management. Ontologies in the hydrology domain support the comprehension, monitoring, and representation of the hydrologic cycle’s complex structure, as well as the predictions of its processes. They contribute to the development of ontology-based information and decision support systems; understanding of en- vironmental and atmospheric phenomena; development of climate and water resiliency concepts; creation of educational tools with arti- ficial intelligence; and strengthening of related cyberinfrastructures. This review provides an explanation of key issues and challenges in ontology development based on hydrologic processes to guide the development of next generation artificial intelligence applications. The study also discusses future research prospects in combination with artificial intelligence and hydroscience.   Keywords: ontology, hydrology, water resources management, knowledge generation, knowledge representation, knowledge networks, knowledge graph   1. Introduction A massive and expanding amount of data is collected and generated in a wide range of disciplines, from sensors to web, and crowdsourcing in this digital age (Jones et al., 2018). By 2025, the world’s digital data will have grown to 175 zettabytes (IDC, 2018). Furthermore, according to a report by the World Economic Forum, roughly 70% of the data generated is never utilized. The lack of interoperability and connectivity of data in separate silos is the primary cause of this limitation in usage. In addition, software and tools must be able to read data au- tomatically to access, process, and integrate this information. The data is heterogeneous, and problems stemming from hetero- geneity are very common in the domain of Earth science (Demir et al., 2015). Because of the different terminologies that are used to identify these observations and the unstructured, incomplete, and diversified nature of the data, making this data accessible and reconcilable is a major challenge (Masmoudi et al., 2021). Effective tools are required for the management, analysis, and   * Corresponding author. Tel.: +1(319)335-5237; fax: +1(319)335-5238.  E-mail address: ozlem-baydaroglu@uiowa.edu (Ö. Baydaroğlu).  ISSN: 1726-2135 print/1684-8799 online © 2023 ISEIS All rights reserved. doi:10.3808/jei.202300500.  communication of these massive data streams. To overcome these challenges, knowledge graphs and ontology-based data management (Sermet and Demir, 2019) introduced as a new paradigm. From the computer science point of view, ontology deals with the classification and explanation of entities for information integration, retrieval on the Internet ",
        "publication_date": "2022-09-16",
        "authors": "Shikha Mehta, Sanju Tiwari, Patrick Siarry, M. A. Jabbar",
        "file_name": "10!1002%9781394171460.ch2.pdf",
        "file_path": "./PDFs/10!1002%9781394171460.ch2.pdf"
    },
    {
        "title": "Association between <i>CFH, CFB, ARMS2, SERPINF1, VEGFR1</i> and <i>VEGF</i> polymorphisms and anatomical and functional response to ranibizumab treatment in neovascular age-related macular degeneration",
        "implementation_urls": [],
        "doi": "10.1111/aos.13519",
        "abstract": "ABSTRACT.Purpose: We sought to determine if specific genetic single nucleotide polymorphisms(SNPs) influence vascular endothelial growth factor inhibition response to ranibizumab inneovascular age-related macular degeneration (AMD).Methods: A total of 403 Caucasian patients diagnosed with exudative AMD wereincluded. After a three-injection loading phase, a pro re nata regimen was followed. NineSNPs from six different genes (CFH, CFB, ARMS2, SERPINF1, VEGFR1, VEGF)were genotyped. Non-genetic risk factors (gender, smoking habit and hypertension) werealso assessed. Patients were classified as good or poor responders (GR or PR) accordingto functional (visual acuity), anatomical (foveal thickness measured by OCT) and fluidcriteria (fluid/no fluid measured by OCT).Results: Hypertension was the environmental factor with the strongest poor responseassociation with ranibizumab in the anatomical measure after the loading phase(p = 0.0004; OR 3.7; 95% CI, 2.4–5.8) and after 12 months of treatment (p = 10�5;OR 2.3; 95% CI, 1.5–3.4). The genetic variants rs12614 (CFB), rs699947 (VEGFA) andrs7993418 (VEGFR1) predisposed patients to a good response, while rs12603486 andrs1136287 (SERPINF1) were associated with a poor response. The protective genotype ofrs800292 variant (CFH) was also associated with a poor anatomical response (p 0.0048).Conclusion: All these data suggest that genetics play an important role in treatmentresponse in AMD patients.Key words: age-related macular degeneration – choroidal neovascularization – pharmacogeneticstudy – ranibizumab‡These authors contributed equally to this work.Acta Ophthalmol. 2018: 96: e201–e212ª 2017 Acta Ophthalmologica Scandinavica Foundation. Published by John Wiley & Sons Ltddoi: 10.1111/aos.13519IntroductionAge-related macular degeneration(AMD) is a chronic, progressive diseaseof the central retina. Macular changesrelated to ageing are typically classifiedinto one of four categories: normal age-related changes, early AMD, interme-diate AMD or late AMD (Ferris et al.2013). Most visual loss happens in lateAMD due to neovascular (i.e., wetAMD) or geographic atrophy (dryAMD; Lim et al. 2012). Age-relatedmacular degeneration (AMD) is theleading cause of low vision and legalblindness in adults in developed coun-tries, and its prevalence is expected toincrease in the next few decades. Whileno good treatments have yet been foundfor dry AMD, effective treatments—primarily the use of antivascularendothelial growth factor (anti-VEGF)inhibitors—are available for wet AMD",
        "publication_date": "2017-09-19",
        "authors": "Estefanía Cobos, Sergio Recalde, Jaouad Anter, María Hernández‐Sánchez, Carla Barreales, Leticia Olavarrieta, A. Valverde-Megías, Marta Suarez‐Figueroa, Fernando E.S. Cruz, Maximino Abraldes, Julián Pérez‐Pérez, Patricia Fernández‐Robredo, Luís Arias, Alfredo García‐Layana",
        "file_name": "10!1111%aos!13519.pdf",
        "file_path": "./PDFs/10!1111%aos!13519.pdf"
    },
    {
        "title": "Automating the Response to GDPR’s Right of Access",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/access-right-solid",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%faia220462.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A demonstration of the developed Solid application is available at https://protect.oeg.fi.upm.es/access-right/and the public repository of the code is accessible at https://github.com/besteves4/access-right-solid for further development."
                    }
                ]
            }
        ],
        "doi": "10.3233/faia220462",
        "abstract": "Abstract.With the enforcement of the European Union’s General Data Protec-tion Regulation, users of Web services – the ‘data subjects’ –, which arepowered by the intensive usage of personal data, have seen their rightsbe incremented, and the same can be said about the obligations imposedon the ‘data controllers’ responsible for these services. In particular, the‘Right of Access’, which gives users the option to obtain a copy of theirpersonal data as well as relevant details such as the categories of per-sonal data being processed or the purposes and duration of said pro-cessing, is putting increasing pressure on controllers as their executionoften requires a manual response effort, and the wait time is negativelyaffecting the data subjects. In this context, the main goal of this work isthe development of an API, which builds on the previously mentionedstructured information, to assist controllers in the automation of repliesto right of access requests. The implemented API method is then usedin the implementation of a Solid application whose main goal is to assistusers in exercising their right of access to data stored in Solid Pods.Keywords. digital rights management, GDPR, right of access, Solid1. IntroductionWith the enforcement of the European Union’s General Data Protection Reg-ulation (GDPR), users of Web services have seen their rights as GDPR ‘datasubjects’ being expanded when it comes to the processing of their personal data.On the other hand, on top of other GDPR-related obligations, ‘data controllers’,the entities that effectively process the data, have seen an increase in workloadrelated to the response to data subject’s right-related requests. GDPR’s ChapterIII2 details a set of 10 data subject rights, starting with the ‘Right to be Informed’described in Articles 13 and 14 and ending with the ‘Right to object to automateddecision making’ in Article 22. Considering this, data controllers would benefitfrom having the information they need to provide to data subjects in a structuredformat to automate the response to such requests [1]. In particular, the ‘Right of1Corresponding author: beatriz.gesteves@upm.es2https://gdpr-info.eu/chapter-3/Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220462170https://gdpr-info.eu/chapter-3/Access’3 is putting more and more pressure on controllers as they not only haveto provide the purpose for which the data is being used or the types of data beingprocessed but also need to provide a copy of said data. As this task is usuallydone manually, the wait time can negatively affect data subjects.In addition, with the emergence of decentralised data storage solutions, suchas Solid4, as an alternative to the traditional centralised data silos, new challengesappear as the data subject–data controller roles are still not adequately defined inthis decentralised contexts. In this context, the creation of Application Program-ming Interface (API) services would help with the automation of right-relatedrequests as, at their core, they are a ‘request–response’ type of software interface",
        "publication_date": "2022-12-05",
        "authors": "Beatriz Esteves, Victor Rodrı́guez-Doncel, Ricardo Longares",
        "file_name": "10!3233%faia220462.pdf",
        "file_path": "./PDFs/10!3233%faia220462.pdf"
    },
    {
        "title": "A novel deep learning approach using blurring image techniques for Bluetooth-based indoor localisation",
        "implementation_urls": [],
        "doi": "10.1016/j.inffus.2022.10.011",
        "publication_date": "2022-10-17",
        "authors": "Reewos Talla-Chumpitaz, Manuel Castillo‐Cara, Luis Orozco‐Barbosa, Raúl García‐Castro",
        "file_name": "10!1016%j!inffus!2022!10!011.pdf",
        "file_path": "./PDFs/10!1016%j!inffus!2022!10!011.pdf"
    },
    {
        "title": "Challenge-Based Learning in Computational Biology and Data Science.",
        "implementation_urls": [],
        "abstract": "Abstract. Data Science is an interdisciplinary field devoted to extractknowledge from large amounts of data. There is a great variety of pro-grams that address the teaching of this field with a growing demand ofprofessionals. However, data science pedagogy tends to emphasize gen-eral aspects of data and the use of tools instead of the its scientificdimension. This position paper describes an ongoing educational inno-vation project for the use of the Challenge-based Learning approach toteach and learn Data Science. In this approach, students work on solv-ing complex and real world problems while the learning is obtained byiterating through three main phases: engage, investigate, and act.Keywords: Challenge-based learning, active learning, experiential learn-ing, project based learning, data science, computational biology.1 IntroductionData science (DS) is an interdisciplinary field devoted to identify patterns and ex-tract knowledge by mining large amounts of structured and unstructured data.Among others, DS includes: machine learning, data processing, statistical re-search, and their related methods. This science has become a revolution thathas changed our manner of doing business, health, politics, education and in-novation [11]. Scientific breakthroughs will be increasingly assisted by advancedcomputing capabilities and DS methods that help researchers manipulate andexplore massive datasets [9].Challenge-based learning (CBL) is a new learning approach created by AppleInc. in collaboration with teachers and leaders in the education community. CBLis “an engaging, multidisciplinary approach that starts with standards-basedcontent and lets students leverage the technology they use in their daily livesto solve complex, real-world problems” [5]. In CBL, students work with otherstudents, their teachers, and experts in their communities and around the worldto develop deeper knowledge of the subjects they are studying.Data science is in a privileged position with respect to other branches ofknowledge to articulate learning through experiences and challenges [18]. The? ORCID ID: 0000-0001-7587-0703?? ORCID ID: 0000-0002-0792-4156Kaggle platform [2] periodically releases a series of competitions on real problemssuch as “Predicting a Biological Response” [4]; which offered 20,000$ to thebest predictive model that linked a biological response of molecules to theirchemical properties. These public competitions have the potential to involveactively the student in a real, significant, and related problematic situation;including a framework for the implementation of a solution to the challenge.This position paper presents an ongoing educational innovation project fo-cusing on using the CBL approach in a DS course, as part of a ComputationalBiology master degree at the Technical University of Madrid (UPM). Studentswill work on challenges at the level of a Kaggle competition with special pref-erence for active and multidisciplinary problems. Based on the 2016 update forthe CBL framework proposed by Apple Inc. [12], students will learn by followingthree main phases: engage, investigate, and act.The paper outline is as follows: after describing the background of the pre-sented innovation project in section 2, the project details are given in section3. These include the scope and students’ profile, the project goals, the time-line and educational resources, the evaluation, resulting products, and diffusionplan. Section 4 explains the expected contribution to the improvement of learn-",
        "publication_date": "2018-01-01",
        "authors": "Emilio Serrano, Martín Molina, Daniel Manrique, Javier Bajo",
        "file_name": "no_doi_20250624161328.pdf",
        "file_path": "./PDFs/no_doi_20250624161328.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2104/paper_236.pdf"
    },
    {
        "title": "Publishing Tourism Statistics as Linked Data a Case Study of Sri Lanka",
        "implementation_urls": [
            {
                "identifier": "https://github.com/tourism-data/tourism-data.github.io",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-03056-8_17.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "For the sake of reproducibility all the datasets and results are publicly available at Datahub.io2 and at the website3."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-03056-8_17",
        "abstract": "Abstract. Tourism is a crucial component of Sri Lanka’s economy. Intel-ligent business decisions by means of thorough analysis of relevant datacan help the Sri Lankan tourism industry to be competitive. To thisend, Sri Lanka Tourism Development Authority makes tourism statis-tics publicly available. However, they are published as PDF files limitingtheir reuse. In this paper, we present how to transform such data into5-star Linked Open Data by extracting the statistics as structured data;modelling them using the W3C RDF Data Cube vocabulary and trans-forming them to RDF using W3C R2RML mappings. Furthermore, wedemonstrate the benefits of such transformation using two real-world usecases.Keywords: Tourism, Open Data, Linked Data, RDF, RDF Data Cube,R2RML1 IntroductionThe tourism industry in Sri Lanka is a crucial component of the island’s econ-omy. In 2016, it was ranked as the third highest foreign exchange earner inthe country (14.20%) [1]. The Sri Lankan government is taking many initiativesto foster tourism industry in the country. One of these initiatives is to maketourism data available as open data so that stakeholders of the tourism indus-try can make intelligent business decisions backed by thorough analysis of data.Tourism statistics are in the center of such open data because statistical dataplay a key role in policy prediction, planning and formulating growth strate-gies. Furthermore, as most of these decision making processes are increasinglybacked by data-driven approaches and artificial intelligence algorithms, it is im-portant to make these data easily accessible, web-friendly, and provided withexplicit semantics, in a machine understandable manner with enough contextualinformation.The Linked Data principles1 help creating a global data space with typedlinks between data from different sources. Publishing data as Linked Data bringseveral advantages including (a) global identifiers for data that can be accessedusing the Web infrastructure and typed links between data from different sources;1 https://www.w3.org/DesignIssues/LinkedData.html2 Mihindukulasooriya et al.(b) the graph-based RDF data model allows consuming and merging data fromdifferent sources without having to do complex structural transformations; and(c) explicit semantics of data expressed in RDF Schema or OWL ontologies whichcan be aligned and mapped to data models of other applied using techniques suchas ontology matching. Thanks to the typed links to external sources, data canbe enriched and more context about the data can be discovered by traversingthe links.RDF Data Cube [2] is a standard vocabulary published by W3C based onStatistical Data and Metadata Exchange (SDMX) to publish statistical data ina standard manner. In addition to all benefits of Linked Data, RDF Data Cubeenables reuse of standarized tools and components such as visualization tools.Thanks to the semantics defined in RDF Data Cube, the structure of the datacan be defined in a manner that can be automatically discovered and be usedfor generating common visualizations such as line graphs, bar chats, pie charts,etc. R2RML [3] is a W3C recommendation for a mapping language from tabulardata to RDF. R2RML can be used to define the mapping between the tabulardata in the tourism statistics to RDF in a standard reusable manner.",
        "publication_date": "2018-01-01",
        "authors": "Nandana Mihindukulasooriya, Freddy Priyatna, Mariano Rico",
        "file_name": "10!1007%978-3-030-03056-8_17.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-03056-8_17.pdf"
    },
    {
        "title": "Graph-based root cause analysis for service-oriented and microservice architectures",
        "implementation_urls": [],
        "doi": "10.1016/j.jss.2019.110432",
        "abstract": "AbstractService-oriented architectures and microservices define two ways of designing software with the aim ofdividing an application into loosely-coupled services that communicate among each other. This translatesinto rapid development, where each service is developed and deployed by small teams, enabling continuousshipping of new features and fast-evolving applications. However, the underlying complexity of this typeof architecture can hinder observability and maintenance by the user. In particular, identifying the rootcause of an anomaly detected in the application can be a difficult and time-consuming task, consideringthe numerous services and connections to be examined. In this work, we present a root cause analysisframework, based on graph representations of these architectures. The graphs can be used to compareany anomalous situation that happens in the system with a library of anomalous graphs that serves as aknowledge base for the user troubleshooting those anomalies. We use the Grid’5000 testbed to deploy threedifferent architectures and inject a set of anomalies. The results show how our graph-based approach is19.41% more effective than a machine learning method that does not take into account the relationshipbetween elements.Keywords: SOA, Microservices, Root Cause Analysis, Containers, Graphs1. IntroductionAs many industries increasingly rely on information systems to operate efficiently, software developmentand architectures have evolved in different directions. Virtualisation has gained momentum in the last decadethanks to the ability of cloud providers to offer Infrastructure as a Service (IaaS) to their clients. This enablescustomers to use processing power on demand through Virtual Machines (VMs) and it eliminates the burdenof maintaining the infrastructure. It also allows the provider to optimize the utilization of the datacenterby means of VM migration and consolidation. The next step in virtualization has been containerisation. Acontainer is an executable unit of packaged software that includes all the dependencies needed. It runs onthe host operating system, sharing the kernel and using its own user space, isolating it from other containers.This eliminates any boot time overhead in comparison with VM’s, empowering the migration and scalingadvantages of virtualisation. A machine can also host dozens of containers at the same time1 thanks to theirlightweight nature. Finally, from a software development point of view, they eliminate the “it works on mymachine” problem, since the whole running environment is included inside the container.Industry has acknowledged all the containers benefits [1], specially thanks to the adoption of technolo-gies like Docker [2], with DevOps [3] and microservice architectures becoming growing trends and commonEmail addresses: abrandon@fi.upm.com (Álvaro Brandón), marc.solesimo@ca.com (Marc Solé),alberto.huelamosegura@ca.com (Alberto Huélamo), david.solans@ca.com (David Solans), mperez@fi.upm.com (Maŕıa S.Pérez), victor.muntes@ca.com (Victor Muntés-Mulero)1https://www.datadoghq.com/docker-adoption/ (last accessed Dec 2018)Preprint submitted to Elsevier October 7, 2019practices. The philosophy behind microservices follows the same principles used in service-oriented archi-tectures [4]. It consists of breaking the system logic into different, small units where each one has a singletask or responsibility. The different units or microservices communicate and cooperate with each other toprovide the system functionality as a whole. Any of the aforementioned units of logic is executed inside acontainer. In comparison with a monolithic architecture, it allows the user to scale specific components ofthe application by increasing the number of containers responsible for that part. Besides, it also enablesnew paradigms like serverless computing, where certain events trigger the allocation of new containers, ab-stracting the infrastructure needed by the user [5]. Finally, it also facilitates the development process andsoftware reuse [6].Coordinating and scaling these containers require an orchestrating unit, specially if we want to deploythem in a distributed infrastructure with several machines. Tools like Kubernetes2 or DC/OS3 have filledthis gap. These platforms have self-healing features, where unhealthy or faulty containers can be relaunched,or containers migrated to a different machine in case one of their hosts dies. But establishing the root causeof these failures can be really complex, specially when we have a network of different services that depend oneach other. The troubleshooting process normally involves a tedious search through logs across the different",
        "publication_date": "2019-10-15",
        "authors": "Álvaro Brandón, Marc Solé, Alberto Huélamo, David Noguéro, Marı́a S. Pérez, Víctor Muntés-Mulero",
        "file_name": "10!1016%j!jss!2019!110432.pdf",
        "file_path": "./PDFs/10!1016%j!jss!2019!110432.pdf"
    },
    {
        "title": "Keeping up with storage: Decentralized, write-enabled dynamic geo-replication",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2017.06.009",
        "publication_date": "2017-06-29",
        "authors": "Pierre Matri, Marı́a S. Pérez, Alexandru Costan, Luc Bougé, Gabriel Antoniu",
        "file_name": "10!1016%j!future!2017!06!009.pdf",
        "file_path": "./PDFs/10!1016%j!future!2017!06!009.pdf"
    },
    {
        "title": "Inferring Types on Large Datasets Applying Ontology Class Hierarchy Classifiers: The DBpedia Case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/anuzzolese/oke-challenge",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-03667-6_21.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/328606612 Inferring Types on Large Datasets Applying Ontology Class Hierarchy Classifiers: The DBpedia Case Conference Paper in Lecture Notes in Computer Science · November 2018 DOI: 10.1007/978-3-030-03667-6_21 CITATION 1 READS 270 4 authors, including: Mariano Rico Universidad Politécnica de Madrid 58 PUBLICATIONS 538 CITATIONS SEE PROFILE Idafen Santana-Perez Universidad Politécnica de Madrid 22 PUBLICATIONS 220 CITATIONS SEE PROFILE All content following this page was uploaded by Mariano Rico on 14 December 2018."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-03667-6_21",
        "abstract": "Abstract. Adding type information to resources belonging to large knowl-edge graphs is a challenging task, specially when considering those thatare generated collaboratively, such as DBpedia, which usually contain er-rors and noise produced during the transformation process from differentdata sources. It is important to assign the correct type(s) to resourcesin order to efficiently exploit the information provided by the dataset.In this work we explore how machine learning classification models canbe applied to solve this issue, relying on the information defined by theontology class hierarchy. We have applied our approaches to DBpediaand compared to the state of the art, using a per-level analysis. We alsodefine metrics to measure the quality of the results. Our results showthat this approach is able to assign 56% more new types with higherprecision and recall than the current DBpedia state of the art.Keywords: DBpedia, machine learning, type prediction, data quality, linkeddata, semantic web1 IntroductionDBpedia is one of the largest and most relevant knowledge bases available nowa-days, which contains structured information about millions of entities extractedfrom Wikipedia. These resources have different types, from different ontologies,including the DBpedia ontology, as well as from other well-known vocabular-ies such as FOAF or schema.org. Even when considering only classes from theDBpedia ontology, as we do in this work, each resource usually has more thanone type. For example, the resource Cervantes has types Agent, Person andArtist, because the DBpedia ontology defines a hierarchy of classes.According to our study, a large amount of DBpedia resources, around 16%,do not have any type. Having correct types is important when working with∗This work was partially funded by grant CAS18/00333 (Castillejo), and projectsRTC-2016-4952-7 (esTA) and TIN2016-78011-C4-4-R (Datos 4.0), from the SpanishState Investigation Agency of the MINECO and FEDER Funds.semantic information, as it allows for a better data queriability and discover-ability. Thus, the more types we have and the more precise they are, the betterdata quality. DBpedia, in its 3.9 (English) version, has more than four millionresources, but only around 50% of them have a type beyond level 1 (we considerThing as level 0). In this work we aim at improving the overall quality of theDBpedia dataset by (1) providing type(s) to those resources lacking of it, and(2) adding more specialized types to already typed resources. Following the pre-vious example about Cervantes, this would mean inferring that it also has typeWriter, a more specific one, although this fact is not in the dataset.We use machine learning methods to create a predictive model from thedataset and the class hierarchy of the ontology. In this paper we use the DBpediadataset and its ontology. This model is used to predict the types for resourceswith no type, and to predict new types for already typed resources. In this paperwe show the results of two different approaches and the comparison to relevantprevious approaches.Our study starts with a naive approach based on multi-class classifiers. Wecompare the results of this approach (approach 1) with the state of the art,achieving worse results in general. Our second approach considers the ontologyclass hierarchy and achieves better results than the state of the art. We alsoevaluate our approach with a well-known gold standard of curated types, andour best approach achieves higher results in terms of precision and recall.",
        "publication_date": "2018-01-01",
        "authors": "Mariano Rico, Idafen Santana-Pérez, Pedro del Pozo-Jiménez, Asunción Gómez‐Pérez",
        "file_name": "10!1007%978-3-030-03667-6_21.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-03667-6_21.pdf"
    },
    {
        "title": "A Tool Suite to Enable Web Designers, Web Application Developers and End-users to Handle Semantic Data",
        "implementation_urls": [],
        "doi": "10.4018/jswis.2010070103",
        "abstract": "’97 extended abstracts on human factors in computing systems(pp. 168–169). New York, NY, USA: ACM. (Los Angeles, USA.April 18-23)Pietriga, E. (2001). Isaviz: A visual authoring tool for rdf.see http://www.w3.org/2001/11/isaviz/. Available from http://www.w3.org/2001/11/IsaViz/Quan, D., & Karger, D. (2004). How to Make a Semantic WebBrowser. In International world wide web conference. proceed-ings of the 13th international conference on world wide web.session: Semantic interfaces and owl tools. (ISBN:1-58113-844-X)Rico, M., Camacho, D., & Corcho Óscar. (2008). VPOET: Usinga Distributed Collaborative Platform for Semantic Web Appli-cations. In C. Badica, G. Mangioni, V. Carchiolo, & D. Bur-descu (Eds.), Intelligent distributed computing, systems and ap-plications. proc. 2nd international symposium on intelligent dis-tributed computing (idc’2008) (pp. 167–176). Springer. (ISBN:978-3-540-85256-8)Rico, M., Camacho, D., & Corcho Óscar. (2009a). Macros vs.scripting in VPOET. In 5th Workshop on Scripting and De-velopment for the Semantic Web (SFSW2009) at the 6th AnnualEuropean Semantic Web Conference (ESWC). CEUR online pro-ceedings, Volume 449.Rico, M., Camacho, D., & Corcho Óscar. (2009b). VPOET Tem-plates to Handle the Presentation of Semantic Data Sources inWikis. In Fourth Workshop on Semantic Wikis: The SemanticWiki Web (SemWiki2009) at the 6th Annual European Seman-tic Web Conference (ESWC). CEUR online proceedings, Volume464 (pp. 186–190).Rico, M., Camacho, D., & Corcho Óscar. (2010). A Contribution-based Framework for the Creation of Semantically-enabled WebApplications. Journal of Information Sciences, 180(10), 1850–1864.Rochen, R., Rosson, M., & Pérez, M. (2006). End user Devel-opment of Web Applications. In H. Lieberman, F. Paternò, &V. Wulf (Eds.), (p. 161-182). Springer.Souzis, A. (2006). Bringing the wiki-way to the seman-tic web with rhizome. In M. Völkel & S. Schaffert (Eds.),Semwiki2006, proceedings of the first workshop on semanticwikis. CEUR-WS.org. Available from http://www.ceur-ws.org/Vol-206/paper19.pdf (Budva, Montenegro, June 12,2006)",
        "publication_date": "2010-01-01",
        "authors": "Mariano Rico, Óscar Corcho, José A. Macías, David Camacho",
        "file_name": "10!4018%jswis!2010070103.pdf",
        "file_path": "./PDFs/10!4018%jswis!2010070103.pdf"
    },
    {
        "title": "A Proposal for Semantic Integration of Crime Data in Mexico City",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-59872-3_3",
        "abstract": "Abstract. Crime is a common problem in big cities where the authorities regu-larly update data crime reports. In Mexico City, the crime reports are available asopen data. However, other relevant data are not connected to them (e.g., socioeco-nomic data). Therefore, the socioeconomic and geographic data can help under-stand how the crime is characterized and what social indicators are related to it.In this research, we explore how data crime reports are described and how theycan be associated in an Ontology with other data, such as socioeconomic and geo-graphic data. The goal is to discover the social indicators related to a particularcrime in a specific area by using SPARQL queries from a knowledge represen-tation. Then, data sets from crime reports, socioeconomic and geographic datafrom 2016 were integrated to explore crime behavior in Mexico City. The workuses a NeOn methodology in which resources from existing ontologies or non-ontological resources can be mixed. Next, a set of SPARQL queries is defined toextract the knowledge from ontology and discover the associations between crimein geographic and socioeconomic domains. The results showed a set of querieswhere it is possible to know where a crime occurred and what other factors areassociated with the crime and help to identify possible patterns among them.Keywords: Crime data ·Mixed data · Behavior crime data · Ontology1 IntroductionCrime occurrence is a phenomenon that affects the geographic area and its inhabitants.Impacting the status of the area, the living of the inhabitants, and how they make deci-sions. Today, data crime incidence is generated and published as open data in MexicoCity. Institutions and organizations use this information to treat and understand thephenomena from different perspectives.The traditional approach to generating and storing the crime reports consists ofschemas of data (files and databases), limiting the capability to retrieve the relevant infor-mation, identifying associations, or combining the information with other data sourcesto reveal trends and patterns. Thus, a knowledge representation model (ontology) can© Springer Nature Switzerland AG 2020M. F. Mata-Rivera et al. (Eds.): GIS LATAM 2020, CCIS 1276, pp. 30–48, 2020.https://doi.org/10.1007/978-3-030-59872-3_3http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-59872-3_3&domain=pdfhttps://doi.org/10.1007/978-3-030-59872-3_3A Proposal for Semantic Integration of Crime Data in Mexico City 31be built to represent the crime reports; it could improve the access and retrieve the rele-vant data to identify the associations and patterns. It generates that the original dataset’sinteroperability is increased.In this work, data from three domains are integrated socioeconomic, geographic, andcrime to reveal associations. Crime reports and socioeconomic data can be transformedinto knowledge, represented using the knowledge representationmodel [1]. The ontologyis the model used in this work, which is defined as a formal, explicit specification of ashared conceptualization [2]. It represents formal knowledge as a set of concepts of adomain using a vocabulary. This vocabulary is used to declare the types, properties, andrelations between the concepts.The dataset of crime reports from Mexico City is described as an ontology; it wasdeveloped based on scenario-based methodology. These scenarios emphasize the join,re-engineering, and reuse of ontological and non-ontological resources. Ontology is builtusing Protégé [available at https://protege.stanford.edu/], using the resources to definethe concepts of the dataset, the properties, and relations.The ontology can be used to develop an RDF file, where the dataset is described as",
        "publication_date": "2020-01-01",
        "authors": "Francisco Carrillo-Brenes, Luis M. Vilches‐Blázquez, Félix Mata",
        "file_name": "10!1007%978-3-030-59872-3_3.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-59872-3_3.pdf"
    },
    {
        "title": "An extension of Thing Descriptions from the Web of Things for Digital Twins",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/cogito_wrapper_module",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!12688%openreseurope!15280!1.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Software availability Source code available from: https://github.com/oeg-upm/cogito_wrapper_module/tree/main/example Archived source code at time of publication: https://doi.org/10.5281/zenodo.73043686 License: Creative Commons Attribution 4.0 International Page 8 of 15 Open Research Europe 2022, 2:144 Last updated: 13 NOV 2024 https://doi.org/10.5281/zenodo.7304368 https://github.com/oeg-upm/cogito_wrapper_module/tree/main/example https://doi.org/10.5281/zenodo.7304368 https://creativecommons.org/licenses/by/4.0/legalcode References 1."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.7304368",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.12688/openreseurope.15280.1",
        "abstract": "Abstract Background: Digital Twins are becoming ubiquitous in a range of domains, such as construction or e-health. Due to the numerous applications where Digital Twins can be adopted, they have to manage heterogeneous cross-domain data that is consumed by third-party service or domain experts. Digital Twins may allow for performing operational commands that are transformed into actions in the physical world; these operations can be grouped into processes. In the domain of construction, a European Project named COGITO was proposed, in which a Digital Twin is used to simulate the construction processes carried out in three railway stations in Europe (Spain, Denmark and Austria). The lack of interoperable data due to heterogeneity in the APIs of the physical layer, the complexity that entails managing these data at the digital model, and the difficulty of orchestrating the data exchanged among the advanced features have been the roadblocks in the project. Methods: The problems have been addressed by adopting the W3C Web of Things (WoT) standard, by using Thing Descriptions. Although the challenges are specific for the COGITO project, they could be found in any Digital Twin proposal. Thus, the article presents how they are solved and addressed in COGITO. Results: The results demonstrate that relying on a Knowledge Graph for the provenance, another for describing a distributed architecture of semantic interoperability services and a final one for data, allows to build a distributed Digital Twin over heterogeneous data sources that provide semantic interoperable data in each digital layer. Conclusions: The lack of a Digital Twin standard highlights the necessity of adopting existing standards and extending them in order to adapt their suitability to the Digital Twin context. Hence, the article presents how WoT is helpful for addressing the challenges, and how it could be extended in the future to support Digital Twins.Open Peer ReviewApproval Status    1 2 3version 121 Dec 2022 view view viewAlessandro Ricci , University of Bologna, Bologna, Italy1. Marco Picone , Universita degli Studi di Modena e Reggio Emilia, Modena, Italy2. Riku Ala-Laurinaho , Aalto University, Espoo, Finland3. Any reports and responses or comments on the article can be found at the end of the article.Open Research Europe Page 1 of 15",
        "publication_date": "2022-12-21",
        "authors": "Salvador González-Gerpe, Andrea Cimmino, Socorro Bernardos, Raúl García‐Castro, María Poveda‐Villalón, Kyriakos Katsigarakis, Georgios N. Lilis, Dimitrios Rovas",
        "file_name": "10!12688%openreseurope!15280!1.pdf",
        "file_path": "./PDFs/10!12688%openreseurope!15280!1.pdf"
    },
    {
        "title": "An Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus",
        "implementation_urls": [],
        "doi": "10.26342/2022-69-14",
        "publication_date": "2022-09-01",
        "authors": "Carlos Badenes-Olmedo, Alvaro Alonso, Oscar Corcho",
        "file_name": "10!26342%2022-69-14.pdf",
        "file_path": "./PDFs/10!26342%2022-69-14.pdf"
    },
    {
        "title": "Accepted Tutorials at The Web Conference 2022",
        "implementation_urls": [],
        "doi": "10.1145/3487553.3547182",
        "abstract": "AbstractLarge-scale applications are ever-increasingly geo-distributed. Maintaining the highest possible data locality is crucialto ensure high performance of such applications. Dynamic replication addresses this problem by dynamically creatingreplicas of frequently accessed data close to the clients. This data is often stored in decentralized storage systems suchas Dynamo or Voldemort, which offer support for mutable data. However, existing approaches to dynamic replicationfor such mutable data remain centralized, thus incompatible with these systems. In this paper we introduce a write-enabled dynamic replication scheme that leverages the decentralized architecture of such storage systems. We proposean algorithm enabling clients to locate tentatively the closest data replica without prior request to any metadata node.Large-scale experiments on various workloads show a read latency decrease of up to 42% compared to other state-of-the-art, caching-based solutions.Keywords: cloud, replication, geo-replication, storage, fault-tolerance, consistency, database, key-value store1. IntroductionLarge-scale applications such as social networks arebeing increasingly deployed over multiple, geograph-ically distributed datacenters (or sites). Such geo-distribution provides fast data access for end-usersworldwide while improving fault-tolerance, disaster-recovery and minimizing bandwidth costs. Today’scloud computing services [1, 2] allow a wider range ofapplications to benefit from these advantages as well.However, designing geo-distributed applications is dif-ficult due to the high and often unpredictable latencybetween sites [3].A key factor impacting application performance isdata locality, i.e. the location of the data relativelyto the application. Accessing remote data is orders ofmagnitude slower than using local data. Although suchremote accesses may be acceptable for rarely-accesseddata (cold data), they hinder application performanceEmail addresses: pmatri@fi.upm.es (Pierre Matri),mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),luc.bouge@ens.ens-rennes.fr (Luc Bougé),gabriel.antoniu@inria.fr (Gabriel Antoniu)for frequently-used data (hot data). For instance, in asocial network application, popular profiles should bereplicated at all sites whereas others can remain locatedat fewer locations. Finding the right balance betweenreplication and storage is critical: replicating too manyprofiles wastes memory, while failing to replicate popu-lar ones results in degraded application performance.Dynamic replication [4] proposes to solve this issueby dynamically replicating hot data as close as possibleto the applications that access it. This technique is lever-aged in Content Delivery Networks (CDN) to cache im-mutable data close to the final user [5, 6]. Similarily, itis used in storage systems such as GFS [7] or HDFS [8]to replicate mutable data, by relying on the centralizedmetadata management of these systems [9, 10]. Yet,such an approach contradicts the design principles of",
        "publication_date": "2022-04-25",
        "authors": "Riccardo Tommasini, Senjuti Basu Roy, Xuan Wang, Hongwei Wang, Heng Ji, Jiawei Han, Preslav Nakov, Giovanni Da San Martino, Firoj Alam, Markus Schedl, Elisabeth Lex, Akash Bharadwaj, Graham Cormode, Milan Dojchinovski, Jan Forberg, Johannes Frey, Pieter Bonte, Marco Balduini, Matteo Belcao, Emanuele Della Valle, Junliang Yu, Hongzhi Yin, Tong Chen, Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Jamell Dacon, Lingjuan Lye, Jiliang Tang, Aristides Gionis, Stefan Neumann, Bruno Ordozgoiti, Simon Razniewski, Hiba Arnaout, Shrestha Ghosh, Fabian M. Suchanek, Lingfei Wu, Yu Chen, Yunyao Li, Bang Liu, Filip Ilievski, Daniel Garijo, Hans Chalupsky, Pedro Szekely, Ilias Kanellos, Dimitris Sacharidis, Thanasis Vergoulis, Nurendra Choudhary, Nikhil Rao, Karthik Subbian, Srinivasan H. Sengamedu, Chandan K. Reddy, Friedhelm Victor, Bernhard Haslhofer, George Katsogiannis- Meimarakis, Georgia Koutrika, Shengmin Jin, Danai Koutra, Reza Zafarani, Yulia Tsvetkov, Vidhisha Balachandran, Sachin Kumar, Xiangyu Zhao, Bo Chen, Huifeng Guo, Yejing Wang, Ruiming Tang, Yang Zhang, Wenjie Wang, Peng Wu, Fuli Feng, Xiangnan He",
        "file_name": "10!1145%3487553!3547182.pdf",
        "file_path": "./PDFs/10!1145%3487553!3547182.pdf"
    },
    {
        "title": "Ontologies Supporting Research-Related Information Foraging Using Knowledge Graphs: Literature Survey and Holistic Model Mapping",
        "implementation_urls": [
            {
                "identifier": "https://github.com/nvbach91/iga-knerd",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-61244-3_6.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The full coverage table of 73 model concepts and 35 ontologies can be found in our research repository on GitHub1, where we also include the set of competency questions and all other relevant resources."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-61244-3_6",
        "abstract": "Abstract. We carried out a literature survey on ontologies dealing withthe scholarly and research domains, with a focus on modeling the knowl-edge graphs that would support information foraging by researcherswithin the different roles they fulfill during their career. We identified43 relevant ontologies, of which 35 were found sufficiently documentedto be reusable. At the same time, based on the analysis of extensive CVsand activity logs of two senior researchers, we formulated a structuredset of competency questions that could be answered through informationforaging on the web, and created a high-level conceptual model indicat-ing the data structures that would provide answers to these questionsvia a holistic knowledge graph. We then studied the retrieved ontologiesand mapped them on the entities and relationships from our conceptualmodel. We identified many overlaps between the ontologies, as well as afew missing features. Preliminary proposals for dealing with some of theoverlaps and gaps were formulated.Keywords: Scholarly ontology · Literature survey · Competencyquestions · Knowledge graph · Information foraging1 IntroductionOn a daily basis, researchers find themselves in situations where they need toacquire information from resources on the Web. The nature of such informationneeds differs based on the specific academic role of the researcher at the givenmoment, such as that of a paper writer, event organizer, scientific evaluator,advisor of other researchers, or project coordinator. Yet, many of these informa-tion needs revolve around a small set of generic entity types and their relation-ships on which information is sought, such as people, institutions, publications,scientific venues, projects, topics, problems, arguments, or research artifacts.c© Springer Nature Switzerland AG 2020C. M. Keet and M. Dumontier (Eds.): EKAW 2020, LNAI 12387, pp. 88–103, 2020.https://doi.org/10.1007/978-3-030-61244-3_6http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-61244-3_6&domain=pdfhttp://orcid.org/0000-0003-2709-3297http://orcid.org/0000-0002-2256-2982http://orcid.org/0000-0002-1212-0101http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-030-61244-3_6Ontologies Supporting Research-Related Information Foraging 89This common basis is relatively generic across research fields and makes itpossible to proceed from textual search to the exploitation of structureddatabases on the web. Further, the rise of RDF-based knowledge graphs (KGs)may help overcome the rigidity of traditional database schemas; informationfrom independent resources could nowadays be integrated and searched withless overhead. Yet, academic KGs spanning over many different entity types arestill scarce; most published RDF datasets are only restricted to a few of theseentity types, e.g., publications and their authors, paper citations, or projects andthe institutions involved. Researchers who look for research-related informationthus still have to either deal with multiple databases or delve into unstructuredtextual search. In any case, the process can be characterized as information forag-ing, the term having been originally coined in the narrower context of followingweb hyperlinks [6]. Namely, given the limited amount of time the researcherscan devote to the search (this not being the prime activity they are paid for),",
        "publication_date": "2020-01-01",
        "authors": "Viet Bach Nguyen, Vojtěch Svátek, Gollam Rabby, Óscar Corcho",
        "file_name": "10!1007%978-3-030-61244-3_6.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-61244-3_6.pdf"
    },
    {
        "title": "Polypharmacy and Drug–Drug Interactions in People Living With Human Immunodeficiency Virus in the Region of Madrid, Spain: A Population-Based Study",
        "implementation_urls": [],
        "doi": "10.1093/cid/ciz811",
        "publication_date": "2019-08-17",
        "authors": "Beatriz López Centeno, Carlos Badenes-Olmedo, Angel Mataix-San-Juan, Katie McAllister, José M. Bellón, Sara Gibbons, Pascual Balsalobre, Leire Pérez‐Latorre, Juana Benedı́, Catia Marzolini, Ainhoa Aranguren-Oyarzábal, Saye Khoo, María J Calvo-Alcántara, Juan Berenguer",
        "file_name": "10!1093%cid%ciz811.pdf",
        "file_path": "./PDFs/10!1093%cid%ciz811.pdf"
    },
    {
        "title": "Practical challenges of ODRL and potential courses of action",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3587628",
        "abstract": "ABSTRACT The Open Digital Rights Language (ODRL) is a standard widely adopted to express privacy policies. This article presents several challenges identifed in the context of the European project AURO-RAL in which ODRL is used to express privacy policies for Smart Communities and Rural Areas. The article presents that some chal-lenges should be addressed directly by the ODRL standardisation group to achieve the best course of action, although others ex-ists. For others, the authors have presented a potential solution, in particular, for considering dynamic values coming from external data sources into privacy policies. Finally, the last challenge is an open research question, since it revolves around the interoperability of privacy policies that belong to diferent systems and that are expressed with diferent privacy languages. KEYWORDS Privacy, Open Digital Rights Language (ODRL), RDF Materialisation ACM Reference Format: Andrea Cimmino, Juan Cano-Benito, and Raúl García-Castro. 2023. Prac-tical challenges of ODRL and potential courses of action. In Companion Proceedings of the ACM Web Conference 2023 (WWW ’23 Companion), April 30–May 04, 2023, Austin, TX, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3543873.3587628 1 INTRODUCTION The use of digital content has increased dramatically in recent years, leading to the need for a standard way to manage the rights and permissions associated with that content [14]. The Open Digital Rights Language (ODRL) provides a solution to this need by ofering a common language for expressing and managing digital rights and obligations. ODRL is a widely adopted standard promoted by the W3C [6, 7]. However, to the authors’ knowledge, the adoption of this standard, and the documents and articles related to it, focus on how these policies can be expressed or how existing restrictions of rights and permissions can or cannot be written using ODRL [2, 3]. For example, they consider whether the General Data Protection Regulation (GDPR) can be expressed using ODRL and not how such a policy is evaluated or verifed. As a result, how these policies must be evaluated by a software system is a topic uncovered by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. WWW ’23 Companion, April 30–May 04, 2023, Austin, TX, USA © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9419-2/23/04. . . $15.00 https://doi.org/10.1145/3543873.3587628 the standard. Furthermore, the standard does not specify how to ",
        "publication_date": "2023-04-28",
        "authors": "Andrea Cimmino, Juan Cano-Benito, Raúl García‐Castro",
        "file_name": "10!1145%3543873!3587628.pdf",
        "file_path": "./PDFs/10!1145%3543873!3587628.pdf"
    },
    {
        "title": "Typology-based semantic labeling of numeric tabular data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/ttla",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-200397.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of the experiment and the data are published [31] (also available on GitHub22)."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-200397",
        "abstract": "Abstract.A lot of tabular data are being published on the Web. Semantic labeling of such data may help in their understanding andexploitation. However, many challenges need to be addressed to do this automatically. With numbers, it can be even harderdue to the possible difference in measurement accuracy, rounding errors, and even the frequency of their appearance. Multipleapproaches have been proposed in the literature to tackle the problem of semantic labeling of numeric values in existing tabulardatasets. However, they also suffer from several shortcomings: closely coupled with entity-linking, rely on table context, needto profile the knowledge graph, and require manual training of the model. Above all, however, they all treat different types ofnumeric values evenly. In this paper, we tackle these problems and validate our hypothesis: whether taking into account thetypology of numeric data in semantic labeling yields better results.Keywords: Semantic Labeling, Semantic Annotation, Levels of Measurements, Typology of Numbers, Fuzzy Clustering,Semantic Web1. IntroductionThe number of structured data published on theweb is constantly growing thanks to initiatives suchas the Open Data movement (e.g., data.gov1) andmore specialized platforms (e.g., Kaggle2, Figshare3,data.world4). One of the most fundamental problemsis enabling the understanding of the dataset content,which could be beneficial for different groups, suchas researchers working on the expansion of Knowl-edge Bases5 (KB) [1], for data scientists who found a1https://www.data.gov/2https://www.kaggle.com/3https://figshare.com/4https://data.world/5In this work, we use the terms “Knowledge Base” and “Knowl-edge Graph” interchangeably.dataset and want to use it for their knowledge discov-ery task [2], or for expanding meta-data describing adataset which could be utilized further for data search(e.g., Google Dataset Search Engine [3]).There are several approaches focusing on the prob-lem of understanding the semantic meaning of the con-tent of datasets [4–8]. They focus on detecting seman-tic labels for specific dataset cells (such as [6]) or as-signing semantic labels to the whole column (such as[4, 5]). However, the majority of the existing efforts fo-cused mainly on textual columns with not much atten-tion being drawn to numerical columns present in thedataset. Recently, the research focusing on the problemof assigning semantic labels to numerical columns in adataset started to get traction. In the work of Mitlöhn-ert et al. [9], it was pointed out that numerical columns1570-0844/0-1900/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:aalobaid@fi.upm.esmailto:ocorcho@fi.upm.esmailto:e.kacprzak@soton.ac.uk2 A. Alobaid et al. / Typology-based Semantic Labeling1 12 2",
        "publication_date": "2020-09-29",
        "authors": "Ahmad Alobaid, Emilia Kacprzak, Óscar Corcho",
        "file_name": "10!3233%sw-200397.pdf",
        "file_path": "./PDFs/10!3233%sw-200397.pdf"
    },
    {
        "title": "Benchmarking the efficiency of RDF-based access for blockchain environments",
        "implementation_urls": [],
        "doi": "10.18293/SEKE2020-104",
        "publication_date": "2020-06-01",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raul Garcia-Castro",
        "file_name": "10!18293%SEKE2020-104.pdf",
        "file_path": "./PDFs/10!18293%SEKE2020-104.pdf"
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activi-ties when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even leadto unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor tech-niques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extend-ing those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluationson both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document similarity, information search and retrieval, clustering, topic models, hashing1. IntroductionHuge amounts of documents are publicly avail-able on the Web offering the possibility of extractingknowledge from them (e.g. scientific papers in digitaljournals). Document similarity comparisons in manyinformation retrieval (IR) and natural language pro-cessing (NLP) areas are too costly to be performed insuch huge collections of data and require more effi-*Corresponding author. E-mail: cbadenes@fi.upm.es.cient approaches than having to calculate all pairwisesimilarities.In this paper we address the problem of program-matically generating annotations for each of the itemsinside big collections of textual documents, in away that is computationally affordable and enables asemantic-aware exploration of the knowledge inside itthat state-of-the-art methods relying on topic modelsare not able to materialize.Most text mining algorithms represent documents ina common feature space that abstracts the specific se-This article is published online with Open Access and distributed under the terms of the Creative Commons Attribution License (CC BY 4.0).1570-0844/20/$35.00 © 2020 – IOS Press and the authors.mailto:cbadenes@fi.upm.eshttps://orcid.org/0000-0002-2753-9917mailto:jluisred@amazon.comhttps://orcid.org/0000-0002-7413-447Xmailto:ocorcho@fi.upm.eshttps://orcid.org/0000-0002-9260-0753mailto:cbadenes@fi.upm.eshttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-200373&domain=pdf&date_stamp=2020-04-30736 C. Badenes-Olmedo et al. / Topic-based hashing algorithmquence of words used in each document and, with ap-propriate representations, facilitate the analysis of re-lationships between documents even when written us-ing different vocabularies. Although a sparse word orn-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. Latent",
        "publication_date": "2020-05-01",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": "./PDFs/10!3233%sw-200373.pdf"
    },
    {
        "title": "Extension of the BiDO Ontology to Represent Scientific Production",
        "implementation_urls": [],
        "doi": "10.1145/3318396.3318422",
        "abstract": "ABSTRACT The SPAR Ontology Network is a suite of complementary ontology modules to describe the scholarly publishing domain. BiDO Standard Bibliometric Measures is part of its set of ontologies. It allows describing of numerical and categorical bibliometric data such as h-index, author citation count, journal impact factor. These measures may be used to evaluate scientific production of researchers. However, they are not enough. In a previous study, we determined the lack of some terms to provide a more complete representation of scientific production. Hence, we have built an extension using the NeOn Methodology to restructure the BiDO ontology. With this extension, it is possible to represent and measure the number of documents from research, the number of citations from a paper and the number of publications in high impact journals according to its area and discipline.  CCS Concepts • Computing methodologies➝Ontology engineering. Keywords BiDO; RDF; Ontology; Scientific Production; Scholarly Publishing; SPAR Ontology Network; SPARQL. 1.  INTRODUCTION One of the criteria used for the evaluation of Higher Education Institutions is the quality and quantity of research outputs, mostly focused on expressions such as articles, book chapters, conference papers, among others. These contributions can be stored in institutional repositories (DSpace, ePrints) and different databases such as Scopus, Web of Science, Google Scholar, Crossref. There are existing works in the state of art focused on homogenizing the representation of bibliographic resources [1] [2] [3] [4] [5].  The SPAR Ontology Network [6] is a complete project to describe the scholarly publishing domain. It is composed of some orthogonal and complementary ontology modules. These ontologies (listed in parentheses) allow describing bibliographic resources and their parts (FaBiO, FRBL-DL, DoCO, DEO, DataCite), describing citations of scholarly resources (CiTO, BiRO, C4O, FOCO, C2W), describing the publishing workflow (PRO, PSO, PWO, SCoRO, FRAPO, FR), and describing metrics and statistics for bibliographic resources (BiDO, FiveStars). They are developed in OWL 2 DL1 for the creation of comprehensive machine-readable RDF metadata for every aspect of semantic publishing and referencing, and it is well documented in www.sparontologies.net. In a previous study [7] we evaluated the capability of the SPAR Ontology Network to cover the publishing domain and some measures that represent scientific production, such as author h-index, author citations count, and journal impact factor. However, other measures such as author number of documents, paper citations count, journal h-index, rank and quartile in the area of study cannot be represented with them. Hence, the objective of ",
        "publication_date": "2019-03-02",
        "authors": "Mariela Tapia-León, Idafen Santana-Pérez, María Poveda‐Villalón, Paola Espinoza-Arias, Janneth Chicaiza, Óscar Corcho",
        "file_name": "10!1145%3318396!3318422.pdf",
        "file_path": "./PDFs/10!1145%3318396!3318422.pdf"
    },
    {
        "title": "Requirements Behaviour Analysis for Ontology Testing",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_8",
        "abstract": "Abstract. In the software engineering field, every software productis delivered with its pertinent associated tests which verify its correctbehaviour. Besides, there are several approaches which, integrated in thesoftware development process, deal with software testing, such as unittesting or behaviour-driven development. However, in the ontology engi-neering field there is a lack of clearly defined testing processes that can beintegrated into the ontology development process. In this paper we pro-pose a testing framework composed by a set of activities (i.e., test design,implementation and execution), with the goal of checking whether therequirements identified are satisfied by the formalization and analysis oftheir expected behaviour. This testing framework can be used in differ-ent types of ontology development life-cycles, or concerning other goalssuch as conformance testing between ontologies. In addition to this, wepropose an RDF vocabulary to store, publish and reuse these test casesand their results, in order to allow traceability between the ontology, thetest cases and their requirements. We validate our approach by integrat-ing the testing framework into an ontology engineering process where anontology network has been developed following agile principles.Keywords: Ontology testing · Ontology requirementsOntology development1 IntroductionThe increasing uptake of semantic technologies and ontologies has led duringthe past years to the study of new ontology development methodologies, fromagile (e.g., [1,12]) to collaborative approaches (e.g., [15,17]). The majority ofthese methodologies take into account the importance of functional1 ontologyThis work is partially supported by the H2020 project VICINITY: Open virtual neigh-bourhood network to connect intelligent buildings and smart objects (H2020-688467)and by a Predoctoral grant from the I+D+i program of the Universidad Politécnicade Madrid.1 This term is borrowed from the Software Engineering field, in which functionalrequirements refer to the functionalities the software system should have.c© Springer Nature Switzerland AG 2018C. Faron Zucker et al. (Eds.): EKAW 2018, LNAI 11313, pp. 114–130, 2018.https://doi.org/10.1007/978-3-030-03667-6_8http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-03667-6_8&domain=pdfhttps://doi.org/10.1007/978-3-030-03667-6_8Requirements Behaviour Analysis for Ontology Testing 115requirements [16] which, written in natural language as competency questions[8], define the knowledge the ontology has to represent.Nowadays, in software engineering it is inconceivable to deliver a softwareproduct without its pertinent tests which guarantee that it fulfils all its require-ments. Besides, there are several approaches integrated into the software devel-opment process whose aim is to test the software. Unit testing [9], which vali-dates that each unit of the software performs as designed, and behaviour-drivendevelopment [19], which focuses on the behaviour the software product is imple-menting, are examples of these approaches.However, in ontology engineering there is a lack of clearly defined testing pro-cesses in order to be able to ascertain whether an ontology satisfies the require-ments. Even though there are approaches to generate tests (e.g., [10,13]), they donot cover the entire testing workflow or are limited to checking for the presence",
        "publication_date": "2018-01-01",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "10!1007%978-3-030-03667-6_8.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-03667-6_8.pdf"
    },
    {
        "title": "WoT Store: Enabling Things and Applications Discovery for the W3C Web of Things",
        "implementation_urls": [],
        "doi": "10.1109/ccnc.2019.8651786",
        "abstract": "Abstract—The Web of Things (WoT) architecture recentlyproposed by the W3C working group constitutes a promisingapproach to handle interoperability issues among heterogeneousdevices and platforms, by semantically describing interfaces andinteraction patterns among the Things. One of the main advan-tage of the W3C architecture is the possibility to decouple thedescription of the Things’ behavior from their implementationand communication strategies, hence greatly simplifying the de-ployment of novel applications and services on top of it. Startingfrom such state-of-art, and envisioning a Web of seamlesslyinteracting W3C Things, this paper focuses on the next steps,i.e.: how to effectively support the discovery of Things? and:how to ease the distribution of applications running on Things?We answer to both the questions above through the proposalof the WOT STORE, a novel software platform supporting thedistribution, discovery and installation of applications for theW3C WoT. The WOT STORE allows users to perform semanticdiscovery of the available Things, to search for compatibleapplications available on the market, and to install them over thetarget devices, all within the same framework. We describe theplatform architecture and its proof-of-concept implementation,providing two alternative interfaces to interact with our tool: aWeb portal, and new modules developed for the popular Node-RED platform. Finally, we discuss two realistic use-cases of theWOT STORE for industrial IoT and home automation systems,remarking the advantages of our solution in terms of deploymentcosts and interoperability support.I. INTRODUCTIONToday’s Internet of Things (IoT) is a vibrant yet chaoticworld characterized by the exponential growth of investmentsworldwide, as well as by an apparently endless spiral of noveltechnologies, hardware/software solutions and application do-mains. Guaranteeing the interoperability among heterogeneousdevices is becoming a key issue for current Industrial IoT(IIOT) systems, as well as an open field for novel scenariosand hence for novel business opportunities. It is easy to believethat the presence of machines using different communicationtechnologies, programming languages and data format cansignificantly increase the complexity and cost of existing IIoTdeployment and integration [1]. At the same time, the op-portunity to create fully connected eco-systems composed ofenergy management, home automation and security platformsis estimated to produce extra revenues for more than ten billiondollars, just for the smart home industry [2].Research on interoperability solutions for IoT systems hasfollowed two main approaches. On the one hand, novelparadigms like the Web of Things (WoTs) have investigatedhow to envision interoperability at the application layer, byre-using well-accepted standards and technologies adoptedin the World Wide Web (WWW); however, this approach",
        "publication_date": "2019-01-01",
        "authors": "Luca Sciullo, Cristiano Aguzzi, Marco Di Felice, Tullio Salmon Cinotti",
        "file_name": "10!1109%ccnc!2019!8651786.pdf",
        "file_path": "./PDFs/10!1109%ccnc!2019!8651786.pdf"
    },
    {
        "title": "Are distributed ledger technologies ready for intelligent transportation systems?",
        "implementation_urls": [],
        "doi": "10.1145/3410699.3413789",
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activi-ties when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even leadto unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor tech-niques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extend-ing those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluationson both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document similarity, information search and retrieval, clustering, topic models, hashing1. IntroductionHuge amounts of documents are publicly avail-able on the Web offering the possibility of extractingknowledge from them (e.g. scientific papers in digitaljournals). Document similarity comparisons in manyinformation retrieval (IR) and natural language pro-cessing (NLP) areas are too costly to be performed insuch huge collections of data and require more effi-*Corresponding author. E-mail: cbadenes@fi.upm.es.cient approaches than having to calculate all pairwisesimilarities.In this paper we address the problem of program-matically generating annotations for each of the itemsinside big collections of textual documents, in away that is computationally affordable and enables asemantic-aware exploration of the knowledge inside itthat state-of-the-art methods relying on topic modelsare not able to materialize.Most text mining algorithms represent documents ina common feature space that abstracts the specific se-This article is published online with Open Access and distributed under the terms of the Creative Commons Attribution License (CC BY 4.0).1570-0844/20/$35.00 © 2020 – IOS Press and the authors.mailto:cbadenes@fi.upm.eshttps://orcid.org/0000-0002-2753-9917mailto:jluisred@amazon.comhttps://orcid.org/0000-0002-7413-447Xmailto:ocorcho@fi.upm.eshttps://orcid.org/0000-0002-9260-0753mailto:cbadenes@fi.upm.eshttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-200373&domain=pdf&date_stamp=2020-04-30736 C. Badenes-Olmedo et al. / Topic-based hashing algorithmquence of words used in each document and, with ap-propriate representations, facilitate the analysis of re-lationships between documents even when written us-ing different vocabularies. Although a sparse word orn-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. Latent",
        "publication_date": "2020-09-04",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1145%3410699!3413789.pdf",
        "file_path": "./PDFs/10!1145%3410699!3413789.pdf"
    },
    {
        "title": "An Agent-Based Model for Exploring Pension Law and Social Security Policies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/emilioserra/SugarscapePensions",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-58790-1_4.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A Web version [21] of the model implementation as well as its source code, documentation, and extended experiments are available online to allow the inter-ested researcher to use, reproduce and extend the contributions presented here [20]."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-58790-1_4",
        "abstract": "Abstract. The increase in life expectancy and the decrease in birthrates pose a structural challenge for the pension systems of developedcountries such as Japan and Spain. Pension law and social security sys-tem of these countries is complex. Moreover, describing or predicting theeffects of changes in these laws is even more challenging. We contributewith an agent-based model (ABM) for computer-aided law educationin this field. This model is a simplified representation of the complexreality of pension systems, to the point that the reality is more under-standable and analytically manageable. The proposed model extendsthe wealth distribution scenario in the Sugarscape model, which is con-sidered the first social simulation where the notion of modeling peoplewas extended to consider entire cities. The proposed ABM encouragesthe exploration about different theories for the sustainability of pensionsystems through experimentation in a simple and controllable scenario.Experimental results indicate that a constant or increasing populationof uniformly distributed ages is not enough to ensure the sustainabilityof pension systems as backbone of the welfare state. A Web version ofmodel implementation as well as its source code, documentation, andextended experiments are available online.Keywords: Pension law · Social security · Agent-based modeling ·Agent-based social simulation · Multi-agent-based simulation1 IntroductionAgent-based modeling (ABM) is a computational modeling paradigm based ondescribing agents’ behaviors. An agent is an autonomous computational indi-vidual or object with particular properties and actions. ABM can be used tomodel and describe a wide variety of processes, phenomena, and situations, butespecially complex systems [25]. Examples of complex systems are ecosystems,economies, immune systems, molecular systems, minds, stock market, and demo-cratic government. ABM allows studying the global patterns that emerge incomplex system from agents’ local interactions and decisions.c© Springer Nature Switzerland AG 2020M. Sakamoto et al. (Eds.): JSAI-isAI 2019, LNAI 12331, pp. 50–63, 2020.https://doi.org/10.1007/978-3-030-58790-1_4http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-58790-1_4&domain=pdfhttp://orcid.org/0000-0001-7587-0703https://doi.org/10.1007/978-3-030-58790-1_4An ABM for Exploring Pension Law and Social Security Policies 51The public pension system is the backbone of the welfare state of a countryand the intergenerational solidarity. The increase in life expectancy and thedecrease in birth rates pose a structural challenge for the pension systems ofdeveloped countries. Japan was the world’s most aged population in 2017 (33%aged 60 or over) [5]. Europe is expected to account for five of the ten mostaged countries or areas in 2050. In the case of Spain, the problem is aggravatedbecause the low average salary compared to Europe and the high unemploymentrates of recent years.We present an extension of the Sugarscape model [11] inspired by Spanishand Japanese laws for the pension system. This new model offers a simplifieddescription of the complicated pension law and can serve as a computer-aidedlaw education system. An online version of the model [21], which can be runin any Web browser, encapsulates this knowledge in an easily transferable way.",
        "publication_date": "2020-01-01",
        "authors": "Emilio Serrano, Ken Satoh",
        "file_name": "10!1007%978-3-030-58790-1_4.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-58790-1_4.pdf"
    },
    {
        "title": "Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies",
        "implementation_urls": [],
        "doi": "10.1145/3360901.3364444",
        "arxiv": "2101.03026",
        "abstract": "ABSTRACTWith the ongoing growth in number of digital articles in a wider setof languages and the expanding use of different languages, we needannotation methods that enable browsing multi-lingual corpora.Multilingual probabilistic topic models have recently emerged as agroup of semi-supervised machine learning models that can be usedto perform thematic explorations on collections of texts in multiplelanguages. However, these approaches require theme-aligned train-ing data to create a language-independent space. This constraintlimits the amount of scenarios that this technique can offer solu-tions to train and makes it difficult to scale up to situations wherea huge collection of multi-lingual documents are required duringthe training phase. This paper presents an unsupervised documentsimilarity algorithm that does not require parallel or comparablecorpora, or any other type of translation resource. The algorithmannotates topics automatically created from documents in a sin-gle language with cross-lingual labels and describes documents byhierarchies of multi-lingual concepts from independently-trainedmodels. Experiments performed on the English, Spanish and Frencheditions of JCR-Acquis corpora reveal promising results on classi-fying and sorting documents by similar content.CCS CONCEPTS• Information systems → Digital libraries and archives; In-formation retrieval.KEYWORDScross-lingual semantic similarity; large-scale text analysis; topicmodels1 INTRODUCTIONCross-language information extraction deals with the retrieval ofdocuments written in languages different from the language of theuser’s query. At execution time, the query in the source language istypically translated into the target language of the documents withthe help of a dictionary or a machine-translation system. But formany languages we may not have access to translation dictionariesor a full translation system, or they can be expensive to apply in anonline search system. In such situations it is useful to rely on smallerannotation units derived from the text so the full content doesn’tneed to be translated, for instance by finding correspondences withregard to the topics discussed. In this case, it may be advisable toautomatically learn cross-lingual topics to browse multi-lingualdocument collections.Multi-lingual topic models discover language-specific descrip-tions of each topic from documents in multi-lingual corpora. Theyare mainly based on Latent Dirichlet Allocation (LDA) [4], addingsupervised associations between languages by using parallel corpus,with sentence-aligned documents (e.g. Europarl1 corpora), or com-parable corpus, with theme-aligned documents (e.g. Wikipedia2articles), in multiple languages. These requirements restrict thekind of corpora that can be used for training since large parallelcorpora are rare in most of the use cases, especially for languages",
        "publication_date": "2019-09-23",
        "authors": "Carlos Badenes-Olmedo, Jose-Luis Redondo García, Óscar Corcho",
        "file_name": "10!1145%3360901!3364444.pdf",
        "file_path": "./PDFs/10!1145%3360901!3364444.pdf"
    },
    {
        "title": "On The Modeling Of P2P Systems as Temporal Networks: a Case Study With Data Streaming",
        "implementation_urls": [],
        "doi": "10.23919/annsim55834.2022.9859513",
        "abstract": "AbstractTemporal networks are a useful tool to model complex systems’ dynamics, especially when they are charac-terized by high dynamicity. While there is strong literature on simulation tools for complex and dynamicalsystems, there is a lack of viable solutions to model and exploit temporal graphs in simulation. In this work,we present a system devised to simulate complex systems and their evolution by using temporal graphs datastructures. As a use case, we focus on data dissemination over peer-to-peer systems characterized by a rele-vant presence of churns. The simulation of dissemination algorithms on temporal graphs involves evaluatingtheir efficiency in terms of coverage, delay, and number of messages sent. In particular, a reasonable trade-off between the speed of delivery and the generated network traffic must be found. In this work, besidestraditional gossip strategies, a more complex scenario is considered, where a second overlay, structured asa tree, is built for more efficient propagation of data from the source to the interested peers only. Thisscenario is analyzed using multiple simulated strategies. More specifically, we investigate how the simu-lation methodology can be used for evaluating the efficiency of dissemination protocols in a peer-to-peerenvironment but with a specific focus on the modelling of churn.Keywords: simulation, temporal networks, peer-to-peer, dissemination, gossip.1 INTRODUCTIONComplex systems are quite often modeled and simulated as a set of entities that somehow interact in anenvironment. Interactions are made possible among those entities that share some specific properties (e.g.,they are at a short distance, given whatever definition of distance). The possibility of interaction betweentwo entities is often represented as a link between the two entities, thus creating an interaction network. Thelogical connections among the entities involved, depending on the nature of the system, might be subject tochanges over time. That means either that (i) new entities enter the system at some point while other entitiesmay disappear, or that (ii) the links between entities are not stable and can change over time. Thus, uponthe need to model and study the dynamics of such complex systems, modelling and simulation techniquesneed to consider the temporariness of the links, which can actually be relevant for the outcome of the tests.66Authorized licensed use limited to: Univ Politecnica de Madrid. Downloaded on June 25,2025 at 17:10:52 UTC from IEEE Xplore.  Restrictions apply. Serena, Zichichi, D’Angelo, and FerrettiSpecifically, in our paper, we employ temporal networks in order to simulate a peer-to-peer (P2P) streamingscenario, where multiple nodes (i.e. downloaders) listen to a stream of data emitted from a specific peer(i.e. the streamer node). In a P2P scenario, the nodes form an overlay network (i.e. a logical network builton top of another network, like the Internet), and the structuring of the overlay changes over time, due to newparticipants joining the system and other nodes leaving the network. The purpose of this paper is to proposea methodology and a simulation tool that enables the analysis and the comparison of different strategies forstreaming-like applications. Modelling and simulation techniques permit investigating the impact of certainstrategies and parameters on the performance of the application, analyzing also how the dynamic topologyof the overlay affects the functioning of the system. For example, the delay experienced for messages deliv-ery and the amount of network traffic are big concerning issues for P2P systems, and it is desirable to findan appropriate trade-off, finding solutions that enable the reduction of the number of messages sent whilenot increasing too considerably the average delay. Due to the volatility of the participants, we think thattemporal graphs are an appropriate data structure for the modelling of this kind of system.Through the use of LUNES-Temporal, a simulator specifically designed for distributed systems, we wereable to model the temporal graphs and the P2P protocols, focusing on the overlay aspects of the system.Specifically, the most relevant aspects considered in the modelling of the system are: (i) the usage of atemporal graph for the adequate representation of churns, (ii) different weights in terms of messages sizefor a precise estimation of the overall network traffic, (iii) information about nodes obtained only throughthe exchange of messages, (iv) hops as the measure for expressing the delay, according to a time-steppedsimulation approach, where a time-step represents an atomic unit of time (v) possibility of nimbly changingnetwork and protocols parameters in order to evaluate their impact.In this work, we want to display some techniques for modelling dynamics systems through the use of tem-",
        "publication_date": "2022-07-18",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!23919%annsim55834.2022.9859513.pdf",
        "file_path": "./PDFs/10!23919%annsim55834.2022.9859513.pdf"
    },
    {
        "title": "Why are ontologies not reused across the same domain?",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2018.12.010",
        "abstract": "to DOLCE, one of the main characteristics of an abstract entity is that it has neither spatial nor temporal qualities. Examples of such abstract entities are mathematical ones: a triangle, a cir­cumference, etc. Nevertheless, SUMO defines social role as a sub class of abstract. Let us note that a social role has both spatial and temporal qualities, since it may take place in a particular area along a particular time interval. Let us now suppose that ontology O1 imports SUMO and that O2 imports DOLCE. Let us also suppose that O1 imports O2. In this case, O1 includes contradictory definitions of the term abstract. Given that all the details of the natural language definitions are not formalized in the ontologies, it may be case that no inconsistency appears when a computer reasons with this ontology. However, there will be a contradiction in the intended meaning of the terms of O1. Something similar may happen if a redefinition of classes/prop­erties in the reusing ontology [19,20] takes place. Let us suppose, for example, the following axiom: reused : property rdf : subPropertyOf reusing : property http://www.sparontologies.net/ontologies/scoro/source.htmlhttp://data.semanticweb.org/ns/swc/swc_2009-05-09.htmlhttp://linkedscience.org/teach/ns/http://www.ontoware.org/index.htmlhttp://www.ontologydesignpatterns.org/ont/dul/DUL.owlhttp://www.ontologydesignpatterns.org/ont/d0.owlhttps://w3id.org/def/dul-dolce-zero-en-espannolIf reusing:property were declared as inverse functional, then reused:property would become inverse functional as well. If soft reuse had been carried out, then, formally, the only definition would be the one of the reusing ontology. However, if a third ontology imported both the reused and the reusing ones, there would be, in the third ontology, aformal definition from the reused ontology and another formal definition in the reusing ontology. Concerning the information that was obtained through the OTN use case, several heterogeneity problems were detected. First of all, there are ontologies, in the academic domain, focused on different aspects of such domain but out of the scope of OTN ontology, for example content management, social networking, etc.. Some ontologies represent the same domain as OTN, but from a different perspective. Others do not use Spanish, being the lack of localiza­tion [21]a key factor that hampers reuse. In fact, it has been proved that just 27.04% of the LOV ontologies use languages other than English. Other reasonstonot reuse LOV ontologies belonging tothe Academy category have been deficiencies in the documentation of some candidate ontologies and unreachable imported ontologies. 5. Related work One of the first analysis on ontology reuse was done by Sim-perl [22]. The author presents different use cases on ontology reuse as well as methods and tools. The main conclusion is that the reuse approach is a decision-making problem in which the developer ",
        "publication_date": "2018-12-27",
        "authors": "Mariano Fernández‐López, María Poveda‐Villalón, Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez",
        "file_name": "10!1016%j!websem!2018!12!010.pdf",
        "file_path": "./PDFs/10!1016%j!websem!2018!12!010.pdf"
    },
    {
        "title": "Towards metrics-driven ontology engineering",
        "implementation_urls": [],
        "doi": "10.1007/s10115-021-01545-9",
        "abstract": "AbstractThe software engineering field is continuously making an effort to improve the effectivenessof the software development process. This improvement is performed by developing quan-titative measures that can be used to enhance the quality of software products and to moreaccurately describe, better understand and manage the software development life cycle. Evenif the ontology engineering field is constantly adopting practices from software engineering,it has not yet reached a state in which metrics are an integral part of ontology engineeringprocesses and support making evidence-based decisions over the process and its outputs. Upto now, ontology metrics are mainly focused on the ontology implementation and do not takeinto account the development process or other artefacts that can help assessing the quality ofthe ontology, e.g. its requirements. This work envisions the need for a metrics-driven ontol-ogy engineering process and, as a first step, presents a set of metrics for ontology engineeringwhich are obtained from artefacts generated during the ontology development process andfrom the process itself. The approach is validated by measuring the ontology engineeringprocess carried out in a research project and by showing how the proposed metrics can beused to improve the efficiency of the process by making predictions, such as the effort neededto implement an ontology, or assessments, such as the coverage of the ontology according toits requirements.Keywords Metrics · Ontology engineering · Requirements · Ontology developmentB Alba Fernández-Izquierdoalbafernandez@fi.upm.esMaría Poveda-Villalónmpoveda@fi.upm.esAsunción Gómez-Pérezasun@fi.upm.esRaúl García-Castrorgarcia@fi.upm.es1 Ontology Engineering Group, Escuela Técnica Superior de Ingenieros Informáticos, UniversidadPolitécnica de Madrid, Madrid, Spain123http://crossmark.crossref.org/dialog/?doi=10.1007/s10115-021-01545-9&domain=pdfhttp://orcid.org/0000-0003-2011-3654https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-3037-0331https://orcid.org/0000-0002-0421-452X868 A. Fernández-Izquierdo et al.1 IntroductionSoftware metrics play an important role in the software engineering field, supporting bothdevelopment and managerial decision-making during the software life cycle. These softwaremetrics are not only related to the source code itself, but also to other artefacts that are part ofthe software main product (e.g. requirements, documentation and tests) and to the activitiesneeded to obtain such artefacts. This diversity of metrics enables software engineers to haveenough information to make different types of predictions, assessments and trade-offs, suchas effort and time predictions or software quality analysis [15].Similarly, in the ontology engineering field different metrics exist which try to assess thequality of ontologies by measuring reliability, reusability or cohesion, among other aspects.However, themetrics proposed until now aremostly focused on the ontology implementation,and they do not take into account other artefacts produced during the ontology developmentprocess or even the development process itself. Moreover, they only consider the structureof the ontology [55].",
        "publication_date": "2021-02-23",
        "authors": "Alba Fernández-Izquierdo, María Poveda‐Villalón, Asunción Gómez‐Pérez, Raúl García‐Castro",
        "file_name": "10!1007%s10115-021-01545-9.pdf",
        "file_path": "./PDFs/10!1007%s10115-021-01545-9.pdf"
    },
    {
        "title": "Enhancing Public Procurement in the European Union Through Constructing and Exploiting an Integrated Knowledge Graph",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/ocds-ontology",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-62466-8_27.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The OCDS ontology is available on GitHub in two versions29: one with the core OCDS terms and another with the extensions."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_27",
        "abstract": "Abstract. Public procurement is a large market affecting almost ev-ery organisation and individual. Governments need to ensure efficiency,transparency, and accountability, while creating healthy, competitive andvibrant economies. In this context, we built a platform, consisting of aset of modular APIs and ontologies to publish, curate, integrate, analyse,and visualise an EU-wide, cross-border, and cross-lingual procurementknowledge graph. We developed end-user tools on top of the knowledgegraph, for anomaly detection and cross-lingual document search. Thispaper describes our experiences and challenges faced in creating sucha platform and knowledge graph and demonstrates the usefulness ofSemantic Web technologies for enhancing public procurement.Keywords: Public procurement · Knowledge graph · Linked data.1 IntroductionThe market around public procurement is large enough so as to affect almostevery single citizen and organisation across a variety of sectors. For this reason,public spending has always been a matter of interest at local, regional, andnational levels, and even more so, in times of great austerity and increased publicscrutiny. Primarily, governments need to be efficient in delivering services, ensuretransparency, prevent fraud and corruption, and build healthy and sustainableeconomies [1, 12]. In the European Union, every year, over 250.000 public authori-ties spend around 2 trillion euros (about 14% of GDP) on the purchase of services,works, and supplies1; while OECD estimates that more than 82% of fraud andcorruption cases remain undetected across all OECD countries [18] costing as1 https://ec.europa.eu/growth/single-market/public-procurement_en2 A. Soylu et al.high as 990 billion euros a year in the EU [10]. Moreover, SMEs are often lockedout of markets due to the high cost of obtaining the required information, wherelarger companies can absorb the cost. This leads to a tendency for governmentsto rely on monolithic suppliers without adequate competition to deliver goodvalue for the taxpayers.The availability of high quality, open, and integrated procurement data couldalleviate some of the aforementioned challenges. This includes government agen-cies assessing purchasing options, companies exploring new business contracts,and other parties (such as journalists, researchers, local communities, businessassociations, transparency activists, and individual citizens) looking for a betterunderstanding of the intricacies of the public procurement landscape throughdecision-making and analytic tools. Projects such as the UK’s GCloud (Govern-ment Cloud)2 have already shown that small businesses can compete effectivelywith their larger counterparts, given the right environment. However, managingthese competing priorities at a national level and coordinating them across differ-ent states and many disparate agencies is notoriously difficult. There are severaldirectives put forward by the European Commission (e.g., Directive 2003/98/ECand Directive 2014/24/EU8) for improving public procurement practices. Theseled to the emergence of national public procurement portals living togetherwith regional, local as well as EU-wide public portals [9]. Yet, there is a lack ofcommon agreement across the EU (in many cases, even inside the same country)on the data formats for exposing such data sources and on the data models forrepresenting such data, leading to a highly heterogeneous technical landscape.To this end, in order to deal with the technical heterogeneity and to con-nect disparate data sources currently created and maintained in silos, we built a",
        "publication_date": "2020-01-01",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": "10!1007%978-3-030-62466-8_27.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-62466-8_27.pdf"
    },
    {
        "title": "The Challenge: From MPEG Intellectual Property Rights Ontologies to Smart Contracts and Blockchains [Standards in a Nutshell]",
        "implementation_urls": [],
        "doi": "10.1109/msp.2019.2955207",
        "publication_date": "2020-02-26",
        "authors": "Panos Kudumakis, Thomas Wilmering, M. Sandler, Victor Rodrı́guez-Doncel, Laurent Boch, Jaime Delgado",
        "file_name": "10!1109%msp!2019!2955207.pdf",
        "file_path": "./PDFs/10!1109%msp!2019!2955207.pdf"
    },
    {
        "title": "A Review of Bias and Fairness in Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.11.001",
        "abstract": "the abstract creation, the work, which is the result of any intellectual endeavour with enough creativity. Works are pure, abstract entities, with no material incarnation whatsoever. Derivative works are a special type of works,  that have been derived from an existing work. Works are fixated into physical manifestations, which are the very first incarnation of works. Manifestations can be instanced, and copied, or they can be transformed into commercial products. Whereas the logical schema of IP entities resembles the Functional Requirements for Bibliographic Records (FRBR) chain [13], the source is somewhat different: MVCO catering for the needs of music and media stakeholders codifies the IP entities mentioned by copyright legislation (as defined by worldwide agreed treaties such as the Berne Convention), whereas FRBR is inspired by the needs of librarians.  A user is defined as an individual or organization, acting in the media value chain. The types of roles, a user  could undertake, revolve around the IP entities, e.g., a creator is defined as the user who creates a work, an adaptor is the user who adapts a work to produce an adaptation. These roles or very similar ones are also acknowledged by copyright legislation. Other roles include producer, distributor and, finally the end-user.  The types of actions that can be performed also revolve around the IP entities. Create work is the action whose result is a new work, produce is the action whose result is a product and so forth. In addition, some other actions do not produce any new IP entity, such as, a public communication or an end-user action (e.g., play and print) but they are legal concepts with explicit mentions and provisions in copyright legislation.  The relationship between a user and a particular IP entity type (e.g., work, adaptation, product, copy) is specified through the concept of role. The actions that a user performs on a given IP entity determine the role of that user with respect to the IP entity in question. Users get roles (e.g., creator, adaptor, producer, end-user) that attribute them rights over actions (e.g., create work, make adaptation, produce, distribute, synchronise) that can be exercised on specific IP entities. Any given user may undertake any number of roles within a given value chain. Figure 1 illustrates these relationships between actions, users and IP entities.  Fig. 1. MVCO defined relationships between actions, users and IP entities.  Authorisation Model The MVCO by defining the relationships between users, actions and IP entities serves well to depict a static picture of the IP information. However, in real life rights are transferable and this dynamic nature of rights was required to be supported in the MVCO.  Transfer of rights are born with the signature of agreements or contracts which grant permissions. A permission relates an IP entity with a right in transit between the original rights owner and the new rights owner. Permissions have an intrinsic dynamic nature: they are granted, invoked and revoked. Instances of a user class will probably be actual companies or persons; instances of works will be actual works. However, instances of permissions are far more interesting due to that they could refer either to the past or in the future. That is, an instance permission (e.g., Alice’s permission to play a song) would be related to both: an end-user instance (e.g., Alice) and an action instance (e.g., play a song). However, what is the interpretation of an action instance? It might be an action effectively executed in the past (e.g., Alice played a song), but it might also be  an action to be performed in the future, as a mere possibility (e.g., Alice can play a song). This is commonly referred in the literature as event factuality, and suggests that action instances can be marked as executed acts or as possible acts. Permissions can also be granted conditionally, that is, subject to certain conditions (facts). Facts can be seen as propositions with an alethic (e.g., true or false) value. These propositions can be combined with logical operators (e.g., conjunction and disjunction) to create more complex ",
        "publication_date": "2023-11-14",
        "authors": "Rubén González-Sendino, Emilio Serrano, Javier Bajo, Paulo Nováis",
        "file_name": "10!9781%ijimai!2023!11!001.pdf",
        "file_path": "./PDFs/10!9781%ijimai!2023!11!001.pdf"
    },
    {
        "title": "Modelling of the Internet Computer Protocol Architecture: The Next Generation Blockchain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-21229-1_1",
        "abstract": "Abstract. The Internet Computer Protocol is described as a third-generation blockchain system that aims to provide secure and scalabledistributed systems through blockchains and smart contracts. In thisposition paper, this innovative architecture is introduced and then dis-cussed in view of its modeling and simulation aspects. In fact, a properlydefined digital twin of the Internet Computer Protocol could help its de-sign, development, and evaluation in terms of performance and resilienceto specific security attacks. To this extent, we propose a multi-level sim-ulation model that follows an agent-based paradigm. The main issuesof the modeling and simulation, and the main expected outcomes, aredescribed and discussed.Keywords: Internet Computer · Distributed Ledger Technology · Mod-elling and Simulation · Blockchain.1 IntroductionCloud computing has undoubtedly been the fastest growing and most success-ful in delivering technical and economic benefits for application and systemdevelopment in recent years [30, 26]. Starting from startups up to large com-panies, everyone is adopting cloud computing to get rid of the risk of capitalinvestment, cutting the cost of hardware and software infrastructure, and avail-ing themselves of services according to their demand. This is why paradigmssuch as ’Infrastructure-as-a-Service (IaaS)’, ’Platform-as-a-Service (PaaS)’, and’Software-as-a-Service (SaaS)’ have emerged. In general, however, cloud serviceproviders maintain their customers with an opaque knowledge about the location⋆ This work has received funding from the EU H2020 research and innovation pro-gramme under the MSCA ITN grant agreement No 814177 LAST-JD-RIoE; and theresearch grant (No.: RP/ESCA-04/2020) offered by Macao Polytechnic University.2 Li et al.and storage of data, the privacy offered to users, and the type of hardware in-frastructure used. This leads firstly to a problem of trust by users [19]. Secondly,security and privacy are undermined by the centrality of these solutions, whichmore easily attracts cyber-attacks, i.e. single points of failure [30]. In addition, itshould not be forgotten that centralized solutions will not be able to support thehuge amount of data generated globally by users and Internet-of-Things devicesfor much longer [26]. Finally, it is commonly difficult to assess if Quality of Ser-vice (QoS) guarantees are met and Service Level Agreements (SLA) negotiatedbetween users and the cloud provider are satisfied, due to the absence of trustedlogs [5]. All this motivates the transition towards a completely decentralized ap-proach. The benefits of this solution are many. In fact, the decentralization ofthe system removes the presence of a single point of failure, allows for inher-ently increasing scalability, curbs illicit activities of malicious nodes, and canalso provide for accountability guarantees. Clearly, in order to realize a similarkind of system, it becomes necessary to encourage node participation that canbe somehow rewarded through incentive mechanisms [33].The Internet Computer Protocol (ICP) architecture5 aims to establish a net-work of networks by defining a protocol for combining the resources of severaldecentralized computers into the reading, replication, modification, and pro-curement of an application state. A network of nodes runs the protocol throughindependently-operated data centers to provide general-purpose (largely) trans-parent computations for end-users. On the other hand, the development of appli-cations on top of the ICP is facilitated by reliable message delivery, transparent",
        "publication_date": "2023-01-01",
        "authors": "AoXuan Li, Luca Serena, Mirko Zichichi, Su-Kit Tang, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!1007%978-3-031-21229-1_1.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-21229-1_1.pdf"
    },
    {
        "title": "MOATcoin",
        "implementation_urls": [
            {
                "identifier": "https://github.com/interwork-alliance/TokenTaxonomyFramework",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3410699!3413798.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "ACM ISBN 978-1-4503-8079-9 The final published version is available online at: https://doi.org/10.1145/3410699.3413798 Rights/License: The terms and conditions for the reuse of this version of the manuscript are specified in the publishing policy."
                    }
                ]
            }
        ],
        "doi": "10.1145/3410699.3413798",
        "abstract": "ABSTRACT In this paper we present MOATcoin, a gamelike experiment that enabled us to practically investigate the open issues related to the governance and legal facets of smart contract based decentralized applications. After presenting the MOATcoin system architecture, we first tackle the problems of decentralized governance, its limits and the shift of trust that it entails; then, we explore the possible legal implications of the given scenario and, particularly, the consequences of code-written contracts. Finally, we offer taxonomical remarks on the concepts of “token” and “coin” and offer an insight from a regulatory perspective. CCS CONCEPTS Applied computing → Law; Computer systems organization → Distributed architectures. KEYWORDS Distributed Ledger Technologies, Smart Contract, Blockchain, Governance, Tokens, Crypto-assets, dApp, Law and Technology   mailto:biagio.distefano@univie.ac.atmailto:nadia.pocher@uab.catmailto:mirko.zichichi@upm.eshttps://www.last-jd-rioe.eu/http://lbl.cirsfid.unibo.it/1 INTRODUCTION Distributed Ledger Technologies (DLTs), originally introduced in the form of blockchain [20], enable the advent of a new vision to consider finance, trust in communication and governance. The distributed ledger ensures the immutable persistence of data, thus providing untampered data to applications when necessary. For this reason, DLTs represent an interesting technology for the development of reliable, decentralized applications (hereinafter: DApp) and services, based on Smart Contracts [3, 5]. In this paper we investigate the multidisciplinary facets, challenges and legal implications of smart contract-powered social interaction governance tools exploiting a “decentralized” game called MOATcoin1. Although this Dapp is just a game, with its simplification of a real-life scenario, it showcases crucial implications of real-world smart contract use cases. This made it possible to critically analyse what we consider to be the crucial matters of DApps that claim to have legal relevance. In order to perform this experiment, we set up a simple set of rules for a game that had to be executed on a decentralized environment. The game scenario pictures twenty Law, Science and Technology Joint Doctorate PhD candidates talking incessantly about work, even after office hours. To solve this, a simple rule was imposed: whoever pronounces certain work-related keywords (e.g., “GDPR”, “blockchain”) outside the office premises, has to buy a beer to another candidate. Since all twenty candidates are peers and no governing bodies were to be formed, we adopted a decentralized architecture based on Ethereum smart contracts [5]2. MOATcoin enables us to question the actual level of decentralization of DApps, as well as to explore governance and legal impacts of software architecture choices and the relevance of the concept of “token”, to the end of highlighting problems and possible solutions. Along with this, a complementary issue needs to be addressed. DLT-related discourse is flooded with terms such as “coins”, “cryptoassets”, “tokens”, “virtual currencies”, “cryptocurrencies”, “digital assets”, “virtual assets”, etc. From a legal and regulatory perspective, these expressions give way to endless considerations and uncertainties. From a practical perspective, it is pivotal to find a balance that avoids the loss of any technical and non-technical common ground. The remainder of this paper is organized as follows: Sections 2 and Section 3 introduce some background and related work and outline the architecture of MOATcoin. From Section 4 we start discussing the experiment results by providing an overview of MOATcoin’s governance model and highlighting weak points and possible solutions. Then, Section 5 offers a perspective on the relation between smart and legal contracts. In Section 6 we place MOATcoin within the world of tokens, by introducing taxonomy considerations and relevant implications. Finally, Section 7 provides some concluding remarks.  ",
        "publication_date": "2020-09-04",
        "authors": "Biagio Distefano, Nadia Pocher, Mirko Zichichi",
        "file_name": "10!1145%3410699!3413798.pdf",
        "file_path": "./PDFs/10!1145%3410699!3413798.pdf"
    },
    {
        "title": "Grammatically uniform population initialization for grammar-guided genetic programming",
        "implementation_urls": [],
        "doi": "10.1007/s00500-020-05061-w",
        "abstract": "Abstract The initial population distribution is an es-sential issue in evolutionary computation performance.Population initialization methods for grammar guidedgenetic programming have some difficulties generatinga representative sample of the search space, which nega-tively affects the overall evolutionary process. This pa-per presents a grammatically uniform population ini-tialization method to address this issue by improvingthe initial population uniformity: the equiprobabilityof obtaining any individual of the search space definedby the context-free grammar. The proposed initializa-tion method assigns and updates probabilities dynam-ically to the production rules of the grammar to pur-sue uniformity, and includes a code bloat control mech-anism. We have conducted empirical experiments tocompare the proposed algorithm with a standard ini-tialization approach very often used in grammar-guidedgenetic programming. The results report that the pro-posed initialization method approximates very well auniform distribution of the individuals in the searchP. Ramos CriadoAturing Research, Salamanca, SpainE-mail: pablo.ramos@aturing.comD. Barrios RolańıaDepartamento de Matemáticas del Área IndustrialUniversidad Politécnica de Madrid, SpainE-mail: dolores.barrios.rolania@upm.esD. Manrique  Departamento de Inteligencia Artificial Universidad Politécnica de Madrid, Spain ORCID ID: 0000-0002-0792-4156 E-mail: daniel.manrique@upm.esE. SerranoDepartamento de Inteligencia Artificial Universidad Politécnica de Madrid, Spain ORCID ID: 0000-0001-7587-0703 E-mail: emilioserra@fi.upm.esspace. Moreover, the overall evolutionary process thattakes place after the population initialization performsbetter in terms of convergence speed and quality of thefinal solutions achieved when the proposed method gen-erates the initial population than when the usual ap-proach does. The results also show that these perfor-mance differences are more significant when the exper-iments involve large search spaces.Keywords Grammar-guided genetic programming ·Initialization · Genotypic uniformity · Stochasticcontext-free grammar1 IntroductionEvolutionary computation (Kari and Rozenberg, 2008)",
        "publication_date": "2020-06-12",
        "authors": "Pablo Ramos Criado, D. Barrios Rolanı́a, Daniel Manrique, Emilio Serrano",
        "file_name": "10!1007%s00500-020-05061-w.pdf",
        "file_path": "./PDFs/10!1007%s00500-020-05061-w.pdf"
    },
    {
        "title": "Combination of Multi-Agent Systems and Wireless Sensor Networks for the Monitoring of Cattle",
        "implementation_urls": [],
        "doi": "10.3390/s18010108",
        "abstract": "Abstract: Precision breeding techniques have been widely used to optimize expenses and increaselivestock yields. Notwithstanding, the joint use of heterogeneous sensors and artificial intelligencetechniques for the simultaneous analysis or detection of different problems that cattle may presenthas not been addressed. This study arises from the necessity to obtain a technological tool thatfaces this state of the art limitation. As novelty, this work presents a multi-agent architecture basedon virtual organizations which allows to deploy a new embedded agent model in computationallylimited autonomous sensors, making use of the Platform for Automatic coNstruction of orGanizationsof intElligent Agents (PANGEA). To validate the proposed platform, different studies have beenperformed, where parameters specific to each animal are studied, such as physical activity,temperature, estrus cycle state and the moment in which the animal goes into labor. In addition, a setof applications that allow farmers to remotely monitor the livestock have been developed.Keywords: birth sensor; bovine embedded hardware; ambient intelligence; virtual organizationsof agents1. IntroductionThe last decade saw a breakthrough in the field of Ambient Intelligence (AmI), resulting in animprovement in the quality of people’s lives. The main objective of AmI is to adapt technology tothe needs of the people in a way that allows users to interact in a natural and effortless manner withthe different systems that make up the environment. Technology must act in a transparent way byadapting to the individuals and their context; simplifying the accomplishment of daily tasks and thecommunication between them and the environment. To achieve this goal, it is necessary to develop newtechnological models that allow users to interact with multiple devices simultaneously. The devicesmust collaborate in the accomplishment of daily tasks without the individuals being aware of it. Dueto the miniaturization of sensors and the reduction of costs in manufacturing processes, AmI can nowprovide new solutions to daily problems that were unthinkable just a few years ago. One field to whichAmI can be applied to is that of livestock and natural resources management.Currently in Spain, there are more than 23 million head of swine and more than 18 million ofsheep, representing 15% of the total of the European Union [1]. The continuous growth that thefarms are experiencing and the increasing demand for farm produce, make technology an essentialinstrument of continuous improvement in the development of farms. Requirements are also becomingincreasingly strict for both the breeders and the livestock, so the study of factors such as food, physicalactivity or the animal’s health is necessary. On large farms, it is difficult to devote time to observing theSensors 2018, 18, 108; doi:10.3390/s18010108 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18010108http://www.mdpi.com/journal/sensorsSensors 2018, 18, 108 2 of 27behavior of each animal. Technology must provide solutions that simplify farmers’ work, especiallyin repetitive and difficult tasks, such as those that entail the study of the factors mentioned above.The research and development of systems that detect anomalies in animals at early stages, is vitallyimportant. Numerous research groups are working on the use of electronic systems in livestock withthe aim of improving productivity and reducing operating costs. AmI allows farmers to remotelyaccess up to date information on the animals and obtain their complete traceability. The joint use ofinformation technology and electronic devices, allows to monitor and study parameters as well asthe consumption of energy on the farm, the level of food in the feeders, the lighting, climatology, thestate of animals’ health, their physical activity, etc. Smart-farming or precision farming consists inapplying information and communication technologies to livestock and agriculture. Its main objectiveis to increase the efficiency and quality of production through rapid decision making in cases where",
        "publication_date": "2018-01-02",
        "authors": "Alberto López Barriuso, Gabriel Villarrubia González, Juan F. De Paz, Álvaro Lozano Murciego, Javier Bajo",
        "file_name": "no_doi_20250624161628.pdf",
        "file_path": "./PDFs/no_doi_20250624161628.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/1/108/pdf?version=1516694198"
    },
    {
        "title": "Analysis of ontologies and policy languages to represent information flows in GDPR",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/SotAResources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-223009.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "5148947, and its public repository can be accessed by the community at https://github.com/besteves4/SotAResources for further development."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5148947",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-223009",
        "abstract": "Abstract. This article surveys existing vocabularies, ontologies and policy languages that can be used to represent informationalitems referenced in GDPR rights and obligations, such as the ‘notification of a data breach’, the ‘controller’s identity’ or a‘DPIA’. Rights and obligations in GDPR are analyzed in terms of information flows between different stakeholders, and acomplete collection of 57 different informational items that are mentioned by GDPR is described. 13 privacy-related policylanguages and 9 data protection vocabularies and ontologies are studied in relation to this list of informational items. ODRLand LegalRuleML emerge as the languages that can respond positively to a greater number of the defined comparison criteria ifcomplemented with DPV and GDPRtEXT, since 39 out of the 57 informational items can be modelled. Online supplementarymaterial is provided, including a simple search application and a taxonomy of the identified entities.Keywords: Privacy policy languages, data protection ontologies, GDPR, rights, obligations1. IntroductionWestin [96] shaped the way we define online privacy before the web existed at all. One of his two major postulateswas that individuals should be able to determine to what extent information about them is communicated to others.The second of these postulates was that technological artifacts could be used to achieve this goal. His books in thelate sixties and the seventies exerted significant influence on the privacy legislation that was enacted in the followingyears, and even today, the European General Data Protection Regulation (GDPR), which came into full effect onMay 25th of 2018, owes much to his work. Any information system has data representational needs, and privacy anddata protection related information systems will have to represent ideas such as ‘consent’ or ‘the right to erasure’. Ifthese applications are to interoperate, then the need for standard formats is clear, and the adoption of semantic-webenabled technologies that facilitate privacy-related data exchange is advantageous such as in data portability.Machine-readable policy languages have been on the scene for some decades. Policy languages allow us to repre-sent the will of an individual or organization to grant access to a certain resource, and they govern the operation ofactual systems over actual data. They seem perfectly aligned with Alan Westin’s vision and indeed several privacy-related policy languages have been defined and used in real scenarios. On the other hand, computers can also help*Corresponding author. E-mail: beatriz.gesteves@upm.es.1570-0844 © 2024 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:beatriz.gesteves@upm.esmailto:beatriz.gesteves@upm.eshttps://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-223009&domain=pdf&date_stamp=2022-06-07710 B. Esteves and V. Rodríguez-Doncel / Analysis of ontologies and policy languages to represent information flows in GDPRin other privacy and data protection tasks different from enforcing access to personal data, and policy languages arenot enough to cover every representational need. Thus, in the last few years, vocabularies and computer ontologieshave appeared to formalize concepts and rules in the domain that can be used either to simply represent informationas RDF, or to govern ontology-based information systems. Not all of them, however, had the GDPR specifically astheir framework of reference.This paper surveys existing policy languages, vocabularies and ontologies in the domain of privacy and dataprotection, and it analyses their adequacy to support GDPR-related applications. These GDPR-related applicationsmay either support individuals to manage their personal information or to support data controllers, data processorsand other stakeholders to better manage compliance with the GDPR. This joint analysis of needs (individual-orientedand company-oriented) is based on the claim that these tools may converge in a near future, and that having commonvocabulary elements and common data models to refer to GDPR rights and obligations and to denote specific GDPRconcepts would permit heterogeneous applications to speak in the same terms and interoperate. Taking into accountthis rationale, we focus on the above motivations to address the following research question: Are the existing policylanguages and vocabularies suitable to meet the representational needs brought on by the newly applicableGDPR’s rights and obligations?.Moreover, the main contributions of this paper are:(i) a study of GDPR in terms of flows of information in different deontic modalities, systematized in Fig. 1, andfurther specified in Table 1 where the informational elements necessary for the management of each GDPRright and obligation are specified;",
        "publication_date": "2022-06-07",
        "authors": "Beatriz Esteves, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%sw-223009.pdf",
        "file_path": "./PDFs/10!3233%sw-223009.pdf"
    },
    {
        "title": "Towards CBDC-based machine-to-machine payments in consumer IoT",
        "implementation_urls": [],
        "doi": "10.1145/3477314.3507078",
        "abstract": "ACM must be honored. Abstracting with credit is permitted. To copy otherwise,to republish, to post on servers, or to redistribute to lists, requires prior specificpermission and/or  a fee. Request permissions from permissions@acm.org orPublicationsDept., ACM, Inc., fax +1 (212) 869-0481.https://www.acm.org/publications/policies/copyright-policyThis item was downloaded from IRIS Università di Bologna (https://cris.unibo.it/)When citing, please refer to the published version.http://dx.doi.org/10.1145/3477314.3507078https://cris.unibo.it/https://www.acm.org/publications/policies/copyright-policyTowards CBDC-based Machine-to-Machine Payments inConsumer IoTNadia Pocher∗Universitat Autònoma de BarcelonaLaw, Science and Technology RIoE EJDnadia.pocher@uab.catMirko Zichichi∗Universidad Politécnica de MadridLaw, Science and Technology RIoE EJDmirko.zichichi@upm.esABSTRACTThe technological advancement of the Internet of Things (IoT) is awell-known phenomenon that mainly affects industrial sectors butalso consumers in everyday life. The use of Consumer IoT, i.e. CIoT,devices is increasing, and they are paving the way for a Machine-to-Machine (M2M) communication that could highly enrich consumerservices. In this paper we position ourselves in the narrowing gapbetween the world of CIoT and the world of money, and we explorethe emerging interaction between the payment needs of a M2MEconomy and the “newways of payment”. Indeed, the advent of Dis-tributed Ledger Technology and cryptocurrencies has introduceda tech-oriented dynamism in the monetary and financial sphere.Accordingly, central banks all over the world have started investi-gations into digital fiat money , i.e., “retail” Central Bank DigitalCurrencies (CBDCs). Against this backdrop, we analyze the inte-gration of retail CBDC models into M2M and CIoT dynamics, whileheeding regulation-by-design and compliance-by/through-designmethodologies, and we propose a preliminary model of integrationbetween a two-tier retail CBDC architecture and CIoT.CCS CONCEPTS• Networks → Peer-to-peer networks; • Applied computing→ Law; Economics; • Human-centered computing → Ubiqui-tous and mobile devices;KEYWORDSCentral Bank Digital Currency, Machine-to-Machine, Internet ofThings, Distributed Ledger TechnologiesACM Reference Format:Nadia Pocher and Mirko Zichichi[1]. 2022. Towards CBDC-based Machine-to-Machine Payments in Consumer IoT. In The 37th ACM/SIGAPP Sympo-sium on Applied Computing (SAC ’22), April 25–29, 2022, Virtual Event, .ACM,",
        "publication_date": "2022-04-25",
        "authors": "Nadia Pocher, Mirko Zichichi",
        "file_name": "10!1145%3477314!3507078.pdf",
        "file_path": "./PDFs/10!1145%3477314!3507078.pdf"
    },
    {
        "title": "Closing the Awareness Gap Between IT Practice and IT Law",
        "implementation_urls": [],
        "doi": "10.3233/faia190006",
        "abstract": "Abstract. Some of the ordinary activities of IT practitioners require a certain degreeof knowledge of IT law. Assuming these professionals will acquire legal knowl-edge better if expressed in terms familiar to them, this Chapter explores differentmanners of organising and presenting legal knowledge for its better cognition byIT professionals. This proposal features data models and knowledge organisationrooted in the specific legal theory of critical legal positivism of Kaarlo Tuori. It hasbeen evaluated with an experiment, where BSc students in Computer Science havebeen provided with models and reference material describing the EU legislationon cookies, and have been asked specific questions. In sight of the new theoreticalframework and the experiment results, we postulate that models and ontologies canbridge the knowledge gap and serve as lingua franca between the legal and the ITprofession.Keywords. legal knowledge, critical legal positivism, cognition of law, semi-formal models1. IntroductionA model is a representation of a reality, an abstraction, a simplification, a depiction.Modelling law can be a necessity for a number of reasons: legal drafting, analysis ofcourt cases, development of computer programs implementing or enforcing law, teach-ing law [1]. This Chapter discusses the pros and cons of using formal or semi-formalmodels for representing the law with didactic purposes, specifically when introduced toIT practitioners.The use of ontologies and semi-formal models in education environments is not new[2]. Semantic Web Technologies have been used for e-Learning [3], to align curriculumand syllabuses with learning objectives [4] or to support adaptive learning [5]. This workproposes their novel application to the legal domain – which has its own idiosincracy– and addressing a specific collective – IT experts. The contribution of this Chapter re-volves around the two proposed ideas: (i) that this target group is likely to understandbetter UML diagrams and related documentation and (ii) that legal knowledge represen-tation must consider a theoretical framework and lean on a rich tradition: languages andmethods for software and ontological engineering make explicit an idea that has beentraditionally part of legal thinking, the idea that legal concepts have a structure and arelinked one to another [6].BKnowledge of the Law in the Big Data AgeG. Peruginelli and S. Faro (Eds.)© 2019 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA19000641We engage Tuori’s critical legal positivist theory [7], which conceives law as a multi-layered phenomenon, as a feasible way to pre-conceptual modelling1. Tuori refers to an‘upper level’ concerned with legislative acts and case law, a “middle level, mediatinglevel in the law” related to the practical knowledge which lawyers and judges requireto interpret the law, and finally at the ‘lower level’, or the ‘deep structure’ of law, acompendium of the most basic principles and habits of mind by which we think andargue about the law. Critical legal positivism acknowledges the two faces of the law:“on the one hand, the law is a symbolic normative phenomenon, and on the other hand,it can be defined as a set of specific social practices”. This Chapter assumes that oneinteresting mapping from a pragmatic point of view is that of the mediating legal culture",
        "publication_date": "2019-01-01",
        "authors": "Marie François, Rodríguez-Doncel Victor, Pompeu Casanovas",
        "file_name": "10!3233%faia190006.pdf",
        "file_path": "./PDFs/10!3233%faia190006.pdf"
    },
    {
        "title": "Extraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law",
        "implementation_urls": [
            {
                "identifier": "https://github.com/pmchozas/term_relex",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!26342%2022-69-9.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This work (code, input and output data) is openly available and published in a GitHub repository3."
                    }
                ]
            }
        ],
        "doi": "10.26342/2022-69-9",
        "abstract": "Abstract: Despite the freedom of information and the development of various opendata repositories, the access to legal information to general audience remains hin-dered due to the difficulty of understanding and interpreting it. In this paper weaim at employing modern language models to extract the most important infor-mation from legal documents and structure this information in a knowledge graph.This knowledge graph can later be used to retrieve information and answer legalquestion. To evaluate the performance of different models we formalize the task asevent extraction and manually annotate 133 instances. We evaluate two models:GRIT and Text2Event. The latter model achieves a better score of « 0.8 F1 scorefor identifying legal classes and 0.5 F1 score for identifying roles in legal relations.We demonstrate how the produced legal knowledge graph could be exploited with2 example use cases. Finally, we annotate the whole Workers’ Statute using thefine-tuned Text2Event model and publish the results in an open repository.Keywords: Information Extraction. Knowledge Graphs. Semantic Web. LegalDomain.Resumen: A pesar de la actual libertad de información y del desarrollo de difer-entes repositorios de datos abiertos, el acceso a la información juŕıdica al públicogeneral sigue suponiendo un problema debido a la dificultad de comprensión e in-terpretación de dicha información. En este art́ıculo, nuestro objetivo es emplearmodelos de lenguaje punteros para extraer información relevante de documentosjuŕıdicos; aśı como estructurar esta información en un grafo de conocimiento, con elobjetivo de que este grafo pueda utilizarse más adelante para recuperar informacióny responder preguntas sobre el dominio juŕıdico. Para evaluar el rendimiento de losdiferentes modelos, hemos formalizado este proceso como una tarea como extracciónde eventos, y hemos anotado manualmente 133 instancias. Evaluamos dos modelos:GRIT y Text2Event. El último modelo consigue mejores resultados, de « 0.8 F1 paraidentificar clases juŕıdicas y de 0.5 F1 para identificar roles en relaciones juŕıdicas.Asimismo, ejemplificamos cómo el grafo producido podŕıa explotarse con diferentescasos de uso. Finalmente, hemos anotado todo el Estatuto de los Trabajadores conel modelo Text2Event y publicado los resultados en un repositorio abierto.Palabras clave: Extracción de Información. Grafos de Conocimiento. WebSemántica. Dominio Juŕıdico.1 IntroductionDue to its specific nature, the legal domainhas always been a complex area for non le-gal users. The challenges include finding thecorrect document for a purpose and inter-preting the document. With the recent riseof the data sharing and open data technolo-gies in the last decade, legal knowledge ismore accessible than ever. Well-known legalpractitioners have already exposed their legaldata in open and machine readable formats,developing platforms such as the EuropeanData Portal1, a platform funded by the Eu-1https://data.europa.eu/Procesamiento del Lenguaje Natural, Revista nº 69, septiembre de 2022, pp. 105-116 recibido 31-03-2022 revisado 12-05-2022 aceptado 30-05-2022ISSN 1135-5948. DOI 10.26342/2022-69-9 © 2022 Sociedad Española para el Procesamiento del Lenguaje Naturalropean Union and managed by the Publica-tions Office that gathers legal data from dif-",
        "publication_date": "2022-09-01",
        "authors": "Artem Revenko, Patricia Martın-Chozas",
        "file_name": "10!26342%2022-69-9.pdf",
        "file_path": "./PDFs/10!26342%2022-69-9.pdf"
    },
    {
        "title": "Legal Linked Data Ecosystems and the Rule of Law",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_5",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "no_doi_20250624161723.pdf",
        "file_path": "./PDFs/no_doi_20250624161723.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_5.pdf"
    },
    {
        "title": "OBOE: an Explainable Text Classification Framework",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2022.11.001",
        "abstract": "Abstract This chapter introduces the notions of meta-rule of law and socio-legalecosystems to both foster and regulate linked democracy. It explores the way ofstimulating innovative regulations and building a regulatory quadrant for the rule oflaw. The chapter summarises briefly (i) the notions of responsive, better and smartregulation; (ii) requirements for legal interchange languages (legal interoperability);(iii) and cognitive ecology approaches. It shows how the protections of the sub-stantive rule of law can be embedded into the semantic languages of the web of dataand reflects on the conditions that make possible their enactment and implemen-tation as a socio-legal ecosystem. The chapter suggests in the end a reusablemulti-levelled meta-model and four notions of legal validity: positive, composite,formal, and ecological.Keywords Web of data � Socio-legal ecosystem � Rule of law �Meta-rule of law �Semantic languages � Governance � Linked democracy � Semantic web regulatorymodels � Regulatory quadrant � Legal validity5.1 Introduction: The Rule of Law in a New Brave WorldWewill expand in this chapter someways of implementing linked democracy on legaland political bases. Linked democracy is not only a theoretical approach incorpo-rating open linked data to theories of democracy. It consists of practices and the realbehaviour of people exercising their political rights on everyday bases. Thus, it alsopossesses a personal and cultural dimension that should be valued and protected. Lawis an obvious element. Behaviour on the web should be ‘fair’ and ‘legal’. What does itmean? Different states have different jurisdictions, and despite the international trendsof the global market, law has been, and still is, dependent on national states.How could we incorporate regulatory forms of empowering people on the web?How could algorithmic governance, data analytics, and semantics be used tofoster the principles of linked democracy that we have just presented at the end ofChap. 4?© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_587http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_5We will contend that there are two ways to reach such objectives: (i) embeddingthe principles of the substantive rule of law into the web of linked data (what wewill call the meta-rule of law), and (ii) incentivising the creation of socio-legalecosystems, i.e. the social conditions that are required to implement the meta-rule oflaw online and outline them among all stakeholders and users.We admit that this can be easier said than done. These two objectives might havean idealistic flavour. A few corporations have a dominant position on the web, theycan trade and invade privacy, and they usually do. As Shadbolt and Hampson(2018) have nicely put it, we live in a hyper-complex environment, shaped by ourown tools. This is a good breeding ground for elites to thrive. They also point outthat “what has changed is human potential, thanks to our transformative new tools.[…] The point is not that machines might wrest control from the elites. The problemis that most of us might never be able to wrest control of the machines from thepeople that occupy the command posts” (Shadbolt and Hampson 2018, 63).Power is certainly a problem. In our hyper-connected world, we barely know in",
        "publication_date": "2024-01-01",
        "authors": "Raúl A. del Águila Escobar, Mari Carmen Suárez-Figueroa, Mariano Fernández‐López",
        "file_name": "10!9781%ijimai!2022!11!001.pdf",
        "file_path": "./PDFs/10!9781%ijimai!2022!11!001.pdf"
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "publication_date": "2020-11-24",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": "./PDFs/10!1109%nca51143!2020!9306721.pdf"
    },
    {
        "title": "ONETT: Systematic Knowledge Graph Generation for National Access Points.",
        "implementation_urls": [],
        "abstract": "Abstract. In this paper, we describe our implemented approach for theusage and exploitation of declarative mappings for the publication ofopen transport data from transport authorities and operators into anontology based on Transmodel. This allows a homogeneous represen-tation of transport data across EU transport-related organisations andminimises the need to understand ad-hoc heterogeneous representationformats for transport data as currently published by them. We show howwe create and use RML mappings for the specific case of transformingGTFS data into a Transmodel-based ontology. In the future, such datamay be further transformed into other formats such as NeTEx.Keywords: Transmodel · GTFS · NAP · RML1 IntroductionTransport data is being currently published by transport authorities and opera-tors in many different formats, some of which are well-known de-facto standards,such as the General Transit Feed Specification or GTFS, and some others are ad-hoc data formats whose structure is decided by the data publisher (e.g., currentdatasets and APIs published by Empresa Municipal de Transportes de Madridin its open data portal1, tram information in Zaragoza2, etc.)All of these datasets have similarities, associated to the fact that they are de-scribing overlapping sets of information (schedules, stops, vehicles, lines, etc.).They are also made available, commonly, using tabular data formats. For ex-ample, GTFS feeds are essentially zip-compressed files containing sets of CSVfiles following the GTFS specification. And other data sources such as thosementioned above as examples provide the data either in CSV or JSON.Having all this data available in a homogeneous manner would actually reducethe total cost of reusing data sources, especially across operators/authorities andcities/regions. That is, developers may be able to develop one application thatwould be deployable in any city in the world with minor adaptations. This isalready happening with GTFS, which is not only being used by Google MapsCopyright c© 2019 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).1 https://opendata.emtmadrid.es/2 https://www.zaragoza.es/sede/servicio/catalogo/327https://opendata.emtmadrid.es/https://www.zaragoza.es/sede/servicio/catalogo/3272 Chaves-Fraga et al.to provide data about transport infrastructure, but also for route planning, butalso by other route planners, such as Navita.io and OpenTripPlanner.To achieve this homogeneity, there are several options that may be followed:– Transport authorities and operators may agree on using the same data for-mat and hence publish according to such data format. They know well thetype of data that they handle, the quality properties on such data, etc., sothey should be able to provide this data easily. To some extent, this is whatis happening currently with GTFS, and what should happen in the near fu-ture in the European Union with NeTex, according to directive 2010/40/EUand regulation 2017/1926 (MMTIS).– 3rd parties (as well as operators and authorities themselves) may be able tocreate transformation rules that allow transforming the original data sourcesinto other generally-agreed formats, republishing such transformed data ei-ther in the original data portals, if allowed to do so, or in other servers.",
        "publication_date": "2019-01-01",
        "authors": "David Chaves-Fraga, Ana A. Antón, Jhon Toledo, Óscar Corcho",
        "file_name": "no_doi_20250624161753.pdf",
        "file_path": "./PDFs/no_doi_20250624161753.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2447/paper8.pdf"
    },
    {
        "title": "TýrFS: Increasing Small Files Access Performance with Dynamic Metadata Replication",
        "implementation_urls": [],
        "doi": "10.1109/ccgrid.2018.00072",
        "abstract": "Abstract—Small files are known to pose major performancechallenges for file systems. Yet, such workloads are increasinglycommon in a number of Big Data Analytics workflows or large-scale HPC simulations. These challenges are mainly caused bythe common architecture of most state-of-the-art file systemsneeding one or multiple metadata requests before being able toread from a file. Small input file size causes the overhead ofthis metadata management to gain relative importance as thesize of each file decreases. In this paper we propose a set oftechniques leveraging consistent hashing and dynamic metadatareplication to significantly reduce this metadata overhead. Weimplement such techniques inside a new file system named TýrFS,built as a thin layer above the Týr object store. We prove thatTýrFS increases small file access performance up to one order ofmagnitude compared to other state-of-the-art file systems, whileonly causing a minimal impact on file write throughput.I. INTRODUCTIONA large portion of research in data storage, managementand retrieval focuses on optimizing access performance forlarge files [1]–[5]. Yet, handling a large number of small filesraises other difficult challenges, that are partly related to thevery architecture of current file systems. Such small files,with a size inferior to a few megabytes, are very commonin large-scale facilities, as shown by multiple studies [6], [7].They can be generated by data-intensive applications suchas CM1 [8] or HACC [9], Internet of Things or StreamProcessing applications, as well as large scale workflows suchas Montage [10], CyberShake [11] or LIGO [12]. Improvingfile access performance for these applications is critical forscalability in order to handle ever-growing data sets on large-scale systems [13].As the amount of data to be transferred for storage op-erations on any single small file is intuitively small, the keyto optimizing access performance for such files lies in im-proving the efficiency of the associated metadata management.Actually, as the data size for each file decreases, the relativeoverhead of opening a file is increasingly significant. In ourexperiments, with small enough files, opening a file may takeup to an order of magnitude more time than reading the datait contains. One key cause of this behavior is the separationof data and metadata inherent to the architecture of currentfile systems. Indeed, to read a file, a client must first retrievethe metadata for all folders in its access path, that may belocated on one or more metadata servers, to check that the userhas the correct access rights or to pinpoint the location of thedata in the system. The high cost of network communicationsignificantly exceeds the cost of reading the data itself.We advocate that a different file system architecture isnecessary to reduce the cost of metadata management forsuch workloads involving many small files. While one could",
        "publication_date": "2018-05-01",
        "authors": "Pierre Matri, Marı́a S. Pérez, Alexandru Costan, Gabriel Antoniu",
        "file_name": "10!1109%ccgrid!2018!00072.pdf",
        "file_path": "./PDFs/10!1109%ccgrid!2018!00072.pdf"
    },
    {
        "title": "Towards human-guided machine learning",
        "implementation_urls": [],
        "doi": "10.1145/3301275.3302324",
        "publication_date": "2019-02-19",
        "authors": "Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti P. Gadewar, Qifan Yang, Neda Jahanshad",
        "file_name": "10!1145%3301275!3302324.pdf",
        "file_path": "./PDFs/10!1145%3301275!3302324.pdf"
    },
    {
        "title": "Enhancing energy management at district and building levels via an EM-KPI ontology",
        "implementation_urls": [],
        "doi": "10.1016/j.autcon.2018.12.010",
        "abstract": "Abstract 1 The use of information and communication technologies facilitates energy management (EM) at 2 both district and building levels but also generates a considerable amount of data. To gain insights 3 into such data, it is essential to resolve the cross-domain data interoperability problem and 4 determine an approach to exchange performance information and insightful data amongst various 5 stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange 6 key performance information and data for districts and buildings. The ontology contains two 7 components: namely KPIs and EM master data; these, respectively, represent multi-level 8 performance information for energy performance tracking and the key data for data exploitation. 9 Through a demonstration, a sample linked dataset generated using the data correlation predefined 10 in the ontology is presented. The linked data analysis proves the feasibility of the ontology for 11 exchanging data among different stakeholders and for exploring insights in relation to 12 performance improvements.  13 Key words:  14 District; building; energy management; stakeholders; ontology; linked data. 15 1. Introduction 16 Buildings account for approximately 40% of the total final energy use in EU countries [1]. 17 However, a large portion of existing buildings are either designed or operated inefficiently [2]. 18 Energy management (EM) is a measure adopted to improve energy efficiency in buildings. 19 Furthermore, there is an increasing need to manage energy not only in a single building, but also 20 on a district scale [3]. Since the implementation of smart cities involves increasing distributed 21 electricity generation such as solar panels in energy distribution networks, EM at a district level, 22 for the purpose of combining the electricity supply and demand of buildings, is pivotal [4,5]. The 23 use of information and communication technologies (ICTs) facilitates the realisation of joint EM 24 that integrates the energy supply and demand sides.  25 Meanwhile, the use of ICTs also generates a massive amount of data and information, which could 26 provide new analysis possibilities for data-driven decision support and offer insights in relation 27 to potential performance improvement [6]. According to the National Institute of Standards and 28 Technology (NIST) in the United States, it could save up to $2 trillion in energy costs by 2030, 29 through exploiting the data from smart grids [7]. Although the expansion of data presents great 30 opportunities for energy performance improvement, there are still challenges faced in the effort 31 to make sense of this data. The problem is twofold. Firstly, there is an interoperability problem 32 between the cross-domain heterogeneous data. Secondly, the solution requires the extraction of 33 insightful data in order to avoid unnecessary analysis.  34 The extraction of core, insightful data is the primary challenge encountered when seeking to 35 access a large amount of data. Data exploitation is valuable only if they address the issues related 36 to the stakeholders. It is important to focus on data that is worth collecting, analyses which are 37 worth sharing and problems which are worth solving [8]. Master data offers a way to represent 38 key data that provides the most valuable information in an organisation [9]. In this case, master 39 data refers to the critical data objects that need to be shared across or beyond an organisation 40 which support decision-making. Master data was initially used for enterprise data management 41 due to the large volumes of data generated during business processes [10]. In the context of energy 42 management, a similar situation is encountered. Introducing the concept of master data into the 43 energy field can help make the large amount of energy-related data actionable, thus bringing 44 additional insight and value through improved decision-making.  45 The master data involved in EM should be shared among different stakeholders; therefore, it is 46 essential to support their performance concerns. In our previous study, we defined stakeholders 47 as those who have an interest in, who have influence in and who are impacted by the actions of 48 energy management; a detailed methodology was developed for selecting the KPIs (key 49 performance indicators) that underpin stakeholders’ performance goals; additionally, the use of 50 ",
        "publication_date": "2018-12-20",
        "authors": "Yehong Li, Raúl García‐Castro, Nandana Mihindukulasooriya, James O’Donnell, Sergio Vega-Sánchez",
        "file_name": "2019_YehongLi- EMKPI_OntologyForDistribution.pdf",
        "file_path": "./PDFs/2019_YehongLi- EMKPI_OntologyForDistribution.pdf"
    },
    {
        "title": "Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2019.03.001",
        "abstract": "ABSTRACT Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user’s knowledge about the data available.  We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements. CCS CONCEPTS • Human-centered computing KEYWORDS Human-guided machine learning; Automated machine learning (AutoML); Task analysis; Scientific workflows. ACM Reference format: Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti Gadewar, Qifan Yang and Neda Jahanshad. 2019. Towards Human-Guided Machine Learning. In 24th International Confer- ence on Intelligent User Interfaces (IUI ’19), March 17–20, 2019, Marina del Rey, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3301275.3302324 1 Introduction In recent years, Automated Machine Learning (AutoML) approaches are making great strides to automatically search for machine learning solutions from a large space of possible kinds of models.  Typically, a solution is created by choosing a model (e.g., random forest, SVM, etc.) and then configuring those models by assigning (hyper)parameter values [1]–[4]. A series of challenges and workshops have led to steadfast improvements [5]. Commercial products are now becoming available that automate machine learning, notably for image classification and select natural language processing tasks [6].  While fully automated model learning is appropriate for many applications, there are many contexts where full automation is not desirable or possible.  This is the case when users have knowledge that supplements the available data, particularly in a scientific research context. An AutoML system would look at a set of instances or images the same whether they are about tumor tissues or ad placements.  However, biologists would bring to bear extensive knowledge about human disease in the development of a machine learning model. Without this knowledge, machine learning ",
        "publication_date": "2019-03-18",
        "authors": "Mariano Rico, Daniel Vila-Suero, Iuliana Botezán, Asunción Gómez‐Pérez",
        "file_name": "10!1016%j!websem!2019!03!001.pdf",
        "file_path": "./PDFs/10!1016%j!websem!2019!03!001.pdf"
    },
    {
        "title": "On learning context-aware rules to link RDF datasets",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AndreaCimminoArriaga/Sorbas",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1093%jigpal%jzaa043.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "1Our prototype is available at https://github.com/AndreaCimminoArriaga/Sorbas."
                    }
                ]
            }
        ],
        "doi": "10.1093/jigpal/jzaa043",
        "abstract": "AbstractIntegrating RDF datasets has become a relevant problem for both researchers and practitioners. In the literature, there aremany genetic proposals that learn rules that allow to link the resources that refer to the same real-world entities, which isparamount to integrating the datasets. Unfortunately, they are context-unaware because they focus on the resources and theirattributes but forget about their neighbours. This implies that they fall short in cases in which different resources have similarattributes but refer to different real-world entities or cases in which they have dissimilar attributes but refer to the same real-world entities. In this article, we present a proposal that learns context-aware rules that take into account both the attributes ofthe resources and their neighbours. We have conducted an extensive experimentation that proves that it outperforms the mostadvanced genetic proposal. Our conclusions were checked using statistically sound methods.1 IntroductionAn RDF dataset is a collection of resources that describe real-world entities. Such datasets arecommonly used to feed a variety of automated business processes [5]. Typically, this requires tointegrate them by linking the resources that refer to the same real-world entities [1]. The resourcesare described by means of properties that can be either data properties or object properties; theformer model the attributes of the resources and the latter relate them to their neighbours.There are several state-of-the-art proposals that use genetic approaches to learn link rules [9, 10,16, 17]. Such rules basically compute the similarity of two resources by comparing their attributesusing a series of string transformations and similarity functions. If the resources are similar enough,then they are linked because they are assumed to describe the same real-world entity; otherwise, theyare kept apart. It is not difficult to realize that such context-unaware rules are imprecise in cases inwhich two resources have similar attributes but describe different real-world entities (e.g. differentpeople who have similar names or ages) or have dissimilar attributes but refer to the same real-worldentities (e.g. resources that describe different facets of a person).In this article, we present an approach to learn context-aware rules building on the context-unawarerules learnt by any of the previous proposals. By context-aware, we mean that the rule takes intoaccount the attributes of the resources being linked and the attributes of their neighbours. This is anovel approach since our analysis of the related work reveals that this is the first time that context-aware rules have been explored in this context. We have also performed an extensive experimental∗E-mail: cimmino@fi.upm.es∗∗E-mail: corchu@us.esstudy in which we sought to prove two hypothesis, namely, (i) exploring the context helps improve the effectiveness of the link rules and (ii) learning context-aware rules helps improve the efficiency of the linking process. Our experimental results and the statistical analysis that we have conducted validate these hypotheses, which prove that our proposal is very promising. Our proposal is related to a previous one in which our goal was to link two datasets on the f ly [3]; our experimentation confirms that our new proposal is as effective as the previous one but increases efficiency significantly.The rest of the article is organized as follows: Section 2 reports on the related work; Section 3 provides the details of our proposal; Section 4 presents our experimental analysis; finally, Section 5 summarizes our conclusions.2 Related workLearning link rules originated in the field of relational databases, where the problem was known as de-duplication [15], collective matching [18] or entity matching [13]. Unfortunately, it is not straightforward to adapt these results to RDF datasets because of the gap between the underlying data models.Some authors have developed a number of proposals that are specifically tailored to working with RDF datasets. Unfortunately, some of them work on a single dataset [8, 14] and others require the datasets to be modelled using OWL ontologies [4, 6, 7, 12], which hinders their general applicability. There are a few proposals that work on two RDF datasets without an explicit model [9, 10, 16, 17, 20], which makes them generally applicable. We analyse them below.Isele and Bizer [9, 10] devised GenLink, which is a genetic approach that uses a tournament ",
        "publication_date": "2020-08-06",
        "authors": "Andrea Cimmino, Rafael Corchuelo",
        "file_name": "10!1093%jigpal%jzaa043.pdf",
        "file_path": "./PDFs/10!1093%jigpal%jzaa043.pdf"
    },
    {
        "title": "A High-Level Ontology Network for ICT Infrastructures",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/devops-infra",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/2021_ISWC_DevOps_Infra.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3 https://github.com/oeg-upm/devops-infra 4 This data model is not available publicly for confidentiality reasons https://github.com/oeg-upm/devops-infra 6 Corcho et al."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-88361-4_26",
        "abstract": "Abstract. The ICT infrastructures of medium and large organisationsthat offer ICT services (infrastructure, platforms, software, applications,etc.) are becoming increasingly complex. Nowadays, these environmentscombine all sorts of hardware (e.g., CPUs, GPUs, storage elements, net-work equipment) and software (e.g., virtual machines, servers, microser-vices, services, products, AI models). Tracking, understanding and act-ing upon all the data produced in the context of such environments ishence challenging. Configuration management databases have been sofar widely used to store and provide access to relevant information andviews on these components and on their relationships. However, differentdatabases are organised according to different schemas. Despite existingefforts in standardising the main entities relevant for configuration man-agement, there is not yet a core set of ontologies that describes theseenvironments homogeneously, and which can be easily extended whennew types of items appear. This paper presents an ontology networkcreated with the purpose of serving as an initial step towards an homo-geneous representation of this domain, and which has been already usedto produce a knowledge graph for a large ICT company.Resource type: OntologyLicense: CC BY 4.0 InternationalDOI: 10.5281/zenodo.4701264URL: http://w3id.org/devops-infraKeywords: Configuration Management Database · Ontology Network· Knowledge Graph1 IntroductionMost ICT organisations (IT service providers, cloud providers, telecom industry,etc.) are witnessing, in recent years, the growing amount and interdependenciesof hardware and software components that they need to handle as part of their in-frastructure. Distinctions between hardware and software-related functionalitieshttp://w3id.org/devops-infra2 Corcho et al.are sometimes blurring due to virtualisation: some hardware items may now bevirtualised as software (e.g., virtual machines, DNSs). Terms like infrastructureas a service (IaaS), platform as a service (PaaS), software as a service (SaaS),etc., are now part of the ICT infrastructure jargon, and new terms are appear-ing (e.g., AI as a service -AIaaS-). This makes these environments increasinglydifficult to track and understand.Information about all these physical or virtual components has been tra-ditionally handled in several types of (often loosely interconnected) databases:configuration management databases (CMDB), IT Service Management (ITSM)systems, IT Asset Management (ITAM) databases and tools, etc. The first groupof databases (CMDBs) store and provide access to relevant information on thesecomponents and their relationships, providing organised views of configurationdata and dynamic views on their functioning. These databases have evolved inrecent years to reflect not only those components, but also the DevOps universeof technology and practices, including software configuration scripts, containers,cloud resources, etc. The second group (ITSM) is focused more exclusively onservice management KPIs. The latter (ITAM) is usually more static and pro-vide general information about the lifecycle of hardware components (purchaseinformation, suppliers, disposal, etc.). There are many other types of products,",
        "publication_date": "2021-01-01",
        "authors": "Óscar Corcho, David Chaves-Fraga, Jhon Toledo, Julián Arenas-Guerrero, Carlos Badenes-Olmedo, Mingxue Wang, Peng Hu, Nicholas Burrett, José Ferrater Mora, Puchao Zhang",
        "file_name": "2021_ISWC_DevOps_Infra.pdf",
        "file_path": "./PDFs/2021_ISWC_DevOps_Infra.pdf"
    },
    {
        "title": "TheyBuyForYou platform and knowledge graph: Expanding horizons in public procurement with open linked data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/data-sources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-210442.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The data collected from OpenOpps and OpenCorporates is openly available under the Open Database License (ODbl).23 It is available on GitHub24 in JSON format and is updated on a monthly basis."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210442",
        "abstract": "Abstract. Public procurement is a large market affecting almost every organisation and individual; therefore, governments needto ensure its efficiency, transparency, and accountability, while creating healthy, competitive, and vibrant economies. In this con-text, open data initiatives and integration of data from multiple sources across national borders could transform the procurementmarket by such as lowering the barriers of entry for smaller suppliers and encouraging healthier competition, in particular byenabling cross-border bids. Increasingly more open data is published in the public sector; however, these are created and main-tained in siloes and are not straightforward to reuse or maintain because of technical heterogeneity, lack of quality, insufficientmetadata, or missing links to related domains. To this end, we developed an open linked data platform, called TheyBuyForYou,consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border,and cross-lingual procurement knowledge graph. We developed advanced tools and services on top of the knowledge graph foranomaly detection, cross-lingual document search, and data storytelling. This article describes the TheyBuyForYou platform andknowledge graph, reports their adoption by different stakeholders and challenges and experiences we went through while creatingthem, and demonstrates the usefulness of Semantic Web and Linked Data technologies for enhancing public procurement.Keywords: Public procurement, knowledge graph, linked datas, open data, ontology*Corresponding author. E-mail: ahmet.soylu@oslomet.no.1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:ahmet.soylu@oslomet.nomailto:ocorcho@fi.upm.esmailto:cbadenes@fi.upm.esmailto:fyedro@fi.upm.esmailto:brian.elvesater@sintef.nomailto:till.lech@sintef.nomailto:dumitru.roman@sintef.nomailto:t.blount@soton.ac.ukmailto:matej.kovacic@ijs.simailto:matej.posinkovic@ijs.simailto:ian@spendnetwork.commailto:chris.taggart@opencorporates.commailto:elena.simperl@kcl.ac.ukmailto:ahmet.soylu@oslomet.nohttps://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-210442&domain=pdf&date_stamp=2021-09-03266 A. Soylu et al. / TheyBuyForYou platform and knowledge graph1. IntroductionThe market around public procurement is large enough so as to affect almost every single citizen and organisationacross a variety of sectors. For this reason, public spending has always been a matter of interest at local, regional, andnational levels, and even more so, in times of great austerity and increased public scrutiny. Primarily, governmentsneed to be efficient in delivering services, ensure transparency, prevent fraud and corruption, and build healthy andsustainable economies [4,20]. For example, in the European Union (EU), every year, over 250.000 public authoritiesspend around 2 trillion euros (about 14% of GDP) on the purchase of services, works, and supplies;1 while theOrganisation for Economic Co-operation and Development (OECD) estimates that more than 82% of fraud andcorruption cases remain undetected across all OECD countries [27] costing as high as 990 billion euros a year in theEU alone [40]. Moreover, small and medium-sized enterprises (SMEs) are often locked out of markets and restrictedby borders due to the high cost of obtaining the required information, where larger companies can absorb the cost.This leads to a tendency for governments to rely on monolithic suppliers without adequate competition to delivergood value for the taxpayers.The availability of good quality, open, and integrated procurement data, coming from multiple sources acrossnational borders, could alleviate the aforementioned challenges [16]. This includes government agencies assessingpurchasing options, companies exploring new business contracts and placing cross-border bids, and other parties(such as journalists, researchers, local communities, business associations, transparency activists, and individual",
        "publication_date": "2021-09-03",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Tom Blount, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": "10!3233%sw-210442.pdf",
        "file_path": "./PDFs/10!3233%sw-210442.pdf"
    },
    {
        "title": "TermitUp: Generation and enrichment of linked terminologies",
        "implementation_urls": [],
        "doi": "10.3233/sw-222885",
        "publication_date": "2022-05-27",
        "authors": "Patricia Martín-Chozas, Karen Vázquez-Flores, Pablo Calleja, Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%sw-222885.pdf",
        "file_path": "./PDFs/10!3233%sw-222885.pdf"
    },
    {
        "title": "Towards a Taxonomy of AI Risks in the Health Domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Pret-a-LLOD/termitup",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%transai54797!2022!00007.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "TermitUp is available in a public GitHub repository,46 as a Python project licensed under Apache License 2.0 terms."
                    }
                ]
            }
        ],
        "doi": "10.1109/transai54797.2022.00007",
        "abstract": "Abstract. Domain-specific terminologies play a central role in many language technology solutions. Substantial manual effortis still involved in the creation of such resources, and many of them are published in proprietary formats that cannot be easilyreused in other applications. Automatic term extraction tools help alleviate this cumbersome task. However, their results areusually in the form of plain lists of terms or as unstructured data with limited linguistic information. Initiatives such as theLinguistic Linked Open Data cloud (LLOD) foster the publication of language resources in open structured formats, specificallyRDF, and their linking to other resources on the Web of Data. In order to leverage the wealth of linguistic data in the LLODand speed up the creation of linked terminological resources, we propose TermitUp, a service that generates enriched domainspecific terminologies directly from corpora, and publishes them in open and structured formats. TermitUp is composed offive modules performing terminology extraction, terminology post-processing, terminology enrichment, term relation validationand RDF publication. As part of the pipeline implemented by this service, existing resources in the LLOD are linked with theresulting terminologies, contributing in this way to the population of the LLOD cloud. TermitUp has been used in the frameworkof European projects tackling different fields, such as the legal domain, with promising results. Different alternatives on how tomodel enriched terminologies are considered and good practices illustrated with examples are proposed.Keywords: Terminology generation, terminology enrichment, linguistic linked data, Multilingualism1. IntroductionInternational institutions have become major producers of multilingual terminology databases, understood asresources that account for the specialised words used in a particular field in multiple languages. Since its foundation,the European Union has maintained initiatives to cater for the collection, maintenance and creation of terminologies,thesauri or vocabularies, to cover their internal communication needs and to support translators. Some of the bestknown resources are available from TermCoord1 (Terminology Coordination Unit of the European Parliament), incharge of the interinstitutional terminology database IATE2 (InterActive Terminology for Europe) since 2004, or the*Corresponding author. E-mail: pmchozas@fi.upm.es.1https://termcoord.eu/2https://iate.europa.eu/1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:pmchozas@fi.upm.esmailto:kvazquez@delicias.dia.fi.upm.esmailto:pcalleja@fi.upm.esmailto:emontiel@fi.upm.esmailto:vrodriguez@fi.upm.esmailto:pmchozas@fi.upm.eshttps://termcoord.eu/https://iate.europa.eu/https://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-222885&domain=pdf&date_stamp=2022-05-26968 P. Martín-Chozas et al. / TermitUp: Generation and enrichment of linked terminologiesFig. 1. Motivating scenario for the development of TermitUp.EU Vocabularies site,3 maintained by the Publications Office, that is also in charge of the upkeep of the multilingualthesaurus EuroVoc.4The creation and curation of such vocabularies has not only supported translators, documentalists and legaldrafters at EU institutions, but has also become a reference for translators and language professionals outside theEU. Nowadays, curated language resources have proven to be more relevant than ever in light of natural languageprocessing (NLP) tasks that rely on sound linguistic data. For example, query expansion using WordNet,5 the well-known English lexicon [49], disambiguation based on BabelNet,6 a multilingual encyclopedic dictionary [46] andtext classification applying DBpedia,7 the semantically structured version of the Wikipedia [23], to mention but afew.Initiatives such as the Linguistic Linked Open Data cloud8 (henceforward LLOD) are focused on collecting andpublishing language resources in Semantic Web formats according to the Linked Data principles [7]. When develop-ing NLP services, one of the main challenges is finding language resources on a certain subject area with acceptable",
        "publication_date": "2022-09-01",
        "authors": "Delaram Golpayegani, Joshua Hovsha, Leon Rossmaier, Rana Saniei, Jana Mišić",
        "file_name": "10!1109%transai54797!2022!00007.pdf",
        "file_path": "./PDFs/10!1109%transai54797!2022!00007.pdf"
    },
    {
        "title": "Trusting Decentralised Knowledge Graphs and Web Data at the Web Conference",
        "implementation_urls": [],
        "doi": "10.1145/3543873.3589756",
        "abstract": "ABSTRACT Knowledge Graphs have become a foundation for sharing data on the web and building intelligent services across many sectors and also within some of the most successful corporations in the world. The over centralisation of data on the web, however, has been raised as a concern by a number of prominent researchers in the feld. For example, at the beginning of 2022 a € 2.7B civil lawsuit was launched against Meta on the basis that it has abused its market dominance to impose unfair terms and conditions on UK users in order to exploit their personal data. Data centralisation can lead to a number of problems including: lock-in/siloing efects, lack of user control over their personal data, limited incentives and opportuni-ties for interoperability and openness, and the resulting detrimental efects on privacy and innovation. A number of diverse approaches and technologies exist for decentralising data, such as federated querying and distributed ledgers. The main question is, though, what does decentralisation really mean for web data and Knowl-edge Graphs? What are the main issues and tradeofs involved? These questions and others are addressed in this workshop. CCS CONCEPTS • Web Data Description Languages; • Query languages for non-relational engines; • Privacy protections; KEYWORDS Knowledge Graphs, Web Data, Decentralisation, Trust, Distributed Ledgers, Personal Data Stores, Personal Data Policy, Federated Querying Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWW ’23 Companion, April 30–May 04, 2023, Austin, TX, USA © 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9419-2/23/04. https://doi.org/10.1145/3543873.3589756 ACM Reference Format: John Domingue, Aisling Third, Maria-Esther Vidal, Philipp D. Rohde, Juan Cano, Andrea Cimmino, and Ruben Verborgh. 2023. Trusting Decentralised Knowledge Graphs and Web Data at the Web Conference. In Companion Proceedings of the ACM Web Conference 2023 (WWW ’23 Companion), April 30–May 04, 2023, Austin, TX, USA. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3543873.3589756 “We demonstrated that the Web had failed instead of served humanity, as it was supposed to have done, and failed in many places,” he told me. The increas-ing centralization of the Web, he says, has “ended up producing—with no deliberate action of the people who designed the platform—a large-scale emergent phenomenon which is anti-human.”1 Since its beginnings in the Semantic Web [1] the application and ",
        "publication_date": "2023-04-28",
        "authors": "John Domingue, Allan Third, María-Esther Vidal, Philipp D. Rohde, Juan Cano-Benito, Andrea Cimmino, Ruben Verborgh",
        "file_name": "3543873.3589756.pdf",
        "file_path": "./PDFs/3543873.3589756.pdf"
    },
    {
        "title": "Prediction and Decision-Making in Intelligent Environments Supported by Knowledge Graphs, A Systematic Review",
        "implementation_urls": [],
        "doi": "10.3390/s19081774",
        "abstract": "Abstract: Ambient Intelligence is currently a lively application domain of Artificial Intelligenceand has become the central subject of multiple initiatives worldwide. Several approaches insidethis domain make use of knowledge bases or knowledge graphs, both previously existing and adhoc. This form of representation allows heterogeneous data gathered from diverse sources to becontextualized and combined to create relevant information for intelligent systems, usually followinghigher level constraints defined by an ontology. In this work, we conduct a systematic review of theexisting usages of knowledge bases in intelligent environments, as well as an in-depth study of thepredictive and decision-making models employed. Finally, we present a use case for smart homesand illustrate the use and advantages of Knowledge Graph Embeddings in this context.Keywords: knowledge base; knowledge graph; intelligent environment; ambient intelligence;reasoning model; knowledge graph embedding1. IntroductionAccording to Augusto et al.: “An Intelligent Environment is one in which the actions of numerousnetworked controllers is orchestrated by self-programming pre-emptive processes in such a way asto create an interactive holistic functionality that enhances occupants experiences” [1]. Although inthis particular context the term “environment” is popularly associated with homes, it also encompassesbroader scenarios, such as buildings, streets or other areas. Intelligent environments are technologicallybased on the combination of several socio-technical innovations such as the Internet of Things (IoT),mobile Internet access, smartphones, data analytics, open data initiatives, and sharing economymodels [2]. These advances allow intelligent environments to manage assets and resources efficientlyby services enhanced with intelligence such as traffic management or healthcare systems.Developing responsive and smarter environments is one of the main present objectives, as shownby the number of research projects developed to pursue this goal, such as Km4City [3] or RoomPathy [4].Although there exists a considerable heterogeneity among the existing works in terms of objectives,methods, and areas of application, the use of knowledge bases (KBs) or knowledge graphs (KGs) is inthe core of a large number of these works.KBs play a key role in multiple ambient intelligence applications, as they are an essential part ofthe conversion of heterogeneous, numerical data provided by sensors into contextualized and semanticinformation. The transformation procedure is usually performed using ontologies, which enable theSensors 2019, 19, 1774; doi:10.3390/s19081774 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-6838-1266https://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/0000-0002-0792-4156https://orcid.org/0000-0001-9461-7922http://www.mdpi.com/1424-8220/19/8/1774?type=check_update&version=1http://dx.doi.org/10.3390/s19081774http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticlePrediction and Decision-Making in IntelligentEnvironments Supported by Knowledge Graphs,A Systematic Review1 1,Elvira Amador-Dominguez *, Emilio Serrano , Daniel Manrique 7© and Juan F. De Paz?1 Ontology Engineering Group, Department of Artificial Intelligence, ETSI Informaticos,Universidad Politécnica de Madrid, 28660 Madrid, Spain; eamador@fi.upm.esArtificial Intelligence Lab, Department of Artificial Intelligence, ETSI Informaticos,Universidad Politécnica de Madrid, 28660 Madrid, Spain; dmanrique@fi.upm.es",
        "publication_date": "2019-04-13",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Juan F. De Paz",
        "file_name": "no_doi_20250624162016.pdf",
        "file_path": "./PDFs/no_doi_20250624162016.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/19/8/1774/pdf?version=1555141210"
    },
    {
        "title": "ODRL Profile for Expressing Consent through Granular Access Control Policies in Solid",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/odrl-access-control-profile",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/ODRL_Profile_for_Expressing_Consent_through_Granular_Access_Control_Policies_in_Solid.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "These examples, as well as the profile serialisation, can be accessed at https://github.com/besteves4/odrl-access-control-profile."
                    }
                ]
            }
        ],
        "doi": "10.1109/eurospw54576.2021.00038",
        "abstract": "Abstract—Solid, the emerging technology for organizing datain decentralized stores, relies on a simple authorization mecha-nism for granting access to data. Solid’s personal online datas-tores (Pods) are ideal for keeping personal data, as they allowindividuals to represent the access permissions in a very simplemanner using Access Control Language (ACL) expressions.Whereas these expressions suffice for yes/no and read/write per-missions, they cannot represent more complex rules nor invokeregulation-specific concepts. This paper describes an extensionof the ACL language and algorithm to implement consent anddata requests. The extension is based on the Open Digital RightsLanguage (ODRL) policy language, which allows expressing richrules, and the Data Privacy Vocabulary (DPV), which permitsinvoking privacy and data protection-specific terms. Some usageexamples illustrate this proposal.Index Terms—access control, consent, data protection, decen-tralized datastores, privacy, DPV, GDPR, regulatory complianceI. INTRODUCTIONSolid is a specification1 for decentralised personal datastores (called ‘Pods’) based on the tenets of user controland interoperability using linked data and web standards.Access to data stored within Pods is governed by the accesscontrol specification Web Access Control2 (WAC) which usesInternationalized Resource Identifiers (IRIs) to represent re-sources and agents, and stores authorisation statements withinan Access Control List (ACL) defined per resource or inheritedfrom the parent resource within the IRI. Currently, WACsupports four operations: read, write, append, and control (ofACLs) which can be declared for specific agents or trustedapps, and is interpreted as a prohibition by default unless thereis an ACL authorisation permitting it. As per Solid, the usercontrolling the ACL is the entity responsible for deciding whohas access to the data. In this manner, Solid intends to provideand standardise implementations for users to store their owndata and have control over who they wish to share it with.Given that Solid provides a way for personal data to bestored and managed by individuals, it is important to considerthe impact and application of data protection and privacy lawssuch as the European General Data Protection Regulation31https://solidproject.org/TR/protocol2https://solid.github.io/web-access-control-spec/3https://eur-lex.europa.eu/eli/reg/2016/679/(GDPR) that defines specific concepts and obligations for howpersonal data can be collected, stored, used, and shared. Inaddition, such laws also specify requirements for provisionand validity of legal bases such as consent used to justifyprocessing of data. While we focus on GDPR in this article,the argument applies to other existing and emerging lawsfollowing similar trends4.GDPR requires controllers to provide information such as",
        "publication_date": "2021-09-01",
        "authors": "Beatriz Esteves, Harshvardhan J. Pandit, Victor Rodrı́guez-Doncel",
        "file_name": "ODRL_Profile_for_Expressing_Consent_through_Granular_Access_Control_Policies_in_Solid.pdf",
        "file_path": "./PDFs/ODRL_Profile_for_Expressing_Consent_through_Granular_Access_Control_Policies_in_Solid.pdf"
    },
    {
        "title": "Students’ Evaluation of a Virtual World for Procedural Training in a Tertiary-Education Course",
        "implementation_urls": [],
        "doi": "10.1177/0735633117706047",
        "abstract": "Abstract This article presents an investigation on the educational value of virtual worlds intended for the acquisition of procedural knowledge. This investigation takes as a case of study a virtual laboratory on biotechnology. A remarkable feature in this virtual laboratory is an automatic tutor that supervises student’s actions and provides tutoring feedback when it is appropriate. The study presented in this article covers two different aspects of the system. First, it analyzes the impact of this virtual world in learning some concepts related to a biotechnology practice; and second, it surveys the students’ opinion on the virtual world by means of three open questions (posi­tive, negative, and general impressions). Results demonstrated that the virtual world had a positive influence in the students’ knowledge, and it was well received by them. 1School of Computer Engineering, UPM, Spain 2Department of Artificial Intelligence, UPM, Spain 3MONTES (School of Forest Engineering and Natural Environment), UPM, Spain Corresponding Author: Jaime Ram ı́rez, Escuela Sup. de Ing. Informa t́icos, Calle de los Ciruelos Montegancedo Campus, Universidad Polite´cnica de Madrid, Building 5, Floor 1, Office 5112, 28660 Boadilla del Monte, Madrid, Spain. Email: jramirez@fi.upm.es There are many examples in the literature that show how interactive simulations are successfully employed for educational purposes. Among these interactive simulations, three-dimensional (3D) virtual environments represent a group of applications that each day is attracting more and more the interest of the edu­cational community. This is because these systems are engaging and allow students to learn new concepts and procedures by recreating situations that in the real world would be too expensive, dangerous, or simply out of reach for the students. We can see 3D virtual environments as a tool for implementing the ‘‘learning by doing’’ approach derived from the constructivist pedagogy (Huang, Rauch, & Liaw, 2010). When several users can visit a shared 3D virtual environment through Internet and interact by means of desktop devices, this 3D virtual environment is usually referred to as a virtual world. In addition to the benefits of 3D simulations, virtual worlds also provide the possibility of reproducing real scen­arios where several people (students or teachers) are able to chat and share rooms, instruments, and so forth. Moreover, as students connect to virtual worlds through Internet, they can perform virtual practices at home, without the physical presence of a teacher close to them. This opens the door to virtual practices from distant places where sufficiently equipped real labs are not available. In recent years, the development of 3D virtual worlds has become more inex­pensive thanks to platforms such as Second Life (http://secondlife.com/), OpenSimulator (OS; http://opensimulator.org/), Open Wonderland (http://open-wonderland.org/), or Bitmanagement Software Collaborate System (http://www. bitmanagement.com/), which provide developers with a basic layer of function­ality. As a result of this, more academic institutions have been able to develop and use their own virtual worlds. This article presents a study on the educational value of a virtual world for procedural learning. In the literature on virtual worlds, we can find many works on how this kind of systems can support different learning activities. However, there are just a few works that show how this kind of systems can support task ",
        "publication_date": "2017-05-08",
        "authors": "Jaime Ramírez, Mariano Rico, Diego Riofrío-Luzcando, Marta Berrocal‐Lobo, Angélica de Antonio",
        "file_name": "10!1177%0735633117706047.pdf",
        "file_path": "./PDFs/10!1177%0735633117706047.pdf"
    },
    {
        "title": "Models to represent linguistic linked data",
        "implementation_urls": [],
        "doi": "10.1017/s1351324918000347",
        "abstract": "AbstractAs the interest of the Semantic Web and computational linguistics communities in linguisticlinked data (LLD) keeps increasing and the number of contributions that dwell on LLDrapidly grows, scholars (and linguists in particular) interested in the development of LLDresources sometimes find it difficult to determine which mechanism is suitable for their needsand which challenges have already been addressed. This review seeks to present the state ofthe art on the models, ontologies and their extensions to represent language resources as LLDby focusing on the nature of the linguistic content they aim to encode. Four basic groupsof models are distinguished in this work: models to represent the main elements of lexicalresources (group 1), vocabularies developed as extensions to models in group 1 and ontologiesthat provide more granularity on specific levels of linguistic analysis (group 2), cataloguesof linguistic data categories (group 3) and other models such as corpora models or service-oriented ones (group 4). Contributions encompassed in these four groups are described, high-lighting their reuse by the community and the modelling challenges that are still to be faced.1 Introduction1.1 Background and motivationLanguage resources (dictionaries, terminologies, corpora, etc.) developed in the fieldsof corpus linguistics, computational linguistics and natural language processing(NLP) are often encoded in heterogeneous formats and developed in isolation fromone another. This makes their discovery, reuse and integration for both the develop-ment of NLP tools and daily linguistic research a difficult and cumbersome task. In∗We are very grateful to the anonymous reviewers for their meticulous reading of the surveyand for providing us with numerous insightful and constructive suggestions to improve it.We would also like to thank Dr Guadalupe Aguado-de-Cea for her help in proofreadingthis manuscript. This work is supported by the Spanish Ministry of Education, Cultureand Sports through the Formación del Profesorado Universitario (FPU) program, and bythe Spanish Ministry of Economy and Competitiveness through the project 4V (TIN2013-46238-C4-2-R) within the FEDER funding scheme, the Juan de la Cierva program, andthe Excellence Network ReTeLe (TIN2015-68955-REDT).https://doi.org/10.1017/S1351324918000347 Published online by Cambridge University Presshttp://orcid.org/0000-0001-6433-4649https://doi.org/10.1017/S1351324918000347812 J. Bosque-Gil et al.order to alleviate such an issue and to enhance interoperability of language resourceson the Web, a community of language technologies experts and practitioners hasstarted adopting techniques coming from the area of study of linked data (LD).The LD paradigm emerges as a series of best practices and principles for ‘exposing,sharing and connecting data on the Web’ (Bizer, Heath and Berners-Lee 2011),independently of the domain. These principles state that unique resource identifiersshould be used to name things in a way that allows people to look them up, finduseful information represented with standard formalisms and discover more thingsthat are linked to those resources. LD emerged in the context of the SemanticWeb, an extension of the Web ‘in which information is given well-defined meaning,better enabling computers and people to work in cooperation’ (Berners-Lee et al.2001). General and domain-specific on-line ontologies (such as the ones we revisitin this survey) provide this well-defined meaning on the Semantic Web and areused to represent the data that will be linked to other data. Following this line,an ontology is properly defined as a ‘formal, explicit specification of a sharedconceptualisation’ (Studer, Benjamins and Fensel 1998). This definition refers to anontology formalised as a machine-readable, explicit description of a domain common",
        "publication_date": "2018-10-04",
        "authors": "Julia Bosque-Gil, Jorge Gracia, Elena Montiel-Ponsoda, Asunción Gómez‐Pérez",
        "file_name": "10!1017%s1351324918000347.pdf",
        "file_path": "./PDFs/10!1017%s1351324918000347.pdf"
    },
    {
        "title": "Decentralized Personal Data Marketplaces: How Participation in a DAO Can Support the Production of Citizen-Generated Data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iotaledger/streams",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162047.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/iotaledger/streams/blob/develop/specification/Streams_Specification_1_0A.pdf (accessed on 24 May 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/s22166260",
        "abstract": "Abstract: Big Tech companies operating in a data-driven economy offer services that rely on theirusers’ personal data and usually store this personal information in “data silos” that prevent trans-parency about their use and opportunities for data sharing for public interest. In this paper, we presenta solution that promotes the development of decentralized personal data marketplaces, exploiting theuse of Distributed Ledger Technologies (DLTs), Decentralized File Storages (DFS) and smart contractsfor storing personal data and managing access control in a decentralized way. Moreover, we focus onthe issue of a lack of efficient decentralized mechanisms in DLTs and DFSs for querying a certain typeof data. For this reason, we propose the use of a hypercube-structured Distributed Hash Table (DHT)on top of DLTs, organized for efficient processing of multiple keyword-based queries on the ledgerdata. We test our approach with the implementation of a use case regarding the creation of citizen-generated data based on direct participation and the involvement of a Decentralized AutonomousOrganization (DAO). The performance evaluation demonstrates the viability of our approach fordecentralized data searches, distributed authorization mechanisms and smart contract exploitation.Keywords: distributed ledger technology; decentralized file storage; distributed hash table; datamarketplace; keyword-based search; citizen-generated data1. IntroductionRecent scandals have shown the harm that current data collection, storage and sharingpractices can cause with regard to the misuse of personal data [1,2]. As the world isbecoming more “smart”, so-called smart environments, of which smart cities [3] stand outthe most, have in common the ability to transform data (in particular, personal data) intomeaningful information needed by the liveness of the ecosystem they generate. Basedon this transformation, indeed, they provide services that are becoming more and moretargeted towards individuals. For instance, it is commonly known that personal informationis used to recommend opportunities to individuals and to make their life easier. However,entities that control these data might not always operate with the aim of social good [4].Many Big Tech companies rely on data collected about their users, usually storing thispersonal information in corporate databases, i.e., data silos, and transacting it to thirdparties with not enough transparency for individuals.Meanwhile, among the many technologies used for general-purpose data manage-ment and storage, Distributed Ledger Technologies (DLTs) are rising up as powerful toolsfor avoiding control centralization. DLT and the realm of decentralized systems, suchas Decentralized File Storages (DFS), that are emerging as solutions able to tackle theissue of obtaining large amounts of data that are not of dubious or of false origin, whileproviding more disintermediated processes [5,6]. DLTs, in this context, provide a new wayof handling personal data, such as recording, storage and transfer. This can be carriedout in combination with cryptographic schemes to ensure data confidentiality. By theirSensors 2022, 22, 6260. https://doi.org/10.3390/s22166260 https://www.mdpi.com/journal/sensorshttps://doi.org/10.3390/s22166260https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/sensorshttps://www.mdpi.comhttps://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0003-1076-2511https://doi.org/10.3390/s22166260https://www.mdpi.com/journal/sensorshttps://www.mdpi.com/article/10.3390/s22166260?type=check_update&version=2Sensors 2022, 22, 6260 2 of 32",
        "publication_date": "2022-08-20",
        "authors": "Mirko Zichichi, Stefano Ferretti, Victor Rodrı́guez-Doncel",
        "file_name": "no_doi_20250624162047.pdf",
        "file_path": "./PDFs/no_doi_20250624162047.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/22/16/6260/pdf?version=1661238706"
    },
    {
        "title": "Using machine learning to optimize parallelism in big data applications",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2017.07.003",
        "abstract": "AbstractIn-memory cluster computing platforms have gained momentum in the last years, due to their ability toanalyse big amounts of data in parallel. These platforms are complex and difficult-to-manage environments.In addition, there is a lack of tools to better understand and optimize such platforms that consequentlyform backbone of big data infrastructure and technologies. This directly leads to underutilization of avail-able resources and application failures in such environment. One of the key aspects that can address thisproblem is optimization of the task parallelism of application in such environments. In this paper, wepropose a machine learning based method that recommends optimal parameters for task parallelization inbig data workloads. By monitoring and gathering metrics at system and application level, we are able tofind statistical correlations that allow us to characterize and predict the effect of different parallelism set-tings on performance. These predictions are used to recommend an optimal configuration to users beforelaunching their workloads in the cluster, avoiding possible failures, performance degradation and wastageof resources. We evaluate our method with a benchmark of 15 Spark applications on the Grid5000 testbed.We observe up to a 51% gain on performance when using the recommended parallelism settings. The modelis also interpretable and can give insights to the user into how different metrics and parameters affect theperformance.Keywords: machine learning, spark, parallelism, big data1. IntroductionBig data technology and services market is esti-mated to grow at a CAGR1 of 22.6% from 2015 to2020 and reach $58.9 billion in 2020 [1]. Highly vis-ible early adopters such as Yahoo, eBay and Face-book have demonstrated the value of mining com-plex information sets, and now many companies areeager to unlock the value in their own data. In orderto address big data challenges, many different par-allel programming frameworks, like Map Reduce,Apache Spark or Flink have been developed [2, 3, 4].Planning big data processes effectively on theseplatforms can become problematic. They involvecomplex ecosystems where developers need to dis-cover the main causes of performance degradationin terms of time, cost or energy. However, process-ing collected logs and metrics can be a tedious andEmail address: abrandon@fi.upm.es (Álvaro BrandónHernández)1Compound Annual Growth Ratedifficult task. In addition, there are several param-eters that can be adjusted and have an importantimpact on application performance.While users have to deal with the challenge ofcontrolling this complex environment, there is afundamental lack of tools to simplify big data in-frastructure and platform management. Some toolslike YARN or Mesos [5, 6] help in decoupling theprogramming platform from the resource manage-ment. Still, they don’t tackle the problem of opti-mizing application and cluster performance.One of the most important challenges is findingthe best parallelization strategy for a particular ap-",
        "publication_date": "2017-07-17",
        "authors": "Álvaro Brandón, Marı́a S. Pérez, Smrati Gupta, Víctor Muntés-Mulero",
        "file_name": "Optimising_Parallelism_Using_Machine_Learning.pdf",
        "file_path": "./PDFs/Optimising_Parallelism_Using_Machine_Learning.pdf"
    },
    {
        "title": "Systematic Construction of Knowledge Graphs for Research-Performing Organizations",
        "implementation_urls": [
            {
                "identifier": "https://github.com/RMLio/rmlmapper-java",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162110.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/RMLio/rmlmapper-java (accessed on 1 October 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info13120562",
        "abstract": "Abstract: Research-Performing Organizations (e.g., research centers, universities) usually accumulatea wealth of data related to their researchers, the generated scientific results and research outputs, andpublicly and privately-funded projects that support their activities, etc. Even though the types of datahandled may look similar across organizations, it is common to see that each institution has developedits own data model to provide support for many of their administrative activities (project reporting,curriculum management, personnel management, etc.). This creates obstacles to the integration andlinking of knowledge across organizations, as well as difficulties when researchers move from oneinstitution to another. In this paper, we take advantage of the ontology network created by the SpanishHERCULES initiative to facilitate the construction of knowledge graphs from existing informationsystems, such as the one managed by the company Universitas XXI, which provides support tomore than 100 Spanish-speaking research-performing organizations worldwide. Our effort is notjust focused on following the modeling choices from that ontology, but also on demonstrating howthe use of standard declarative mapping rules (i.e., R2RML) guarantees a systematic and sustainableworkflow for constructing and maintaining a KG. We also present several real-world use casesin which the proposed workflow is adopted together with a set of lessons learned and generalrecommendations that may also apply to other domains. The next steps include researching in theautomation of the creation of the mapping rules, the enrichment of the KG with external sources, andits exploitation though distributed environments.Keywords: knowledge graph; research-performing organizations; declarative mapping rules1. IntroductionResearch-performing organizations such as universities and research centers collectand accumulate a large amount of data related to their activities (e.g., scientific results,project outputs, academic courses, etc.). Although there are some common informationmodels (e.g., EuroCRIS [1]), in this domain it is common practice for each of these organiza-tions to develop their own information system to support all their activities. This fact has anegative impact on integrating and exploiting knowledge across institutions and also makesit difficult for researchers to manage their data when they move from one organizationto another. The effectiveness of semantic web technologies and knowledge graphs [2] forcomplex data management tasks has already been demonstrated in several domains [3–5],by companies (e.g., Google [6], Amazon [7]) and public communities (e.g., DBpedia [8],Wikidata [9]).HERCULES is a multi-anual project, promoted by the main Spanish association ofuniversities (CRUE) [10], that aims to build a semantic layer to harmonize the knowl-edge and data of the information systems of Spanish research-performing organizations.The main objective is to address data interoperability problems across organizations interms of schemes and formats, ensuring efficient exploitation of combined knowledge.Information 2022, 13, 562. https://doi.org/10.3390/info13120562 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13120562https://doi.org/10.3390/info13120562https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/info13120562https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13120562?type=check_update&version=1Information 2022, 13, 562 2 of 14",
        "publication_date": "2022-11-30",
        "authors": "David Chaves-Fraga, Óscar Corcho, Francisco Yedro Martínez, Roberto Moreno, Juan Olías, Alejandro De La Azuela",
        "file_name": "no_doi_20250624162110.pdf",
        "file_path": "./PDFs/no_doi_20250624162110.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/13/12/562/pdf?version=1669804108"
    },
    {
        "title": "Ontological Model for the Semantic Description of Syllabuses",
        "implementation_urls": [],
        "doi": "10.1145/3357419.3357442",
        "abstract": "ABSTRACT The syllabus is a relevant document to organize how the teaching-learning process will be carried out during an academic course in Higher Education Institutions (HEI). Usually, this document is written in a human-readable format that do not enable automatic processing through intelligent services to support teaching and learning. Therefore, we created OntoSyllabus ontology for the representation of syllabuses applying the NeOn methodology. The semantic model of a syllabus will allow the comprehension for both: machines and humans, and it will facilitate the interchange of data between different services and applications. The ontology was created based on the results of our three previous studies, which helped us to determinate the terms and relations in the syllabus ontology. The documentation and the computable model are available on the Internet for their reuse.   CCS Concepts • CCS →  Computing methodologies →  Artificial intelligence →  Knowledge representation and reasoning →  Ontology engineering Keywords Higher Education Institution; Ontology; NeOn Methodology; Semantic Web; Syllabus. 1. INTRODUCTION Higher Education Institutions (HEI) should be an example of the use of Information and Communication Technologies [1]. This kind of institutions generates a vast amount of information related to teaching, learning, or educational processes. For that reason, it is necessary to use semantic technologies like ontologies to facilitate communication, storing, searching, and knowledge sharing. The syllabus is a relevant document, generally elaborated by professors, to organize how the teaching-learning process will be carried out during an academic course [2]. The syllabus usually is written in different format files like PDF, DOC, or HTML. Those are human-readable formats that do not allow the automatic processing through intelligent services focused on support teaching and learning such as syllabus matching and interlinking, syllabus versioning and history tracking, syllabus recommendation, syllabus-based adaptive learning pathway, semantic searching and browsing of syllabus knowledge bases [3], among others. In this paper, we explain a systematic process that carried out to develop an ontological model for the semantic description of syllabuses named OntoSyllabus. We used the NeOn methodology  and the waterfall life-cycle model. Finally, the syllabus ontology model and its documentation were available in GitHub site to make it accessible for anyone that need to reuse it1. This paper is structured as follows. Section 2 briefly explains the previous work and the NeOn methodology. Section 3 describes the syllabus ontology creation according to NeOn. Section 4 ",
        "publication_date": "2019-08-23",
        "authors": "Mariela Tapia-León, Carlos Aveiga, Janneth Chicaiza, Mari Carmen Suárez-Figueroa",
        "file_name": "3357419.3357442.pdf",
        "file_path": "./PDFs/3357419.3357442.pdf"
    },
    {
        "title": "Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific Publications",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2012.01953",
        "publication_date": "2020-01-01",
        "authors": "Carlos Badenes-Olmedo, David Chaves-Fraga, María Poveda‐Villalón, Ana Iglesias-Molina, Pablo Calleja, Socorro Bernardos, Patricia Martín-Chozas, Alba Fernández-Izquierdo, Elvira Amador-Domínguez, Paola Espinoza-Arias, Luis Pozo-Gilo, Edna Ruckhaus, Esteban González-Guardia, R. Cedazo, Beatriz López Centeno, Óscar Corcho",
        "file_name": "arxiv.2012.01953",
        "file_path": "10.48550/arxiv.2012.01953"
    },
    {
        "title": "Reuse and Reengineering of Non-ontological Resources in the Legal Domain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_24",
        "abstract": "Abstract. Instead of custom-building a new ontology from scratch, knowledgeresources can be elicited, reused and engineered to develop legal ontologies withthe goal of promoting the application of good practices and speeding up theontology development process. This paper focuses on the specificities of non-ontological resources in the legal domain, and provides some guidelines of howthese can be reused and engineered to enable heterogeneous resources inte-gration within a legal ontology. The paper presents some examples of theseprocesses using a case-study in the consumer law domain.Keywords: Ontology engineering � Non-ontological resourcesLegal ontology � MeLOn methodology � NeOn methodology1 IntroductionInstead of custom-building a new legal ontology from scratch, knowledge resources areelicited from the legal domain, reused and engineered to develop legal ontologies1,promoting the application of good practices.Knowledge resources have been classified as ontological resources (ORs) or non-ontological resources (henceforth named NORs) [1]. This division regards the level offormalization. We will focus on the latter type. There is much literature for reusing andreengineering ORs [2, 3] and also ontology design patterns, but little about extractingknowledge from NORs in the legal domain, probably due to its specificities, delved in1 Ontologies are the chosen artifact to support the integration of data from multiple, heterogeneouslegal sources, making information explicit and enabling the sharing of a common understanding of adomain.© Springer Nature Switzerland AG 2018U. Pagallo et al. (Eds.): AICOL VI-X 2015–2017, LNAI 10791, pp. 350–364, 2018.https://doi.org/10.1007/978-3-030-00178-0_24http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_24&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_24&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_24&amp;domain=pdfthis paper. This subject is relevant as it has consequences at different levels, fromknowledge acquisition, to ontology engineering.There is a large amount of NORs that embody knowledge in the legal domain, thatrepresent some degree of consensus for the legal community and possess relatedsemantics that allows interpreting the knowledge contained therein. In fact, within thisdomain, NORs may correspond to some legal sources which consist on legislation, butalso other relevant sources of e.g., case law, doctrinal interpretations, social rules; it isessential to connect this existing legal material to the ontology, even if its majority isnot formalised, and hence not necessarily interoperable. NORs from this realm can beembedded in different and scattered sources of hard and soft law, such as classificationschemes, thesauri, lexicons2, textual corpora, among others, in a “patchwork” of “lego”pieces. The heterogeneity of the legal sources is observed at multiples levels: structural,semantic, and syntactic. To integrate information from multiple and heterogeneousknowledge sources, it is important to cope with the problem of legal knowledge rep-resentation, that consists in the balance between consensus and authoritativeness3 [4]or, from the socio-legal perspective, dialogue and bindingness [5].On the one hand, domain legal experts lack competencies in data modeling, andthey often adopt technical tools (e.g., Protégé) without the necessary awareness of thetechnical consequences [6]. On the other hand, ontology developers, besides the datamodeling perspective, should consider likewise compliance with the specificities of thejuristic nature of legal NORs and of expert knowledge. A balanced combination wouldyield reliable actionable knowledge in a real world context, for a thorough under-",
        "publication_date": "2018-01-01",
        "authors": "Cristiana Santos, Pompeu Casanovas, Victor Rodrı́guez-Doncel, Leendert van der Torre",
        "file_name": "069.pdf",
        "file_path": "./PDFs/069.pdf"
    },
    {
        "title": "XSD2SHACL: Capturing RDF Constraints from XML Schema",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dtai-kg/XSD2SHACL",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/duan2023xsd2shacl.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are published with the code1."
                    }
                ]
            }
        ],
        "doi": "10.1145/3587259.3627565",
        "abstract": "ABSTRACTSHACL shapes describe the constraints of RDF subgraphs whichare constructed from heterogeneous data, such as RDBs, JSONs,XMLs, etc. These heterogeneous data often already have constraintsdefined in their schemas, e.g., JSON Schema for JSON or XSD forXML, but this information is ignored when the RDF graph is con-structed, as there are currently not many works that translate suchschemas into SHACL. In this paper, we focus on the incorporationof XSD constraints for XML data sources in SHACL shapes. We de-fine a translation from XSD to SHACL, and provide a correspondingsystem. We compare our solution with XMLSchema2ShEx whichtranslates XSD constraints to ShEx and validate our solution againsttwo use cases. Our solution provides the desired SHACL shapes ina reasonable time. This allows us to automatically derive SHACLshapes for some original raw data without any manual effort.CCS CONCEPTS• Information systems→ Semanticweb description languages;• Computing methodologies→ Knowledge representation andreasoning; • Software and its engineering→ Constraints.KEYWORDSSHACL, XSD, XML Schema, RDF shapes, ValidationACM Reference Format:XueminDuan, David Chaves-Fraga, andAnastasia Dimou. 2023. XSD2SHACL:Capturing RDF Constraints from XML Schema. In Knowledge Capture Con-ference 2023 (K-CAP ’23), December 05–07, 2023, Pensacola, FL, USA. ACM,New York, NY, USA, 9 pages. https://doi.org/10.1145/3587259.36275651 INTRODUCTIONShape constraint languages, such as Shapes Constraint Language(SHACL) [13] and the Shapes Expression language (ShEx) [14],were proposed to validate RDF graphs against a set of constraints.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.K-CAP ’23, December 05–07, 2023, Pensacola, FL, USA© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0141-2/23/12. . . $15.00https://doi.org/10.1145/3587259.3627565Different methods have been considered so far to define shapes foran RDF graph. Such shapes may be manually, e.g., for RINF or TED(see Section 5.2), or automatically generated. In the latter case, theSHACL shapes may be derived from RDF graphs [7–9, 15, 16, 18],ontologies [2, 11], or mapping rules that define how the RDF graphshould be constructed from some raw data [4].However, while these RDF graphs are often derived from rawdata [21], e.g., CSV, XML, or JSON, and these raw data may alreadyhave constraints defined in their schema, e.g., SQL [12] for relational",
        "publication_date": "2023-11-29",
        "authors": "X.R. Duan, David Chaves-Fraga, Anastasia Dimou",
        "file_name": "duan2023xsd2shacl.pdf",
        "file_path": "./PDFs/duan2023xsd2shacl.pdf"
    },
    {
        "title": "Semantic Modelling of Plans and Execution Traces for Enhancing Transparency of IoT Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TrustLens/EP-PLAN",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%iotsms48152!2019!8939260.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://www.w3.org/TR/2013/REC-prov-o-20130430/[4] D."
                    }
                ]
            }
        ],
        "doi": "10.1109/iotsms48152.2019.8939260",
        "abstract": "Abstract—Transparency of IoT systems is an essential require-ment for enhancing user’s trust towards such systems. Prove-nance mechanisms documenting the execution of IoT systemsare often cited as an enabler of such transparency. However,provenance records often lack detailed descriptions of a system’sexpected behaviour. Plan specifications describe the steps neededto achieve a certain goal by a human or an automated system.Once plans reach a certain level of complexity, they are typicallydecomposed in different levels of abstraction. However, thisdecomposition makes it difficult to relate high level abstract plansto their granular execution traces. This paper introduces EP-Plan, a vocabulary for linking the different levels of granularityof a plan with their respective provenance traces. EP-Plan alsoprovides the means to describe plan metadata such as constraints,policies, rationales, and expected participating agents associatedwith a plan.I. INTRODUCTIONAn increasing number of systems provide means to docu-ment provenance records of past executions [1] (i.e., executiontraces), in order to inspect and explain the creation process ofexisting results or behaviour of a system. In this paper, weargue that recording such provenance can be further enhancedby recording the set of planned steps that guided its execution,and we refer to such records as plans.Plans document intended system behaviour, which is ben-eficial at the point when no runtime provenance is available(e.g. to assess risks associated with a planned IoT deployment).Plans are also critical to understand errors, enabling a pointof reference for comparison when the execution deviates fromwhat was planned to happen.Gateway CloudPlanRelay data from sensorsPlanProcess and store dataPlanDeliver smart temperature monitoring at Alice’s homeSensorPlanProduce observationsIoT SystemExecution TraceA log of activities generating and publishing raw data values and timestampsExecution TraceA log of activities receiving and uploading sensor data  Execution Trace",
        "publication_date": "2019-10-01",
        "authors": "Milan Marković, Daniel Garijo, Peter Edwards, Wamberto Vasconcelos",
        "file_name": "10!1109%iotsms48152!2019!8939260.pdf",
        "file_path": "./PDFs/10!1109%iotsms48152!2019!8939260.pdf"
    },
    {
        "title": "The landscape of multimedia ontologies in the last decade",
        "implementation_urls": [],
        "doi": "10.1007/s11042-011-0905-z",
        "abstract": "Abstract Many efforts have been made in the area of multimedia to bridge the so-called “semantic-gap” with the implementation of ontologies from 2001 to the present. Inthis paper, we provide a comparative study of the most well-known ontologies related tomultimedia aspects. This comparative study has been done based on a frameworkproposed in this paper and called FRAMECOMMON. This framework takes into accountprocess-oriented dimension, such as the methodological one, and outcome-orienteddimensions, like multimedia aspects, understandability, and evaluation criteria. Finally,we derive some conclusions concerning this one decade state-of-art in multimediaontologies.Keywords Ontology . Multimedia . RDF(S) . OWL . Comparative Framework1 IntroductionVision and sound are the most used senses to communicate experiences and knowledge. Theseexperiences or knowledge are normally recorded in media objects, which are generallyassociated to text, image, sound, video and animation. In this regard, a multimedia object can beconsidered as a composite media object (text, image, sound, video, or animation) that iscomposed of a combination of different media objects.Multimed Tools ApplDOI 10.1007/s11042-011-0905-zM. C. Suárez-Figueroa (*) : O. CorchoOntology Engineering Group (OEG). Facultad de Informática,Universidad Politécnica de Madrid (UPM), Madrid, Spaine-mail: mcsuarez@fi.upm.esO. Corchoe-mail: ocorcho@fi.upm.esURL: http://www.oeg-upm.net/G. A. AtemezingEurecom, MM Department, Sophia-Antipolis, Franceemail: ghislain.atemezing@gmail.comNowadays, a growing amount of multimedia data is being produced, processed, andstored digitally. We are continuously consuming multimedia contents in different formatsand from different sources using Google1, Flickr2, Picasa3, YouTube4, and so on. Theavailability of huge amounts of multimedia objects implies the need for efficientinformation retrieval systems that facilitate storage, retrieval, and browsing of not onlytextual, but also image, audio, and video objects. One potential approach can be based onthe semantic annotation of the multimedia content to be semantically described andinterpreted both by human agents (users) and machines agents (computers). Hence, there isa strong need of annotating multimedia contents to enhance the agents’ interpretation andreasoning for an efficient search.The annotation of multimedia objects is difficult because of the so-called semantic gap[24]; that is, the disparity between low level features (e.g., colour, textures, fragments) thatcan be derived automatically from the multimedia objects and high level concepts (mainlyrelated to domain content), which are typically derived based on human experience andbackground. In other words, the semantic gap refers to the lack of coincidence between theinformation that machines can extract from the visual data and the interpretation that thesame data have for a particular person in a given situation. The challenge of unifying bothlow level elements and high level descriptions of multimedia contents in a unique ontologyis one of the ways to contribute to bridge this semantic gap.The need for a high level representation that captures the true semantics of amultimedia object led at the beginning to the development of the MPEG-7 standard[9] for describing multimedia documents. This standard provides metadata descriptors",
        "publication_date": "2011-11-21",
        "authors": "Mari Carmen Suárez-Figueroa, Ghislain Auguste Atemezing, Óscar Corcho",
        "file_name": "10!1007%s11042-011-0905-z.pdf",
        "file_path": "./PDFs/10!1007%s11042-011-0905-z.pdf"
    },
    {
        "title": "Creating and Querying Personalized Versions of Wikidata on a Laptop",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2108.07119",
        "publication_date": "2021-01-01",
        "authors": "Hans Chalupsky, Pedro Szekely, Filip Ilievski, Daniel Garijo, Kartik Shenoy",
        "file_name": "arxiv.2108.07119",
        "file_path": "10.48550/arxiv.2108.07119"
    },
    {
        "title": "Impact of Text Length for Information Retrieval Tasks based on Probabilistic Topics",
        "implementation_urls": [],
        "doi": "10.26342/2021-67-2",
        "abstract": "Abstract: Information retrieval has traditionally been approached using vectormodels to describe texts. In large document collections, these models need to reducethe dimensions of the vectors to make the operations manageable without compro-mising their performance. Probabilistic topic models (PTM) propose smaller vectorspaces. Words are organized into topics and documents are related to each otherfrom their topic distributions. As in many other AI techniques, the texts used totrain the models have an impact on their performance. Particularly, we are inter-ested on the impact that length of texts may have to create PTM. We have studiedhow it influences to semantically relate multilingual documents and to capture theknowledge derived from their relationships. The results suggest that the most ade-quate texts to train PTM should be of equal or greater length than those used tomake inferences later and documents should be related by hierarchy-based similaritymetrics at large-scale.Keywords: probabilistic topics, text similarity, hierarchical topics, document re-trieval.Resumen: La recuperación de información ha utilizado tradicionalmente modelosvectoriales para describir los textos. A gran escala, estos modelos necesitan reducirlas dimensiones de los vectores para que las operaciones sean manejables sin com-prometer su rendimiento. Los modelos probabiĺısticos de tópicos (MPT) proponenespacios vectoriales más pequeños. Las palabras se organizan en tópicos y los doc-umentos se relacionan entre śı a partir de sus distribuciones de tópicos. Como enmuchas otras técnicas de IA, los textos utilizados para entrenar los modelos influyenen su rendimiento. En particular, nos interesa el impacto de la longitud de los textosal crear MPT. Hemos estudiado cómo influye al relacionar semánticamente docu-mentos multilingües y al capturar el conocimiento derivado de sus relaciones. Losresultados sugieren que los textos más adecuados deben ser de igual o mayor longi-tud que los utilizados para hacer inferencias posteriormente y las relaciones debenbasarse en métricas de similitud jerárquicas.Palabras clave: topicos probabiĺısticos, semejanza de textos, jerarqúıa de tópicos,recuperación de documentos.1 IntroductionProbabilistic Topic Models (PTM) (Hof-mann, 2001) (Blei, Ng, and Jordan, 2003)are statistical methods based on bag-of-wordsthat analyze the words of the original textsto discover the themes that run throughthem, how those themes are connected toeach other, or how they change over time.PTM do not require any prior annotationsor labeling of the documents. The topicsemerge, as hidden structures, from the anal-ysis of the original texts. These structuresare topic distributions, per-document topicdistributions or per-document per-word topicassignments. In turn, a topic is a distribu-tion over terms that is biased around thosewords associated to a single theme. Figure 1shows some topics that have emerged whencreating a topic model with the collection ofWikipedia articles to better understand what",
        "publication_date": "2021-09-06",
        "authors": "Carlos Badenes-Olmedo, Borja Lozano-Álvarez, Óscar Corcho",
        "file_name": "no_doi_20250624162145.pdf",
        "file_path": "./PDFs/no_doi_20250624162145.pdf",
        "pdf_link": "http://rua.ua.es/dspace/bitstream/10045/117485/1/PLN_67_02.pdf"
    },
    {
        "title": "Fuzzy Semantic Labeling of Semi-structured Numerical Datasets",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_2",
        "abstract": "Abstract. SPARQL endpoints provide access to rich sources of data(e.g. knowledge graphs), which can be used to classify other less struc-tured datasets (e.g. CSV files or HTML tables on the Web). We proposean approach to suggest types for the numerical columns of a collection ofinput files available as CSVs. Our approach is based on the applicationof the fuzzy c-means clustering technique to numerical data in the inputfiles, using existing SPARQL endpoints to generate training datasets.Our approach has three major advantages: it works directly with liveknowledge graphs, it does not require knowledge-graph profiling before-hand, and it avoids tedious and costly manual training to match val-ues with types. We evaluate our approach against manually annotateddatasets. The results show that the proposed approach classifies most ofthe types correctly for our test sets.Keywords: Fuzzy Clustering, Semantic Labeling, Semantic Web1 IntroductionA massive number of data are stored and made publicly available on the Webin semi-structured formats, such as spreadsheets. This is especially the case foropen data made available by public administrations, since the publication ofCSV data grants them three stars in the 5-star open data scheme1.A major drawback of the publication of data in spreadsheets is the difficultyfor potential data consumers to understand and interpret their content. This isbecause the terms used for column headings in these files are commonly not suf-ficiently informative and lack a data dictionary where their meaning is provided.Therefore, the automatic classification of such semi-structured data sources maybe useful to improve their usage. For example, such characterization may allowsearch engines to improve the relevancy of results [4]. It may also be used to(partially) automate the generation of mappings (e.g. RML [7] and R2RML [6])that may be used to generate RDF on the fly without actually transforming thedata.Meanwhile, data are also exposed on the Web by means of Linked Dataprinciples or via SPARQL endpoints, which can be considered as rich sources1 http://5stardata.info/en/https://doi.org/10.1007/978-3-030-03667-6_2.2 A. Alobaid et al.of more structured and well-described data. Our hypothesis is that such datacan be useful to train models that are able to characterize the numerical semi-structured data sources that we were referring to in the previous paragraph.In this paper, we describe an approach for the characterization of semi-structured data sources (e.g., CSVs) that uses the content available in SPARQLendpoints for such characterization. Our approach is based on the usage of thefuzzy c-means clustering technique. We have identified the following advantages:– It is domain agnostic. That is, it performs the semantic labeling of semi-structured data sources regardless of their domain, what makes it applicableto a wide range of datasets.– No manual training is needed. It does not require users to manually typesamples of the data beforehand or to use a training dataset that has beenconstructed before. Instead, it works with existing data available as SPARQLendpoints.– It does not require exact matches for the numerical values whose columnsit classifies. The correct typing of data sources is not prevented by having",
        "publication_date": "2018-01-01",
        "authors": "Ahmad Alobaid, Óscar Corcho",
        "file_name": "fuzzy_semantic_labeling_self.pdf",
        "file_path": "./PDFs/fuzzy_semantic_labeling_self.pdf"
    },
    {
        "title": "What Are the Parameters that Affect the Construction of a Knowledge Graph?",
        "implementation_urls": [
            {
                "identifier": "https://github.com/SDM-TIB/KGC-Param-Eval",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-030-33246-4_43.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The testbeds used to conduct this evaluation are available online4."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-33246-4_43",
        "abstract": "Abstract. A large number of datasets are made publicly available on awide range of formats. Due to interoperability problems, the construc-tion of RDF-based knowledge graphs (KG) using declarative mappinglanguages has emerged with the aim of integrating heterogeneous sourcesin a uniform way. Although the scientific community has actively con-tributed with several engines to solve the problem of knowledge graphconstruction, the lack of testbeds has prevented reproducible bench-marking of these engines. In this paper, we tackle the problem of eval-uating knowledge graph creation, and analyze and empirically studya set of variables and configurations that impact on the behaviour ofthese engines (e.g. data size, data distribution, mapping complexity).The evaluation has been conducted on RMLMapper and the SDM-RDFizer, two state-of-the-art engines that interpret the RDF MappingLanguage (RML) and transform (semi)-structured data into RDF knowl-edge graphs. The results allow us to discover unknown relations betweenthese engines that cannot be observed in other configurations.Keywords: Knowledge graph construction · RDFizers · Testbeds1 IntroductionFollowing the FAIR principles [19] and Open data initiatives, the size of publiclyavailable data has grown exponentially in the last decade, expecting a fastergrowth rate in the following years as a result of the advances in the technologiesfor data generation and ingestion. In order to extract values for existing datasets,several data integration approaches have been proposed in the literature [5]. TheSemantic Web community has also proposed various approaches that enable theintegration of data presented in diverse formats into a knowledge graph. Knowl-edge graphs comprise data and the knowledge that describe the main charac-teristics of the integrated data following a graph-based data model, e.g. RDFc© Springer Nature Switzerland AG 2019H. Panetto et al. (Eds.): OTM 2019, LNCS 11877, pp. 695–713, 2019.https://doi.org/10.1007/978-3-030-33246-4_43http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-33246-4_43&domain=pdfhttps://doi.org/10.1007/978-3-030-33246-4_43696 D. Chaves-Fraga et al.[18]. With the aim of transforming structured data in tabular or nested formatslike CSV, relational, JSON, and XML, into RDF knowledge graphs, diverse map-ping languages have been proposed. Exemplary mapping languages include RDFMapping Language (RML) [3], R2RDF [16], xR2RML [12], and R2RML [2], aswell as tools like KARMA [4], SPARQL-Generate [11], and DIG [9]. Despitethese developments, the absence of testbeds has prevented the community fromconducting fair evaluations of the existing tools for knowledge graph creation.This testbed deficiency has also impeded for a holistic understanding about thepros and cons of the state of the art, as well as for clear directions to advance thearea. Given the expected growth rate of available data, testbeds are demandedin order to devise the next generation of tools able to integrate data at scale.Our Goals: We study the process of knowledge graph creation and analyze vari-ous variables and configurations that can impact on the performance of RDFizers– tools for transforming (semi)-structured data following mapping rules specifiedin the RDF Mapping Language (RML). The relevant parameters studied in thispaper include selectivity of the joins between mapping rules, types of relations,and percentage of duplicates. We also present diverse examples that evidence the",
        "publication_date": "2019-01-01",
        "authors": "David Chaves-Fraga, Kemele M. Endris, Enrique Iglesias, Óscar Corcho, María-Esther Vidal",
        "file_name": "978-3-030-33246-4_43.pdf",
        "file_path": "./PDFs/978-3-030-33246-4_43.pdf"
    },
    {
        "title": "Enhancing virtual ontology based access over tabular data with Morph-CSV",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-210432.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The description and features of each query are also available online26."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210432",
        "arxiv": "2001.09052",
        "abstract": "Abstract. Ontology-Based Data Access (OBDA) has traditionally focused on providing a unified view of heterogeneous datasets(e.g., relational databases, CSV and JSON files), either by materializing integrated data into RDF or by performing on-the-fly querying via SPARQL query translation. In the specific case of tabular datasets represented as several CSV or Excel files,query translation approaches have been applied by considering each source as a single table that can be loaded into a relationaldatabase management system (RDBMS). Nevertheless, constraints over these tables are not represented (e.g., referential integrityamong sources, datatypes, or data integrity); thus, neither consistency among attributes nor indexes over tables are enforced.As a consequence, efficiency of the SPARQL-to-SQL translation process may be affected, as well as the completeness of theanswers produced during the evaluation of the generated SQL query. Our work is focused on applying implicit constraints onthe OBDA query translation process over tabular data. We propose Morph-CSV, a framework for querying tabular data thatexploits information from typical OBDA inputs (e.g., mappings, queries) to enforce constraints that can be used together withany SPARQL-to-SQL OBDA engine. Morph-CSV relies on both a constraint component and a set of constraint operators. Fora given set of constraints, the operators are applied to each type of constraint with the aim of enhancing query completenessand performance. We evaluate Morph-CSV in several domains: e-commerce with the BSBM benchmark; transportation with abenchmark using the GTFS dataset from the Madrid subway; and biology with a use case extracted from the Bio2RDF project.We compare and report the performance of two SPARQL-to-SQL OBDA engines, without and with the incorporation of Morph-CSV. The observed results suggest that Morph-CSV is able to speed up the total query execution time by up to two orders ofmagnitude, while it is able to produce all the query answers.Keywords: Knowledge Graphs, Tabular Data, Mapping Languages, Constraints1. IntroductionGuided by the Open Data principles, governmentsand private organizations are regularly publishing vastamounts of public data in open data portals. For exam-ple, almost a million datasets are available in the Eu-ropean Open Data Portal (EODP)1, and many of themare available in tabular formats (e.g., CSV, Excel), asobserved in Table 1. Both the simplicity of a tabularrepresentation and the variety of tools to manage a ta-*Corresponding author. E-mail: dchaves@fi.upm.es.1https://www.europeandataportal.euble (e.g., Excel, Calc) have influenced the popularityof tabular formats to represent open data.Albeit extensively utilized, tabular representationsimposed various data management challenges to ad-vanced users (e.g., developers, data scientists). Thelack of a unified way to query tabular data, some-thing available in other formats (e.g., RDB, JSON,XML), hinders the integration of sources, especiallythose having datatype inconsistencies. Moreover, datamay not be normalized, and information about rela-tionships or column names are not always descriptiveor homogeneous. Hence, data consumers are usuallyforced to apply ad-hoc or manual data wrangling pro-cesses to consume data via open data portals.arXiv:2001.09",
        "publication_date": "2021-04-09",
        "authors": "David Chaves-Fraga, Edna Ruckhaus, Freddy Priyatna, María-Esther Vidal, Óscar Corcho",
        "file_name": "10!3233%sw-210432.pdf",
        "file_path": "./PDFs/10!3233%sw-210432.pdf"
    },
    {
        "title": "Enhancing Drug Repurposing on Graphs by Integrating Drug Molecular Structure as Feature",
        "implementation_urls": [
            {
                "identifier": "https://github.com/pckroon/pysmiles",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Enhancing_Drug_Repurposing_on_Graphs_by_Integrating_Drug_Molecular_Structure_as_Feature.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/pckroon/pysmiles [23] NetworkX, ‘NetworkX — NetworkX documentation’."
                    }
                ]
            }
        ],
        "doi": "10.1109/cbms58004.2023.00215",
        "abstract": "Abstract— Drug repurposing has become increasingly important, particularly in light of the COVID-19 pandemic. This process involves identifying new therapeutic uses for existing drugs, which can significantly reduce the cost, risk, and time associated with developing new drugs, de novo development. A previous conducted study proved that Deep Learning can be used to streamline this process by identifying drug repurposing hypotheses. The study presented a model called REDIRECTION, which utilized the rich biomedical information available in graph form and combined it with Geometric Deep Learning to find new indications for existing drugs. The reported metrics for this model were 0.87 for AUROC and 0.83 for AUPRC. In this current study, the importance of node features in GNNs is explored. Specifically, the study used GNNs to embed two-dimensional drug molecular structures and obtain corresponding features. These features were incorporated into the drug repurposing graph, along with some other enhancements, resulting in an improved model called DMSR. Performance score for the reported metrics values raised by 0.0448 in AUROC and 0.0919 in AUPRC. Based on these findings, we believe that the method used for embedding drug molecular structures is interesting and captures valuable information about drugs. Its incorporation in the graph for drug repurposing can significantly benefit the process, leading to improved performance evaluation metrics. Keywords—Drug repurposing; Graph deep learning; Drug molecular structure; Graph Neural Networks; Graph Autoencoder; DISNET knowledge base  I. INTRODUCTION  In our prior work, Ayuso et al. (2022) [1], it was proved that incorporating diverse biomedical data into a multi-layered graph is a helpful method for gaining a deeper understanding of diseases and their associated components and characteristics. Using Graph Neural Networks (GNNs) and DISNET biomedical graph [2] a model called REDIRECTION was presented. DISNET is a biomedical integrated knowledge base containing information regarding diseases and their associations to symptoms and drugs, among others. REDIRECTION is based on graph deep learning [3], [4] and link  prediction. Its aim is to generate drug repurposing hypotheses. Drug repurposing or repositioning involves identifying new therapeutic uses for drugs that have already been approved. The model performed well, scoring 0.87 for the area under the receiving operating characteristics curve (AUROC) and 0.83 for the area under the precision-recall curve (AUPR) for a selected subset of already-tested repurposing hypotheses, RepoDB test. GNNs benefit enormously from the addition of node features, due to the foundational role these features play in the message passing framework upon which GNNs are built [5], [6]. This is an intuitive outcome, as the incorporation of relevant ",
        "publication_date": "2023-06-01",
        "authors": "Adrián Ayuso-Muñoz, Lucía Prieto Santamaría, Andrea Álverez-Pérez, Belén Otero-Carrasco, Emilio Serrano, Alejandro Rodríguez‐González",
        "file_name": "Enhancing_Drug_Repurposing_on_Graphs_by_Integrating_Drug_Molecular_Structure_as_Feature.pdf",
        "file_path": "./PDFs/Enhancing_Drug_Repurposing_on_Graphs_by_Integrating_Drug_Molecular_Structure_as_Feature.pdf"
    },
    {
        "title": "An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2021.02.018",
        "abstract": "AbstractKnowledge graphs (KGs) are one of the most common frameworks for knowl-edge representation. However, they suffer from a severe scalability problemthat hinders their usage. KG embedding aims to provide a solution to thisissue. Nonetheless, general approaches are incapable of representing and rea-soning about information not previously contained in the graph. This paperproposes to leverage semantic and ontological information for a significant ben-efit of knowledge graph completion, focusing on triple classification. The goalof this task is to determine whether a given fact holds. Furthermore, this paperalso considers the classification of facts that include entities that have not beenseen during training, denoted out-of-knowledge-base or OOKB entities. An in-cremental method is presented, composed of six stages. Although the proposalcan be applied to any KG embedding model, this work focuses on its applicationfor semantic matching models, such as ComplEx and DistMult. Compared toother approaches, our proposal is model-agnostic, computationally inexpensive,and does not require retraining. The results show that triple classification ac-curacy scales up to 15% with the proposed approach, as well as accelerating theconvergence of the model to its optimal solution. Furthermore, facts containingOOKB entities can be classified with a reasonable accuracy.Key words: Knowledge Graph Embeddings, Entity Initialization, KnowledgeGraph Completion, Word Embeddings, Ontological Information1. IntroductionKnowledge graphs (KGs) are currently one of the primary forms of knowledgerepresentation. In these structures, knowledge is stored in the form of facts, or∗Corresponding authorEmail address: emilioserra@fi.upm.es (Emilio Serrano)Preprint submitted to Information Science December 1, 2020 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ",
        "publication_date": "2021-02-20",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Patrick Hohenecker, Thomas Lukasiewicz",
        "file_name": "9292552.pdf",
        "file_path": "./PDFs/9292552.pdf"
    },
    {
        "title": "Using LOT methodology to develop a noise pollution ontology: a Spanish use case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/opencitydata/medio-ambiente-contaminacion-acustica",
                "type": "git",
                "paper_frequency": 5,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/The_noise_pollution_ontology.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The ontology code is available in our GitHub repository, as explained in subsection 3.1.1."
                    }
                ]
            }
        ],
        "doi": "10.1007/s12652-019-01561-2",
        "abstract": "Abstract Local administrations generate large amounts of data due to theprocesses followed to attend administrative governance issues and the needsof its citizenry. Sadly, in most cases this data is not fully exploited and re-mains within the institutions, making their reutilization difficult. Currently,open data initiatives had gained ground worldwide and more cities are takingadvantage of adopting an open data strategy, which are visible at the orga-nizational level and at user level. In this respect, there is a need to generateguidelines that allow cities: a) to identify datasets to be shared, for examplepollution, commercial premises, public services, etc. and b) to publish qualitydata on their portals. Data should be accurate and interoperable among citiesto facilitate reuse. This work describes the development process of an ontologyto represent the acoustic pollution data collected by measurement stations lo-cated in cities, providing a common model for data publication. The developedontology reuses several well-known ontologies and includes classes, propertiesand instances specifically created to cover this domain. This work also includesreal examples about how to instantiate the ontology.Keywords noise pollution · open data · ontologyPaola Espinoza-Arias* Corresponding authorE-mail: pespinoza@fi.upm.esMaŕıa Poveda-VillalónE-mail: mpoveda@fi.upm.esOscar CorchoE-mail: ocorcho@fi.upm.es1 Ontology Engineering Group, ETSI Informáticos, Universidad Politécnica de Madrid,28660 Madrid, Spain.2 Paola Espinoza-Arias et al.1 IntroductionNoise pollution is one of the environmental problems affecting the health andwell-being of citizens in large towns and cities. According to the World HealthOrganization, noise is the second largest environmental cause of health prob-lems, just after the impact of air quality [5]. Bearing in mind this public healthissue, many cities around the world have defined several regulations in order toassess and manage environmental noise. In the case of the European Union, in2002 the Directive 2002/49/EC [10], also known as the Environmental NoiseDirective (END), was passed in order to identify noise pollution levels and totrigger the necessary action both at Member State and at European Unionlevel. The END defines the environmental noise pollution as noise caused byroad, rail and airport traffic, industry, construction, as well as some otheroutdoor activities. In accordance with one of the key areas related with thisdirective, Member States are committed to reporting their strategic noise mapsevery five years. The purpose of these maps is presenting and assessing the cal-culated/measured noise levels over a geographical area in order to determinethe population exposed by this pollution. In the context of national legislationand local regulations, in Spain the Ley del Ruido 37/2003 [3] has also beendefined for protecting citizens from excessive noise pollution.In order to detect the noise level of cities, several sensors, located at strate-gic places (e.g. train stations, airports, wide boulevards, etc.), are used to col-lect these data. These sensors, in most cases, are part of an interconnected sen-sor network deployed over the city. Although cities are reporting their strategic",
        "publication_date": "2019-10-31",
        "authors": "Paola Espinoza-Arias, María Poveda‐Villalón, Óscar Corcho",
        "file_name": "The_noise_pollution_ontology.pdf",
        "file_path": "./PDFs/The_noise_pollution_ontology.pdf"
    },
    {
        "title": "Multi-perspective approach for curating and exploring the history of climate change in Latin America within digital newspapers",
        "implementation_urls": [],
        "doi": "10.2298/csis220110008v",
        "abstract": "Abstract. This paper introduces a multi-perspective approach to deal with curation14and exploration issues in historical newspapers. It has been implemented in the15platform LACLICHEV (Latin American Climate Change Evolution platform).16Exploring the history of climate change through digitalized newspapers published17around two centuries ago introduces four challenges: (1) curating content for track-18ing entries describing meteorological events; (2) processing (digging into) collo-19quial language (and its geographic variations5) for extracting meteorological events;20(3) analyzing newspapers to discover meteorological patterns possibly associated21with climate change; (4) designing tools for exploring the extracted content.22LACLICHEV provides tools for curating, exploring, and analyzing historical news-23paper articles, their description and location, and the vocabularies used for referring24to meteorological events. This platform makes it possible to understand and iden-25tify possible patterns and models that can build an empirical and social view of the26history of climate change in the Latin American region.27Keywords: data curation, metadata extraction, data collections exploration, data28analytics.291. Introduction30Ninety-seven per cent of climate scientists agree that climate-warming trends over the31past century are very likely due to human activities6. Some observation reports and studies32reveal that the planet’s average surface temperature has risen about 2.0 degrees Fahren-33heit (1.1 degrees Celsius) since the late 19th century. The hypothesis is that this change34has been mainly driven by increased carbon dioxide and other human-made atmospheric35emissions.365 In Iberoamerica, Spanish has variations in the different countries, even if all Spanish-speaking people canperfectly understand each other.6 https://climate.nasa.gov/scientific-consensus/2 Authors Suppressed Due to Excessive LengthTechnological advances have allowed understanding of phenomena and complex sys-1tems by collecting many different types of information. Data collections are exported un-2der different releases with different sizes and formats (e.g., CSV, text, excel), sometimes3with various quality features. Tools helping to understand, consolidate and correlate data4collections are crucial. Even if there is an increasing interest in analysing digital data col-5lections for performing historical studies on climatologic events, the history of climate6behaviour is still an open issue that has not revealed missing knowledge. Long histor-7ical data studies could make it possible to compute more complete models of climatic8phenomena and the conditions in which they emerged. However, meteorology is a young9science that started around the 19th century. It is supported by more or less recent data,10making it challenging to run an analysis that can give more historical pictures of climatic11evolution and its implications using observations instead of extrapolations. Those willing12to promote changes in the behaviour of society and industry to reduce emissions that have13a role in climate change must convince civil society of the importance of the challenges.14For this reason, our work addressed the problem of collecting and analyzing the history15of meteorological events to explore how they were described, lived and perceived by civil16society. In this sense, the digitalization of data collections has an increasingly vital role17in collecting vast amounts of hidden data. Thus, considering that digital archives become18more easily accessible every time and contain explicit and implicit spatio-temporal in-19formation, researchers in GIScience [18], are becoming aware of these new data sources20[10], [9], [34], [41]. Moreover, digital data collections make it possible to have an analytic21vision of the evolution of environmental, administrative, economic and social phenomena.22In this context, our work deals with data collections that report the emergence of mete-23",
        "publication_date": "2023-01-01",
        "authors": "Genoveva Vargas‐Solar, José-Luis Zechinelli-Martini, Javier A. Espinosa-Oviedo, Luis M. Vilches‐Blázquez",
        "file_name": "10!2298%csis220110008v.pdf",
        "file_path": "./PDFs/10!2298%csis220110008v.pdf"
    },
    {
        "title": "A framework for connecting two interoperability universes: OGC Web Feature Services and Linked Data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/GeoKnow/TripleGeo",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/frameworkWebFeature.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This triple store is available at https://ec2-54-94-208-47.sa-east-1.compute.amazonaws.com:8080/parlia‐ ment/."
                    }
                ]
            }
        ],
        "doi": "10.1111/tgis.12496",
        "abstract": "AbstractDiverse studies have shown that about 80% of all available data are related to a spatial location. Most of these geo‐spatial data are available as structured and semi‐struc‐tured datasets, and often use distinct data models, are encoded using ad‐hoc vocabularies, and sometimes are being published in non‐standard formats. Hence, these data are isolated within silos and cannot be shared and in‐tegrated across organizations and communities. Spatial Data Infrastructures (SDIs) have emerged and contributed to significantly enhance data discovery and accessibility based on OGC (Open Geospatial Consortium) Web ser‐vices. However, finding, accessing, and using data dissemi‐nated through SDIs are still difficult for non‐expert users. Overcoming the current geospatial data challenges in‐volves adopting the best practices to expose, share, and integrate data on the Web, that is, Linked Data. In this ar‐ticle, we have developed a framework for generating, en‐riching, and exploiting geospatial Linked Data from multiple and heterogeneous geospatial data sources. This proposal allows connecting two interoperability universes (SDIs, more specifically Web Feature Services, WFS, and Semantic Web technologies), which is evaluated through a study case in the (geo)biodiversity domain.mailto:﻿http://orcid.org/0000-0001-5799-469Xmailto:lmvilches@javeriana.edu.cohttp://crossmark.crossref.org/dialog/?doi=10.1111%2Ftgis.12496&domain=pdf&date_stamp=2018-11-28     |  23VILCHES‐BLÁZQUEZ and SAAVEDRA1  | INTRODUC TIONDiverse studies have shown that about 80% of all available data are related to a spatial location (Densham & Goodchild, 1989; Li & Li, 2014). Nowadays, this statement has been strengthened, due to the fact that Web 2.0, mobile devices, citizen participation, and crowdsourcing have become a relevant stream in the collection of geo‐related data with the advancement of network deployment (Salk, Sturn, See, Fritz, & Perger, 2016). These new streams are significant and have become complementary efforts to the traditional and official organizations associated with geospatial information production. Likewise, in‐situ sensor network data, GPS trace data from mobile devices, geo‐social media data (e.g. Twitter), and crowdsourcing/volunteered geographic information data (e.g. OpenStreetMap) contribute to the increased volume and variety in data (Guo et al., 2017).Most of these geospatial data are available as structured1 (e.g. CSV, Microsoft Excel, relational databases, etc.) and semi‐structured2 (e.g. shapefile, XML, JSON, Web services, etc.) datasets. Besides, these data often use distinct data models, are encoded using ad‐hoc vocabularies, and are sometimes being published in non‐standard formats. Hence, these resources are isolated in silos and cannot be shared and integrated across organizations and communities. As a result, data use and analysis become a privilege owned by some domain‐specific well‐educated users, and much data may not be analyzed and sufficiently exploited (Zhao, Foerster, & Yue, 2012).In this scenario, Spatial Data Infrastructures (SDIs) have emerged and contributed to integrate data sources, systems, network linkages, standards, and institutional issues to deliver geospatial data from heterogeneous sources to the widest possible audience (Giuliani et al., 2017). These issues motivated such initiatives (SDIs) to focus on interoperability to significantly enhance data discovery and accessibility based on OGC (Open Geospatial Consortium) Web services. However, although SDIs constitute a mature technology widely de‐",
        "publication_date": "2018-11-28",
        "authors": "Luis M. Vilches‐Blázquez, Jhonny Saavedra",
        "file_name": "frameworkWebFeature.pdf",
        "file_path": "./PDFs/frameworkWebFeature.pdf"
    },
    {
        "title": "FMonE: A Flexible Monitoring Solution at the Edge",
        "implementation_urls": [],
        "doi": "10.1155/2018/2068278",
        "abstract": "Abstract. This paper introduces a multi-perspective approach to deal with curation14and exploration issues in historical newspapers. It has been implemented in the15platform LACLICHEV (Latin American Climate Change Evolution platform).16Exploring the history of climate change through digitalized newspapers published17around two centuries ago introduces four challenges: (1) curating content for track-18ing entries describing meteorological events; (2) processing (digging into) collo-19quial language (and its geographic variations5) for extracting meteorological events;20(3) analyzing newspapers to discover meteorological patterns possibly associated21with climate change; (4) designing tools for exploring the extracted content.22LACLICHEV provides tools for curating, exploring, and analyzing historical news-23paper articles, their description and location, and the vocabularies used for referring24to meteorological events. This platform makes it possible to understand and iden-25tify possible patterns and models that can build an empirical and social view of the26history of climate change in the Latin American region.27Keywords: data curation, metadata extraction, data collections exploration, data28analytics.291. Introduction30Ninety-seven per cent of climate scientists agree that climate-warming trends over the31past century are very likely due to human activities6. Some observation reports and studies32reveal that the planet’s average surface temperature has risen about 2.0 degrees Fahren-33heit (1.1 degrees Celsius) since the late 19th century. The hypothesis is that this change34has been mainly driven by increased carbon dioxide and other human-made atmospheric35emissions.365 In Iberoamerica, Spanish has variations in the different countries, even if all Spanish-speaking people canperfectly understand each other.6 https://climate.nasa.gov/scientific-consensus/2 Authors Suppressed Due to Excessive LengthTechnological advances have allowed understanding of phenomena and complex sys-1tems by collecting many different types of information. Data collections are exported un-2der different releases with different sizes and formats (e.g., CSV, text, excel), sometimes3with various quality features. Tools helping to understand, consolidate and correlate data4collections are crucial. Even if there is an increasing interest in analysing digital data col-5lections for performing historical studies on climatologic events, the history of climate6behaviour is still an open issue that has not revealed missing knowledge. Long histor-7ical data studies could make it possible to compute more complete models of climatic8phenomena and the conditions in which they emerged. However, meteorology is a young9science that started around the 19th century. It is supported by more or less recent data,10making it challenging to run an analysis that can give more historical pictures of climatic11evolution and its implications using observations instead of extrapolations. Those willing12to promote changes in the behaviour of society and industry to reduce emissions that have13a role in climate change must convince civil society of the importance of the challenges.14For this reason, our work addressed the problem of collecting and analyzing the history15of meteorological events to explore how they were described, lived and perceived by civil16society. In this sense, the digitalization of data collections has an increasingly vital role17in collecting vast amounts of hidden data. Thus, considering that digital archives become18more easily accessible every time and contain explicit and implicit spatio-temporal in-19formation, researchers in GIScience [18], are becoming aware of these new data sources20[10], [9], [34], [41]. Moreover, digital data collections make it possible to have an analytic21vision of the evolution of environmental, administrative, economic and social phenomena.22In this context, our work deals with data collections that report the emergence of mete-23",
        "publication_date": "2018-01-01",
        "authors": "Álvaro Brandón, Marı́a S. Pérez, Jesús Montes, Alberto Sánchez",
        "file_name": "10!1155%2018%2068278.pdf",
        "file_path": "./PDFs/10!1155%2018%2068278.pdf"
    },
    {
        "title": "A Community Roadmap for Scientific Workflows Research and Development",
        "implementation_urls": [],
        "doi": "10.1109/works54523.2021.00016",
        "arxiv": "2110.02168",
        "abstract": "Abstract—The landscape of workflow systems for scientificapplications is notoriously convoluted with hundreds of seeminglyequivalent workflow systems, many isolated research claims, anda steep learning curve. To address some of these challenges andlay the groundwork for transforming workflows research anddevelopment, the WorkflowsRI and ExaWorks projects partneredto bring the international workflows community together. Thispaper reports on discussions and findings from two virtual“Workflows Community Summits” (January and April, 2021).The overarching goals of these workshops were to develop a viewof the state of the art, identify crucial research challenges in theworkflows community, articulate a vision for potential communityefforts, and discuss technical approaches for realizing this vision.To this end, participants identified six broad themes: FAIR com-putational workflows; AI workflows; exascale challenges; APIs,interoperability, reuse, and standards; training and education;and building a workflows community. We summarize discussionsand recommendations for each of these themes.Index Terms—Scientific workflows, community roadmap, datamanagement, AI workflows, exascale computing, interoperabilityI. INTRODUCTIONScientific workflow systems are used almost universallyacross scientific domains for solving complex and large-scale computing and data analysis problems, and have un-derpinned some of the most significant discoveries of thepast decades [1]. Many of these workflows have significantcomputational, storage, and communication demands, and thusmust execute on a wide range of large-scale platforms, fromlocal clusters over science or public clouds to upcomingexascale HPC platforms [2]. Managing these executions isoften a significant undertaking, requiring a sophisticated andversatile software infrastructure.Historically, many of these infrastructures for workflowexecution consisted of complex, integrated systems, developedin-house by workflow practitioners with strong dependencieson a range of legacy technologies—even including sets ofad-hoc scripts. Due to the increasing need to support work-flows, dedicated workflow systems were developed to provideabstractions for creating, executing, and adapting workflowsconveniently and efficiently while ensuring portability. Whilethese efforts are all worthwhile individually, there are nowhundreds of independent workflow systems [3]. These arecreated and used by thousands of researchers and developers,leading to a rapidly growing corpus of workflows researchpublications. The resulting workflow system technology land-scape is fragmented, which may present significant barriersfor future workflow users due to the tens of seemingly com-parable, yet usually mutually incompatible, systems that exist.In the current workflow research, there are conflicting theo-retical bases and abstractions for what constitutes a workflow",
        "publication_date": "2021-11-01",
        "authors": "Rafael Ferreira da Silva, Henri Casanova, Kyle Chard, İlkay Altıntaş, Rosa M. Badía, Bartosz Baliś, Tainã Coleman, Frederik Coppens, Frank Di Natale, Bjoern Enders, Thomas Fahringer, Rosa Filgueira, Grigori Fursin, Daniel Garijo, Carole Goble, Dorran Howell, Shantenu Jha, Daniel S. Katz, Daniel Laney, Ulf Leser, M. Malawski, Kshitij Mehta, Loïc Pottier, Jonathan Ozik, J. L. Peterson, Lavanya Ramakrishnan, Stian Soiland‐Reyes, Douglas Thain, Matthew Wolf",
        "file_name": "10!1109%works54523!2021!00016.pdf",
        "file_path": "./PDFs/10!1109%works54523!2021!00016.pdf"
    },
    {
        "title": "Towards a Knowledge Graph Based Platform for Public Procurement",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-14401-2_29",
        "abstract": "Abstract. Procurement affects virtually all sectors and organizationsparticularly in times of slow economic recovery and enhanced transparency.Public spending alone will soon exceed EUR 2 trillion per annum in theEU. Therefore, there is a pressing need for better insight into, andmanagement of government spending. In the absence of data and toolsto analyse and oversee this complex process, too little consideration isgiven to the development of vibrant, competitive economies when buyingdecisions are made. To this end, in this short paper, we report our ongoingwork for enabling procurement data value chains through a knowledgegraph based platform with data management, analytics, and interaction.Keywords: Procurement · Knowledge graphs · Analytics · Interaction.1 IntroductionProcurement affects virtually all sectors and organisations. In the public sectoralone, spending on goods and services across the EU is estimated to exceed e 2trillion per annum1. Governments and state-owned enterprises are confrontedwith massive challenges: they must deliver services with greatly reduced budgets;prevent losses through fraud and corruption; and build healthy, sustainableeconomies. Managing these competing priorities is notoriously difficult. In timesof slow economic recovery and enhanced transparency and accountability, there isa pressing need for better insight into, and management of government spending? This work is funded by EU H2020 TheyBuyForYou project (780247).?? Corresponding author. Email: ahmet.soylu@sintef.no1 http://ec.europa.eu/DocsRoom/documents/20679http://ec.europa.eu/DocsRoom/documents/206792 E. Simperl et al.(cf. [1]). In the absence of data and tools to analyse and oversee this complexprocess, too little consideration is given to the development of vibrant, competitiveeconomies when buying decisions are made.To this end, within an EU project called TheyBuyForYou2, we work towardsenabling the procurement data value chains through a knowledge graph basedplatform paired with data management, analytics, and interaction. Our objective,from a technical point of view, is to build a technology platform, consisting of a setof modular, web-based services and APIs to publish, curate, integrate, analyse, andvisualize a comprehensive, cross-border and cross-lingual procurement knowledgegraph, including public spending and corporate data from multiple sources acrossthe EU. In this context, the first challenge is the heterogeneity of the underlyingdata (cf. [5,10]), which covers structured (e.g., statistics, financial news) aswell as unstructured (e.g., text, social media) sources in different languagesand using their own terminology and formats (CSV, PDF, databases, websites,APIs etc.). A second challenge will be turning this vast array of informationinto a semantic knowledge graph [17,18], an interconnected semantic knowledgeorganization structure using Web URIs and linked data vocabularies, which canbe analysed in depth to identify patterns and anomalies in procurement processesand networks. Finally, the last challenge is to support analytics and interactionwith the knowledge graph by different groups of stakeholders.In this short paper, we report our ongoing work. The rest of the paper isstructured as follows. In Sect. 2, we present an overview of the related work.We present key challenges and our solution approach in Sect. 3 and Sect. 4respectively, and finally we conclude the paper in Sect. 5.2 Related Work",
        "publication_date": "2019-01-01",
        "authors": "Elena Simperl, Óscar Corcho, Marko Grobelnik, Dumitru Roman, Ahmet Soylu, María Jesús Fernández Ruiz, Stefano Gatti, Chris Taggart, Urška Skok Klima, Annie Ferrari Uliana, Ian Makgill, Till Christopher Lech",
        "file_name": "10!1007%978-3-030-14401-2_29.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-14401-2_29.pdf"
    },
    {
        "title": "Lynx: A knowledge-based AI service platform for content processing, enrichment and analysis for the legal domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/trellis-ldp/trellis",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1016%j!is!2021!101966.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Indeed, for all documents (Table 2) it 46 The dataset is openly available at https://zenodo.org/record/4590265."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.is.2021.101966",
        "abstract": "ncluding courts, judges, jurisdictions, abstract legal ideas andther general concepts. The Lynx Legal Knowledge Graph, how-ver, does not contain such an assortment of entities, the focuss placed instead on documents and terminological information,erving the purpose to represent multilingual legal information.he main entity, the Lynx Document, comprises both data andetadata and is the most important entity in the Legal Knowl-dge Graph. Lynx Documents can be grouped in Collections andventually enriched with Annotations.Lynx Documents are the basic information units in Lynx: iden-ified pieces of text, possibly with structure, metadata and anno-ations. A Lynx Document Part is one part of a Lynx Document,ossibly arranged hierarchically (chapter, section, article, etc.).ollections are groups of logically related Lynx Documents, e. g.,ne collection per use case, jurisdiction, etc. Annotations are en-ichments of Lynx Documents, such as summaries, translations,ecognised entities, etc., these annotations are NIF-compatibleNLP Interchange Format). Original documents are harvested inheir original form first, transformed into Lynx Documents andhen enriched with annotations.6 https://participedia.net/case/51827 http://www.oasis-open.org8 http://lkg.lynx-project.eu/def/6Lynx documents can reference each other in different manners(e. g., a law implementing a directive, a standard referencinglegislation), and they can reference terms and concepts presentin terminologies — documents and terminologies are the mainelements in the LKG. In addition, a few other elements are na-tively present in the LKG, i. e., companies and relevant persons.Finally, every element in the LKG is connected to external en-tities: companies are connected to their external reference (theone provided by Refinitiv’s PermID) and to NACE codes to classifytheir activity, terminologies are connected to other term banks(IATE, Wikidata, etc.), documents are connected to the originalsources etc. (see Fig. 6). The LKG is not a closed graph but it isconnected to other entities, many of them in the Linked DataCloud.9The LKG has multiple advantages. Citizens and companies areprovided with better access to legislation by finding homoge-neous practices throughout Europe; companies are able to runlegal information systems more smoothly, and documents areuniquely identified and explicitly described in this context.3.1.1. Data validationData integrity is critical for the design, implementation andusage of any system which stores, processes or retrieves data.The concept becomes even more crucial in our case, since weuse an RDF-based data model, which offers a certain amount offlexibility, i. e., any node can in principle have any number ofvalues, possibly of different type, for any given property. How-",
        "publication_date": "2021-12-06",
        "authors": "Julián Moreno Schneider, Georg Rehm, Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel, Patricia Martín-Chozas, María Navas-Loro, Martin Kaltenböck, Artem Revenko, Sotirios Karampatakis, Christian Sageder, Jorge Gracia, Filippo Maganza, Ilan Kernerman, Dorielle Lonke, Andis Lagzdiņš, Julia Bosque-Gil, Pieter Verhoeven, Elsa Gomez Diaz, Pascual Boil Ballesteros",
        "file_name": "10!1016%j!is!2021!101966.pdf",
        "file_path": "./PDFs/10!1016%j!is!2021!101966.pdf"
    },
    {
        "title": "The apertium bilingual dictionaries on the web of data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170258",
        "abstract": "Abstract. Bilingual electronic dictionaries contain collections of lexical entries in two languages, with explicitly declared trans-lation relations between such entries. Nevertheless, they are typically developed in isolation, in their own formats and accessiblethrough proprietary APIs. In this paper we propose the use of Semantic Web techniques to make translations available on theWeb to be consumed by other semantic enabled resources in a direct manner, based on standard languages and query means.In particular, we describe the conversion of the Apertium family of bilingual dictionaries and lexicons into RDF (Resource De-scription Framework) and how their data have been made accessible on the Web as linked data. As a result, all the converteddictionaries (many of them covering under-resourced languages) are connected among them and can be easily traversed from oneto another to obtain, for instance, translations between language pairs not originally connected in any of the original dictionaries.Keywords: Linguistic linked data, multilingualism, Apertium, bilingual dictionaries, lexicons, lemon, translation1. IntroductionThe publication of bilingual and multilingual lan-guage resources as linked data (LD) on the Web canlargely benefit the creation of the critical mass of cross-lingual connections required by the vision of the mul-tilingual Web of Data [9]. The benefits of sharing lin-guistic information on the Web of Data have beenrecently recognized by the language resources com-munity, which has shown increasing interest in pub-lishing their linguistic data and metadata as LD onthe Web [2]. As a result of interlinking multilingualand open language resources, the Linguistic LinkedOpen Data (LLOD) cloud is emerging,1 that is, a new*Corresponding author. E-mail: jgracia@fi.upm.es.1An updated picture of the current LLOD cloud can be found athttp://linguistic-lod.org/linguistic ecosystem based on the LD principles thatwill allow the open exploitation of such data at globalscale.In this article we will focus on the case of electronicbilingual dictionaries as a particular type of languageresources. Bilingual dictionaries are specialized dictio-naries that describe translations of words or phrasesfrom one language to another. They can be unidirec-tional or bidirectional, allowing translation, in the lat-ter case, to and from both languages. A bilingual dic-tionary can contain additional information such as partof speech, gender, declension model and other gram-matical properties.Electronic bilingual dictionaries have been typicallydeveloped in isolation, in their own formats and acces-sible through proprietary APIs. We propose the use ofSemantic Web techniques to make translations avail-mailto:jgracia@fi.upm.esmailto:asun@fi.upm.esmailto:marta.villegas@upf.edumailto:nuria.bel@upf.edumailto:jgracia@fi.upm.eshttp://linguistic-lod.org/able on the Web to be consumed by other semantic en-abled resources in a direct manner, based on standard",
        "publication_date": "2017-01-24",
        "authors": "Jorge Gracia, Marta Villegas, Asunción Gómez‐Pérez, Núria Bel",
        "file_name": "10!3233%sw-170258.pdf",
        "file_path": "./PDFs/10!3233%sw-170258.pdf"
    },
    {
        "title": "Discovering Hidden Mental States in Open Multi-Agent Systems by Leveraging Multi-Protocol Regularities with Machine Learning",
        "implementation_urls": [],
        "doi": "10.3390/s20185198",
        "abstract": "Abstract: The agent paradigm and multi-agent systems are a perfect match for the design of smartcities because of some of their essential features such as decentralization, openness, and heterogeneity.However, these major advantages also come at a great cost. Since agents’ mental states are hiddenwhen the implementation is not known and available, intelligent services of smart cities cannotleverage information from them. We contribute with a proposal for the analysis and prediction ofhidden agents’ mental states in a multi-agent system using machine learning methods that learnfrom past agents’ interactions. The approach employs agent communication languages, which is acore property of these multi-agent systems, to infer theories and models about agents’ mental statesthat are not accessible in an open system. These mental state models can be used on their own orcombined to build protocol models, allowing agents (and their developers) to predict future agents’behavior for various tasks such as testing and debugging them or making communications moreefficient, which is essential in an ambient intelligence environment. This paper’s main contribution isto explore the problem of building these agents’ mental state models not from one, but from severalinteraction protocols, even when the protocols could have different purposes and provide distinctambient intelligence services.Keywords: open multi-agent system; smart city; agent communication languages; agent-orientedsoftware engineering1. IntroductionSmart cities are technologically based on the combination of several socio-technical innovationssuch as: the Internet of Things (IoT), mobile Internet access, smartphones, data analytics, open datainitiatives, and sharing economy models, among others [1]. This allows these cities to manage assetsand resources efficiently by services enhanced with intelligence such as: traffic management, hospitals,transportation systems, power and water plants, waste management, etcetera.The open and heterogeneous nature of multi-agent systems (MASs) addresses naturally thedynamism and scalability problems of smart cities and ambient intelligence [2]. High-level interactionprotocols and communications are a cornerstone of MASs, which are capable of establishing conversationby following these protocols, sending and receiving messages, or sharing vocabularies.However, how do we leverage information from an open MAS to provide citizens with intelligentservices? MAS platforms and frameworks usually allow developers to analyze agents’ mental statesand interactions among agents for testing, debugging, and verification purposes [3]. Regarding theSensors 2020, 20, 5198; doi:10.3390/s20185198 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/ORCID ID: 0000-0002-4392-4743http://www.mdpi.com/1424-8220/20/18/5198?type=check_update&version=1http://dx.doi.org/10.3390/s20185198http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleDiscovering Hidden Mental States in OpenMulti-Agent Systems by Leveraging Multi-ProtocolRegularities with Machine LearningtEmilio Serrano *\"\\ and Javier BajoOntology Engineering Group, Department of Artificial Intelligence, Universidad Politécnica de Madrid,28660 Madrid, Spain; jbajo@fi.upm.es* Correspondence: emilioserra@fi.upm.es+ Current address: Departamento de Inteligencia Artificial, Escuela Técnica Superior de Ingenieros Informaticos,Universidad Politécnica de Madrid, Boadilla del Monte, 28660 Madrid, Spain.",
        "publication_date": "2020-09-12",
        "authors": "Emilio Serrano, Javier Bajo",
        "file_name": "no_doi_20250624162248.pdf",
        "file_path": "./PDFs/no_doi_20250624162248.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/20/18/5198/pdf?version=1599893740"
    },
    {
        "title": "Ontology Management in an Industrial Environment: The BASF Governance Operational Model for Ontologies (GOMO)",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.7007495",
        "abstract": "ABSTRACT Governance on ontology development and maintenance practices within an organization has many advantages over ungoverned, siloed approaches that many organizations exhibit nowadays. This paper presents the BASF Governance Operational Model for Ontologies (GOMO), which addresses all stages of the ontology lifecycle and provides a framework for the development and maintenance of ontologies within BASF. GOMO is comprised of Principles, Standards and Quality Assurance criteria, Best Practices, Training and Outreach and is the result of a collaborative effort between industry and academia in the semantic web field. GOMO Principles, Standards and Best Practices are being applied to all running ontology-based projects in BASF. Through outreach presentations with sections of the BASF community, GOMO has reached a wider audience to foster understanding on the utility and implementation of ontologies. Finally, GOMO stands as a framework that is fit for adoption by other organizations facing similar challenges in ontology governance. 1 INTRODUCTION  Ontologies have been widely used for decades for different purposes and applied to a wide variety of domains. They have been developed and implemented in both academic and industrial contexts (Poveda-Villalón, Fernández-Izquierdo, Fernández-López, & García-Castro, 2022). Some of the reasons for their adoption are (i) their potential for data semantification, (ii) their provision of standardized meta-, reference- and master data, (iii) their ability to make curated, granular domain knowledge reusable and standardized, (iv) their ability to infer new knowledge, and (v) their use in data integration: enabling interoperability through alignment of schema and controlled vocabularies across multiple data/information source systems. In large organizations such as BASF, which are adopting ontologies as a way to describe data across the organization, it is not uncommon to see the application of ungoverned ontology development methods, relying on ad-hoc practices, hence producing ontologies that are not interoperable, discoverable or understandable. Indeed, this generates obstacles to data integration and results in duplication of efforts and poor coordination across stakeholders such as data and knowledge stewards, data scientists, application owners and users.  Given this starting point, which is common in many organizations, the need for a holistic approach for ontology governance has led BASF to create GOMO, a Governance Operational Model for BASF ontologies that enables (i) interoperability, (ii) development and use of common tools, (iii) the application of best practices and (iv) addressing the skills gap. The model, its components and how it is being used at BASF is described in the remaining of this paper. 2 RELATED WORK In recent years the increase in the volume of data handled by organizations has had an impact on how data is governed, since it has become a critical asset in industrial, scientific and public administration settings. For that reason, data governance has gained importance and several frameworks have been developed and adopted by organizations (Alhassan, Sammon, & Daly, 2016). One of the most relevant frameworks is DAMA (Brackett & Earley, 2009). This framework defines Data Governance as “the exercise of authority, control and shared decision making (planning, monitoring and enforcement) over the management of data assets”.  In relation to ontologies, several communities have developed general guidelines that are applied in the creation, development and maintenance of ontologies. This is the case of the OBO Foundry (Smith, et al., 2007) for biological sciences, or the ",
        "publication_date": "2022-08-18",
        "authors": "Ana Iglesias-Molina, José Antonio Bernabé-Díaz, P.R. Deshmukh, Paola Espinoza-Arias, Aarón Ayllón-Benítez, Alba Fernández-Izquierdo, José María Ponce-Bernabé, Sara Isabel Pérez, Edna Ruckhaus, Óscar Corcho, José Luis Sánchez-Fernández",
        "file_name": "bio-ontologies_paper_GOMO.pdf",
        "file_path": "./PDFs/bio-ontologies_paper_GOMO.pdf"
    },
    {
        "title": "On the Use of Deep Neural Networks for Security Vulnerabilities Detection in Smart Contracts",
        "implementation_urls": [
            {
                "identifier": "https://github.com/tintinweb/smart-contract-sanctuary",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/On_the_Use_of_Deep_Neural_Networks_for_Security_Vulnerabilities_Detection_in_Smart_Contracts.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/tintinweb/smart-contract-sanctuary [22] S."
                    }
                ]
            }
        ],
        "doi": "10.1109/percomworkshops56833.2023.10150302",
        "abstract": "Abstract—In this paper, we investigate the use of deep learningtechniques to identify and classify smart contract code vulnerabil-ities. We collected a large-scale dataset of smart contracts that weused to train different Convolutional Neural Networks (CNNs)models. In particular, we used two variants of 2-dimensionalCNNs working on RGB images corresponding to contract byte-code, a 1-dimensional CNN working on the bytecode directly,and a Long Short-Term Memory (LSTM) neural network. Givena set of vulnerability detectors, we employed five classes ofvulnerabilities. Our results show that CNNs provide a good levelof accuracy and demonstrate the viability of using deep learningtechniques to identify smart contract vulnerabilities.Index Terms—smart contract, code vulnerability, blockchain,deep learning, convolutional neural networksI. INTRODUCTIONSmart contracts gained momentum in recent years. Thegeneral interest concerns their unique features, such as im-mutability, being executed in blockchains, and automatic en-forceability. This has led to lively research activity regardingthe development of tools for building contracts, as well ascareful analysis comparing smart contracts to legal contracts.A recent European Union regulation proposal confirms thisgeneral interest [1]. In particular, in the Data Act proposal,smart contracts are defined as programs on digital ledgersthat execute and settle transactions based on pre-determinedconditions. They are considered a system to promote datasharing, providing data holders and recipients with guaranteesthat conditions for sharing data are respected [2]. However, thisneed for ”guarantees” raises smart contract security concernsthat need to be solved [3].At the time of writing, the most popular blockchain platformto run smart contracts is Ethereum. Ethereum’s smart con-tracts are written in Solidity, a Turing complete programminglanguage. This allows blockchain developers to implementcomplex business logic solutions and to develop decentralizedapplications (dApps) [4]–[6]. However, it also allows for abigger chance of bugs and code vulnerabilities. This is asevere issue since these problems cannot be patched afterdeployment due to the immutable nature of the ledger [7].Thus, a developer should check potentially vulnerable piecesThis work has received funding from the EU H2020 research and innovationprogramme under the MSCA ITN European Joint Doctorate grant agreementNo 814177 LAST-JD - RIoE and from the University of Urbino Carlo Bothrough the “Bit4Food” research project.of code before deploying its contracts: this is often doneby comparing against security patterns, which help ensurethat the code is reliable but is expert-made and expensiveto produce. Automated vulnerability detection methods onlyfocus on known bugs and are too time-consuming or relyon expert-made detection rules. For this reason, some ma-",
        "publication_date": "2023-03-13",
        "authors": "Martina Rossini, Mirco Zichichi, Stefano Ferretti",
        "file_name": "On_the_Use_of_Deep_Neural_Networks_for_Security_Vulnerabilities_Detection_in_Smart_Contracts.pdf",
        "file_path": "./PDFs/On_the_Use_of_Deep_Neural_Networks_for_Security_Vulnerabilities_Detection_in_Smart_Contracts.pdf"
    },
    {
        "title": "eWoT: A Semantic Interoperability Approach for Heterogeneous IoT Ecosystems Based on the Web of Things",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/eWoT",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162312.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Availability: the implementation of eWoT is publicly available at https://github.com/oeg-upm/eWoT."
                    }
                ]
            }
        ],
        "doi": "10.3390/s20030822",
        "abstract": "Abstract: With the constant growth of Internet of Things (IoT) ecosystems, allowing them tointeract transparently has become a major issue for both the research and the software developmentcommunities. In this paper we propose a novel approach that builds semantically interoperableecosystems of IoT devices. The approach provides a SPARQL query-based mechanism to transparentlydiscover and access IoT devices that publish heterogeneous data. The approach was evaluated inorder to prove that it provides complete and correct answers without affecting the response time andthat it scales linearly in large ecosystems.Keywords: semantic interoperability; IoT devices; context-based search; content-based search1. IntroductionIn the last decade, the IoT devices available through the Web have become pervasive [1],allowing users and applications to easily monitor and interact with their devices [2]. Far fromreaching a scenario in which the number of IoT devices will stop growing, researchers foresee aconstant growth in the number of such devices available through the Web in the mid-term future [3,4].The different IoT devices are developed and distributed by different vendors [5]; as a result, these IoTdevices rely on a wide number of heterogeneous models, data formats, and APIs [6].Semantic interoperability aims at dealing with IoT device heterogeneity [7], by enablingan environment in which IoT devices can be included, forming an IoT ecosystem. The ecosystem mustenable transparent mechanisms through which such devices are discoverable [8]. Discovery shouldenable context- and content-based searching (i.e., the discovery of sensors by some meta-data(context-based) or by values that devices are capturing (content-based)). For this purpose,W3C standards have become widely adopted [9], specifically SPARQL [10], to transparently interactwith IoT ecosystems, and ontologies are used to describe the meta-data of the IoT devices. In particular,the Thing Description (TD) model defined by the W3C Web of Things working group [11] is one of themost adopted to profile IoT devices and developing discovery mechanisms [12].A thorough analysis of the current proposals that address semantic interoperability for IoTecosystems was presented by Zout et al. [9], involving approximately 50 proposals. There are mainlytwo approaches for Web-available IoT devices based on the W3C standards that perform both context-and content-based searches: federation and centralised approaches. The former requires each IoTdevice in the ecosystem to enable a SPARQL endpoint, so when a query is issued it is federatedto all the devices in the ecosystem [13]. The latter consists of storing all the sensors’ meta-data ina central triple store, and it requires the IoT devices to push their fresher data into the triple store.Sensors 2020, 20, 822; doi:10.3390/s20030822 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-1823-4484https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttp://www.mdpi.com/1424-8220/20/3/822?type=check_update&version=1http://dx.doi.org/10.3390/s20030822http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleeWoT: A Semantic Interoperability Approach forHeterogeneous IoT Ecosystems Based on the Webof ThingsAndrea Cimmino **®, Maria Poveda-Villal6n **® and Rat Garcia-Castro tOntology Engineering Group (OEG), Universidad Politécnica de Madrid, 28660 Madrid, Spain;rgarcia@fi.upm.es* Correspondence: cimmino@fi.upm.es (A.C.); mpoveda@fi.upm.es (M.P.-V.)+ These authors contributed equally to this work.",
        "publication_date": "2020-02-04",
        "authors": "Andrea Cimmino, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "no_doi_20250624162312.pdf",
        "file_path": "./PDFs/no_doi_20250624162312.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/20/3/822/pdf?version=1580812003"
    },
    {
        "title": "A sustainable process and toolbox for geographical linked data generation and publication: a case study with BTN100",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/btn100",
                "type": "git",
                "paper_frequency": 10,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1186%s40965-019-0060-4.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Original and transformed files, plugin and scripts are available at https://github.com/oeg-upm/btn100."
                    }
                ]
            }
        ],
        "doi": "10.1186/s40965-019-0060-4",
        "abstract": "AbstractWe describe the process and tools that we have used to generate and publish the BTN100 Linked Dataset, based onthe original data from the Spanish Topographic Base (1:100.000 scale) from the Spanish Instituto Geográfico Nacional.We have taken into account the limitations and lessons learned from our initial experience on the generation andpublication of Linked Data from a range of geographical sources in Spain, in 2010, and we have now refined theprocess in order to facilitate: declarative mappings for the transformations from existing open data (shapefiles),automation of transformations whenever there are changes in the original data sources, version control, andalignment with INSPIRE URIs. As a result of this transformation and publication process we have also updated thereference ontology for geographical features and aligned with general ontologies such as GeoSPARQL.Keywords: Geospatial data, Linked data, Linked dataset, OntologyIntroductionOne of the activities of the Spanish Instituto GeográficoNacional1 (IGN) is to produce geographical informationfor all the territorial entities in Spain. IGN is responsi-ble for maintaining and making accessible cartographicand topographic databases for the representation of theSpanish territory. Their catalogs publish data related totransport networks, geodetic information, administrativeunits, etc. making it possible for everyone to downloadthem from their data portal2 under an open data licensecompatible with CC By 4.03.Governments, via their many agencies and organiza-tions, are constantly producing data that may be highlyinterrelated, but in practice become isolated data dueto lack of interoperability. Cartographic and topographicinformation from IGN may easily enrich informationfrom other government entities data, e.g. data fromthe National Institute of Statistics, Institute of CulturalHeritage, General Direction of Cadastre, Geological andMining Institute, etc.*Correspondence: pespinoza@fi.upm.es1Ontology Engineering Group, Universidad Politécnica de Madrid, Boadilla delMonte 28660, SpainFull list of author information is available at the end of the articleHowever, the generalized lack of use of semantics stan-dards in the descriptions of the data elements within thedata sources make it difficult to reuse them. Althoughprogress in data availability, there are still plenty issuesrelated to semantic interoperability; this is the ability ofinformation systems to exchange data with unambiguous,shared meaning.There are several initiatives around the world that havefocused on generating and publishing Linked Data from arange of geospatial data sources, and a W3C/OCGWork-ing Group was running between 2015 and 2017 with thetitle “Spatial Data on the Web” producing recommenda-tions on how to publish different types of geospatial, sen-sor, and temporal data on the web in a principled manner.The LinkedGeoData initiative4 aimed to make availablethe information collected by Open Street Map5 as RDF",
        "publication_date": "2019-03-13",
        "authors": "Paola Espinoza-Arias, Miguel Ángel García-Delgado, Óscar Corcho, Pedro Vivas-White, Hugo Potti-Manjavacas",
        "file_name": "10!1186%s40965-019-0060-4.pdf",
        "file_path": "./PDFs/10!1186%s40965-019-0060-4.pdf",
        "pdf_link": "https://opengeospatialdata.springeropen.com/track/pdf/10.1186/s40965-019-0060-4"
    },
    {
        "title": "Semantic Web and Databases",
        "implementation_urls": [],
        "doi": "10.1007/b106149",
        "abstract": "Abstract. We present R2O, an extensible and declarative language to describe mappings between relational DB schemas and ontologies implemented in RDF(S) or OWL. R2O provides an extensible set of primitives with well-defined semantics. This language has been conceived expressive enough to cope with complex mapping cases arisen from situations of low similarity between the ontology and the DB models.  1   Introduction and Motivations There is a large quantity of data on Web pages generated from relational DBs. This information is often referred to as the Deep Web [2] as opposed to the surface Web comprising all static Web pages. In this paper we face the problem of “upgrading” this large amount of existing content into Semantic Web content. Let us set the following scenario: we have a legacy DB and we want to generate Semantic Web content from it. Until now, three approaches have been reported. The first one, described in [11,12] is based in the semi-automatic generation of an ontology from our DB’s relational model. Then mappings are defined between the DB and the generated ontology. Because the level of similarity between both is very high, mappings will be quite direct and complex mapping situations do not usually appear. This approach does not allow the population of an existing ontology, which is a big limitation. A second approach [6], proposes the manual annotation of dynamic Web pages which publish DB content, with information about the underlying DB and how each content item in a page is extracted from the DB. This approach does not deal neither with complex mapping situations and assumes we want to make our database schema public, which is not always the case. The third approach is the one proposed in this paper. It tries to map an existing DB to an appropriate existing ontology implemented in RDF(S) or OWL. The term appropriate may mean, depending on the situation: The one whose domain has a higher coverage of the domain modeled in the DB, the one whose domain best covers the specific part of the DB to be migrated to the Semantic Web or the one that maximizes the extraction of information according to a particular metric, among others. The literature references very few languages for expressing mappings between ontologies and DBs. Recent approaches like D2R MAP [4] and extended D2R [1] have tackled this problem but they lack of expressiveness for writing complex mapping transformations and are not fully declarative. The language presented in this paper is intended to extend and enhance the mapping description capabilities of these two ones. Complementary approaches to this work can be also found in the Intelligent Information Integration area, in which data from existing heterogeneous DBs are extracted according to ontologies and then combined. The main differences with respect to our approach is that in these systems the mapping between the ontologies and the DBs from which the ontology instances are extracted are not created declaratively but with ad-hoc software implementations. Examples of such systems are Observer [8] and Picsel [5], among others.  The most important aspect of our approach is that we will use the DB and the ontology “as they are” and we will just define a declarative specification of the mappings between their modeling components. That is the reason why the R2O (Relational to Ontology) language which is the base of our approach, has been conceived expressive enough to cope with complex mapping situations arisen from low similarity between the ontology and the DB model (one of them is richer, more generic or specific, better structured, etc., than the other).  This paper is organized as follows: Section 2 describes the main features of the ",
        "publication_date": "2005-01-01",
        "authors": "Christoph Bußler, Val Tannen, Irini Fundulaki",
        "file_name": "10!1007%b106149.pdf",
        "file_path": "./PDFs/10!1007%b106149.pdf"
    },
    {
        "title": "Analysis of the Confidence in the Prediction of the Protein Folding by Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-38079-2_9",
        "abstract": "Abstract. The determination of protein structure has been facilitated using deep learning models, which can predict protein folding from protein sequences. In some cases, the predicted structure can be compared to the already-known distribution if there is information from classic methods such as nuclear magnetic resonance (NMR) spectroscopy, X-ray crystallography, or electron microscopy (EM). However, challenges arise when the proteins are not abundant, their structure is heterogeneous, and protein sample preparation is difficult. To determine the level of confidence that supports the prediction, different metrics are provided. These values are important in two ways: they offer information about the strength of the result and can supply an overall picture of the structure when different models are combined. This work provides an overview of the different deep-learning methods used to predict protein folding and the metrics that support their outputs. The confidence of the model is evaluated in detail using two proteins that contain four domains of unknown function.  Keywords: Protein Structure Prediction, Machine Learning Metrics, Model Confidence.  1 Introduction  Protein folding refers to the mechanism through which a polypeptide chain transforms into its biologically active protein in its 3D structure, and it has a significant impact on different applications, e.g., drug design, protein-protein interaction, and understanding the molecular mechanism of some diseases. There are classic methods to determine the structure of a protein, such as X-ray, NMR, and EM. These methods can be costly and time-consuming because they require significant resources and expertise. Protein folding is a complex process that is challenging for different reasons, including a large number of possible conformations, the crowded cellular environment, and the complex energy landscape required to reach the final structure. The emergence of deep learning methods for predicting protein folding has revolutionized traditional biochemistry. These methods enable in silico predictions, followed by laboratory validation of the findings, offering a new approach to the field. The Critical Assessment of Protein Structure Prediction (CASP) experiments aim to evaluate the current state of the art in protein structure prediction, track the progress made so far, and identify areas where future efforts may be most productively focused. The bi-annual CASP meeting has shown that deep learning methods like AlphaFold, Rosetta, RoseTTAFold and trRosetta, are more effective than traditional approaches that explicitly model the folding process.  AlphaFold2 was introduced as a new computational approach that can predict protein structures with near-experimental accuracy in most cases. The artificial intelligence system, AlphaFold, was submitted to the CASP 14 competition as AlphaFold2, using a completely different model from the previous AlphaFold system in CASP13 [1]. A multimer version has been released, which allows scientists to predict protein complexes and detect protein-protein interactions. The use of AlphaFold-Multimer leads to improved accuracy in predicted multimeric interfaces compared to the input-adapted single-chain AlphaFold, while maintaining a high level of intra-chain accuracy [2]. To fully utilize these methods, researchers require access to powerful computing resources, which is why alternative platforms have been developed. ColabFold is one such platform that offers an accelerated prediction of protein structures and complexes by combining the fast homology search of MMseqs2 with AlphaFold2 [3]. From early 2022, the Galaxy server is able to run AlphaFold 2.0. as part of the tools offered for bioinformatics analysis. DeepMind and EMBL’s European Bioinformatics Institute (EMBL-EBI) did indeed collaborate to create AlphaFold DB, which offers open access to over 200 million protein structure predictions, with the goal of accelerating scientific research [1, 4]. CAMEO (Continuous Automated Model EvaluatiOn) is a community project, developed by the Computational Structural Biology Group, at the SIB Swiss Institute of Bioinformatics and the Biozentrum of the University of Basel. CAMEO is a service that continuously evaluated the accuracy of protein prediction servers based on known experimental structures released by the PDB. Users can submit several models for a target protein in CAMEO, and it will evaluate up to 5 models. Through CAMEO, Robetta, a protein prediction service, undergoes continual evaluation. While deep learning models can achieve impressive accuracy on a wide range of tasks, it’s important to carefully evaluate and interpret their predictions to ensure they are reliable and useful. This involves not only selecting appropriate metrics, but also understanding the underlying assumptions and limitations of the model, and how these can impact its predictions.  The paper is organized in the following order: Section 2 provides a description of the metrics and scores, while Section 3 describes the tools and resources used in this paper. In Section 4, the results obtained are presented and discussed in Section 5. Finally, Section 6 presents the conclusions of this work and outlines future directions for research. ",
        "publication_date": "2023-01-01",
        "authors": "Paloma Tejera-Nevado, Emilio Serrano, Ana González-Herrero, Rodrigo Bermejo-Moreno, Alejandro Rodríguez‐González",
        "file_name": "BioRchiv_ Tejera-Nevado_2023.pdf",
        "file_path": "./PDFs/BioRchiv_ Tejera-Nevado_2023.pdf"
    },
    {
        "title": "The Zaragoza’s Knowledge Graph: Open Data to Harness the City Knowledge",
        "implementation_urls": [],
        "abstract": "Abstract: Public administrations handle large amounts of data in relation to their internal processesas well as to the services that they offer. Following public-sector information reuse regulations andworldwide open data publication trends, these administrations are increasingly publishing their dataas open data. However, open data are often released without agreed data models and in non-reusableformats, reducing interoperability and efficiency in data reuse. These aspects hinder interoperabilitywith other administrations and do not allow taking advantage of the associated knowledge in anefficient manner. This paper presents the continued work performed by the Zaragoza city councilover more than 15 years in order to generate its knowledge graph, which constitutes the key piece oftheir data management system, whose main strengthen is the open-data-by-default policy. The mainfunctionalities that have been developed for the internal and external exploitation of the city’s opendata are also presented. Finally, some city council experiences and lessons learned during this processare also explained.Keywords: public administration; linked open data; knowledge graph; interoperability; ontologies;open government data; vocabularies; API1. IntroductionPublic administrations are large data producers and collectors due to the need to provide supportfor their internal processes, as well as for services offered to citizens. These data are traditionallyavailable in several formats, according to their own information models, and are managed in different,sometimes isolated, data sources. From the point of view of the organization, the importance of datais undeniable. A proper organizational data management improves decision-making, operationalefficiency and service provision. From the side of citizens, the public administration’s data are anessential asset that must be made available in order to enhance transparency and the accountability [1].From the point of view of re-users, having these data available enables developing value-addedservices and applications, consequently stimulating innovation and economy [2]. Having all thesebenefits in mind, public administrations are increasingly publishing their data as Open Data, definedas data that can be freely used, re-used and redistributed by anyone—subject only, at most, to therequirement to attribute and sharealike [3]. An Open Government Data strategy ensures that dataare made available across city departments and to third parties, contributes to citizen engagement,increases democracy and serves to drive economic growth and social improvement [4,5].However, these data are mostly represented with information models defined without previousagreements or consensus processes with other institutions, which hinders the possibility of using andInformation 2020, 11, 129; doi:10.3390/info11030129 www.mdpi.com/journal/informationhttp://www.mdpi.com/journal/informationhttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0002-9260-0753http://dx.doi.org/10.3390/info11030129http://www.mdpi.com/journal/informationhttps://www.mdpi.com/2078-2489/11/3/129?type=check_update&version=20101001010 information fmoPrZ01010ArticleThe Zaragoza’s Knowledge Graph: Open Data toHarness the City KnowledgePaola Espinoza-Arias ’*, Maria Jestis Fernandez-Ruiz 2, Victor Morlan-Plo 2,Rubén Notivol-Bezares 2 and Oscar Corcho !1 Ontology Engineering Group, ETSI Informaticos, Universidad Politécnica de Madrid, 28660 Madrid, Spain;ocorcho@fi.upm.es",
        "file_name": "no_doi_20250624162340.pdf",
        "file_path": "./PDFs/no_doi_20250624162340.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/11/3/129/pdf"
    },
    {
        "title": "Multiagent Systems on Virtual Games: A Systematic Mapping Study",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Unity-Technologies/ml-agents",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Multiagent_Systems_on_Virtual_Games_A_Systematic_Mapping_Study.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/Unity-Technologies/ml-agents RTS games [21], [30] or 3-D with an isometric perspective that can be abstracted from the interaction of a 2-D game, such as Starcraft [32], [33], [38]."
                    }
                ]
            }
        ],
        "doi": "10.1109/tg.2022.3214154",
        "abstract": "Abstract—Context: Games are a well-established scenario to testAI and multiagent systems (MAS) proposals due to their popularityand defiance. However, there is no big picture of the application ofthis technology to games, the evolution of the kind of problem tack-led, or the game scenarios in which agents have been experimented.Objective: To perform a systematic mapping to characterize thestate of the art in the field of MAS applied to virtual games and toidentify trends, strengths, and gaps for further research. Method:A systematic mapping study has been conducted to find primarystudies in the field. A search was performed on title, abstracts,and keywords, whilst classification, data extraction, and furtheranalysis were performed according to specific criteria focused onMAS papers with experimentation and evidence in a game scenario.Results: 78 studies published between 1998 and 2021 were found.Studies have been classified according to the MAS problem facedand the agent reasoning strategy. We detect that machine learning isthe most common AI technique for MAS in games, considering bothreinforcement learning and evolutionary techniques. MAS are usedin a variety of gaming genres, especially in real-time strategy (RTS),sports, and simulation. Conclusions: RTS and sports games are wellsuited for concrete MAS problems, such as multiagent planning andtask allocation. Expanding evidence and experimentation on otheraspects related to scalability and usability issues is discussed. ThoseMAS problems and experiments that remain slightly modeled ongames or are not thoroughly studied yet have been also identified.Index Terms—Games, multiagent systems (MASs), systematicmapping study (SMS), virtual environments.I. INTRODUCTIONIN THE last decade, the emergence and improvement of AImechanisms have provided new opportunities to combinethese novel techniques with multiagent systems (MAS), leadingresearchers to elicit and analyze the benefits, problems, andchallenges of that combination.Manuscript received 12 February 2021; revised 16 April 2022 and 25 July2022; accepted 8 October 2022. Date of publication 12 October 2022; date ofcurrent version 16 June 2023. (Corresponding author: Jose Barambones.)Jose Barambones, Ismael Sánchez-Rivero, and Ricardo Imbert are with theMadrid HCI Lab, Department of Computer Languages and Systems and Soft-ware Engineering, Universidad Politécnica de Madrid, 28660 Madrid, Spain, andalso with the Escuela Técnica Superior de Ingenieros Informáticos, UniversidadPolitécnica de Madrid, 28660 Madrid, Spain (e-mail: j.barambones@upm.es;i.sanchez@madhcilab.es; ricardo.imbert@upm.es).Juan Cano-Benito is with the Escuela Técnica Superior de Ingenieros Infor-máticos, Universidad Politécnica de Madrid, 28660 Madrid, Spain, and also withthe Ontology Engineering Group, Department of Artificial Intelligence, Spain(e-mail: jcano@fi.upm.es).Florian Richoux is with the National Institute of Advanced Indus-trial Science and Technology (AIST), Tokyo 135-0064, Japan (e-mail: flo-rian.richoux@aist.go.jp).Color versions of one or more figures in this article are available at",
        "publication_date": "2022-10-12",
        "authors": "Jose Barambones, Juan Cano-Benito, Ismael Sánchez-Rivero, Ricardo Imbert, Florian Richoux",
        "file_name": "Multiagent_Systems_on_Virtual_Games_A_Systematic_Mapping_Study.pdf",
        "file_path": "./PDFs/Multiagent_Systems_on_Virtual_Games_A_Systematic_Mapping_Study.pdf"
    },
    {
        "title": "SAREF4INMA: A SAREF extension for the industry and manufacturing domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/idafensp/ar2dtool",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-200402.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "More details related to LOT are available online in its website.12 The following sections present the main definitions and guidelines provided by the methodology for each of the above-mentioned activities."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-200402",
        "abstract": "Abstract. The IoT landscape is characterized by a fragmentation of standards, platforms and technologies, often scattered amongdifferent vertical domains. To prevent the market to continue to be fragmented and power-less, a protocol-independent semanticlayer can serve as enabler of interoperability among the various smart devices from different manufacturers that co-exist in aspecific industry domain, but also across different domains. To that end, the SAREF ontology was created in 2015 with theintention to interconnect data, enabling the communication between IoT devices that use different protocols and standards. Anumber of industrial sectors consequently expressed their interest to extend SAREF into their domains in order to fill the gaps ofthe semantics not yet covered by their communication protocols. Therefore, the SAREF4INMA ontology was recently createdto extend SAREF for describing the Smart Industry & Manufacturing domain. SAREF4INMA is based on several standards andIoT initiatives, as well as on real use cases, and includes classes, properties and instances specifically created to cover the industryand manufacturing domain. This work describes the approach followed to develop this ontology, specifies its requirements andalso includes a practical example of how to use it.Keywords: industry 4.0, ontology, standard, SAREF, SAREF4INMA1. IntroductionThis paper presents the resulting model after extending the Smart Applications REFerence ontology (SAREF)for the Industry & Manufacturing domain1 together with the methodology followed and modelling decisions takenduring the development. This paper builds on the success achieved in the past years with SAREF2, which is areference ontology for IoT created in close interaction with the industry [1] during a study requested by the EuropeanCommission in 20153. SAREF is published as an ETSI Technical Specification series that also includes dedicatedextensions to specific domains (TS 103 410, parts 1-6). A proof-of-concept solution based on SAREF in the energydomain and implemented on existing commercial products4 was demonstrated in 2017 [2].The motivation behind SAREF is that the IoT landscape is characterized by a fragmentation of standards, plat-forms and technologies, often scattered among different vertical domains [3, 4] . To prevent the market to continue*Corresponding author. E-mail: mike.deroode@tno.nl.1https://portal.etsi.org/STF/stfs/STFHomePages/STF5342https://ec.europa.eu/digital-single-market/en/blog/new-standard-smart-appliances-smart-home3https://sites.google.com/site/smartappliancesproject4https://ec.europa.eu/digital-single-market/en/news/digitalising-energy-sector-common-language-consumer-centric-world1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:mike.deroode@tno.nlmailto:laura.daniele@tno.nlmailto:albafernandez@fi.upm.esmailto:mpoveda@fi.upm.esmailto:rgarcia@fi.upm.esmailto:mike.deroode@tno.nl2 M.A.W. de Roode et al. / SAREF for Industry and Manufacturing1 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 15",
        "publication_date": "2020-10-16",
        "authors": "Mike de Roode, Alba Fernández-Izquierdo, Laura Daniele, María Poveda‐Villalón, Raúl García‐Castro",
        "file_name": "10!3233%sw-200402.pdf",
        "file_path": "./PDFs/10!3233%sw-200402.pdf"
    },
    {
        "title": "On The Modeling Of P2P Systems as Temporal Networks: a Case Study With Data Streaming",
        "implementation_urls": [],
        "doi": "10.23919/annsim55834.2022.9859513",
        "abstract": "AbstractTemporal networks are a useful tool to model complex systems’ dynamics, especially when they are charac-terized by high dynamicity. While there is strong literature on simulation tools for complex and dynamicalsystems, there is a lack of viable solutions to model and exploit temporal graphs in simulation. In this work,we present a system devised to simulate complex systems and their evolution by using temporal graphs datastructures. As a use case, we focus on data dissemination over peer-to-peer systems characterized by a rele-vant presence of churns. The simulation of dissemination algorithms on temporal graphs involves evaluatingtheir efficiency in terms of coverage, delay, and number of messages sent. In particular, a reasonable trade-off between the speed of delivery and the generated network traffic must be found. In this work, besidestraditional gossip strategies, a more complex scenario is considered, where a second overlay, structured asa tree, is built for more efficient propagation of data from the source to the interested peers only. Thisscenario is analyzed using multiple simulated strategies. More specifically, we investigate how the simu-lation methodology can be used for evaluating the efficiency of dissemination protocols in a peer-to-peerenvironment but with a specific focus on the modelling of churn.Keywords: simulation, temporal networks, peer-to-peer, dissemination, gossip.1 INTRODUCTIONComplex systems are quite often modeled and simulated as a set of entities that somehow interact in anenvironment. Interactions are made possible among those entities that share some specific properties (e.g.,they are at a short distance, given whatever definition of distance). The possibility of interaction betweentwo entities is often represented as a link between the two entities, thus creating an interaction network. Thelogical connections among the entities involved, depending on the nature of the system, might be subject tochanges over time. That means either that (i) new entities enter the system at some point while other entitiesmay disappear, or that (ii) the links between entities are not stable and can change over time. Thus, uponthe need to model and study the dynamics of such complex systems, modelling and simulation techniquesneed to consider the temporariness of the links, which can actually be relevant for the outcome of the tests.Serena, Zichichi, D’Angelo, and FerrettiSpecifically, in our paper, we employ temporal networks in order to simulate a peer-to-peer (P2P) streamingscenario, where multiple nodes (i.e. downloaders) listen to a stream of data emitted from a specific peer(i.e. the streamer node). In a P2P scenario, the nodes form an overlay network (i.e. a logical network builton top of another network, like the Internet), and the structuring of the overlay changes over time, due to newparticipants joining the system and other nodes leaving the network. The purpose of this paper is to proposea methodology and a simulation tool that enables the analysis and the comparison of different strategies forstreaming-like applications. Modelling and simulation techniques permit investigating the impact of certainstrategies and parameters on the performance of the application, analyzing also how the dynamic topologyof the overlay affects the functioning of the system. For example, the delay experienced for messages deliv-ery and the amount of network traffic are big concerning issues for P2P systems, and it is desirable to findan appropriate trade-off, finding solutions that enable the reduction of the number of messages sent whilenot increasing too considerably the average delay. Due to the volatility of the participants, we think thattemporal graphs are an appropriate data structure for the modelling of this kind of system.Through the use of LUNES-Temporal, a simulator specifically designed for distributed systems, we wereable to model the temporal graphs and the P2P protocols, focusing on the overlay aspects of the system.Specifically, the most relevant aspects considered in the modelling of the system are: (i) the usage of atemporal graph for the adequate representation of churns, (ii) different weights in terms of messages sizefor a precise estimation of the overall network traffic, (iii) information about nodes obtained only throughthe exchange of messages, (iv) hops as the measure for expressing the delay, according to a time-steppedsimulation approach, where a time-step represents an atomic unit of time (v) possibility of nimbly changingnetwork and protocols parameters in order to evaluate their impact.In this work, we want to display some techniques for modelling dynamics systems through the use of tem-poral networks. We show that it is possible to create efficient and scalable simulators, which allow theusers to carry out different experiments by easily customizing the behaviour of the agents and the network",
        "publication_date": "2022-07-18",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "24modeling.pdf",
        "file_path": "./PDFs/24modeling.pdf"
    },
    {
        "title": "PaCTS 1.0: A Crowdsourced Reporting Standard for Paleoclimate Data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/idafensp/ar2dtool",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1029%2019pa003632.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "More details related to LOT are available online in its website.12 The following sections present the main definitions and guidelines provided by the methodology for each of the above-mentioned activities."
                    }
                ]
            }
        ],
        "doi": "10.1029/2019pa003632",
        "abstract": "Abstract. The IoT landscape is characterized by a fragmentation of standards, platforms and technologies, often scattered amongdifferent vertical domains. To prevent the market to continue to be fragmented and power-less, a protocol-independent semanticlayer can serve as enabler of interoperability among the various smart devices from different manufacturers that co-exist in aspecific industry domain, but also across different domains. To that end, the SAREF ontology was created in 2015 with theintention to interconnect data, enabling the communication between IoT devices that use different protocols and standards. Anumber of industrial sectors consequently expressed their interest to extend SAREF into their domains in order to fill the gaps ofthe semantics not yet covered by their communication protocols. Therefore, the SAREF4INMA ontology was recently createdto extend SAREF for describing the Smart Industry & Manufacturing domain. SAREF4INMA is based on several standards andIoT initiatives, as well as on real use cases, and includes classes, properties and instances specifically created to cover the industryand manufacturing domain. This work describes the approach followed to develop this ontology, specifies its requirements andalso includes a practical example of how to use it.Keywords: industry 4.0, ontology, standard, SAREF, SAREF4INMA1. IntroductionThis paper presents the resulting model after extending the Smart Applications REFerence ontology (SAREF)for the Industry & Manufacturing domain1 together with the methodology followed and modelling decisions takenduring the development. This paper builds on the success achieved in the past years with SAREF2, which is areference ontology for IoT created in close interaction with the industry [1] during a study requested by the EuropeanCommission in 20153. SAREF is published as an ETSI Technical Specification series that also includes dedicatedextensions to specific domains (TS 103 410, parts 1-6). A proof-of-concept solution based on SAREF in the energydomain and implemented on existing commercial products4 was demonstrated in 2017 [2].The motivation behind SAREF is that the IoT landscape is characterized by a fragmentation of standards, plat-forms and technologies, often scattered among different vertical domains [3, 4] . To prevent the market to continue*Corresponding author. E-mail: mike.deroode@tno.nl.1https://portal.etsi.org/STF/stfs/STFHomePages/STF5342https://ec.europa.eu/digital-single-market/en/blog/new-standard-smart-appliances-smart-home3https://sites.google.com/site/smartappliancesproject4https://ec.europa.eu/digital-single-market/en/news/digitalising-energy-sector-common-language-consumer-centric-world1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:mike.deroode@tno.nlmailto:laura.daniele@tno.nlmailto:albafernandez@fi.upm.esmailto:mpoveda@fi.upm.esmailto:rgarcia@fi.upm.esmailto:mike.deroode@tno.nl2 M.A.W. de Roode et al. / SAREF for Industry and Manufacturing1 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 15",
        "publication_date": "2019-09-03",
        "authors": "Deborah Khider, Julien Emile‐Geay, Nicholas P. McKay, Yolanda Gil, Daniel Garijo, Varun Ratnakar, Montserrat Alonso‐García, Sébastien Bertrand, Oliver Bothe, Peter W. Brewer, Andrew G. Bunn, Manuel Chevalier, Laia Comas‐Bru, Adam Csank, Émilie Pauline Dassié, Kristine L. DeLong, Thomas Felis, Pierre Francus, Amy Frappier, William R. Gray, Simon Goring, Lukas Jonkers, Michael Kahle, Darrell S. Kaufman, Natalie Kehrwald, Belén Martrat, Helen McGregor, Julie N. Richey, Andreas Schmittner, Nick Scroxton, Elaine Kennedy Sutherland, Kaustubh Thirumalai, Kathryn Allen, Fabien Arnaud, Yarrow Axford, Timothy T. Barrows, Lucie Bazin, Suzanne E. Pilaar Birch, Elizabeth Bradley, Joshua C. Bregy, Émilie Capron, Olivier Cartapanis, Hong‐Wei Chiang, K. M. Cobb, Maxime Debret, René Dommain, Jianghui Du, Kelsey A. Dyez, Suellyn Emerick, Michael P. Erb, Georgina Falster, Walter Finsinger, Daniel Fortier, Nicolas Gauthier, S. E. George, Eric C. Grimm, J. E. Hertzberg, Fiona Hibbert, Aubrey L. Hillman, Will Hobbs, Matthew Huber, Anna L.C. Hughes, Samuel L. Jaccard, Jiaoyang Ruan, Markus Kienast, Bronwen Konecky, Gaël Le Roux, Vyacheslav Lyubchich, Valdir F. Novello, Lydia Olaka, J. W. Partin, Christof Pearce, Steven J. Phipps, Cécile Pignol, Natalia Piotrowska, Maria-Serena Poli, Alexander A. Prokopenko, Franciéle Schwanck, Christian Stepanek, George E. A. Swann, Richard J. Telford, Elizabeth R. Thomas, Zoë Thomas, S. A. Truebe, Lucien von Gunten, A. J. Waite, Nils Weitzel, Bruno Wilhelm, John W. Williams, Joseph W. Williams, Mai Winstrup, Ning Zhao, Yuxin Zhou",
        "file_name": "10!1029%2019pa003632.pdf",
        "file_path": "./PDFs/10!1029%2019pa003632.pdf"
    },
    {
        "title": "Introduction to Linked Data and Its Lifecycle on the Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-39784-4_1",
        "publication_date": "2013-01-01",
        "authors": "Sören Auer, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Amrapali Zaveri",
        "file_name": "no_doi_20250624162447.pdf",
        "file_path": "./PDFs/no_doi_20250624162447.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_1.pdf"
    },
    {
        "title": "DLT-based Data Mules for Smart Territories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/luca-Serena/lunes-tdm-islands",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/DLT-based_Data_Mules_for_Smart_Territories.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/luca-Serena/lunes-tdm-islands [16] M."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn54977.2022.9868916",
        "abstract": "Abstract—Many services that are taken for granted in smartcities are not even remotely available in dislocated areas yet, dueto the lack of or too costly wide area network connectivity. Withthe aim to offer a practical and secure way to transport dataand allow for communications in such constrained scenarios, wefocus on the problem of incentivizing to data mules, i.e. devicesdedicated to enable the data transfer even in the absence of theInternet. Our solution combines the use of several distributedtechnologies for verifying the correct behavior of all the partici-pants and incentivize them. We focus on the use of state channelsto support the flow of smart-contract-based tokens as a form ofpayment, in a condition where participants communicate onlywith others in physical proximity. Furthermore, we validate theviability of the application through the simulation of peer-to-peerinteractions between the participants. In this work we achievepositive results in terms of communication latency and percentageof client nodes which are able to benefit from the system.Index Terms—Data Mules, Distributed Ledger Technologies,State Channels, Smart TerritoriesI. INTRODUCTIONNowadays, we are at a crossroad. People are to some extentbeing constrained to move to big, smart cities, due to thehigher opportunities in terms of work and offered services.Conversely, recent events, such as the COVID-19 pandemic,have shown how this trend can be reversed, with an increasingnumber of people deciding to move to the countryside andrural areas. Almost, since for sure many services that are takenfor granted in (smart) cities, are not even remotely available indislocated areas yet. For some underprivileged territories it isnot possible to implement (costly) smart cities services due tothe very different economic circumstances or due to unavail-able, unreliable or too expensive network infrastructures [1].We argue that what is needed is a set of novel opportunisticsolutions, which allows us to share and reuse data, services,computation and bandwidth. Such a solution would simplifythe development of new services and the integration of legacytechnologies into new ones. Well-known examples consist oftechnologies such as multi-homing mobile services, mobilead-hoc networks, opportunistic networks, peer-to-peer andfog computing systems [1]. In this novel “smart territory”case, however, such applications might not be supported bya wide area network connectivity, and certain networkingsolutions might result as too costly (e.g. satellite connections).This work has received funding from the EU H2020 research and innovationprogramme under the MSCA ITN European Joint Doctorate grant agreementNo 814177 LAST-JD - RIoE and from the University of Urbino through the“Bit4Food” research project.Data Mules (that is an acronym for Mobile Ubiquitous LANExtensions [2]) allow for communication and data transfereven in the absence of Internet, and they can be important tools",
        "publication_date": "2022-07-01",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "DLT-based_Data_Mules_for_Smart_Territories.pdf",
        "file_path": "./PDFs/DLT-based_Data_Mules_for_Smart_Territories.pdf"
    },
    {
        "title": "Understanding the phenomenology of reading through modelling",
        "implementation_urls": [],
        "doi": "10.3233/sw-200396",
        "abstract": "Abstract This chapter presents Linked Data, a new form of distributed data on theweb which is especially suitable to be manipulated by machines and to shareknowledge. By adopting the linked data publication paradigm, anybody can publishdata on the web, relate it to data resources published by others and run artificialintelligence algorithms in a smooth manner. Open linked data resources maydemocratize the future access to knowledge by the mass of internet users, eitherdirectly or mediated through algorithms. Governments have enthusiasticallyadopted these ideas, which is in harmony with the broader open data movement.Keywords Linked data � Semantic web � Democracy � Ontologies � Knowledgerepresentation � eDemocracy1.1 IntroductionMore than half of the world’s population has access to the Internet. Vast amounts ofknowledge accumulated in roughly 2 billion websites are available to anyone whois able to read and can afford an internet connection.Entertainment habits, interpersonal human relations and almost any conceivableaspect of human life have been profoundly transformed with the arrival of theinternet. Yet modern democracies have remained relatively unaffected. It is true thatpropaganda techniques have undergone changes, political parties organize theircampaign strategies differently and the idea of eDemocracy is perhaps about tohatch; but the public institutions, the habits of citizens and the overall political gameare all apparently the same.We have to indulge—Internet is a new thing. But a careful observation of theevolution of technologies and the new organizational forms they enable revealdiscrete signs of change, now with little effect but potentially of much impact.This chapter introduces some new technologies and ideas which may seemirrelevant today, but which will probably exert a powerful influence on the forth-coming transformations of the concept of democracy.© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_11.2 The World Wide Web as a Source of Dataand Knowledge1.2.1 Data, Information and KnowledgeMarshall McLuhan described technology as extensions of man (McLuhan 1964),whereby our bodies and our senses are extended beyond their natural limits.Certainly, a shovel is an improvement of our hands when we dig a trench andtelescopes are augmented eyes when we look at the stars. In top level chess tour-naments, chess players prepare their games and study their opponents with a jointteam of humans and machine—machines also extend human’s capabilities forthinking.In order to make a value judgement, we need data—this is a truism. But todaywe also need machines which need data. Whenever we take an important decision,we usually google for some related information. Our decisions are mediated byinformation provided by a company, or a handful of companies, whose interestsmay not match our interests. Maybe in the future we will have a wider range of",
        "publication_date": "2020-09-29",
        "authors": "Alessio Antonini, Mari Carmen Suárez-Figueroa, Alessandro Adamou, Francesca Benatti, François Vignale, Guillaume Gravier, Lucia Lupi",
        "file_name": "10!3233%sw-200396.pdf",
        "file_path": "./PDFs/10!3233%sw-200396.pdf"
    },
    {
        "title": "A Linked Data Terminology for Copyright Based on Ontolex-Lemon",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_28",
        "abstract": "Abstract. Ontolex-lemon is the de facto standard to represent lexica relative toontologies and it can be used to encode term banks as RDF. Amulti-lingual, multi-jurisdictional term bank of copyright-related concepts has been published aslinked data based on the ontolex-lemonmodel. The terminology links informationfrom WIPO (concepts and definitions), IATE (multilingual terms, usage notes)and other sources as Creative Commons (multilingual definitions) or DBpedia(general concepts). The terms have been hierarchically arranged, spanning mul-tiple languages and targeting different jurisdictions. The term bank has beenpublished as a TBX dump file and is publicly accessible as linked data. The termbank has been used to annotate common licenses in the RDFLicense dataset.Keywords: Term bank � Linked data � Copyright � Legal localizationMultilingualism1 IntroductionLegal translations, namely the translations of texts within the field of law, are amongthe most difficult types of translations. The legal system referred by the source text maybe different from the legal system referred by the target text, and the translation of theparts with a specific legal significance must be particularly precise at ensuring thecorrespondence of concepts at both sides. The mistranslation of a clause in a contractcan lead to lawsuits or loss of money.A term bank (also known as term base or more informally as terminology) is adatabase of concepts and terminological data related to a particular field. Terminologieshelp keeping translations consistent and help choosing the most adequate term whenprecision is required. Further, the localization of legal texts require of specializedterminologies where the exact concept in a legal system must be invoked.The work presented in this paper describes a terminology created in a half-automated process, where terms and their definitions have been extracted and inte-grated from different lexical sources and mapped in a supervised process.© Springer Nature Switzerland AG 2018U. Pagallo et al. (Eds.): AICOL VI-X 2015–2017, LNAI 10791, pp. 410–423, 2018.https://doi.org/10.1007/978-3-030-00178-0_28http://orcid.org/0000-0003-1076-2511http://orcid.org/0000-0002-0980-2371http://orcid.org/0000-0002-3037-0331http://orcid.org/0000-0001-6452-7627http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_28&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_28&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00178-0_28&amp;domain=pdfThe resulting terminology has been published1 in the TBX format – ISO 30042 [1]– which is the standard for the exchange of terminologies; and it has also been pub-lished in Resource Description Format (RDF)2, according to the schema described byCimiano et al. [4]. The RDF version is especially suitable for establishing links withother resources (like DBpedia3) and with other terminologies. IATE4, the inter-institutional database of the European Union (EU), has been taken as the externalreference for some of the extracted terms.Plain texts can be annotated, makin-g reference to concepts or terms in a term bank.This work also presents the text of a license that has been annotated with the terms inthe copyright terminology here presented.The use of a terminology of legal terms found in licenses is not exhausted with themere translation or localization. Once in a digital format, it can alleviate the task ofidentifying the key elements in new licenses as in [5] or can help the study of com-",
        "publication_date": "2018-01-01",
        "authors": "Victor Rodrı́guez-Doncel, Cristiana Santos, Pompeu Casanovas, Asunción Gómez‐Pérez, Jorge Gracia",
        "file_name": "978-3-030-00178-0_28.pdf",
        "file_path": "./PDFs/978-3-030-00178-0_28.pdf"
    },
    {
        "title": "Ontological Representation of Smart City Data: From Devices to Cities",
        "implementation_urls": [],
        "doi": "10.3390/app9010032",
        "publication_date": "2018-12-22",
        "authors": "Paola Espinoza-Arias, María Poveda‐Villalón, Raúl García‐Castro, Óscar Corcho",
        "file_name": "no_doi_20250624162516.pdf",
        "file_path": "./PDFs/no_doi_20250624162516.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/9/1/32/pdf?version=1545833153"
    },
    {
        "title": "Coming to Terms with FAIR Ontologies",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-61244-3_18",
        "abstract": "Abstract. Ontologies are widely used nowadays for many different pur-poses and in many different contexts, like industry and research, and indomains ranging from geosciences, biology, chemistry or medicine. Whenused for research, ontologies should be treated as other research artefacts,such as data, software, methods, etc.; following the same principles usedto make them findable, accessible, interoperable and reusable (FAIR) toothers. However, in comparison to the number of guides, indicators andrecommendations available for making research data FAIR, not muchattention has been paid so far on how to publish ontologies following theFAIR principles. This position paper reviews the technical and socialneeds required to define a roadmap for generating and publishing FAIRontologies on the Web. We analyze four initiatives for ontology publica-tion, aligning them in a common framework for comparison. The paperconcludes by opening a discussion about existing, ongoing and requiredinitiatives and instruments to facilitate FAIR ontology sharing on theWeb.Keywords: FAIR principles · Ontologies · Semantics1 IntroductionSince its inception in 2016, the FAIR (Findable, Accessible, Interoperable,Reusable) data principles [35] have gained an increasing importance in the con-text of research data management, and are being adopted by a large numberof private and public organisations worldwide, including initiatives such as theEuropean Open Science Cloud1 (EOSC) or the Research Data alliance2 (RDA).Ontologies play a relevant role in some of the FAIR data principles, espe-cially in relation to providing support for data “interoperability” and “reusabil-ity”. The need for ontologies (also called vocabularies) is pointed out in the1 https://www.eosc-portal.eu/.2 https://www.rd-alliance.org/.c© Springer Nature Switzerland AG 2020C. M. Keet and M. Dumontier (Eds.): EKAW 2020, LNAI 12387, pp. 255–270, 2020.https://doi.org/10.1007/978-3-030-61244-3_18http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-61244-3_18&domain=pdfhttp://orcid.org/0000-0003-3587-0367http://orcid.org/0000-0002-3938-2064http://orcid.org/0000-0003-0454-7145http://orcid.org/0000-0002-9260-0753https://www.eosc-portal.eu/https://www.rd-alliance.org/https://doi.org/10.1007/978-3-030-61244-3_18256 M. Poveda-Villalón et al.following principles: data and metadata should (I2)3 use vocabularies that fol-low FAIR principles, (I1) use a formal, accessible, shared, and broadly applicablelanguage for knowledge representation; (I3) include qualified references to other(meta)data, and (R1.3) meet domain-relevant community standards. Ontologiesare also relevant in terms of “findability”, (F2) requiring to describe data withrich metadata.The research community has already acknowledged the need for ontologiesto follow the FAIR principles [7]. First, there is a clear movement towardsexpanding the application of the FAIR principles beyond research data, asdescribed in the ongoing EOSC Interoperability Framework [8]. Since ontolo-",
        "publication_date": "2020-01-01",
        "authors": "María Poveda‐Villalón, Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "978-3-030-61244-3_18.pdf",
        "file_path": "./PDFs/978-3-030-61244-3_18.pdf"
    },
    {
        "title": "Legal Ontologies for the Spanish e-Government",
        "implementation_urls": [],
        "doi": "10.1007/11881216_32",
        "abstract": "Abstract. The Electronic Government is a new field of applications for the se-mantic web where ontologies are becoming an important research technology. The e-Government faces considerable challenges to achieve interoperability given the semantic differences of interpretation, complexity and width of scope. In this paper we show the results obtained in an ongoing project commissioned by the Spanish government that seeks strategies for e-Government to reduce the problems encountered when delivering services to citizens. Here we present an e-Government ontology model; within this model a set of legal ontologies are devoted to represent the Real-estate transaction domain used to illustrate this paper; some examples of use of these legal ontologies are given. 1   Introduction and Motivation Electronic Government (e-Gov) is an important application field [2] for the transfor-mations that governments and public administrations will have to undergo in the next decades. Therefore to transform the e-Gov into the e-Governance, the e-Gov research needs to be based on a robust theory, on modelling approaches, and on planning. In this scenario, it is crucial to manage the legal knowledge for improving the systems applications in different ways.  For over more than two decades the AI and Law community has been very active and productive. In the early 80´s, research was focused on logic programming, and all the efforts were centered on legislation and on legal reasoning. Another approach adopted was the case-based reasoning, which was not as formal as logic was, aimed at finding similarities in legal cases and allowed retrieving relevant cases for the judges. Knowledge engineering was also of interest for the research community and the field most applied; this area allowed developing and using the legal ontologies that underlie the growing of the Semantic Web. The Semantic Web was proposed by Tim Berners-Lee [7] as a new field of re-search, and according to World Wide Web Consortium1 (W3C) the Semantic Web is defined as “an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. It is based on the idea of having data on the Web defined and linked such that it can be                                                            1 http://www.w3.org/2001/sw used for more effective discovery, automation, integration, and reuse across various applications”. The Semantic Web at e-Gov is new; it features knowledge representation, knowl-edge engineering, database design, information systems, database integration, natural language understanding, information retrieval and semantic portals, among others. The Semantic Web is considered to be the infrastructure upon which all intelligent e-Gov applications will be built in the near future. Within the objectives of the Semantic Web the ontologies play an important role. “Ontology” is a word taken from Philosophy where it is used as a systematic explana-tion of \"existence\". In the field of the Artificial Intelligence, Neches [11] defined an ontology for the first time in the following way: \"Ontology defines the basic terms and the relations that include the vocabulary of a specific area, in addition to the rules to combine terms and relations to define extensions to the vocabulary\". It is possible to say that this definition serves us as a kind of guide to construct ontologies. Accord-ing to Neche´s definition, an ontology does not include only the terms that explicitly are defined in it, but also those that can be inferred using rules. Gruber defines the ontology as: \"An explicit specification of a conceptualization\" [5, 6]. The e-Gov has been strengthened with all these previous studies carried out by the ",
        "publication_date": "2006-01-01",
        "authors": "Asunción Gómez‐Pérez, Fernando Ortiz, Boris Villazón-Terrazas",
        "file_name": "Legal_Ontologies_for_the_Spanish_e-Government.pdf",
        "file_path": "./PDFs/Legal_Ontologies_for_the_Spanish_e-Government.pdf"
    },
    {
        "title": "Ontology evolution for personalised and adaptive activity recognition",
        "implementation_urls": [],
        "doi": "10.1049/iet-wss.2018.5209",
        "abstract": "Abstract: Ontology-based knowledge-driven activity recognition (AR) models play a vital role in realm of Internet of Things(IoTs). However, these models suffer the shortcomings of static nature, inability of self-evolution, and lack of adaptivity. Also, ARmodels cannot be made comprehensive enough to cater all the activities and smart home inhabitants may not be restricted toonly those activities contained in AR model. So, AR models may not rightly recognise or infer new activities. Here, a frameworkhas been proposed for dynamically capturing the new knowledge from activity patterns to evolve behavioural changes in ARmodel (i.e. ontology based model). This ontology-based framework adapts by learning the specialised and extended activitiesfrom existing user-performed activity patterns. Moreover, it can identify new activity patterns previously unknown in AR model,adapt the new properties in existing activity models and enrich ontology model by capturing change representation to enrichontology model. The proposed framework has been evaluated comprehensively over the metrics of accuracy, statisticalheuristics, and Kappa coefficient. A well-known dataset named DAMSH has been used for having an empirical insight into theeffectiveness of proposed framework that shows a significant level of accuracy for AR models.1 IntroductionHuman activity recognition (HAR) determines the activities thathave been performed by humans based upon certain knowledgeand context. Earlier, activity recognition (AR) was performed byobserving and analysing human activities through surveillancecameras. Such manual observation-driven AR seemed cost-intensive and demanding around the clock e.g. personneldeployment in homecare services was infeasible financially.However, automated HAR systems resolved the issues byproviding efficient and cost-effective measures instead of human-centred observations and analysis. Continuous scientific andtechnical progress has directed the human expectation from HARtowards personalised HAR [1] for personalised service provision.A rich growth of data-driven and knowledge-driven modellingtechniques have been proposed in [2–4]. Limitations of data-drivenproblems are cold start problem and non-reusability [2, 5].Whereas knowledge-driven techniques are static in nature,incomplete, and non-adaptable [3, 5]. One recent contribution inknowledge-driven techniques is based upon ontologies [4, 6].Compared with rest of the approaches, ontology-based modelsprovide higher degree of automation, better reasoning ability, andsolid technological foundations but still lacking the self-evolution.Here, we extend our work described in [7] for ontology evolution.Proposed ontological model for AR adopts hybrid activitymodelling approach (knowledge-driven and data-driven) in whichseed knowledge about activities is modelled in an ontology. Seedknowledge comprises of set of actions necessary to perform anactivity called perceptible activity models (PAMs). Modeldescribed in [7] transfer the sensor stream into action properties(AP). This sensor stream is used with different ontological contextssuch as duration, location, object type, temporal dependenciesamong actions, and feature-based semantic similarities [7] torecognise personalised activity patterns.Practically, in activity modelling, it is not possible tocompletely model all the activities at once due to followingreasons:• Inhabitants of the smart homes are not restricted to performspecific activities modelled in ontology. Instead, they mayperform activities (with existing home-objects) which have not",
        "publication_date": "2019-03-02",
        "authors": "Muhammad Safyan, Zia Ul Qayyum, Sohail Sarwar, Muddesar Iqbal, Raúl García Castro, Anwer Al‐Dulaimi",
        "file_name": "activity_recognition.pdf",
        "file_path": "./PDFs/activity_recognition.pdf"
    },
    {
        "title": "Smart Contracts Vulnerability Classification through Deep Learning",
        "implementation_urls": [],
        "doi": "10.1145/3560905.3568175",
        "abstract": "Abstract: Existing smart city ontologies allow representing different types of city-related data fromcities. They have been developed according to different ontological commitments and hence do notshare a minimum core model that would facilitate interoperability among smart city informationsystems. In this work, a survey has been carried out in order to study available smart city ontologiesand to identify the domains they are representing. Taking into account the findings of the survey anda set of ontological requirements for smart city data, a list of ontology design patterns is proposed.These patterns aim to be easily replicated and provide a minimum set of core concepts in order toguide the development of smart city ontologies.Keywords: ontology; smart cities; ontology design patterns1. IntroductionThe term smart city refers to a city that manages, in an intelligent way, all its associated resourceswith the aim to enhance the quality of the services provided to citizens and to improve their qualityof life [1,2]. The smart city domain has been a topic of interest in many sectors around the world.Standardization bodies, for example, the International Telecommunications Union (https://www.itu.int) and the International Standards Organization (https://www.iso.org), have been working ondefining standards and recommendations in order to provide a unified way to refer to and managethis particular field. In addition, several initiatives and projects in the smart city field have emerged,which denote the efforts and investments that industries, countries, and regions are making in orderto manage the city resources in a better manner. In the case of city coalitions or research groups(e.g., Smart Cities-European Medium-Sized Cities (http://smart-cities.eu), Open and Agile Smart Cities(http://www.oascities.org), Smart Cities Council (http://smartcitiescouncil.com), etc.), their studiesand business reports mention more than 300 smart cities involved [3] with an increasing need for andinterest in exploring solutions in order to improve their city processes. In the case of projects (e.g.,ESPRESSO [4], CityPulse [5], SmartSantander [6], etc.), which are financed by public, private, or a mixof both funds, they aim, in most cases, to provide technological tools to solve requirements in severalcity challenges.In this respect, there is wide agreement about the fact that smart cities are characterized by apervasive use of information and communication technologies [7–9], which, in various urban domains,may help cities make better use of their resources [10]. Some of these technologies include open datainfrastructures, mobile applications, public participation tools, Internet of Things (IoT) platforms, etc.The data handled or produced by all these technologies is very heterogeneous in terms of formats,structure, and delivery mechanisms, both inside the same city and across different cities. Hence,this opens up the opportunity to create common models to allow interoperability inside cities. In thiscontext, ontologies, understood as formal specifications of shared conceptualizations [11], can be usedAppl. Sci. 2019, 9, 32; doi:10.3390/app9010032 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttps://orcid.org/0000-0002-9260-0753https://www.itu.inthttps://www.itu.inthttps://www.iso.orghttp://smart-cities.euhttp://www.oascities.orghttp://smartcitiescouncil.comhttp://dx.doi.org/10.3390/app9010032http://www.mdpi.com/journal/applscihttp://www.mdpi.com/2076-3417/9/1/32?type=check_update&version=2",
        "publication_date": "2022-11-06",
        "authors": "Martina Rossini, Mirko Zichichi, Stefano Ferretti",
        "file_name": "10!1145%3560905!3568175.pdf",
        "file_path": "./PDFs/10!1145%3560905!3568175.pdf"
    },
    {
        "title": "GEnI: A framework for the generation of explanations and insights of knowledge graph embedding predictions",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2022.12.010",
        "abstract": "AbstractKnowledge Graphs (KGs) are among the most commonly used knowledgerepresentation paradigms, being at the core of tasks such as question an-swering or recommendation systems. Knowledge Graph Completion (KGC)is one of the key tasks concerning KGs, where the goal is to mine new el-ements from the existing information. Different approaches have been pro-posed through the years to tackle this challenge. Among them, two analo-gous categories can be distinguished: Rule-Learning and Knowledge GraphEmbeddings (KGE). Different methods have been subsequently proposed tounify both types under a single framework, such that the benefits of bothproposals can be exploited. However, none of these methods consider usingrule-learning models not as a boosting agent for the KGE model but as anexplainability tool. This work presents GEnI, a framework capable of gener-ating insights and explanations for KGE models. GEnI follows a three-phasesequential process, yielding a proper insight for a given prediction. Possi-ble outcomes are rules, correlations, and influence detection. Moreover, theoutput is expressed in natural language, to extend the explainability of theproposal further. GEnI has been successfully evaluated under three criteria:coherence, the meaningfulness of the output, and reliability. Moreover, itcan be used by both translational and bilinear KGE models, offering a broadcoverage. Furthermore, this work also presents an in-depth revision of ex-isting integrative approaches between rule-learning and embedding models,providing a comparative framework between them.This document is a Preprint. The final version has been published in NeurocomputingGEnI: Generation of Explanations of KGE predictions1. IntroductionKnowledge Graphs (KG) are one of the cornerstones of knowledge rep-resentation. They are widely used for tasks such as question answering orrecommendation. YAGO [1], Freebase [2], and WordNet [3] are exampleswhere the information is stored in the form of facts or triples. Each triplefollows the schema (s, r, o) and models the interactions between a subject andan object entity through a relation. KGs are constantly growing, making itnecessary to develop methods that can support the constant introduction ofnew elements into the KG.Knowledge Graph Completion (KGC) methods provide a solution to thisissue, automatically mining new facts from the information contained in thegraph. Amongst the existing KGC proposals, KG embeddings (KGE) are thepredominant option. These models embed the KG into a continuous vectorspace, then using the computed representations to infer new facts.Recent KGE models rely on increasingly complex paradigms, such asGraph Neural Networks [4], operations [5], and representation spaces [6, 7].While the usage of these paradigms benefits the model performance, it in-creases the complexity, thus hindering its interpretability.Rule-based learning models provide an alternative to KGE models, wherethe inference of new facts is performed from a set of mined rules from thedata. While these methods solve the interpretability issue existing withinKGE approaches, they can not encode statistical properties. Moreover, thesemodels can face scalability problems as the search space grows proportionallywith the KG size.Several approaches develop a hybrid KGC solution, unifying both em-",
        "publication_date": "2022-12-07",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique",
        "file_name": "10!1016%j!neucom!2022!12!010.pdf",
        "file_path": "./PDFs/10!1016%j!neucom!2022!12!010.pdf"
    },
    {
        "title": "Towards an Ontology for Public Procurement Based on the Open Contracting Data Standard",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/ocds-ontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-29374-1_19.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This edition of the OCDS ontology is available online in GitHub in two versions15: one version only with core OCDS terms and a second version with extensions (e.g., enquiries, lots, etc.)."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-29374-1_19",
        "abstract": "Abstract. The release of a growing amount of open procurement dataled to various initiatives for harmonising the data being provided. Amongothers, the Open Contracting Data Standard (OCDS) is highly relevantdue to its high practical value and increasing traction. OCDS defines acommon data model for publishing structured data throughout most of thestages of a contracting process. OCDS is document-oriented and focuseson packaging and delivering relevant data in an iterative and event-drivenmanner through a series of releases. Ontologies, beyond providing uniformaccess to heterogeneous procurement data, could enable integration withrelated data sets such as with supplier data for advanced analytics andinsight extraction. Therefore, we developed an ontology, the “OCDSontology”, by using OCDS’ main domain perspective and vocabulary,since it is an essential source of domain knowledge. In this paper, weprovide an overview of the developed ontology.Keywords: Procurement · OCDS · Ontology.1 IntroductionPublic entities worldwide are increasingly required to publish information abouttheir procurement processes (e.g., in Europe, with EU directives 2003/98/ECand 2014/24/EU8) in order to improve effectiveness, efficiency, transparency, andaccountability of public services [8]. As a result, the release of a growing amountof open procurement data led to various initiatives (e.g., OpenPEPPOL4, CENBII5, TED eSenders6, CODICE7, Open Contracting Data Standard (OCDS)8) forharmonising the data being provided. XML formats and file templates are definedwithin these standards to make it possible to structure the messages exchanged bythe various agents involved in electronic procurement. These standards are mostly4 https://peppol.eu5 http://cenbii.eu6 https://simap.ted.europa.eu/web/simap/sending-electronic-notices7 https://contrataciondelestado.es/wps/portal/codice8 http://standard.open-contracting.orghttps://peppol.euhttp://cenbii.euhttps://simap.ted.europa.eu/web/simap/sending-electronic-noticeshttps://contrataciondelestado.es/wps/portal/codicehttp://standard.open-contracting.org2 A. Soylu et al.oriented to achieve interoperability, addressing communication between systems,and hence they usually focus on the type of information that is transmittedbetween the various organizations involved in the process. The structure of theinformation is commonly provided by the content of the documents that areexchanged. Furthermore, there are no generalised standardised practices to referto third parties, companies participating in the process, or even the main objectof contracts. In sum, this still generates a lot of heterogeneity. Ontologies havebeen proposed to alleviate this problem [1,10]. Several ontologies (e.g., PPROC[5], LOTED2 [3], MOLDEAS [7], PCO [6]) have recently emerged, with differentlevels of detail and focus (e.g., legal, process-oriented, pragmatic). However, noneof them has had a wide adoption so far.In this context, OCDS is highly relevant due to its high practical value andincreasing traction. It defines a common data model for publishing structured datathroughout all the stages of a contracting process. It is document-oriented and",
        "publication_date": "2019-01-01",
        "authors": "Ahmet Soylu, Brian Elvesæter, Philip Turk, Dumitru Roman, Óscar Corcho, Elena Simperl, George Konstantinidis, Till Christopher Lech",
        "file_name": "10!1007%978-3-030-29374-1_19.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-29374-1_19.pdf"
    },
    {
        "title": "The Use of Decentralized and Semantic Web Technologies for Personal Data Protection and Interoperability",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_23",
        "abstract": "Abstract. The enactment of the General Data Protection Regulation(GDPR) has been the response of the European Union to the growingdata-driven economy backed up by the largest companies in the world.It provides the data protection and portability needed by individualsthat “unconsciously” generate personal data for “free” services offeredby providers that lack transparency on their use. Meanwhile, the rise ofDistributed Ledger Technologies (DLTs) offers new possibilities for themanagement of general purpose data, hence being suitable for handlingpersonal data in a trustless scenario. These decentralized technologiesbring a new concept of contract called smart because of its ability tobe self-executable. DLTs and smart contracts, together with the use ofSemantic Web standards, allows the creation of a decentralized digitalspace controlled entirely by an individual, where his personal data canbe stored and transacted.Keywords: GDPR · Personal data · Distributed ledger technologies ·Smart contracts · Semantic web1 IntroductionWith the introduction of the General Data Protection Regulation (GDPR) [5]in 2018, operations carried out regarding the management and the movement ofpersonal data have radically changed. Data privacy of European Union’s Citi-zen has been empowered through a series of rights that provide data protectionand portability. GDPR can be seen as a necessary response to the challengesposed by technological advances brought about mainly by Big Tech companies,which generate huge amounts of data without sufficient safeguards for individu-als. A huge business, indeed, lies behind the trade of personal data and severalcompanies make consistent profits operating in this sector. GDPR and currentliteracy help the individual to understand how their personal data is often gen-erate unconsciously and where, how or why the data is being collected, but still,c© Springer Nature Switzerland AG 2021V. Rodŕıguez-Doncel et al. (Eds.): AICOL-XI 2018/AICOL-XII 2020/XAILA 2020, LNAI 13048, pp. 328–335, 2021.https://doi.org/10.1007/978-3-030-89811-3_23http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-89811-3_23&domain=pdfhttps://doi.org/10.1007/978-3-030-89811-3_23The Use of DLTs and Semantic Web for Personal Data Management 329further work is needed to let them develop the necessarily practical and interpre-tive skills [15]. More efforts are needed to reach both transparency and a balancebetween privacy and data sharing.Even if GDPR requires data controllers, i.e. entities that collect and manageindividuals’ personal data, to release to their users the complete dataset theycollected on them, upon request, there are currently no standards for this kindof requests and there is the tendency to hinder the progress of these, causingthe entire process to become almost useless. These data controllers usually storethis personal information in corporate databases, but they can become dataproviders to other parties if the individual agrees –and even if the individualdoes not agree they are obliged to act as data providers in extraordinary cases,e.g. national security. As of today, when these data transactions happen there isno transparency on the individual’s data usage.Meanwhile, between the many technologies that regards general-purpose datamanagement and storage, Distributed Ledger Technologies (DLTs) are raising aspowerful tools to avoid the control centralization. The current use of DLTs is in",
        "publication_date": "2021-01-01",
        "authors": "Mirko Zichichi, Victor Rodrı́guez-Doncel, Stefano Ferretti",
        "file_name": "978-3-030-89811-3_23.pdf",
        "file_path": "./PDFs/978-3-030-89811-3_23.pdf"
    },
    {
        "title": "Refining Terminological Saturation using String Similarity Measures.",
        "implementation_urls": [],
        "abstract": "Abstract. This paper reports on the refinement of the THD algorithm, developed in the OntoElect framework. This baseline THD algorithm used exact string matches for key term comparison. It has been refined by introducing an appro-priate string similarity metric for grouping the terms having similar meaning and looking similar as text strings. To choose the most appropriate metric, several existing metrics have been cross-evaluated on the developed test set of multi-word terms in English. The rationale for creating this test set is also presented. Further, the refined algorithm for measuring terminological difference has been cross-evaluated with the baseline THD algorithm. For this cross-evaluation, the bags of terms extracted from the TIME collection of scientific papers were used. The experiment revealed that using the refined algorithm yielded better and quicker terminological saturation, compared to the baseline. Keywords: Automated Term Extraction, OntoElect, Terminological Difference, Key Term, Linguistic Similarity Metric, Bag of Terms, Terminological Satura-tion. 1 Introduction The research presented in this paper is the part of the development of the methodolog-ical and instrumental components for extracting representative (complete) sets of sig-nificant terms from the representative sub-collections of textual documents having min-imal possible size. These terms are further interpreted as the required features for engi-neering an ontology in a particular domain of interest. Therefore, it is assumed that the documents in a collection cover a single and well circumscribed domain. The main hypothesis, put forward in this work, is that a sub-collection can be considered as rep-resentative to describe the domain, in terms of its terminological footprint, if any addi-tions of extra documents from the entire collection to this sub-collection do not notice-ably change this footprint. Such a sub-collection is further considered as complete and mailto:aluonac@i.uamailto:rodeonpopov@gmail.com therefore yields a representative bag of significant terms describing its domain. The approach to assess the representativeness does so by evaluating terminological satura-tion in a document (sub-)collection [1], [31]. Detecting saturation is done by measuring terminological difference (thd) among the pairs of the consecutive incrementally enlarged datasets, as described in Section 4. This set measure is of course based on measuring differences between individual terms.   A (baseline) THD algorithm [1] has been developed and implemented in the OntoElect project1. This THD algorithm, however, uses a simple string equivalence check for de-tecting similar individual terms. The objective of the research presented in this paper was to find out if it is possible to achieve better performance in measuring terminolog-ical difference by using a proper string similarity measure to compare individual terms.  The remainder of the paper is structured as follows. Section 2 reviews the related work. Section 3 reports on the implementation of the chosen string similarity measures and selecting the proper term similarity thresholds for their use. Section 4 sketches out the approach of OntoElect for measuring thd and our refinement of the baseline THD algorithm. Section 5 presents the set-up and results of our evaluation experiments. Our conclusions and plans for the future work are given in Section 6.  2 Related Work The work reported in this paper aims at improving the measures of terminological dif-ference between the bags of terms extracted from textual documents. The improvement is sought via the proper choice and use of existing string metrics for measuring linguis-",
        "publication_date": "2018-01-01",
        "authors": "Alyona Chugunenko, Victoria Kosa, Rodion Popov, David Chaves-Fraga, Vadim Ermolayev",
        "file_name": "no_doi_20250624162615.pdf",
        "file_path": "./PDFs/no_doi_20250624162615.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2105/10000003.pdf"
    },
    {
        "title": "LYNX: Towards a Legal Knowledge Graph for Multilingual Europe",
        "implementation_urls": [],
        "doi": "10.26826/law-in-context.v37i1.129",
        "abstract": "ABSTRACTLynx is an innovation project in Europe whose objective is to develop services for legal compliance. A legal knowledge graph is built over multilingual, multijurisdictional documents using semantic web technologies. A collection of ser-vices implementing natural language techniques enables better legal information retrieval, cross-lingual answering of questions and information discovery. Three use cases are discussed, as well as the overall impact of the project.Keywords – Lynx Legal Knowledge Graph, compliance services, European legislation, multilingualism  Acknowledgements: This work has been funded by the project Lynx, which has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement no. 780602. For more information: http://www.lynx-project.eu. Disclosure statement – No potential conflict of interest was reported by the authors.License – This work is under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) https://creativecommons.org/licenses/by-nc-sa/4.0/ Suggested citation: Rodríguez-Doncel, V. and Montiel-Ponsoda, E. 2021. “Lynx:Towards a Legal Knowledge Graph for Multilingual Europe”, Law in Context, 37(1): 1-4 , DOI: https://doi.org/10.26826/law-in-context.v37i1.129Summary1.\t Introduction2.\t Results of Lynx project2.1\t A multilingual legal knowledge graph2.2\t Software2.3\t Demonstration in diverse scenarios3.\t Impact of the project4.\t Conclusions5.\t Referenceshttp://journals.latrobe.edu.au/index.php/law-in-context/indexhttps://doi.org/10.26826/law-in-context.v37i1.129\rhttps://orcid.org/0000https://orcid.org/0000http://www.lynx-project.euhttps://creativecommons.org/licenses/by-nc-sa/4.0/https://doi.org/10.26826/law-in-context.v37i1.129\rLaw in Context, Vol 37, Issue 1, 2020\t 176ISSN: 1839-41831.\t INTRODUCTION The European Union (EU), post-Brexit, is comprised of 27 member states populated by approximately 450 million people, who speak over 60 indigenous languages with different legal status. The EU has adopted 24 official languages, and every EU national has the right to use any of these 24 languages to contact the EU institutions, and institutions are obliged to reply in the same language. Although the European Commission favors three of these languages as ‘procedural languages’ (English, French and German), EU law and many other legislative documents are published in all official languages except Irish. How-ever, member states do not translate their legislation into foreign languages and finding the applicable norms in any location in one’s own language is not easy.The double fragmentation (of Law and languages) hampers the development of a unified market with fluid commercial exchanges in Europe. This has been recognized by the EU authorities, who set the completion of the Digital Single Market as one of their 10 political priorities for the ",
        "publication_date": "2020-12-20",
        "authors": "Victor Rodrı́guez-Doncel, Elena Montiel-Ponsoda",
        "file_name": "LYNX_Towards_a_Legal_Knowledge_Graph_for_Multiling.pdf",
        "file_path": "./PDFs/LYNX_Towards_a_Legal_Knowledge_Graph_for_Multiling.pdf"
    },
    {
        "title": "Handling qualitative preferences in SPARQL over virtual ontology-based data access",
        "implementation_urls": [],
        "doi": "10.3233/sw-212895",
        "abstract": "Abstract. With the increase of data volume in heterogeneous datasets that are being published following Open Data initiatives,new operators are necessary to help users to find the subset of data that best satisfies their preference criteria. Quantitativeapproaches such as top-k queries may not be the most appropriate approaches as they require the user to assign weights thatmay not be known beforehand to a scoring function. Unlike the quantitative approach, under the qualitative approach, whichincludes the well-known skyline, preference criteria are more intuitive in certain cases and can be expressed more naturally.In this paper, we address the problem of evaluating SPARQL qualitative preference queries over an Ontology-Based Data Ac-cess (OBDA) approach, which provides uniform access over multiple and heterogeneous data sources. Our main contribution isMorph-Skyline++, a framework for processing SPARQL qualitative preferences by directly querying relational databases. Ourframework implements a technique that translates SPARQL qualitative preference queries directly into queries that can be evalu-ated by a relational database management system. We evaluate our approach over different scenarios, reporting the effects of datadistribution, data size, and query complexity on the performance of our proposed technique in comparison with state-of-the-arttechniques. Obtained results suggest that the execution time can be reduced by up to two orders of magnitude in comparison tocurrent techniques scaling up to larger datasets while identifying precisely the result set.Keywords: Qualitative preference, Skyline, OBDA, query translation, R2RML1. IntroductionEliciting and exploiting preferences in query evaluation over relational databases and triple stores has attractedsustained interest in the last two decades [1,11–13,19,25,26,30,41,44,47]. Such interest is motivated by the need ofusers who are not database experts but are willing to explore large datasets, commonly coming from the integrationof multiple and heterogeneous data sources [25,41,44,47]. Usually, these users do not know, a priori, what usefulinformation they can extract from this data or they do not have a particular result in mind until it is discovered as anoutcome of their data exploration process. During data exploration, they try to identify useful information accordingto their preferences by means of eliciting queries that best meet their criteria. Typical examples include travelers*Corresponding author. E-mail: mgoncalves@usb.ve.1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:mgoncalves@usb.vemailto:marlene.gdasilva@upm.esmailto:dchaves@fi.upm.esmailto:ocorcho@fi.upm.esmailto:mgoncalves@usb.vehttps://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-212895&domain=pdf&date_stamp=2022-01-13660 M. Goncalves et al. / Handling qualitative preferences in SPARQL over virtual OBDAlooking for the best deals on accommodation to visit a city or users looking for laboratories with the best offers onPCR tests in a range of hours more appropriate for them during the COVID-19 global pandemic. Database enginesshould be able to filter useful information to support the requirements of these non-expert users, i.e., evaluatingqueries that represent the desirable properties over the final result during the user’s search process. This kind of userexpects such queries to be easily posed, correctly interpreted by the engine, and computationally efficient.Introducing preference criteria into queries has been tackled in the context of relational databases (RDB) [11–13,30–32]. They are often used to filter information and thus reduce the volume of data being displayed to theuser. At the level of preference criteria, two different approaches can be pursued: qualitative and quantitative. In thequantitative approach [1,26], preference criteria are specified by means of scoring functions that assign a numericalscore to each instance of the query response. Thus, an instance A is preferred to an instance B if the score of A ishigher than the score of B. The problem of lack of expressiveness of this approach is well known in utility theory[22] where preferences are only represented by numerical scoring functions which are not necessarily easy to defineby any user, whereas preferences in the qualitative approach can be expressed more naturally. For example, a typicalscoring function for finding cheap hotels near the sea is a weighted average of price and distance, but the usershould know how to define weights. In addition, the user should normalize the values of price and distance becausethey have different units. In contrast, under the qualitative approach, the user may simply express his preferencesas minimizing price and distance, which is more intuitive. The qualitative approach is strictly more general than",
        "publication_date": "2022-01-14",
        "authors": "Marlene Goncalves, David Chaves-Fraga, Óscar Corcho",
        "file_name": "10!3233%sw-212895.pdf",
        "file_path": "./PDFs/10!3233%sw-212895.pdf"
    },
    {
        "title": "Drivers, standards and platforms for the IoT: Towards a digital VICINITY",
        "implementation_urls": [
            {
                "identifier": "https://github.com/openhab/openhab",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Drivers_standards_and_platforms_for_the_IoT_Towards_a_digital_VICINITY.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: http://vicinity2020.eu/“VICINITY D1.2."
                    }
                ]
            }
        ],
        "doi": "10.1109/intellisys.2017.8324287",
        "abstract": "Abstract—The Internet of Things is created by networkingmany different kind of things, enabling new services and businessmodels. However, things from different manufacturers, variousdomains, and a number of standards have to interact. Un-fortunately, the current situation is characterized by ‘silos’ or‘islands’ that lack interoperability. This paper gives a survey andanalysis of drivers, platforms, and standards for the Internet ofThings (IoT) that provide a basis for interoperability. They areconsidered for requirements of the VICINITY project whose goalis to offer “Interoperability as a Service”.Keywords—Internet of Things (IoT); interoperability; standardsI. INTRODUCTIONThe “Internet of Things (IoT)” has become a quicklygrowing topic that has the potential to change the way peoplelive, work, and think. The vision of ubiquitous, networkeddevices was first formulated by Mark Weiser [1]. The concreteterm “Internet of Things” was coined by Kevin Ashton [2],initially focusing on RFID tags. The International Telecom-munication Union (ITU) defines the Internet of Things as“a global infrastructure for the information society, enablingadvanced services by interconnecting (physical and virtual)things based on existing and evolving interoperable informa-tion and communication technologies” [3], and “ (...) thingsinclude the surrounding environment, industrial robots, goodsand electrical equipment” [3].Following the ITU’s definition of the IoT, the creationof advanced services differentiates the IoT from well-knowncommunication systems, e.g. in automation systems or wirelesssensor networks. As data in the IoT stems from a variety ofheterogeneous things from different domains, interoperabilityis a major challenge for the creation of advanced services.The objective of this paper is to define the basis for thecreation of advanced concepts of interoperability, coveringvarious layers, starting with business and application layer,semantic interoperability, down to technical interoperability.The rest of the paper is organized as follows. In theremainder of this section we describe a typical IoT applicationarchitecture. In Section II we summarize the outcomes of aFig. 1. A five-layer IoT architecture.survey identifying drivers and barriers from the point of viewof various stakeholders. In Section III we give an overviewof standards that strive to provide semantic interoperability.In Section IV we survey the software platforms that providetechnical interoperability, e.g., between ZigBee and Fieldbusprotocols, and hardware platforms on which this can beimplemented. The last section closes the paper and identifiesthe future work.A. Layers of IoT ArchitecturesTo simplify the complexity of IoT architectures, they arestructured in layers that stepwise abstract data to semantic",
        "publication_date": "2017-09-01",
        "authors": "Aida Mynzhasova, Carna Radojicic, Christopher Heinz, Johannes Kölsch, Christoph Grimm, Juan Rico, Keith Dickerson, Raúl García‐Castro, Victor Oravec",
        "file_name": "Drivers_standards_and_platforms_for_the_IoT_Towards_a_digital_VICINITY.pdf",
        "file_path": "./PDFs/Drivers_standards_and_platforms_for_the_IoT_Towards_a_digital_VICINITY.pdf"
    },
    {
        "title": "Review of the state of the art: discovering and associating semantics to tags in folksonomies",
        "implementation_urls": [],
        "doi": "10.1017/s026988891100018x",
        "abstract": "AbstractThis paper describes and compares the most relevant approaches for associating tags with semantics inorder to make explicit the meaning of those tags. We identify a common set of steps that are usuallyconsidered across all these approaches and frame our descriptions according to them, providing aunified view of how each approach tackles the different problems that appear during the semanticassociation process. Furthermore, we provide some recommendations on (a) how and when to use eachof the approaches according to the characteristics of the data source, and (b) how to improve results byleveraging the strengths of the different approaches.Keywords: folksonomies, ontologies, tagging, semantics, clustering.1 IntroductionIn recent years we have witnessed the transition from a Web where the content is generated mainly bythe owners of websites to a more open and social Web where users are not only information consumersbut also producers (prosumers - Tapscott & Williams, 2006). This new age of the Web, also known asWeb 2.01, has brought a diversity of new social applications like wikis, blogs, social networks, socialbookmarks, and photo, music and video sharing sites. These applications made it possible for all Webusers to contribute and share huge amounts of multimedia content, and to tag these content resources withfree-form keywords.Tags serve multiple purposes, such as content organisation, description, and searching. In 2003,Delicious2 was released as a social bookmarking tool where users are able to assign tags to URLs in acollaborative manner. One year later, Flickr3 was presented as a social network for photo sharing whereusers can assign tags to their own photos or to other photos from their colleagues. Nowadays, tagging ispart of many popular applications such as Amazon, YouTube and Last.Fm, to name a few, where users canassign tags to products, videos and songs respectively.In 2004, Vander Wal4 coined the term Folksonomy to describe the new structure of users, tags andobjects. Folksonomy is defined as the result of personal free tagging of information and objects (anything1http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html2http://delicious.com/3http://www.flickr.com/4http://www.vanderwal.net/folksonomy.html2 A. GARCÍA-SILVA ET AL.with a URL) for one’s own retrieval. The tagging is done in a social environment (usually shared and opento others). Folksonomies are good sources of terminology frequently updated by large communities ofusers. This contrasts with other classification schemes, such as thesauri or taxonomies, which are generallycreated and maintained by controlled user groups. Hence one of the advantages of folksonomies is theirability to rapidly adapt to new changes in terminologies and domains. Furthermore, as time goes by userstend to stabilize the vocabulary used to tag a resource (Golder & Huberman, 2006). This stabilization is theresult of several user iterations and of a tag recommendation strategy based on previously assigned tags.These vocabularies can be seen as shared conceptualizations by groups of users with respect to groups ofresources.The success of tagging is attributed to two main factors; (a) they are very easy to create, where usersdo not need any special skills or experience to tag, and (b) the benefits of tagging are immediate (Hothoet al., 2006). However, current tagging technology suffers from two main problems. First problem isthat folksonomies lack a uniform representation to facilitate their sharing and reuse. Some Web 2.0applications provide APIs to export their folksonomies. However, they do it in proprietary formats. Toovercome this problem, ontologies have been proposed to model the tagging activities in folksonomies,with semantic concepts to represent users, tags, resources, etc. (Echarte et al., 2007; Gruber, 2005; Kimet al., 2008a; Knerr, 2006; Newman, 2005; Passant, 2008; Scerri et al., 2007). One example of theseontologies is the SCOT ontology (Kim et al., 2008a), which is depicted in Figure 1. This ontologymodels tagging information, and includes concepts such as User, Item, Tag and Tag Cloud as well as",
        "publication_date": "2012-02-22",
        "authors": "Andrés García-Silva, Óscar Corcho, Harith Alani, Asunción Gómez‐Pérez",
        "file_name": "Review_of_the_state.pdf",
        "file_path": "./PDFs/Review_of_the_state.pdf"
    },
    {
        "title": "Proof of Location through a Blockchain Agnostic Smart Contract Language",
        "implementation_urls": [],
        "doi": "10.1109/icdcsw60045.2023.00016",
        "abstract": "Abstract—Location-based services are at the heart of manyapplications that individuals use every day. However, there isoften no guarantee of the truthfulness of users’ location data,since this information can be easily spoofed without a proof mech-anism. In distributed system applications, preventing users fromsubmitting counterfeit locations becomes even more challengingbecause of the lack of a central authority that monitors dataprovenance. In this work, we propose a decentralized architecturebased on blockchains and decentralized technologies, offering atransparent solution for Proof of Location (PoL). We specificallyaddress two main challenges, i.e., the issuing process of the PoLand the proof verification. We describe a smart contract basedimplementation in Reach, a blockchain-agnostic smart contractlanguage, and the tests we conducted on different blockchains,i.e. Ethereum, Polygon, and Algorand, measuring latency andcosts due to the payment of fees. Results confirm the viability ofthe proposal.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Keyword Search, SmartContractsI. INTRODUCTIONNowadays, many users’ activities are supported by a differ-ent number of mobile applications, leveraging their positionto offer specific location-based services. For example, trust-worthy crowd-sourcing of urban or environmental obstaclesfor accessibility purposes [1], [2], customer-loyalty rewardsystems that offer discounts to users who frequently visit theshop, privacy-preserving contact tracing [3].These kinds of systems have at least three issues to copewith. First, some level of trust is needed in the user that crowd-sources some data related to a certain position. This led to theidea of a Proof-of-Location (PoL) because the location couldbe easily spoofed [4]. Second, there is the need to ensure,on the other hand, some privacy guarantees to the users thatgenerate data, so as to avoid everyone being entitled to knowa specific user location at a certain time. Third, the usualapproach is to resort to a centralized system, where a singleentity is responsible for collecting and storing data, users thatgenerated them, and their associated position representing,somehow, a PoL. While this solution can help in dealing withthe two issues above, it raises some concerns on personal dataThis work received funding from the EU H2020 R&D programme under theMSCA ITN EJD grant agreement No 814177 Law Science and TechnologyJoint Doctorate - RIoE.sovereignty, as location data is one of the most sensitive caseswith respect to users’ data exploitation [5], [6].With this in view, in this paper we propose a decentralizedProof of Location (PoL) system, based on blockchain anddistributed storage technologies. Our system is designed to bedecentralized, so as to avoid the presence of single point of",
        "publication_date": "2023-07-18",
        "authors": "Michele Bonini, Mirko Zichichi, Stefano Ferrettiv, Gabriele D’Angelo",
        "file_name": "10!1109%icdcsw60045!2023!00016.pdf",
        "file_path": "./PDFs/10!1109%icdcsw60045!2023!00016.pdf"
    },
    {
        "title": "Street images classification according to COVID-19 risk in Lima, Peru: A convolutional neural networks analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/jmcastagnetto/lima-atu-covid19-paraderos",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162652.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Outcome (ie, labels: moderate and extreme COVID-19 risk) data are available online: https://sistemas.atu.gob.pe/paraderosCOVID; this information was systematised at https://github.com/jmcastagnetto/lima-atu-covid19-paraderos."
                    }
                ]
            }
        ],
        "doi": "10.1101/2021.09.06.21263188",
        "abstract": "ABSTRACTObjectives  During the COVID-19 pandemic, convolutional neural networks (CNNs) have been used in clinical medicine (eg, X-rays classification). Whether CNNs could inform the epidemiology of COVID-19 classifying street images according to COVID-19 risk is unknown, yet it could pinpoint high-risk places and relevant features of the built environment. In a feasibility study, we trained CNNs to classify the area surrounding bus stops (Lima, Peru) into moderate or extreme COVID-19 risk.Design  CNN analysis based on images from bus stops and the surrounding area. We used transfer learning and updated the output layer of five CNNs: NASNetLarge, InceptionResNetV2, Xception, ResNet152V2 and ResNet101V2. We chose the best performing CNN, which was further tuned. We used GradCam to understand the classification process.Setting  Bus stops from Lima, Peru. We used five images per bus stop.Primary and secondary outcome measures  Bus stop images were classified according to COVID-19 risk into two labels: moderate or extreme.Results  NASNetLarge outperformed the other CNNs except in the recall metric for the moderate label and in the precision metric for the extreme label; the ResNet152V2 performed better in these two metrics (85% vs 76% and 63% vs 60%, respectively). The NASNetLarge was further tuned. The best recall (75%) and F1 score (65%) for the extreme label were reached with data augmentation techniques. Areas close to buildings or with people were often classified as extreme risk.Conclusions  This feasibility study showed that CNNs have the potential to classify street images according to levels of COVID-19 risk. In addition to applications in clinical medicine, CNNs and street images could advance the epidemiology of COVID-19 at the population level.INTRODUCTIONIn COVID-19 research, deep learning tools applied to image analysis (ie, computer vision) have informed the diagnosis and prog-nosis of patients through the classification of X-ray and computer tomography images of the chest.1–3 These tools have helped practi-tioners treating COVID-19 patients.On the other hand, the application of computer vision to study the epidemiology of COVID-19 has been limited. One relevant example is the use of Google Street View images to extract features of the built envi-ronment and associate these with COVID-19 ",
        "publication_date": "2021-09-12",
        "authors": "Rodrigo M. Carrillo‐Larco, Jose Francisco Hernández Santa Cruz",
        "file_name": "no_doi_20250624162652.pdf",
        "file_path": "./PDFs/no_doi_20250624162652.pdf",
        "pdf_link": "https://bmjopen.bmj.com/content/bmjopen/12/9/e063411.full.pdf"
    },
    {
        "title": "Multi-Agent System for Demand Prediction and Trip Visualization in Bike Sharing Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/novafloss/workalendar",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162654.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/novafloss/workalendar (accessed on 5 October 2017)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app8010067",
        "abstract": "Abstract: This paper proposes a multi agent system that provides visualization and prediction toolsfor bike sharing systems (BSS). The presented multi-agent system includes an agent that performsdata collection and cleaning processes, it is also capable of creating demand forecasting models foreach bicycle station. Moreover, the architecture offers API (Application Programming Interface)services and provides a web application for visualization and forecasting. This work aims to make thesystem generic enough for it to be able to integrate data from different types of bike sharing systems.Thus, in future studies it will be possible to employ the proposed system in different types of bikesharing systems. This article contains a literature review, a section on the process of developing thesystem and the built-in prediction models. Moreover, a case study which validates the proposedsystem by implementing it in a public bicycle sharing system in Salamanca, called SalenBici. It alsoincludes an outline of the results and conclusions, a discussion on the challenges encountered in thisdomain, as well as possibilities for future work.Keywords: bike sharing systems (BSS); regression models; open data; data visualization; multi agentsystems; organizations and institutions; socio-technical systems1. IntroductionThere is a consensus in the literature [1,2] which states that bicycles are one of the most sustainablemodes of urban transport and they are suitable for both short trips and medium distance trips.Riding a bicycle does not have any negative impact on the environment [3], it promotes physicalactivity and improves health. Furthermore, its use is cost-effective from the perspective of usersand infrastructure.Moreover, due to the increased CO2 levels, the European Union and other states are takingmeasures to reduce greenhouse gas emissions in every sector of the economy [4].These facts explain the growing popularity of sustainable means of transport such as bike sharingsystems. From 1965 when they came into use in Amsterdam to 2001, there were only few systemsaround the world. Bike sharing systems (BSS) began to spread in 2012, when their number increased toover 400 [5]. By 2014 this number had doubled [6] and nowadays there are approximately 1175 cities,municipalities or district jurisdictions in 63 different countries where these systems are in active use,according to BikeSharingMap [7].Appl. Sci. 2018, 8, 67; doi:10.3390/app8010067 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-0493-4471https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/app8010067http://www.mdpi.com/journal/applsciAppl. Sci. 2018, 8, 67 2 of 21Bike sharing systems allow users to travel in the city at a low cost or even for free. They can pickup a bicycle at one of the stations distributed across the city and leave it at another. These systemshave evolved over time [8] and today the vast majority include sensors that provide information on theinteraction of users with the system. However, the management of these systems and the data collectedby them, is often poor and as a result the numbers of bicycles available at stations are not sufficient.These are the reasons as to why bike sharing systems should be improved with data producedby the systems themselves. They should include predictive models for user behaviour and demand,which will notify the system administrator of the stations where more bicycles are required forsatisfying user demand. This will also allow to set up new stations in places where the demand is highor, on the contrary, to close down the stations at which the demand is too low.This article presents a multi-agent system which collects bike sharing system data together withother useful data. The system uses these data to create demand prediction models and to offer services",
        "publication_date": "2018-01-05",
        "authors": "Álvaro Lozano Murciego, Juan F. De Paz, Gabriel Villarrubia González, Daniel H. de la Iglesia, Javier Bajo",
        "file_name": "no_doi_20250624162654.pdf",
        "file_path": "./PDFs/no_doi_20250624162654.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/8/1/67/pdf?version=1515142581"
    },
    {
        "title": "A quality assessment approach for evolving knowledge bases",
        "implementation_urls": [],
        "doi": "10.3233/sw-180324",
        "abstract": "AbstractKnowledge bases are nowadays essential components for any task that requires automation with some degrees of intelligence.Assessing the quality of a Knowledge Base (KB) is a complex task as it often means measuring the quality of structured informa-tion, ontologies and vocabularies, and queryable endpoints. Popular knowledge bases such as DBpedia, YAGO2, and Wikidatahave chosen the RDF data model to represent their data due to its capabilities for semantically rich knowledge representation.Despite its advantages, there are challenges in using RDF data model, for example, data quality assessment and validation.In this paper, we present a novel knowledge base quality assessment approach that relies on evolution analysis. The proposedapproach uses data profiling on consecutive knowledge base releases to compute quality measures that allow detecting qualityissues. In particular, we propose four quality characteristics: Persistency, Historical Persistency, Consistency, and Completeness.Persistency and historical persistency measures concern the degree of changes and lifespan of any entity type. Consistency andcompleteness measures identify properties with incomplete information and contradictory facts. The approach has been assessedboth quantitatively and qualitatively on a series of releases from two knowledge bases, eleven releases of DBpedia and eightreleases of 3cixty. In particular, a prototype tool has been implemented using the R statistical platform. The capability of Per-sistency and Consistency characteristics to detect quality issues varies significantly between the two case studies. Persistencymeasure gives observational results for evolving KBs. It is highly effective in case of KB with periodic updates such as 3cixtyKB. The Completeness characteristic is extremely effective and was able to achieve 95% precision in error detection for both usecases. The measures are based on simple statistical operations that make the solution both flexible and scalable.Keywords: Quality Assessment, Quality Issues, Temporal Analysis, Knowledge Base, Linked Data1. IntroductionThe Linked Data approach consists in exposing andconnecting data from different sources on the Web bythe means of semantic web technologies. Tim Berners-Lee1 refers to linked open data as a distributed modelfor the Semantic Web that allows any data provider to*Corresponding author. E-mail: mohammad.rashid@polito.it1http://www.w3.org/DesignIssues/LinkedData.htmlpublish its data publicly, in a machine readable for-mat, and to meaningfully link them with other infor-mation sources over the Web. This is leading to the cre-ation of Linked Open Data (LOD) cloud hosting sev-eral Knowledge Bases (KBs) making available billionsof RDF triples from different domains such as Geogra-phy, Government, Life Sciences, Media, Publication,Social Networking, User generated2.2http://lod-cloud.net1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reserved2 M. Rashid et al. / A Quality Assessment Approach for Evolving Knowledge BasesThe knowledge bases are rapidly evolving sinceboth data instances and ontologies are updated, ex-tended, revised, and refactored [15] covering moreand more topical domains. In particular, KB instancesevolve over time given that new resources are added,old resources are removed, and links to resources areupdated or deleted. For instance, DBpedia [2] has beenavailable from the very beginning of the LOD move-ment and released various versions periodically. Alongwith each release, DBpedia proposed changes at bothinstance and schema level over time. In particular, theschema level changes involved classes, properties, ax-",
        "publication_date": "2018-09-11",
        "authors": "Mohammad Rifat Ahmmad Rashid, Marco Torchiano, Giuseppe Rizzo, Nandana Mihindukulasooriya, Óscar Corcho",
        "file_name": "swj1795.pdf",
        "file_path": "./PDFs/swj1795.pdf"
    },
    {
        "title": "Chowlk: from UML-Based Ontology Conceptualizations to OWL",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Chowlk",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/paper_90_Chavez-Feria_et_al.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The diagrams constructed with their corresponding OWL ontologies are available in the GitHub repository of the project19 for its verification."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-06981-9_20",
        "abstract": "Abstract. Ontology conceptualization is an ontology development taskthat consists in generating a preliminary model based on the require-ments that the ontology should represent. This activity is often carriedout by generating the models as diagrams in a blackboard, paper ordigital tools. The generated models drive the ontology implementationactivity, where the model is formalized and completed using an imple-mentation language. Normally, the ontology conceptualization outputserves as guidance for the ontology implementation; however, ontologyimplementation is usually done from scratch using ontology editors. Thegoal of this work is to consider ontology conceptualizations as first-orderartifacts in ontology development in order to boost the ontology im-plementation activity. For doing so we present Chowlk, a framework totransform digital machine-processable ontology conceptualization dia-grams into OWL. Domain experts and ontologists benefit from this ap-proach in several ways: 1) reduce time generating the first versions ofthe OWL file that can be invested on 2) focusing on the conceptualiza-tion diagrams that can be used both for 3) improving communicationbetween ontology users and developers and 4) be reused during the on-tology documentation stage.Keywords: Ontology engineering · ontology conceptualization · OWL1 IntroductionEveryday more and more applications are being built on top of or in combinationwith semantic technologies. Ontologies play a crucial role in this developmentas they allow the representation of knowledge in a formal and structured way,being the OWL [4] language the default choice for their implementation becauseof its high level of expressiveness, reasoning capabilities and the fact that it hasbeen designed for the web environment.One of the first and most important steps in ontology development is the con-ceptualization one, during which the ontology development team defines a set of⋆ This work has been supported by the BIMERR funded from the European Union’sHorizon 2020 research and innovation programme under grant agreement no. 820621.2 Serge Chávez-Feria, Raúl Garćıa-Castro, and Maŕıa Poveda-Villalónconcepts and properties to represent the knowledge of a specific domain. Often,this conceptualization is materialized in a diagram that displays the relation-ships, attributes and axioms of the different concepts of an ontology. From thismodel, the ontology implementation is carried out normally using an ontologyeditor, such as Protégé [11], realizing the model into OWL code.However, in this process the diagram is in most of the cases only used asa guideline to implement the ontology, translating the ontological elements andconstructs to a formal syntax, being this process mostly manual and error-prone.Some tools have been proposed in the last years that allow the graphical creationor modification of ontologies following their respective visual notations [16, 2].In our case, rather than building a graphical ontology editor, the effort isdriven towards the goal of allowing a smoother transition from the concep-tualization activity to a first version of the actual implementation by takingthe conceptualization output as a first order artifact in ontology developmentprojects. For doing so, the Chowlk framework has been designed. The frame-work, shown in Figure 1, consists of: 1) an UML-based visual notation; 2) a pairof diagrams.net templates implementing the visual notation; and 3) a converterfrom diagrams.net XML diagrams to OWL. It should be clarified that the re-",
        "publication_date": "2022-01-01",
        "authors": "Serge Chávez-Feria, Raúl García‐Castro, María Poveda‐Villalón",
        "file_name": "paper_90_Chavez-Feria_et_al.pdf",
        "file_path": "./PDFs/paper_90_Chavez-Feria_et_al.pdf"
    },
    {
        "title": "A graph-based representation of knowledge for managing land administration data from distributed agencies – A case study of Colombia",
        "implementation_urls": [],
        "doi": "10.1080/10095020.2021.2015250",
        "publication_date": "2022-01-07",
        "authors": "Luis M. Vilches‐Blázquez, Jhonny Saavedra",
        "file_name": "10!1080%10095020!2021!2015250.pdf",
        "file_path": "./PDFs/10!1080%10095020!2021!2015250.pdf"
    },
    {
        "title": "Dealing with Demand in Electric Grids with an Adaptive Consumption Management Platform",
        "implementation_urls": [
            {
                "identifier": "https://github.com/jasaavedra/GeoLOD",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1155%2018%4012740.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The knowl edge graphs are available on http://20.115.164."
                    }
                ]
            }
        ],
        "doi": "10.1155/2018/4012740",
        "abstract": "ABSTRACTMultiple efforts have been performed worldwide around diverse aspects of land administration. However, land administration data and systems’ notorious heterogeneity remains a longstanding challenge to develop a harmonized vision. In this sense, the traditional Spatial Data Infrastructures adoption is not enough to overcome this challenge since data sources’ heterogeneity implies needs related to harmonization interoperability, sharing, and integration in land administration development. This paper proposes a graph-based representation of knowledge for integrating multiple and heterogeneous data sources (tables, shapefiles, geodatabases, and WFS services) belonging to two Colombian agencies within a decentralized land administration scenario. These knowledge graphs are developed on an ontology-based knowledge representation using national and international standards for land administration. Our approach aims to prevent data isolation, enable cross-datasets integration, accomplish machine-processable data, and facilitate the reuse and exploitation of multi- jurisdictional datasets in a single approach. A real case study demonstrates the applicability of the land administration data cycle deployed.ARTICLE HISTORY Received 1 September 2021  Accepted 3 December 2021 KEYWORDS Land administration; knowledge graph; ontology; heterogeneity; data integration1. IntroductionLand administration is described as the process of recording and disseminating information about the ownership, use, and value of land and its related resources and incorporates restrictions and responsibilities associated with land rights, use and value, and impact of development processes (UNECE 1996). In this sense, it is undoubted that land administration is the cornerstone for securing tenure, taxation, valuation, land/resources management, and spatial planning (van Oosterom and Lemmen 2015).Traditionally, land administration information was registered in hard-copy formats. Geographic Information Systems (GIS) advancement initiated a digitalization process and made this information available in digital form. Later on, Williamson et al. (2010) argued that effective and efficient Land Administration Systems required Spatial Data Infrastructures (SDI) to progress. Supporting this assertion, we have witnessed SDIs presence in almost every country around the world over the last 20 years. These initiatives have combined systems, data sources, standards, network linkages, and institutional issues to provide geospatial information from different sources to the broadest audience (Giuliani et al. 2017). In this scenario, SDIs ",
        "publication_date": "2018-01-01",
        "authors": "Diego M. Jiménez-Bravo, Juan F. De Paz, Gabriel Villarrubia González, Javier Bajo",
        "file_name": "10!1155%2018%4012740.pdf",
        "file_path": "./PDFs/10!1155%2018%4012740.pdf"
    },
    {
        "title": "Ontology based E-learning framework: A personalized, adaptive and context aware model",
        "implementation_urls": [],
        "doi": "10.1007/s11042-019-08125-8",
        "abstract": "AbstractEnhancing the degree of learner productivity, one of the major challenges in E-Learningsystems, may be catered through effective personalization, adaptivity and context awarenesswhile recommending the learning contents to the learners. In this paper, an E-Learningframework has been proposed that profiles the learners, categorizes the learners based onprofiles, makes personalized content recommendations and performs assessment based contentadaptation. A mathematical model has been proposed for learner categorization using machinelearning techniques (a hybrid of case based reasoning and neural networks). The learningcontents have been annotated through CourseOntology in which three academic courses (eachfor language of C++, C# and JAVA) have been modeled for the learners. A dynamic rule basedrecommender has been presented targeting a ‘relative grading system’ for maximizing thelearner’s productivity. Performance of proposed framework has been measured in terms ofaccurate learner categorization, personalized recommendation of the learning contents, com-pleteness and correctness of ontological model and overall performance improvement oflearners in academic sessions of 2015, 2016 and 2017. The comparative analysis of proposedframework exhibits visibly improved results compared to prevalent approaches. These im-provements are signified to the comprehensive attribute selection in learner profiling, dynamictechniques for learner categorization and effective content recommendation while ensuringpersonalization and adaptivity.Keywords Ontologies . E-Learning . Personalization . Adaptivity . Content Recommender1 IntroductionThe perceptible dominance of internet has affected every aspect of human life that canspecifically be observed on academic landscape in the form of Electronic Learning (or “E-https://doi.org/10.1007/s11042-019-08125-8* Sohail Sarwarsohail.sarwar@seecs.edu.pkExtended author information available on the last page of the articleMultimedia Tools and Applications (2019) 78:34745–34771Published online: 13 September\\ 2019/http://crossmark.crossref.org/dialog/?doi=10.1007/s11042-019-08125-8&domain=pdfhttp://orcid.org/0000-0001-7565-439Xmailto:sohail.sarwar@seecs.edu.pkLearning). The revolutionary advent of knowledge universality through E-Learning hasentailed in upsurge of educational elite in societies through development of professionals byeliminating costs of workforce and infrastructure. Currently, these systems are playing apivotal role in transforming information societies into knowledge societies through widespreaddelivery of didactic contents with a vision to educate future generation of learners (expected togrow from 100 million learners to 250 million by 2025 [3]). Such cyberspace driven learningis paving the way for building hubs of inventive activities through the emergence of E-Learning concepts [29] with target of maximizing the learner’s productivity and effectiveness.E-Learning is not confined merely to prompt deliverance of educational contents, rather it’sa line of packages ranging from content development to maintaining profile of learners,aligning contents to respective learners as per their ability, from maintaining practice exercisesto managing grading, adaptivity and personalization of learning material and searching fromrelevant educational repositories. The multifaceted and ubiquitous view of an E-Learningsystem has been illustrated in Fig. 1, aiming to improvise the role of E-Learning frominformation transmission to knowledge-construction [3] and deliverance targeted to enhancelearner productivity.With core functional components and services, the focus of our work remains on deliver-",
        "publication_date": "2019-09-13",
        "authors": "Sohail Sarwar, Zia Ul Qayyum, Raúl García‐Castro, Muhammad Safyan, Rana Faisal Munir",
        "file_name": "s11042-019-08125-8.pdf",
        "file_path": "./PDFs/s11042-019-08125-8.pdf"
    },
    {
        "title": "An Intelligent System to Generate Chord Progressions from Colors with an Artificial Immune System",
        "implementation_urls": [],
        "doi": "10.1007/s00354-020-00100-4",
        "abstract": "AbstractSynesthesia is a neurological phenomenon in which stimulation of one sensory or cognitive pathway leads to automatic, involuntary experiences in a second sensory or cognitive pathway. Many synesthetes used their capabilities as inspiration for their works. Drawing on this phenomenon, this paper presents a model able to cre‑ate chord progressions using colors as the synesthetic input. In particular, the model extracts sound from colors to create chord progressions by applying an artificial immune system (AIS). The quality of each chord is mapped in the Tonal Interval Space, a geometrical space in which mathematical measures are related to musical properties. The result is an assistive tool that has been evaluated in terms of musical standards and usefulness for the users.Keywords  Synesthesia · Music generation · Chords · Artificial immune systemIntroductionSynesthesia can be defined as a neurological phenomenon in which stimulation of one sensory or cognitive pathway leads to automatic and involuntary experiences in a second cognitive pathway [5]. Although not everyone has this ability, there have been many synesthetes, most of whom were famous artists, writers, or musicians [7]. Many of them used this ability as inspiration for the creation of new artistic works. For example, Russian painter Wassily Kandinsky combined four senses [6]: color, hearing, touch, and smell. Synesthetic composers such as Duke Ellington,  *\t María Navarro‑Cáceres \t maria90@usal.es1\t School of Engineering, University of Edinburgh, Thomas Bayes Rd, EH9 3JG Edinburgh, United Kingdom2\t Computer Sciences Department, University of Salamanca, Calle Espejo, 2, Salamanca 37007, Spain3\t Computer Sciences Department, Polytechnic University of Madrid, Paseo de Juan XXIII, 11, 28040 Madrid, Spainhttp://crossmark.crossref.org/dialog/?doi=10.1007/s00354-020-00100-4&domain=pdf532\t New Generation Computing (2020) 38:531–549123Nikolai Rimsky-Korsakov, and Olivier Messiaen used different types of complex colors that were rendered explicitly in musical chord structures that they invented [2]. All of them use the synesthetic phenomena as inspiration for their works.The synesthesia is usually subjective and it depends on the composer’s emotions and perception. However, many artists try to compose melodies and chord (a set of three or more notes reproduced at the same time) progressions based on a visual sample, in which colors play an essential role. In fact, one of the most interesting applications is helping composers, especially those with less experience, to create harmonic progressions. This might be considered easy for not novice composers. However, the most interesting part of the approach is that the progression can be based on images or colors. This opens a new way to create music in which colors (an image in general) can influence the generation process. The relation between colors and music is widely applied in the creation of original soundtracks, advertisement, ambience music or soundscapes. Thus, this application, evolved, can be useful, for example, as a small aid for movies or for ambient sounds. It could also be applied to create “ambient music” capable of somehow describing visual art for people with certain disabilities. To assist these composers in the selection of chords from an image, we developed the work described in the present manuscript.The state of the art in the context of technology shows that there are many ",
        "publication_date": "2020-06-25",
        "authors": "María Navarro-Cáceres, José A. Castellanos-Garzón, Javier Bajo",
        "file_name": "s00354-020-00100-4.pdf",
        "file_path": "./PDFs/s00354-020-00100-4.pdf"
    },
    {
        "title": "Introduction: A Hybrid Regulatory Framework and Technical Architecture for a Human-Centered and Explainable AI",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_1",
        "abstract": "Abstract. This introduction presents the fifth volume of a series started twelveyears ago: the AI Approaches to the Complexity of Legal Systems (AICOL).The introduction revises the recurrently addressed topics of technology, ArtificialIntelligence and law and presents new challenges and areas of research, suchas the AI ethical and legal turn, hybrid and conflictive intelligences, regulatorycompliance and AI explainability. Other domains not yet fully explored includethe regulatory models of the Web of Data and the Internet of Things that integratelegal reasoning and legal knowledge modelling.Keywords: AICOL workshops · Artificial intelligence and law · Semantic web ·LegalXML ·Web of linked data · Internet of Things · Ethics · Human rights ·Privacy · Rule of law1 IntroductionThis is the Introduction to the fifth volume of AI Approaches to the Complexity of LegalSystems (AICOL). During the past twelve years, AICOL editions have been consistentlyshowing the evolution of the research carried out in the field of technology, ArtificialIntelligence, and law. In this edition, we point out the main trends and developmentsof the last three years, in a time marked by the pandemic, but also by a growing andsustained heed in the regulatory models of the Web of linked Data and the Internet ofThings integratedwith legal reasoning and legal knowledgemodelling. It would be a case© Springer Nature Switzerland AG 2021V. Rodríguez-Doncel et al. (Eds.): AICOL-XI 2018/AICOL-XII 2020/XAILA 2020, LNAI 13048, pp. 1–11, 2021.https://doi.org/10.1007/978-3-030-89811-3_1http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-89811-3_1&domain=pdfhttp://orcid.org/0000-0003-1076-2511http://orcid.org/0000-0002-8557-8084http://orcid.org/0000-0003-2524-3976http://orcid.org/0000-0002-0980-2371http://orcid.org/0000-0001-7981-8849http://orcid.org/0000-0003-2210-0398https://doi.org/10.1007/978-3-030-89811-3_12 V. Rodríguez-Doncel et al.of increasing returns, asmany outcomes of research are just a step ahead, not having beenimplemented yet. In this scenario we are assisting to the cross-fertilization of differentresearch fields: subsymbolic AI should be combined with symbolic AI and SemanticWeb in order to provide better instruments to implement the concept of explicability andthe ethics principles. Human-centered AI needs integrated approach where technical,social, legal and ethics approaches are used together for supporting the Human-in-the-loop principle. This is a matter of time, as more young researchers are joining and areadvancing and figuring out new approaches to cope with the problems raised in digitalscenarios that are becoming the natural ecosystems of our time. But it is also a matterof will, as many solutions require the agreement and collaborative effort of citizens,companies, governments, and social and economic institutions. As we will state in thisvolume, we are facing hybrid and conflictive environments, in which, the simultaneousemergence of new technologies is changing the notion of what it means to be human inthe digital age [1]. The reminder of the Introduction will briefly address these renewedchallenges, focusing on its ethical and legal components, before briefly introducing thecontents of the volume.2 A Latest AI Ethical and Legal TurnScholars and institutions have extensively discussed the ethical (and legal) principles ofAI over the past years. So far, we have got more than a hundred of such declarations,",
        "publication_date": "2021-01-01",
        "authors": "Victor Rodrı́guez-Doncel, Monica Palmirani, Michał Araszkiewicz, Pompeu Casanovas, Ugo Pagallo, Giovanni Sartor",
        "file_name": "978-3-030-89811-3_1.pdf",
        "file_path": "./PDFs/978-3-030-89811-3_1.pdf"
    },
    {
        "title": "Data governance through a multi-DLT architecture in view of the GDPR",
        "implementation_urls": [],
        "doi": "10.1007/s10586-022-03691-3",
        "abstract": "AbstractThe centralization of control over the processing of personal data threatens the privacy of individuals due to the lack oftransparency and the obstruction of easy access to their data. Individuals need the tools to effectively exercise their rights,enshrined in regulations such as the European Union General Data Protection Regulation (GDPR). Having direct controlover the flow of their personal data would not only favor their privacy but also a ‘‘data altruism’’, as supported by the newEuropean proposal for a Data Governance Act. In this work, we propose a multi-layered architecture for the managementof personal information based on the use of distributed ledger technologies (DLTs). After an in-depth analysis of thetensions between the GDPR and DLTs, we propose the following components: (1) a personal data storage based on a(possibly decentralized) file storage (DFS) to guarantee data sovereignty to individuals, confidentiality and data portability;(2) a DLT-based authorization system to control access to data through two distributed mechanisms, i.e. secret sharing (SS)and threshold proxy re-encryption (TPRE); (3) an audit system based on a second DLT. Furthermore, we provide aprototype implementation built upon an Ethereum private blockchain, InterPlanetary File System (IPFS) and Sia and weevaluate its performance in terms of response time.Keywords Distributed Ledger Technology � GDPR � Smart Contracts � Personal Data � Decentralized File Storage �Data Governance1 IntroductionThe control, direct or indirect, that individuals currentlyexercise over their personal data is conditioned by thecentralized platform-based personal information manage-ment techniques, which are then concentrated in a fewinternet service providers (ISPs) for the purpose ofexploring, filtering and obtaining data of interest [1]. Thelack of control by individuals over access to their data is ofgrowing concern and, as a result, several regulations havebeen enacted with the aim of addressing this need. TheGeneral Data Protection Regulation (GDPR) [2] is a prin-cipal example, designed for European citizens to helppromote a view in favor of the interests of individuals,instead of large corporations. It has been followed by otherregulations around the world, such as the California Con-sumer Privacy Act [3] in the USA. The GDPR conveysdata control by imposing a number of accountabilitymeasures on the responsible actors and by assigning a setof rights to individuals, i.e. as ‘‘natural persons should havecontrol of their own personal data’’ (Recital 7). Dedicatedtechnologies can help either companies to comply withGDPR (and similar) and individuals to exercise their rights,with particular regard to address two main issues: the lackof transparency in the management of personal informationand the inability to access and make interoperable personaldata.& Stefano Ferrettistefano.ferretti@uniurb.itMirko Zichichimirko.zichichi@upm.esGabriele D’Angelog.dangelo@unibo.itVı́ctor Rodrı́guez-Doncelvrodriguez@fi.upm.es1 Ontology Engineering Group, Universidad Politécnica de",
        "publication_date": "2022-08-10",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%s10586-022-03691-3.pdf",
        "file_path": "./PDFs/10!1007%s10586-022-03691-3.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s10586-022-03691-3.pdf"
    },
    {
        "title": "Editorial of transport data on the web",
        "implementation_urls": [],
        "doi": "10.3233/sw-223278",
        "publication_date": "2022-10-14",
        "authors": "David Chaves-Fraga, Pieter Colpaert, Mersedeh Sadeghi, Marco Comerio",
        "file_name": "10!3233%sw-223278.pdf",
        "file_path": "./PDFs/10!3233%sw-223278.pdf"
    },
    {
        "title": "Enhancing the Maintainability of the Bio2RDF Project Using Declarative Mappings.",
        "implementation_urls": [],
        "abstract": "Abstract. Bio2RDF is one of the most popular projects that integratesand publishes biomedical datasets as Linked Data. The community hasactively contributed to the generation of these datasets using ad-hoc pro-grammed scripts. In the context of the Semantic Web, Ontology-BasedData Access (OBDA) approaches have been proposed to provide dataaccess and transformation in a more standardized way, using declara-tive mapping languages. In this paper, we propose the use of an OBDAapproach to provide an alternative to the way in which transformationsinto RDF are currently done in the Bio2RDF project, with the aim of en-hancing its methodology in terms of understandability, reusability andmaintainability. We describe the proposed methodology together withthe declarative mappings creation process aiming to improve the afore-mentioned features. We compare the RDF dataset generated using ourproposal with the latest release of Bio2RDF for a subset of the datasources that we have dealt with. Finally, we discuss the set of challengesthat we face with this approach.Keywords: Bio2RDF · OBDA · RML1 IntroductionIn the last decades, the amount of databases that have been created to store andshare biological knowledge has heavily increased [1,2]. According to [3], there aremore than 1600 biological databases that are publicly accessible online, includingwell-known examples, such as PubMed1, UniProt2 or KEGG3. Nowadays, theseresources have become essential for researchers, as they rely on them to conductmuch of their work.Each biological data source contains information specific to its domain. Thismeans that the knowledge of a concept (e.g. enzyme, transcription factor) isdistributed in multiple data sources that are created by different institutions,usually represented in different formats and terminologies. A relevant challengein this domain is how to integrate these data sources in order to provide a1 https://www.ncbi.nlm.nih.gov/pubmed/2 https://www.uniprot.org/3 https://www.genome.jp/kegg/https://www.ncbi.nlm.nih.gov/pubmed/https://www.uniprot.org/https://www.genome.jp/kegg/2 Iglesias-Molina et al.Table 1: Comparison of the methodology of Bio2RDF in its differentreleases and the proposed approach with declarative mappings. Thefeatures compared are the type of tool, how many can be used, and if it allowsmaterialisation or virtualization.FeatureBio2RDFRelease 1Bio2RDFReleases 2 & 3DeclarativeMappingsTool Type Ad-hoc solution Ad-hoc solution General Purpose# Tools 1 (myBio2RDF app) 1 (PHP scripts) ManyMaterialization Yes Yes Yes",
        "publication_date": "2019-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Freddy Priyatna, Óscar Corcho",
        "file_name": "no_doi_20250624162844.pdf",
        "file_path": "./PDFs/no_doi_20250624162844.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2849/paper-01.pdf"
    },
    {
        "title": "LOT: An industrial oriented ontology engineering framework",
        "implementation_urls": [
            {
                "identifier": "https://github.com/SmartDeveloperHub/sdh-vocabulary",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1016%j!engappai!2022!104755.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Users Experts activity flow X (input) Artefact reference Figure 1: LOT methodology base workflow Even though there are many roles which can be involved in ontology development projects, the LOT methodology classifies them into the following groups: 5Online documentation of the methodology is available at https://lot.linkeddata.es/5 https://lot.linkeddata.es/• Ontology developer: An ontology developer is a member of the ontology development team who has high knowledge about ontology development and knowledge representation."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.engappai.2022.104755",
        "abstract": "AbstractOntology Engineering has captured much attention during the last decades leading to the prolif-eration of numerous works regarding methodologies, guidelines, tools, resources, etc. includingtopics which are still being investigated. Even though, there are still many open questions whenaddressing a new ontology development project, regarding how to manage the overall projectand articulate transitions between activities or which tasks and tools are recommended for eachstep. In this work we propose the Linked Open Terms (LOT) methodology, an overall and light-weight methodology for building ontologies based on existing methodologies and oriented tosemantic web developments and technologies. The LOT methodology focuses on the alignmentwith industrial development, in addition to academic and research projects, and software devel-opment, that is making ontology development part of the software industry. This methodologyincludes lessons learnt from more than 20 years in ontological engineering and its application on18 projects is reported.Keywords: Ontology engineering; Ontology development methodology; Ontology developmentsoftware support; Collaborative ontology development; Ontology industrial development1. IntroductionQ11: Should you always write ontology functional requirements in the form of CompetencyQuestions (CQs) (Grüninger and Fox, 1995) when you elaborate the requirements specificationfor an ontology? Q2: Which is the best way to communicate requirements to software engineers?Q3: And to domain experts? Q4: Which are the main problems that you will find when you reusean ontology? Q5: Which is the sequence of activities that has been shown to be successful inpractice in ontology development? Q6: What tools can you use for each activity in ontologydevelopment? Q7: Are there some tools that aid you to identify typical mistakes in modelling?Q8: What do you have to do to transform your Web Ontology Language (OWL) ontology intoan HTML document?Email addresses: mpoveda@fi.upm.es (María Poveda-Villalón), albafernandez@fi.upm.es (AlbaFernández-Izquierdo), mfernandez.eps@ceu.es (Mariano Fernández-López), rgarcia@fi.upm.es (RaúlGarcía-Castro)1The following questions are identified by “QX” to ease the reference to the questions from other sections of thepaper.Preprint submitted to Journal of LATEX Templates 24th January 2022Although it is true that a lot of useful work has been carried out on ontology engineering overthe years, such as the proposal of multiple ontology development methodologies to systematisethe development process and the alignment with agile practices, as of today, there are importantquestions on ontology development that have not been answered, This issue has been exposed bythe recent analysis of the current state, challenges and future directions in ontology engineeringpresented by Tudorache (Tudorache, 2020).The aim of the work presented in this paper is to respond to this situation and answer thequestions presented in the first paragraph by proposing the Linked Open Terms (LOT) methodo-logy, which not only presents the activities to be performed in the ontology development process,but also proposes recommendations, tips and tools to support them. The LOT methodology isbased on the experience of, at least, 18 projects where ontologies have been developed, both bythis paper authors and by external teams, involving both domain experts and software engineers.Our experience is also diverse in other senses, for example, there are projects where the creationof linked open data has been an important result, others where the ontology has been an object-ive itself, others where the ontology has been an standard schema for communication betweensystems, etc. In addition, one of the authors has contributed in the past to two of the most well-known methodologies for building ontologies which brings not only an extensive experience inpractical matters but also a broader view and knowledge about the evolution of the ontology en-gineering field during the last decades. The conclusions and lessons learnt from our experience",
        "publication_date": "2022-03-03",
        "authors": "María Poveda‐Villalón, Alba Fernández-Izquierdo, Mariano Fernández‐López, Raúl García‐Castro",
        "file_name": "10!1016%j!engappai!2022!104755.pdf",
        "file_path": "./PDFs/10!1016%j!engappai!2022!104755.pdf"
    },
    {
        "title": "A guideline for reporting experimental protocols in life sciences",
        "implementation_urls": [],
        "doi": "10.7717/peerj.4795",
        "abstract": "ABSTRACTExperimental protocols are key when planning, performing and publishing researchin many disciplines, especially in relation to the reporting of materials and methods.However, they vary in their content, structure and associated data elements. This articlepresents a guideline for describing key content for reporting experimental protocolsin the domain of life sciences, together with the methodology followed in order todevelop such guideline. As part of our work, we propose a checklist that contains 17 dataelements that we consider fundamental to facilitate the execution of the protocol. Thesedata elements are formally described in the SMART Protocols ontology. By providingguidance for the key content to be reported, we aim (1) to make it easier for authorsto report experimental protocols with necessary and sufficient information that allowothers to reproduce an experiment, (2) to promote consistency across laboratories bydelivering an adaptable set of data elements, and (3) to make it easier for reviewers andeditors to measure the quality of submitted manuscripts against an established criteria.Our checklist focuses on the content, what should be included. Rather than advocatinga specific format for protocols in life sciences, the checklist includes a full descriptionof the key data elements that facilitate the execution of the protocol.Subjects Biochemistry, Biotechnology, Cell Biology, Molecular Biology, Plant ScienceKeywords Checklist, Experimental protocols, Guidelines, Recommendations, Good practices forreporting protocols, Open science, ReproducibilityINTRODUCTIONExperimental protocols are fundamental information structures that support thedescription of the processes by means of which results are generated in experimentalresearch (Giraldo et al., 2017; Freedman, Venugopalan & Wisman, 2017). Experimentalprotocols, often as part of ‘‘Materials and Methods’’ in scientific publications, are centralfor reproducibility; they should include all the necessary information for obtainingconsistent results (Casadevall & Fang, 2010; Festing & Altman, 2002). Although protocolsare an important component when reporting experimental activities, their descriptionsare often incomplete and vary across publishers and laboratories. For instance, whenreporting reagents and equipment, researchers sometimes include catalog numbersand experimental parameters; they may also refer to these items in a generic manner,e.g., ‘‘Dextran sulfate, Sigma-Aldrich’’ (Karlgren et al., 2009). Having this information isimportant because reagents usually vary in terms of purity, yield, pH, hydration state,How to cite this article Giraldo et al. (2018), A guideline for reporting experimental protocols in life sciences. PeerJ 6:e4795; DOI10.7717/peerj.4795https://peerj.commailto:ogiraldo@fi.upm.eshttps://peerj.com/academic-boards/editors/https://peerj.com/academic-boards/editors/http://dx.doi.org/10.7717/peerj.4795http://creativecommons.org/licenses/by/4.0/http://creativecommons.org/licenses/by/4.0/http://dx.doi.org/10.7717/peerj.4795grade, and possibly additional biochemical or biophysical features. Similarly, experimentalprotocols often include ambiguities such as ‘‘Store the samples at room temperature untilsample digestion’’ (Brandenburg et al., 2002); but, how many Celsius degrees? What is theestimated time for digesting the sample? Having this information available not only savestime and effort, it also makes it easier for researchers to reproduce experimental results;adequate and comprehensive reporting facilitates reproducibility (Freedman, Venugopalan& Wisman, 2017; Baker, 2016).",
        "publication_date": "2018-05-28",
        "authors": "Olga Giraldo, Alexander García, Óscar Corcho",
        "file_name": "10!7717%peerj!4795.pdf",
        "file_path": "./PDFs/10!7717%peerj!4795.pdf"
    },
    {
        "title": "An intelligent interface for integrating climate, hydrology, agriculture, and socioeconomic models",
        "implementation_urls": [],
        "doi": "10.1145/3308557.3308711",
        "abstract": "ABSTRACT Understanding the interactions between natural processes and human activities poses major challenges as it requires the integration of models and data across disparate disciplines.  It typically takes many months and even years to create valid end-to-end simulations as different models need to be configured in consistent ways and generate data that is usable by other models. MINT is a novel framework for model integration that captures extensive knowledge about models and data and aims to automatically compose them together. MINT guides a user to pose a well-formed modeling question, select and configure appropriate models, find and prepare appropriate datasets, compose data and models into end-to-end workflows, run the simulations, and visualize the results. MINT currently includes hydrology, agriculture, and socioeconomic models.  CCS CONCEPTS H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous. KEYWORDS Intelligent workflow systems; model integration; environmental modeling, scientific discovery.  ACM Reference format: Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Kelly Cobourn, Ewa Deelman, Chris Duffy, Rafael Ferreira da Silva, Armen Kemanian, Craig Knoblock, Vipin Kumar, Scott Peckham, Yao-Yi Chiang, Ankush Khandelwal, Minh Pham, Jay Pujara, Maria Stoica, Kshitij Tayal, Binh Vu, Dan Feldman, Lele Shu, Rajiv Mayani, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne Pierce. 2018. An Intelligent Interface for Integrating Climate, Hydrology, Agriculture, and Socioeconomic Models. In Proceedings of IUI '19 Companion, March 17–20, 2019, Marina del Rey, CA, USA. https://doi.org/10.1145/3308557.3308711 1 Introduction Understanding how human activities affect natural resources, and how natural processes affect societies requires complex computational simulations that cut across disciplinary boundaries. For example, in order to understand how weather predictions and agriculture practices affect water availability or how flooding affects planting strategies and population migration, modelers integrate physics-based climate and hydrology models with biologically-informed agriculture models and market distribution economic models. While the questions are short fused in order to prepare for natural disasters or to make near-term policy decisions, integrating these diverse models may take many months or even years.  Major challenges include finding relevant models to address a question and datasets with the necessary granularity and quality to run the models, developing sophisticated data transformations to set up and execute a model, and checking for models compatibility. Existing infrastructure supports some aspects of the modeling (e.g., [1]) but there are no comprehensive frameworks that tackle ",
        "publication_date": "2019-02-28",
        "authors": "Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Ewa Deelman, Rafael Ferreira da Silva, Craig A. Knoblock, Yao‐Yi Chiang, Tam Minh Pham, Jay Pujara, Bình Dương Vũ, Dan Feldman, Rajiv Mayani, Kelly M. Cobourn, Christopher Duffy, Armen R. Kemanian, Lele Shu, Vipin Kumar, Ankush Khandelwal, Kshitij Tayal, S. D. Peckham, Maria Stoica, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne A. Pierce",
        "file_name": "10!1145%3308557!3308711.pdf",
        "file_path": "./PDFs/10!1145%3308557!3308711.pdf"
    },
    {
        "title": "Reliability and correlation analysis of computed methods to convert conventional 2D radiological hindfoot measurements to a 3D setting using weightbearing CT",
        "implementation_urls": [],
        "doi": "10.1007/s11548-018-1727-5",
        "publication_date": "2018-03-09",
        "authors": "Arne Burssens, Johannes M. Peeters, Matthias Peiffer, R. Marien, Tom Lenaerts, Geoffroy Vandeputte, Jan Victor",
        "file_name": "286549379.pdf",
        "file_path": "./PDFs/286549379.pdf"
    },
    {
        "title": "ENIGMA and global neuroscience: A decade of large-scale studies of the brain in health and disease across more than 40 countries",
        "implementation_urls": [
            {
                "identifier": "https://github.com/npnl/PALS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624162916.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Preprint at https://doi.org/10.31234/osf.io/jnsb2."
                    }
                ]
            }
        ],
        "doi": "10.1038/s41398-020-0705-1",
        "abstract": "AbstractThis review summarizes the last decade of work by the ENIGMA (Enhancing NeuroImaging Genetics through MetaAnalysis) Consortium, a global alliance of over 1400 scientists across 43 countries, studying the human brain in healthand disease. Building on large-scale genetic studies that discovered the first robustly replicated genetic loci associatedwith brain metrics, ENIGMA has diversified into over 50 working groups (WGs), pooling worldwide data and expertiseto answer fundamental questions in neuroscience, psychiatry, neurology, and genetics. Most ENIGMA WGs focus onspecific psychiatric and neurological conditions, other WGs study normal variation due to sex and gender differences,or development and aging; still other WGs develop methodological pipelines and tools to facilitate harmonizedanalyses of “big data” (i.e., genetic and epigenetic data, multimodal MRI, and electroencephalography data). Theseinternational efforts have yielded the largest neuroimaging studies to date in schizophrenia, bipolar disorder, majordepressive disorder, post-traumatic stress disorder, substance use disorders, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, autism spectrum disorders, epilepsy, and 22q11.2 deletion syndrome. More recentENIGMA WGs have formed to study anxiety disorders, suicidal thoughts and behavior, sleep and insomnia, eatingdisorders, irritability, brain injury, antisocial personality and conduct disorder, and dissociative identity disorder. Here,we summarize the first decade of ENIGMA’s activities and ongoing projects, and describe the successes and challengesencountered along the way. We highlight the advantages of collaborative large-scale coordinated data analyses fortesting reproducibility and robustness of findings, offering the opportunity to identify brain systems involved in clinicalsyndromes across diverse samples and associated genetic, environmental, demographic, cognitive, and psychosocialfactors.IntroductionThe ENIGMA (Enhancing NeuroImaging Geneticsthrough Meta Analysis) Consortium is a collaboration ofmore than 1400 scientists from 43 countries studying thehuman brain. ENIGMA started 10 years ago, in 2009, withthe initial aim of performing a large-scale neuroimaginggenetic study, and has since diversified into 50 workinggroups (WGs), pooling worldwide data, resources andexpertise to answer fundamental questions in neu-roscience, psychiatry, neurology, and genetics (Fig. 1shows a world map of participating sites, broken down byworking group). Thirty of the ENIGMA WGs focus onspecific psychiatric and neurologic conditions. Four studydifferent aspects of development and aging. Others studykey transdiagnostic constructs, such as irritability, and theimportance of evolutionarily interesting genomic regionsin shaping human brain structure and function. Central tothe success of these WGs are the efforts of dedicatedmethods development groups within ENIGMA. There arecurrently 12 WGs that develop and disseminate multi-scale and ‘big data’ analysis pipelines to facilitate harmo-nized analyses using genetic and epigenetic data,multimodal (anatomical, diffusion, functional) magnetic© The Author(s) 2020OpenAccessThis article is licensedunder aCreativeCommonsAttribution 4.0 International License,whichpermits use, sharing, adaptation, distribution and reproductionin any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate ifchangesweremade. The images or other third partymaterial in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to thematerial. Ifmaterial is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtainpermission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.Correspondence: Paul M. Thompson (pthomp@usc.edu)Full list of author information is available at the end of the article.",
        "publication_date": "2020-03-20",
        "authors": "Paul M. Thompson, Neda Jahanshad, Christopher R. K. Ching, Lauren E. Salminen, Sophia I. Thomopoulos, Joanna K. Bright, Bernhard T. Baune, Sara Bertolín, Janita Bralten, Willem B. Bruin, Robin Bülow, Jian Chen, Yann Chye, Udo Dannlowski, Carolien G. F. de Kovel, Gary Donohoe, Lisa T. Eyler, Stephen V. Faraone, Pauline Favre, Courtney A. Filippi, Thomas Frodl, Daniel Garijo, Yolanda Gil, Hans J. Grabe, Katrina L. Grasby, Tomáš Hájek, Laura K. M. Han, Sean N. Hatton, Kevin Hilbert, Tiffany C. Ho, Laurena Holleran, Georg Homuth, Norbert Hosten, Josselin Houenou, Iliyan Ivanov, Tianye Jia, Sinéad Kelly, Marieke Klein, Jun Soo Kwon, Max A. Laansma, Jeanne Leerssen, Ulrike Lueken, Abraham Nunes, Joseph O' Neill, Nils Opel, Fabrizio Piras, Federica Piras, Merel C. Postema, Elena Pozzi, Natalia Shatokhina, Carles Soriano‐Mas, Gianfranco Spalletta, Daqiang Sun, Alexander Teumer, Amanda K. Tilot, Leonardo Tozzi, Celia van der Merwe, Eus J.W. Van Someren, Guido van Wingen, Henry Völzke, Esther Walton, Lei Wang, Anderson M. Winkler, Katharina Wittfeld, Margaret J. Wright, Je‐Yeon Yun, Guohao Zhang, Yanli Zhang‐James, Bhim M. Adhikari, Ingrid Agartz, Moji Aghajani, André Alemán, Robert R. Althoff, André Altmann, Ole A. Andreassen, David Baron, Brenda Bartnik‐Olson, Janna Marie Bas‐Hoogendam, Arielle Baskin–Sommers, Carrie E. Bearden, Laura A. Berner, Premika S.W. Boedhoe, Rachel M. Brouwer, Jan K. Buitelaar, Karen Caeyenberghs, Charlotte A. M. Cecil, Ronald A. Cohen, James H. Cole, Patricia Conrod, Stéphane A. De Brito, Sonja M. C. de Zwarte, Emily L. Dennis, Sylvane Desrivières, Danai Dima, Stefan Ehrlich, Carrie Esopenko, Graeme Fairchild, Simon E. Fisher, Jean‐Paul Fouché, Clyde Francks",
        "file_name": "no_doi_20250624162916.pdf",
        "file_path": "./PDFs/no_doi_20250624162916.pdf",
        "pdf_link": "https://www.nature.com/articles/s41398-020-0705-1.pdf"
    },
    {
        "title": "Optimized Term Extraction Method Based on Computing Merged Partial C-Values",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OntoElect/Data",
                "type": "git",
                "paper_frequency": 8,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-030-39459-2_2.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "8 The partition of the DMKD-300 collection: https://github.com/OntoElect/Data/blob/master/DMKD-300-DCF-Part.zip."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-39459-2_2",
        "abstract": "Abstract. Assessing the completeness of a document collection, regarding ter-minological coverage of a domain of interest, is a complicated task that requiressubstantial computational resource and human effort. Automated term extraction(ATE) is an important step within this task in our OntoElect approach. It outputsthe bags of terms extracted from incrementally enlarged partial document col-lections for measuring terminological saturation. Saturation is measured itera-tively, using our thd measure of terminological distance between the two bags ofterms. The bags of retained significant termsTi andTiþ 1 extracted at i-th and i + 1-st iterations are compared ðthdðTi; Tiþ 1ÞÞ until it is detected that thd went belowthe individual term significance threshold. The flaw of our conventional approachis that the sequence of input datasets is built by adding an increment of severaldocuments to the previous dataset. Hence, the major part of the documentsundergoes term extraction repeatedly, which is counter-productive. In this paper,we propose and prove the validity of the optimized pipeline based on the modifiedC-value method. It processes the disjoint partitions of a collection but not theincrementally enlarged datasets. It computes partial C-values and then mergesthese in the resulting bags of terms. We prove that the results of extraction arestatistically the same for the conventional and optimized pipelines.We support thisformal result by evaluation experiments to prove document collection and domainindependence. By comparing the run times, we prove the efficiency of the opti-mized pipeline. We also prove experimentally that the optimized pipeline effec-tively scales up to process document collections of industrial size.Keywords: Automated term extraction � Terminological saturation � Partial C-value � Merged Partial C-value � Optimization1 IntroductionOntology learning from texts is a developing research field that aims to extract domaindescription theories from document collections or corpora describing the subject domain.It is increasingly acknowledged as a plausible alternative to ontology development based© Springer Nature Switzerland AG 2020V. Ermolayev et al. (Eds.): ICTERI 2019, CCIS 1175, pp. 24–49, 2020.https://doi.org/10.1007/978-3-030-39459-2_2http://orcid.org/0000-0002-7300-8818http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0001-5742-104Xhttp://orcid.org/0000-0002-5159-254Xhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-39459-2_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-39459-2_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-39459-2_2&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-39459-2_2on the interviews of domain knowledge stakeholders. One shortcoming of ontologylearning from texts is that the input document collection has to be quite big for beingrepresentative for the subject domain. Another shortcoming is that learning ontologiesfrom text is expensive, in terms of taken time, as it involves the use of several algorithms,in a pipeline [1], that are computationally hard.Automated term extraction is an essential step at the beginning of the pipeline forontology learning [1, 2], that is known to be bulky in terms of the increase of the runtime with the growth of the input text corpus. Therefore, finding a way to reduce:(i) either the size of the processed text; or (ii) the time spent for term extraction; or(iii) both – is of importance.In our prior work [2–5], we developed the ATE-based approach (OntoElect) that",
        "publication_date": "2020-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Hennadii Dobrovolskyi, Vadim Ermolayev",
        "file_name": "978-3-030-39459-2_2.pdf",
        "file_path": "./PDFs/978-3-030-39459-2_2.pdf"
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "publication_date": "2021-07-19",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": "./PDFs/10!23919%annsim52504!2021!9552126.pdf"
    },
    {
        "title": "Mission possible: Unify HPC and Big Data stacks towards application-defined blobs at the storage layer",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2018.07.035",
        "abstract": "AbstractHPC and Big Data stacks are completely separated today. The storage layer offers opportunities for convergence,as the challenges associated with HPC and Big Data storage are similar: trading versatility for performance. Thismotivates a global move towards dropping file-based, POSIX-IO compliance systems. However, on HPC platformsthis is made difficult by the centralized storage architecture using file-based storage. In this paper we advocate that thegrowing trend of equipping HPC compute nodes with local storage redistributes the cards by enabling object storageto be deployed alongside the application on the compute nodes. Such integration of application and storage not onlyallows fine-grained configuration of the storage system, but also improves application portability across platforms. Inaddition, the single-user nature of such application-specific storage obviates the need for resource-consuming storagefeatures like permissions or file hierarchies offered by traditional file systems. In this article we propose and evaluateBlobs (Binary Large Objects) as an alternative to distributed file systems. We factually demonstrate that it offersdrop-in compatibility with a variety of existing applications while improving storage throughput by up to 28%.1. IntroductionHPC and Big Data platforms are carving new datastorage models. This is made necessary by the ever-increasing scale of the computation and of the datasetsingested and produced by large-scale applications. Thesuccess of key-value stores [1, 2] or block storagesystems [3, 4] on Clouds, and the advent of burstbuffers [5, 6] or advanced I/O libraries [7, 8] for HPCclearly highlight this need.At the heart of these different methods is the movefrom legacy POSIX-compliant storage systems towardssimple storage paradigms designed especially for onepurpose, trading versatility for performance. Indeed,Email addresses: pmatri@fi.upm.es (Pierre Matri),alforov@dkrz.de (Yevhen Alforov), abrandon@fi.upm.es(Álvaro Brandon), mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),gabriel.antoniu@inria.fr (Gabriel Antoniu),michael.kuhn@informatik.uni-hamburg.de (Michael Kuhn),carns@mcs.anl.gov (Philip Carns), ludwig@dkrz.de(Thomas Ludwig)POSIX-IO imposes functionality such as hierarchicalnamespaces or file permissions. While these featuresare often provided for convenience, they are in practicerarely needed by modern applications and can signifi-cantly hinder the storage performance. Indeed, the li-braries and frameworks commonly used to access thestorage on HPC [9] and Big Data platforms [10, 11] pro-vide relaxed semantics (i.e., the set of rules and guaran-tees provided by the system regarding the behavior of itsstorage operations) compared to those of the underlyingfile system.Yet, deploying new storage models on HPC platformsused to be hard or simply impossible. Indeed, paral-lel file systems such as Lustre or GPFS on HPC havebeen the cornerstone of HPC storage for decades and arelikely to remain so in the next few years. This is largelyexplained by the high level of versatility and support for",
        "publication_date": "2018-07-26",
        "authors": "Pierre Matri, Yevhen Alforov, Álvaro Brandón, Marı́a S. Pérez, Alexandru Costan, Gabriel Antoniu, Michael Kühn, Philip Carns, Thomas Ludwig",
        "file_name": "10!1016%j!future!2018!07!035.pdf",
        "file_path": "./PDFs/10!1016%j!future!2018!07!035.pdf"
    },
    {
        "title": "DBtravel: A Tourism-Oriented Semantic Graph",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_19",
        "abstract": "Abstract. We present DBtravel, a tourism-oriented knowledge graphgenerated from the collaborative travel site Wikitravel. Our approachtakes advantage of the recommended guideline for contributors providedby Wikitravel and extracts the named entities available in WikitravelSpanish entries by using a NLP pipeline. Compared to a manually anno-tated gold standard, results show that our approach reaches values forprecision and recall around 80% for some sections of Wikitravel for theSpanish language.Keywords: DBpedia · Name entity recognition · Wikitravel1 IntroductionWikitravel is a web site, inspired by Wikipedia, in which users can contributecontent in a collaborative way in different languages, aimed at providing travelguides. In 21 languages and with 116 thousand entries for English, Wikitravelcontains high valuable and useful information for tourists in natural languagethat currently is only exploited by humans reading its entries. In order to helpuser to create entries, Wikitravel provides a guideline1 to recommend the entrysections and how to structure the information in each section. For example, aWikitravel entry of a city should have these sections: See, Eat and Drink(restaurants and bars), Sleep and Get Out with a specific format of the infor-mation in each section such as lists or relevant concepts in bold. This guidelinecan be considered as a template from which we can extract information, as DBpe-dia does with Wikipedia infoboxes [3]. Figure 1 shows the sections See and GetOut of a Wikitravel entry in Spanish, specifically Valladolid, a medium size citylocated in Spain.The main purpose of this work is to create a knowledge graph of tourism-oriented information by means of exploiting the information stored in Wikitravelentries. Compared to DBpedia [1], the generated graph (DBtravel) is focused ontourism-specific information. The information extraction process is driven by a1 https://wikitravel.org/en/Wikitravel:Manual of style.c© Springer Nature Switzerland AG 2018C. Pautasso et al. (Eds.): ICWE 2018, LNCS 11153, pp. 206–212, 2018.https://doi.org/10.1007/978-3-030-03056-8_19http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-03056-8_19&domain=pdfhttps://wikitravel.org/en/Wikitravel:Manual_of_stylehttps://doi.org/10.1007/978-3-030-03056-8_19DBtravel: A Tourism-Oriented Semantic Graph 207named entity recognition process over different sections of each Wikitravel entry,exploiting the structure information provided by the guideline. In this work wefocus on the Spanish version of Wikitravel, specifically on See and Get Outsections. Section See provides points of interest (POIs) to see in a city suchas monuments, buildings or locations. Section Get Out provides near cities ortowns near the city of the entry. Both sections can be used for holiday planning:POIs can be used to compute how many days a tourist should stay in a particularcity and near cities could help to recommend day-trip destinations a tourist couldfollow when they have extra days. The result of the experiments described in thispaper show that we can identify around 80% of the named entities in Wikitravel.The paper is structured as follows. Section 2 describes related work andSect. 3 proposes the method to exploit Wikitravel entries to extract structuredinformation. Section 4 discusses the obtained results and Sect. 5 presents someconclusions and highlights future work. For sake of reproductibility, all the",
        "publication_date": "2018-01-01",
        "authors": "Pablo Calleja, Freddy Priyatna, Nandana Mihindukulasooriya, Mariano Rico",
        "file_name": "978-3-030-03056-8_19.pdf",
        "file_path": "./PDFs/978-3-030-03056-8_19.pdf"
    },
    {
        "title": "A Flexible and Robust Deep Learning-Based System for Solar Irradiance Forecasting",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iipr/solar-irradiance",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%access!2021!3051839.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/iipr/solar-irradiance [24] Y."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2021.3051839",
        "abstract": "ABSTRACT Most studies about the solar forecasting topic do not analyze and exploit the temporal andspatial components that are inherent to such a task. Furthermore, they mostly focus just on precision and noton other meaningful features, such as flexibility and robustness. With the current energy production trends,where many solar panels are distributed across city rooftops, there is a need to manage all this informationsimultaneously and to be able to add and remove sensors as needed. Likewise, robust models need to beable to cope with (inevitable) sensor failure and continue producing reliable predictions. Due to all of this,solar forecasting models need to be as decoupled as possible from the number of data sources that feedthem and their geographical distribution, enabling also the reusability of the models. This article contributeswith a family of Deep Learning models for solar irradiance forecasting complying with the aforementionedfeatures, i.e. flexibility and robustness. In the first stage, several Artificial Neural Networks are trained as abasis for predicting solar irradiance on several locations at the same time. Thereupon, a family of models thatwork with irradiance maps thanks to Convolutional Long Short-TermMemory layers is presented, obtainingforecast skills between 7.4% and 41% (depending on the location and horizon) compared to the baseline.The latter family comes with flexibility and robustness features, which are required in large-scale IntelligentEnvironments, such as Smart Cities. Working with irradiance maps means that new sensors can be added(or removed) as needed, without requiring rebuilding the model. Experiments carried out show that sensorfailures have a mild impact on the prediction error for several forecast horizons.INDEX TERMS Convolutional long short-termmemory (Conv-LSTM), deep learning, irradiance map, solarirradiance, time series forecasting.I. INTRODUCTIONA. BACKGROUND AND MOTIVATIONDuring the past decade, the field of solar forecasting hasemerged due to the increasing energy demands by indus-try and research institutions in the context of the so-calledSustainable Development Goals (SDGs). Obtaining reliable,fine grain predictions is crucial for the development of manyrenewable energy systems. An example is the optimization ofthe layout and orientation of the solar panels of a Photovoltaic(PV) plant, which can have a big impact on the overall amountof energy produced during its lifetime. As the number of PVplants grows vastly and they are integrated into the electricThe associate editor coordinating the review of this manuscript andapproving it for publication was Frederico Guimarães .grid, anticipating the amount of energy produced is neededto avoid overloads and foresee energy shortage. Furthermore,the management of batteries to accumulate the producedenergy requires accurate forecasts.Several methods are being explored in the field to thisaim, which could be grouped as: Numerical Weather Predic-tion (NWP), image-based, statistical, and Machine Learning(ML). They could also be classified based on their character-istics, meaning if a method takes into account spatio-temporalfeatures, if it is deterministic or probabilistic, if it considersexogenous inputs (other inputs such as physical variables) orjust its data features, etc.The task of solar forecasting inherently presents manyangles of complexity. On the one hand, time granularitydirectly impacts on the variability of the measurements, in the12348 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 9, 2021https://orcid.org/0000-0003-4018-8725",
        "publication_date": "2021-01-01",
        "authors": "Ignacio-Iker Prado-Rujas, Antonio García-Dopico, Emilio Serrano, Marı́a S. Pérez",
        "file_name": "10!1109%access!2021!3051839.pdf",
        "file_path": "./PDFs/10!1109%access!2021!3051839.pdf"
    },
    {
        "title": "Semantic Software Metadata for Workflow Exploration and Evolution",
        "implementation_urls": [
            {
                "identifier": "http://doi.org/10.5281/zenodo.1414552",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1109/escience.2018.00132",
        "abstract": "Abstract Scientific workflow management systems play a major role in the design, execution and documentation of computational experiments. However, they have limited support for managing workflow evolution and exploration because theylack rich metadata for the software that implements workflow components. Such metadata could be used to support scientists in exploring local adjustments to a workflow, replacing components with similar software, or upgrading components upon release ofnewer software versions. To address this challenge, we propose OntoSoft-VFF (Ontology for Software Version, Function and Functionality), a software metadata repository designed to capture information about software and workflow components that is important for managing workflow exploration and evolution. Our approach uses a novel ontology to describe the functionality and evolution through time of any software used to create workflow components. OntoSoft-VFF is implemented as anonline catalog that stores semantic metadata for software to enableworkflow exploration through understanding of software functionality and evolution. The catalog also supports comparisonand semantic search of software metadata. We showcase OntoSoft-VFF using machine learning workflow examples. Wevalidate our approach by testing that a workflow system couldcompare differences in software metadata, explain softwareupdates and describe the general functionality of workflow steps.Keywords—scientific workflows; software metadata; software functions; software registries; workflow evolution.I. INTRODUCTIONWorkflow management systems [1] play a major role in supporting scientists to design, document and execute their computational experiments. During workflow design, scientists use third party software or their own code to implement workflow components. This paper investigates the issues that arise when such software evolves in terms of how a scientist’s workflow is affected.There are many reasons for scientists to modify a workflow that they created, either by changing specific steps of the workflow (also called workflow components) or changing the workflow structure. Changes in software used to implement components are common and could happen for different reasons, e.g., a newer version is available, older software is not maintained. Also, data sources change, e.g. when datasets are updated with new formats, which may require adjustments in existing components and adding new ones. Thus, due to changes in software and data, workflows must be updated accordingly to avoid workflow decay [2] and reproducibility issues [13]. Another important reason to update workflows iswhen scientists are exploring alternative ways of performing a computational experiment. During these exploratory tasks,scientists often want to compare methods or try different approaches to implement a workflow component.",
        "publication_date": "2018-10-01",
        "authors": "Lucas Carvalho, Daniel Garijo, Cláudia Bauzer Medeiros, Yolanda Gil",
        "file_name": "Semantic_Software_Metadata_for_Workflow_Exploration_and_Evolution.pdf",
        "file_path": "./PDFs/Semantic_Software_Metadata_for_Workflow_Exploration_and_Evolution.pdf"
    },
    {
        "title": "Accountable Clouds Through Blockchain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/cloud-chain",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%access!2023!3276240.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/miker83z/cloud-chain [39] E."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2023.3276240",
        "abstract": "ABSTRACT We present a solution for accountability in Cloud infrastructures based on blockchain.We showthat, through smart contracts, it is possible to create an unforgeable log that can be used for auditing andautomatic Service Level Agreement (SLA) verification. As a practical case study, we consider Cloud storageservices and define interaction protocols for registering the outcome of each file operation in the blockchain.We developed a prototype implementation that runs on the GoQuorum, Hyperledger Besu, and Polygonblockchains, using different consensus protocols. Using a dedicated testbed, we discuss the performanceof our implementation in terms of latencies, error rates and gas usage. Results demonstrate the viabilityof our approach over permissioned blockchains, with better performance for the Polygon and GoQuorumRaft decentralized systems. Our implementation enables interoperability, given that it is supported by theEthereum Virtual Machine which currently is underlying several blockchain platforms.INDEX TERMS Blockchain, smart contracts, cloud computing.I. INTRODUCTIONCloud computing is a well-established paradigm for provid-ing computation and storage resources according to a ‘‘payas you go’’ model. In Cloud computing, service providersown computing resources and provide remote access to thoseresources to customers for a fee [2].The level of abstraction at which a customer interactswith a Cloud infrastructure is defined by the servicemodel. In a Software as a Service (SaaS) Cloud, cus-tomers are provided with application services running in theCloud infrastructure. ‘‘Google Workspace’’ and ‘‘MicrosoftOffice Online’’ are examples of widely used SaaS Clouds.A Platform as a Service (PaaS) Cloud provides program-ming languages, tools, and a hosting environment for appli-cations developed by the customer. Examples of PaaS solu-tions are AppEngine by Google, Force.com from SalesForce,The associate editor coordinating the review of this manuscript andapproving it for publication was Nitin Gupta .Microsoft’s Azure, and Amazon’s Elastic Beanstalk. Finally,an Infrastructure as a Service (IaaS) Cloud provides low-levelcomputing capabilities such as processing, storage, and net-works where the customer can run arbitrary software, includ-ing operating systems and applications. Amazon EC2 is anexample of IaaS Cloud.The mode of operation of a Cloud defines its deploymentmodel. A Private Cloud is operated exclusively for a cus-tomer organization; it might be managed or owned by thatorganization, although this is not required. A CommunityCloud is shared by several organizations and supports aspecific community with common concerns (e.g., regulatoryrequirements). A Public Cloud is made available to the gen-eral public and is owned by an organization selling Cloudservices. Finally, a Hybrid Cloud is built upon a combinationof private, public, and community Clouds.Cloud computing allows separation between constructionand operation of the infrastructure and providing end-userservices. This opportunity enables the existence of at least48358This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",
        "publication_date": "2023-01-01",
        "authors": "Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti, Moreno Marzolla",
        "file_name": "10!1109%access!2023!3276240.pdf",
        "file_path": "./PDFs/10!1109%access!2023!3276240.pdf"
    },
    {
        "title": "Ontology-driven semantic unified modelling for concurrent activity recognition (OSCAR)",
        "implementation_urls": [],
        "doi": "10.1007/s11042-018-6318-5",
        "abstract": "Abstract Activity recognition has a vital role in smart home operations. One of the majorchallenges in object-sensor-based activity recognition is to learn the complete activity modelderived from a generic activity model for sequential and parallel activities. Such challengeexists due to erratic degrees of dissimilar activities in which inhabitants perform activities insequential and interleaved fashion while interacting with different objects. The proposed workfocuses on recognizing a complete set of actions (of activity) by exploiting different knowl-edge engineering techniques, ontology-based temporal formalisms and data driven techniques.Semantic Segmentation has been employed to establish the generic activity model. Thespurious semantic segmentation produced by sensor noise or erratic behaviour is removedby Allen’s temporal formalism. Moreover, Tversky’s feature-based similarity has been used toremove the highly similar spurious activities produced as a result of mistaken interactions withwrong home objects. The duration to perform activities varies among inhabitants; suchduration intervals are identified dynamically using the proposed model in order to have acomplete activity model. A comprehensive set of experiments has been carried out forevaluating the proposed model where the results based upon different metrics assert itseffectiveness especially when compared with other contemporary techniques.Keywords Complete activitymodel . Personalized activitymodel . Adaptive system . Domainactivity ontology . Concurrent activity recognitionMultimed Tools Appl (2019) 78:2073–2104https://doi.org/10.1007/s11042-018-6318-5Electronic supplementary material The online version of this article (https://doi.org/10.1007/s11042-018-6318-5) contains supplementary material, which is available to authorized users.* Muhammad Safyanm.safyan@seecs.edu.pk1 Iqra University Islamabad, Islamabad, Pakistan2 University of Gujarat, Gujarat, Pakistan3 Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid, Spain4 GC University, Lahore, Pakistanhttp://crossmark.crossref.org/dialog/?doi=10.1007/s11042-018-6318-5&domain=pdfhttps://doi.org/10.1007/s11042-018-6318-5https://doi.org/10.1007/s11042-018-6318-5mailto:m.safyan@seecs.edu.pk1 IntroductionThe internet of Things (IoT) has been a success since its inception [18]; the intention isto make human life comfortable in different ways such as smart homes [7, 18] and smartcities [5]. BActivity Recognition^ is an important aspect to build such facilities especiallyin smart homes. It transforms a home into a smart home for the inhabitants (focus of ourwork). Activity Recognition (AR) process, as illustrated in Fig. 1, can be comprehendedfrom three perspectives (i) Activity Monitoring (ii) Activity Modeling and (iii) ActivityComplexity. Each of the perspectives is elaborated as given in the following.Activity monitoring describes the process of capturing actions performed in pervasiveenvironments. Some of the prevalent AR techniques are: Vision-based AR: An image-based approach that involves monitoring of inhabitant’s actions, activities and theircomplete behaviour using surveillance cameras. Based on the major features of avision-based approach, it identifies the events by discovering areas of interest, observingmotion patterns and features like walking, hand waving or running [15, 17, 37]. On theother hand, these techniques compromise the inhabitant’s privacy, use higher bandwidthand add a cost of computational resources [41]. Sensor-based AR: In order to identify theevents taking place among inhabitants, sensor streams are received from sensors [8]. Thesensor-based AR techniques have two sub-categories: Wearable-sensor-based AR: Wear-",
        "publication_date": "2018-07-05",
        "authors": "Muhammad Safyan, Zia Ul Qayyum, Sohail Sarwar, Raúl García‐Castro, Mehtab Ahmed",
        "file_name": "s11042-018-6318-5.pdf",
        "file_path": "./PDFs/s11042-018-6318-5.pdf"
    },
    {
        "title": "Knowledge maps: An essential technique for conceptualisation",
        "implementation_urls": [],
        "doi": "10.1016/s0169-023x(99)00050-6",
        "publication_date": "2000-05-01",
        "authors": "A. Gómez, Ana M. Moreno, J. Pazos, Almudena Sierra-Alonso",
        "file_name": "10!1016%s0169-023x(99)00050-6.pdf",
        "file_path": "./PDFs/10!1016%s0169-023x(99)00050-6.pdf"
    },
    {
        "title": "Data Management Documentation in Citizen Science Projects: Bringing Formalisation and Transparency Together",
        "implementation_urls": [],
        "doi": "10.5334/cstp.538",
        "abstract": "ABSTRACTCitizen science (CS) is a way to open up the scientific process, to make it more accessible and inclusive, and to bring professional scientists and the public together in shared endeavours to advance knowledge. Many initiatives engage citizens in the collection or curation of data, but do not state what happens with such data. Making data open is increasingly common and compulsory in professional science. To conduct transparent, open science with citizens, citizens need to be able to understand what happens with the data they contribute. Data management documentation (DMD) can increase understanding of and trust in citizen science data, improve data quality and accessibility, and increase the reproducibility of experiments. However, such documentation is often designed for specialists rather than amateurs.This paper analyses the use of DMD in CS projects. We present analysis of a qualitative survey and assessment of projects’ DMD, and four vignettes of data management practices. Since most projects in our sample did not have DMD, we further analyse their reasons for not doing so. We discuss the benefits and challenges of different forms of DMD, and barriers to having it, which include a lack of resources, a lack of awareness of tools to support DMD development, and the inaccessibility of existing tools to citizen scientists without formal scientific education. We conclude that, to maximise the inclusivity of citizen science, tools and templates need to be made more accessible for non-experts in data management.mailto:gefion.thuermer@kcl.ac.ukhttps://doi.org/10.5334/cstp.538https://orcid.org/0000-0001-7345-0000https://orcid.org/0000-0003-4112-6825https://orcid.org/0000-0002-1044-3943https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-1722-947X2Thuermer et al. Citizen Science: Theory and Practice DOI: 10.5334/cstp.538Citizen science projects can help advance scientific knowledge, and educate participants about specific topics and the scientific process in general (Bonney et al. 2009). These projects occur at different scales, from local, such as the iSPEX project (http://ispex-eu.org), where citizen scientists use sensors to measure air quality (Volten et al. 2018), to international, such as eBird (https://ebird.org), an online platform used globally to record bird observations (Lagoze 2014). Citizens may create such projects from the bottom up, with or without the support of professional scientists; conduct data collection or analysis in scientist-led projects (Wiggins and Crowston 2011); or contribute to scientific publications (Tinati et al. 2015).The implementation of data management policies can make data and projects more scientifically sound, improve data quality and accessibility, and increase reproducibility. In CS projects, data management is an essential activity that enables citizen scientists to produce data that can be relevant and useful for, and trusted by, researchers (Hunter, Alabri and Ingen 2013). However, in many projects, data management policies or documentation are not systematically applied, leading to the perception that ",
        "publication_date": "2023-06-05",
        "authors": "Gefion Thuermer, Esteban González, Neal Reeves, Óscar Corcho, Elena Simperl",
        "file_name": "no_doi_20250624162944.pdf",
        "file_path": "./PDFs/no_doi_20250624162944.pdf",
        "pdf_link": "https://storage.googleapis.com/jnl-up-j-cstp-files/journals/1/articles/538/647dce1d15fad.pdf"
    },
    {
        "title": "OKG-Soft: An Open Knowledge Graph with Machine Readable Scientific Software Metadata",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mintproject/MINT-ModelCatalogIngestionAPI",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/OKG-Soft_An_Open_Knowledge_Graph_with_Machine_Readable_Scientific_Software_Metadata.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The latest version of the ontology is available and documented at https://w3id.org/okn/o/sd."
                    }
                ]
            }
        ],
        "doi": "10.1109/escience.2019.00046",
        "abstract": "Abstract—Scientific software is crucial for understanding, reusing and reproducing results in computational sciences. Software is often stored in code repositories, which may contain human readable instructions necessary to use it and set it up. However, a significant amount of time is usually required to understand how to invoke a software component, prepare data in the format it requires, and use it in combination with other software. In this paper we introduce OKG-Soft, an open knowledge graph that describes scientific software in a machine readable manner. OKG-Soft includes: 1) an ontology designed to describe software and the specific data formats it uses; 2) an approach to publish software metadata as an open knowledge graph, linked to other Web of Data objects; and 3) a framework to annotate, query, explore and curate scientific software metadata. OKG-Soft supports the FAIR principles of findability, accessibility, interoperability, and reuse for software. We demonstrate the benefits of OKG-Soft with two applications: a browser for understanding scientific models in the environmental and social sciences, and a portal to combine climate, hydrology, agriculture, and economic software models.  Keywords—software metadata, software registries, FAIR, knowledge graphs, software composition, software interoperability I. INTRODUCTION  Software is a key product of scientific research, as it can be used to understand and reproduce the findings reported in a publication (e.g., by rerunning a hydrology model, a genome sequence analysis or testing a trained machine learning model). The importance of software is increasingly recognized [1], with publishers and community initiatives encouraging researchers to make their software openly available to others.1 Scientific software created by scientists should be appropriately documented and curated to facilitate reuse by other researchers. Code repositories such as GitHub 2  or BitBucket3 provide the means to store and version code, while software container repositories such as DockerHub4 capture the execution environment required to run software. However, there is usually a lack of important information that makes software difficult to discover and reuse, such as descriptions of the main features of the software, unambiguous usage instructions, incomplete sample data, etc. Moreover, when                                                  1 https://paperswithcode.com 2 github.com/ 3 https://bitbucket.org/ this kind of information is present, it is not machine readable, so it is hard to develop tools to facilitate those tasks for users. A major barrier to reuse is the time and effort required to understand how to run scientific software. Researchers need to understand how to prepare data for software, how to invoke ",
        "publication_date": "2019-09-01",
        "authors": "Daniel Garijo, Maximiliano Osorio, Deborah Khider, Varun Ratnakar, Yolanda Gil",
        "file_name": "OKG-Soft_An_Open_Knowledge_Graph_with_Machine_Readable_Scientific_Software_Metadata.pdf",
        "file_path": "./PDFs/OKG-Soft_An_Open_Knowledge_Graph_with_Machine_Readable_Scientific_Software_Metadata.pdf"
    },
    {
        "title": "Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome",
        "implementation_urls": [],
        "doi": "10.1371/journal.pone.0080278",
        "abstract": "AbstractHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience orintuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify thisdifficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertiseto domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimatingthe time required to reproduce each of the steps in the method described in the original paper and make them part of anexplicit workflow that reproduces the original results. Reproducing the method took several months of effort, and requiredusing new versions and new software that posed challenges to reconstructing and validating the results. The quantificationleads to ‘‘reproducibility maps’’ that reveal that novice researchers would only be able to reproduce a few of the steps in themethod, and that only expert researchers with advance knowledge of the domain would be able to reproduce the methodin its entirety. The workflow itself is published as an online resource together with supporting software and data. The paperconcludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and adesiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducingthe work of others from published papers, but reproducing work from one’s own laboratory.Citation: Garijo D, Kinnings S, Xie L, Xie L, Zhang Y, et al. (2013) Quantifying Reproducibility in Computational Biology: The Case of the TuberculosisDrugome. PLoS ONE 8(11): e80278. doi:10.1371/journal.pone.0080278Editor: Christos A. Ouzounis, The Centre for Research and Technology, Hellas, GreeceReceived September 18, 2012; Accepted October 10, 2013; Published November 27, 2013Copyright: � 2013 Garijo et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permitsunrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Funding: This research is sponsored by Elsevier Labs, the National Science Foundation with award number -0 , the Air Force Office of ScientificResearch with award number FA9550-11-1-0104, internal funds from the University of Southern California’s Information Sciences Institute and from the Universityof California, San Diego, and by a Formacistudy design, data collection and analysis, decision to publish, or preparation of the manuscript.Competing Interests: The research presented here has been sponsored partly by Elsevier Labs. This does not alter the authors’ adherence to all the PLOS ONEpolicies on sharing data and materials.* E-mail: pbourne@ucsd.edu (PEB); gil@isi.edu (YG)IntroductionComputation is now an integral part of the biological scienceseither applied as a technique or as a science in its own right -bioinformatics. As a technique, software becomes an instrument toanalyze data and uncover new biological insights. By reading thepublished article describing these insights, another researcherhopes to understand what computations were carried out, replicatethe software apparatus originally used and reproduce theexperiment. This is rarely the case without significant effort, andsometimes impossible without asking the original authors. In short,reproducibility in computational biology is aspired to, but rarelyachieved. This is unfortunate since the quantitative nature of thescience makes reproducibility more obtainable than in cases whereexperiments are qualitative and hard to describe explicitly.An intriguing possibility where potential quantification exists isto extend articles through the inclusion of scientific workflows thatrepresent computations carried out to obtain the published results,thereby capturing data analysis methods explicitly [1]. This wouldmake scientific results more reproducible because articles wouldhave not only a textual description of the computational processdescribed in the article but also a workflow that, as acomputational artifact, could be analyzed and re-run automati-cally. Consequently, workflows can make scientists more produc-",
        "publication_date": "2013-11-27",
        "authors": "Daniel Garijo, Sarah Kinnings, Li Xie, Lei Xie, Yinliang Zhang, Philip E. Bourne, Yolanda Gil",
        "file_name": "10!1371%journal!pone!0080278.pdf",
        "file_path": "./PDFs/10!1371%journal!pone!0080278.pdf"
    },
    {
        "title": "Nine best practices for research software registries and repositories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/codemeta/codemeta",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!7717%peerj-cs!1023.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available at https://github.com/codemeta/codemeta."
                    }
                ]
            }
        ],
        "doi": "10.7717/peerj-cs.1023",
        "abstract": "ABSTRACTScientific software registries and repositories improve software findability andresearch transparency, provide information for software citations, and fosterpreservation of computational methods in a wide range of disciplines. Registries andrepositories play a critical role by supporting research reproducibility andreplicability, but developing them takes effort and few guidelines are available to helpprospective creators of these resources. To address this need, the FORCE11 SoftwareCitation Implementation Working Group convened a Task Force to distill theexperiences of the managers of existing resources in setting expectations for allstakeholders. In this article, we describe the resultant best practices which includedefining the scope, policies, and rules that govern individual registries andrepositories, along with the background, examples, and collaborative work that wentinto their development. We believe that establishing specific policies such as thosepresented here will help other scientific software registries and repositories betterserve their users and their disciplines.Subjects Computer Education, Databases, Digital LibrariesKeywords Best practices, Research software repository, Research software registry, Softwaremetadata, Repository policies, Research software registry guidelinesINTRODUCTIONResearch software is an essential constituent in scientific investigations (Wilson et al., 2014;Momcheva & Tollerud, 2015; Hettrick, 2018; Lamprecht et al., 2020), as it is often used totransform and prepare data, perform novel analyses on data, automate manual processes,and visualize results reported in scientific publications (Howison & Herbsleb, 2011).Research software is thus crucial for reproducibility and has been recognized by thescientific community as a research product in its own right—one that should be properlydescribed, accessible, and credited by others (Smith, Katz & Niemeyer, 2016; Chue Honget al., 2021). As a result of the increasing importance of computational methods,communities such as Research Data Alliance (RDA) (Berman & Crosas, 2020) (https://www.rd-alliance.org/) and FORCE11 (Bourne et al., 2012) (https://www.force11.org/)How to cite this article Garijo D, Ménager H, Hwang L, Trisovic A, Hucka M, Morrell T, Allen A. et al., 2022. Nine best practices forresearch software registries and repositories. PeerJ Comput. Sci. 8:e1023 DOI 10.7717/peerj-cs.1023Submitted 28 September 2021Accepted 9 June 2022Published 8 August 2022Corresponding authorDaniel Garijo, daniel.garijo@upm.esAcademic editorVarun GuptaAdditional Information andDeclarations can be found onpage 24DOI 10.7717/peerj-cs.1023Copyright2022 Garijo et al.Distributed underCreative Commons CC-BY 4.0https://github.com/force11/force11-sciwghttps://github.com/force11/force11-sciwghttps://www.rd-alliance.org/https://www.rd-alliance.org/",
        "publication_date": "2022-08-08",
        "authors": "Daniel Garijo, Hervé Ménager, Lorraine Hwang, A. Trisovic, Michael Hucka, Thomas E. Morrell, Alice Allen",
        "file_name": "10!7717%peerj-cs!1023.pdf",
        "file_path": "./PDFs/10!7717%peerj-cs!1023.pdf"
    },
    {
        "title": "Latest enhancements in the Spanish DBpedia.",
        "implementation_urls": [],
        "abstract": "Abstract. The Spanish DBpedia is a data source used initially to sup-port the Spanish community. However, our logs show that the Spanishlanguage goes beyond Spanish speakers and many non-Spanish speakersuse the Spanish DBpedia on a daily basis. In the last months we havemade two important enhancements to the Spanish DBpedia: (1) we pub-lish a nonstandard dataset containing the type of resources that in thestandard distribution have no type, and (2) we update automatically ourdata every week by using the DBpedia databus. In this way, we satisfya frequent request made by companies and we foster the usage of theSpanish language, the second mother language by the number of speak-ers (after Chinese), and the second in scientific papers (after English).Keywords: Spanish DBpedia · Resource type · DBpedia data bus.1 Introduction1.1 The rising of the Spanish languageThe data published by the Cervantes Institute in its 2020 report [4] are over-whelming: Spanish speakers have increased by 30% in the last decade, and thenumber of foreigners who study it has grown by 60%. More than 585 millionpeople speak Spanish. Of these, almost 489 million are native Spanish speakers.Furthermore, Spanish is the second mother tongue by number of speakers afterMandarin Chinese, and the third language in the global count of users after En-glish and Mandarin Chinese. On the Internet, it is the third most used and isthe second language, behind English, publishing scientific texts.The DBpedia project has long generated semantic information from EnglishWikipedia. Since June 2011, the information generation process has extractedinformation from Wikipedia in 111 of its languages, but only 18 languages have aDBpedia chapter with a website. One of them is Spanish. The DBpedia Interna-tionalization Committee has assigned a website and a SPARQL [7] endpoint for? Partially supported by HcommonK (RTC2019-007134-7) and Datos4.0 (TIN2013-46238-C4-3-R) projects.Copyright ©2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 S. Sanz-Lucio et al.each of these languages1. In the case of Spanish (with website es.dbpedia.org),the extraction process produces more than 100 million RDF triples from theSpanish Wikipedia. All these triples are available on the SPARQL endpointes.dbpedia.org/sparql using Semantic Web [2] and Linked Data [1] technolo-gies.1.2 The DBpedia datasetsAs we have mentioned previously, DBpedia extracts data from 111 differentlanguage editions of Wikipedia. Then, for each language we have a knowledgebase (a “Knowledge Graph” in modern terminology, abbreviated as KG). Thelargest DBpedia KG is extracted from the English edition of Wikipedia, witharound 400 million facts (triples) that describe 3.7 million resources (Wikipediaentries). The DBpedia knowledge graphs that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 millionadditional resources. Therefore, two-thirds of the information in DBpedia comesfrom non-English Wikipedias.From a technical perspective, the DBpedia project maps Wikipedia infoboxes [12]from 27 different language editions into the DBpedia ontology, a single shared on-tology consisting of 320 classes and 1,650 properties. The mappings are created",
        "publication_date": "2021-01-01",
        "authors": "Sara Sanz-Lucio, Oussama Tahiri-Alaoui, Mariano Rico",
        "file_name": "no_doi_20250624162951.pdf",
        "file_path": "./PDFs/no_doi_20250624162951.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2941/paper8.pdf"
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "publication_date": "2021-05-07",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": "./PDFs/10!3233%ssw51.pdf"
    },
    {
        "title": "Knowledge Base Evolution Analysis: A Case Study in the Tourism Domain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_26",
        "abstract": "Abstract. Stakeholders – curator, consumer, etc. – in the tourism do-main routinely need to combine and compare statistical indicators abouttourism. In this context, various Knowledge Bases (KBs) have beendesigned and developed in the Linked Open Data (LOD) cloud in or-der to support decision-making process in Tourism domain. Such KBsevolve over time: their data (instances) and schemes can be updated, ex-tended, revised and refactored. However, unlike in more controlled typesof knowledge bases, the evolution of KBs exposed in the LOD cloudis usually unrestrained, what may cause data to suffer from a varietyof issues. This paper attempts to address the impact of KB evolution intourism domain by showing how entity evolves over time using the 3cixtyKB. We show that using multiple versions of the KB through time canhelp to understand inconsistency in the data collection process.Keywords: Knowledge Base · Linked Data · Evolution Analysis.1 IntroductionIn the recent years much efforts have been given towards sharing KnowledgeBases (KBs) in the Linked Open Data (LOD) cloud4. Large KBs in the tourismdomain are often maintained by organizations that act as curators to ensuretheir quality [9]. These KBs naturally evolve due to several causes: (i) resourcerepresentations and links that are created, updated, and removed; (ii) the en-tire graph can change or disappear. In general, KBs in the tourism domain arehighly complex and dynamic in nature. Decision-makers often rely on forecast-ing models to predict future demand or on decision support systems to analyzeand compare the relevant stakeholders [9]. Whilst most datasets are published asopen data, the data publishers continuously try to improve the quality of theirdata by updating ontologies and data instances or removing obsolete ones. How-ever, unlike in more controlled types of knowledge bases, the evolution of KBsin the tourism domain may suffer from a variety of issues, both at a semantic(contradiction) and at a pragmatic level (ambiguity, inaccuracies). This situationclearly affects negatively data stakeholders such as consumers, curators.4 http://lod-cloud.net2 M. Rashid et al.Taking into consideration a KB, we believe that understanding this evolu-tion could help to define more suitable strategies for data sources integration,enrichment, and maintenance. One of the common tasks for KB evolution anal-ysis is to perform a detailed data analysis, with data profiling. Data profiling isusually defined as the process of examining data to collect statistics and providerelevant metadata [1]. Based on data profiling we can thoroughly examine andunderstand a KB, its structure, and its properties before usage.In this paper, we explored the impact of KB evolution in the tourism domainusing the 3cixty KB [10]. The core idea in this work is to use dynamic featuresfrom data profiling results for analyzing the evolution of KBs. The main con-tributions of this work are: (1) a fundamental overview about the topic of KBevolution analysis; and (2) the presentation of the 3cixty KB as a use case tounderstand the impact of KB resource evolution. Furthermore, we used two en-tity types to explore the stability characteristics to identify any inconsistencypresent in the data extraction process. In this context, we created a set of APIs5for periodic snapshots generation and maintaining scheduled tasks for automaticand timely checks. We explored KB evolution analysis with lode:Event6 anddul:Places7 entity-type in the 3cixty KB, reporting the benefits of KB evolution",
        "publication_date": "2018-01-01",
        "authors": "Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Marco Torchiano, Nandana Mihindukulasooriya, Óscar Corcho",
        "file_name": "Rashid_Rizzo-TKG.pdf",
        "file_path": "./PDFs/Rashid_Rizzo-TKG.pdf"
    },
    {
        "title": "A framework for the broad dissemination of hydrological models for non-expert users",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mintproject/mic",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/MINT_MIC_Paper_Submitted.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "URL: https://doi.org/10.101775 6/j.gloplacha.2013.02.004, doi:10.1016/j.gloplacha.2013.02.004.776 USArmyCorps of Engineers, 2000."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.envsoft.2023.105695",
        "abstract": "12 ABSTRACT1314Hydrological models are essential in water resources management, but the expertise required to15operate them often exceeds that of potential stakeholders. We present an approach that facili-16tates the dissemination of hydrological models, and its implementation in the Model INTegra-17tion (MINT) framework. Our approach follows principles from software engineering to create18software components that reveal only selected functionality of models which is of interest to19users while abstracting from implementation complexity, and to generate metadata for the model20components. This methodology makes the models more findable, accessible, interoperable, and21reusable in support of FAIR principles. We showcase ourmethodology and its implementation in22MINT using two case studies. We illustrate how the models SWAT and MODFLOW are turned23into software components by hydrology experts, and how users without hydrology expertise can24find, adapt, and execute them. The two models differ in terms of represented processes and in25model design and structure. Our approach also benefits expert modelers, by simplifying model26sharing and the execution of model ensembles. MINT is a general modeling framework that uses27artificial intelligence techniques to assist users, and is released as open-source software.2829ORCID(s):First Author et al.: Preprint submitted to Elsevier Page 1 of 37Short Title of the ArticleHighlights30• An approach that facilitates hydrological model dissemination from expert modelers to non-experts31• Software engineering methods are proposed to simplify model complexity by creating software components32• Non-experts can easily modify selected parameters and execute models provided by experts33• Our approach makes models more findable, accessible, interoperable, and reusable in support of FAIR principles34• Various applications benefited from this approach within the MINT framework35First Author et al.: Preprint submitted to Elsevier Page 2 of 37Short Title of the Article1. Introduction36Hydrological models (HMs) are commonly used for water resources management and are mainly developed and37used by expert researchers or engineers working in the water sector. The results of HMs are important and considered38in decision-making processes of government agencies (Ruiz-Ortiz et al., 2019; Andreu et al., 1996). HM applications39include estimation of water availability (Döll et al., 2003), development of water management strategies (Haasnoot40et al., 2011), flood risk assessment (Merz et al., 2010), climate impact analysis (Krysanova and Hattermann, 2017;41Lobanova et al., 2018; Hattermann et al., 2018), solute transport (Konikow, 2010; Morales et al., 2010) and spatial42characterization of hydrological system variables such as soil water content (Brocca et al., 2017), desalination and43industrial wastewater treatment (Panagopoulos, 2022) as well as groundwater heads (Reinecke et al., 2019). HMs44vary widely in terms of their mathematical description of prevalent hydrological processes and their spatial model45structure, ranging from lumped conceptual models (Bittner et al., 2018; Booij and Krol, 2010) to distributed physical46models (Brunner and Simmons, 2012; Newman et al., 2017).47A fundamental understanding of hydrological processes is needed in order to reasonably set up a hydrologicalmodel48for a new region or modeling problem. This may become an obstacle for the use of HMs by decision-makers and other49users (Lüke and Hack, 2018). In practice, model results are presented to decision-makers as a summary focusing only50on a few specific variables of interest, such as streamflow or groundwater heads. The interests and requirements of51decision-makers and various stakeholders can diverge widely from what may be hydrologically interesting. Decision52makers in water resources management are usually interested in the assessment of the water balance, primarily the53availability of water in space and time. HMs allow a holistic view on the components of the water cycle, from which54insightful information, e.g. limiting factors in space and/or time, can be derived. These variables do not necessarily be55restricted towater availability, but could also refer to evapotranspiration, soil water or precipitation. Miscommunication56between science and non-expert groups is therefore not a rarity (Timmerman and Langaas, 2005). This increases the57",
        "publication_date": "2023-04-10",
        "authors": "Timo Schaffhauser, Daniel Garijo, Maximiliano Osorio, Daniel Bittner, Suzanne A. Pierce, Hernán Vargas, Markus Disse, Yolanda Gil",
        "file_name": "MINT_MIC_Paper_Submitted.pdf",
        "file_path": "./PDFs/MINT_MIC_Paper_Submitted.pdf"
    },
    {
        "title": "VICINITY: IoT Semantic Interoperability Based on the Web of Things",
        "implementation_urls": [],
        "doi": "10.1109/dcoss.2019.00061",
        "abstract": "Abstract—Internet of Things ecosystems have been developedunder different standards and semantics leading to sparseislands of information. The lack of consensus regarding bothstandards and semantics hinders the interoperability amongInternet of Things ecosystems, preventing the exploitation ofthe huge potential expected by integrating such ecosystems.In this paper we present the H2020 project VICINITY, adecentralized bottom-up standards-based platform to integrateInternet of Things ecosystems avoiding the tedious task ofadapting their semantics. VICINITY offers transparent inter-operability among such environments as a service in the cloud.In addition, dynamic discovery of new ecosystems is includedin VICINITY. We aim at implementing VICINITY in severalreal-world pilot scenarios in order to validate our approach.Keywords-Internet of Things, Web of Things, ecosystemsinteroperabilityI. INTRODUCTIONNowadays, Internet of Things (IoT) ecosystems havebecome pervasive in the Web [1]. These infrastructuresprovide access to data from several physical or virtualdevices [2]. IoT ecosystems bring suitable data for end-users, business models, and software agents. Unfortunately,these ecosystems are islands of information from differentdomains that are developed under different standards. Inaddition, there is no consensus among which standard shouldbe used when providing access to a new IoT ecosystem.The Web of Things approach aims at gathering thesesparse islands to bring transparent interoperability amongIoT ecosystems and the possibility of discovering newecosystems as well. Web of Things approach handles IoTecosystems as set of things that are described by means of aThing Description to establish how it should be used (access)and how interact with such thing (semantics).Unfortunately, the lack of consensus in IoT standards hin-ders the interoperability among IoT ecosystems and presentsseveral challenges [3]:• IoT ecosystems are built under different, often propri-etary, non-common standards.• IoT vendors or systems integrators may be reluctantto share the interface specifications due to intellectualproperty.• Large-scale integration imposes rules that are disadvan-tageous for particular participants.• Adapting existing ecosystems towards new standardsrequires significant change of management efforts re-garding IoT users and operators.• Information exchange entails serious privacy issues.The H2020 project VICINITY implements an open virtualneighbourhood to interconnect IoT ecosystems and smartobjects providing transparent interoperability based on the",
        "publication_date": "2019-05-01",
        "authors": "Andrea Cimmino, Viktor Oravec, Fernando Serena, Peter Kostelnik, María Poveda‐Villalón, Athanasios Tryferidis, Raúl García‐Castro, Stefan Vanya, Dimitrios Tzovaras, Christoph Grimm",
        "file_name": "VICINITY_IoT_Semantic_Interoperability_Based_on_the_Web_of_Things.pdf",
        "file_path": "./PDFs/VICINITY_IoT_Semantic_Interoperability_Based_on_the_Web_of_Things.pdf"
    },
    {
        "title": "RDF shape induction using knowledge base profiling",
        "implementation_urls": [
            {
                "identifier": "https://github.com/rifat963/RDFShapeInduction",
                "type": "git",
                "paper_frequency": 10,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Mihindukulasooriya_Rizzo-SAC2018.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The datasets can be accessed online6."
                    }
                ]
            }
        ],
        "doi": "10.1145/3167132.3167341",
        "abstract": "ABSTRACTKnowledge Graphs (KGs) are becoming the core of most artificialintelligent and cognitive applications. Popular KGs such as DBpediaand Wikidata have chosen the RDF data model to represent theirdata. Despite the advantages, there are challenges in using RDFdata, for example, data validation. Ontologies for specifying domainconceptualizations in RDF data are designed for entailments ratherthan validation. Most ontologies lack the granular informationneeded for validating constraints. Recent work on RDF Shapes andstandardization of languages such as SHACL and ShEX providebetter mechanisms for representing integrity constraints for RDFdata. However, manually creating constraints for large KGs is stilla tedious task. In this paper, we present a data driven approach forinducing integrity constraints for RDF data using data profiling.Those constraints can be combined into RDF Shapes and can be usedto validate RDF graphs. Our method is based on machine learningtechniques to automatically generate RDF shapes using profiledRDF data as features. In the experiments, the proposed approachachieved 97% precision in deriving RDF Shapes with cardinalityconstraints for a subset of DBpedia data.CCS CONCEPTS• Information systems→Data cleaning; •Computingmethod-ologies → Knowledge representation and reasoning;KEYWORDSRDF Shape, Knowledge Base, Data Quality, Machine Learning1 INTRODUCTIONKnowledge Graphs (KGs) are becoming the core of most artificialintelligent applications driven by domain knowledge. Popular KGssuch as DBpedia, YAGO2, and Wikidata have chosen the RDF datamodel for knowledge representation. RDF is a graph-based datamodel which is the de facto model in Semantic Web and LinkedData applications. RDF graphs can capture and represent domaininformation in a semantically rich manner using ontologies. Aswith any other data model, in order to specify the conditions thatmust be satisfied by an RDF graph, constraints need to be imposed.In practical settings, constraints are commonly used for three maintasks: (a) specifying the properties that data should hold; (b) handlecontradictions within data or with respect to the domain underconsideration; or (c) as a help for query optimization.Ontology languages are designed for inferring new knowledgeusing known axioms rather than for validating data based on ax-ioms. A reasoner and a validator have different functions, i.e., areasoner is used for inferring new knowledge while a validator isused for finding violations against a set of constraints. The under-pinning principles used in OWL, such as the use of Open WorldAssumption (OWA) and Non-Unique Name Assumption, can lead tounexpected results in a validator. Thus, it is challenging to performcertain validation tasks using such languages.Validation against a pre-defined schema is one of the key stepsin most conventional data storage and publishing paradigms; for",
        "publication_date": "2018-04-09",
        "authors": "Nandana Mihindukulasooriya, Mohammad Rifat Ahmmad Rashid, Giuseppe Rizzo, Raúl García‐Castro, Óscar Corcho, Marco Torchiano",
        "file_name": "Mihindukulasooriya_Rizzo-SAC2018.pdf",
        "file_path": "./PDFs/Mihindukulasooriya_Rizzo-SAC2018.pdf"
    },
    {
        "title": "Cross-Evaluation of Automated Term Extraction Tools by Measuring Terminological Saturation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ontologylearning-oeg/epnoi-legacy",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/kosa2018crossevaluation.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3 The batch service of NaCTeM TerMine is available at http://www.nactem.ac.uk/batch.php."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-76168-8_7",
        "abstract": "Abstract. This paper reports on cross-evaluating the two software tools forautomated term extraction (ATE) from English texts: NaCTeM TerMine andUPM Term Extractor. The objective was to find the most fitting software forextracting the bags of terms to be the part of our instrumental pipeline forexploring terminological saturation in text document collections in a domain ofinterest. The choice of these particular tools from the bunch of the otheravailable is explained in our review of the related work in ATE. The approach tomeasure terminological saturation is based on the use of the THD algorithmdeveloped in frame of our OntoElect methodology for ontology refinement. Thepaper presents the suite of instrumental software modules, experimental work-flow, 2 synthetic and 3 real document collections, generated datasets, and set-upof our experiments. Next, the results of the cross-evaluation experiments arepresented, analyzed, and discussed. Finally the paper offers some conclusionsand recommendations on the use of ATE software for measuring terminologicalsaturation in retrospective text document collections.Keywords: Automated term extraction � Software toolExperimental Cross-Evaluation � Terminological saturationRetrospective document collection � OntoElect1 IntroductionAutomated term extraction (ATE, also known as recognition – ATR) from textualdocuments is an established sub-field in text mining. Its results are further used fordifferent important purposes, for example as inputs in ontology learning. Many© Springer International Publishing AG, part of Springer Nature 2018N. Bassiliades et al. (Eds.): ICTERI 2017, CCIS 826, pp. 135–163, 2018.https://doi.org/10.1007/978-3-319-76168-8_7vadim@ermolayev.comhttp://orcid.org/0000-0002-7300-8818http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0001-5678-4439http://orcid.org/0000-0003-4523-6933http://orcid.org/0000-0002-2753-9917http://orcid.org/0000-0002-5159-254Xhttp://orcid.org/0000-0002-4925-9131http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-76168-8_7&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-76168-8_7&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-76168-8_7&amp;domain=pdfresearch activities are undertaken currently to improve the quality of extraction results.These activities focus on different aspects, including: new or improved extractionalgorithms; combining linguistic and statistical approaches to extraction; developingnew or refined metrics which allow higher quality extraction; developing newextraction tools which yield better results and scale to fit current dataset size require-ments. The mainstream criteria used to assess the quality of extracted results areadopted from information retrieval and based on recall and precision metrics. However,to the best of our knowledge, there were no reports on approaches to assess thecompleteness of the document collection from which extraction is performed. Recallmeasures just inform about how completely the set of terms was extracted from theavailable data but does not hint if the data itself was complete to contain all significantterms characterizing the domain. In other words, there is no way so far to check if thecollection of documents chosen for term extraction is representative. Therefore theapproaches to measure the representativeness of document collections are timely. In",
        "publication_date": "2018-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Dmitriy Naumenko, Eugene Yuschenko, Carlos Badenes-Olmedo, Vadim Ermolayev, Aliaksandr Birukou",
        "file_name": "kosa2018crossevaluation.pdf",
        "file_path": "./PDFs/kosa2018crossevaluation.pdf"
    },
    {
        "title": "Ontology verification testing using lexico-syntactic patterns",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kierendavies/tddonto2",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S0020025521009324-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "5 The last version of TDDOnto is available at the following URL: https://github.com/kierendavies/tddonto2."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.ins.2021.09.011",
        "abstract": "keyword-driven testing [33], which aims to express each test as abstractly as possible, but still precise enough to be executedand interpreted by a test execution tool. Another example is behaviour-driven development [24], which proposes a commonlanguage for specifying system behaviours. Inspired by them, this method describes a testing language to design tests. Sincethe LSPs introduced in Section 3.1 determine the types of requirements and how they should be implemented in an ontology,the testing language is grounded on them.To that end, representative keywords have been extracted from each LSP to define the syntax of the test designs. Thesekeywords are based on the ontology axioms (e.g., rdfs:subClassOf) and on the different types of LSP [14] (e.g., hierarchiesbetween terms). Therefore, these keywords indicate the goal in the ontology for each LSP, such as to define a subsumptionrelation or a cardinality restriction, and are used to propose a catalogue of test expressions that are written following theOWL Manchester syntax.9 Therefore, each test refers to a particular type of requirement and includes a set of keywords,e.g., the tests related to subsumption relations between classes includes the keyword subClassOf. Since these tests are writtenusing keywords, they can be automatically analysed and implemented into queries or axioms to be executed on the ontology,7 http://coralcorpus.linkeddata.es.8 https://doi.org/10.5281/zenodo.1967306.9 https://www.w3.org/TR/owl2-manchester-syntax.95http://coralcorpus.linkeddata.eshttps://doi.org/10.5281/zenodo.1967306https://www.w3.org/TR/owl2-manchester-syntaxTable 3List of lexico-syntactic patterns included in the CORAL dictionary [14].LSP Type of correspondence1 to 1 correspondenceLSP-SC-EN LSP for subclassOf relation ODPLSP-MI-EN LSP for multiple inheritance ODPLSP-EQ-EN LSP for equivalence relation ODPLSP-OP-EN LSP for object property ODPLSP-DP-EN LSP for datatype property ODPLSP-Di-EN LSP for disjoint classes ODPLSP-SV-EN LSP for specified values ODPLSP-PA-EN LSP for participation ODPLSP-PCP-EN LSP for co-participation ODPLSP-LO-EN LSP for location ODPLSP-OR-EN LSP for object-role ODPLSP-DC-SC-EN LSP for defined classes and subclassLSP- SC-Di-EN LSP for subclass relation, disjoint classes and exhaustive classesLSP-OP-UR-EN LSP for object property and universal restrictionLSP-CD-EN LSP for class definitionLSP-Min-and-OP-EN LSP for object property minimum cardinality and object propertyLSP-OP-Min-EN LSP for object property minimum cardinality related to an object propertyLSP-OP-Max-EN LSP for object property maximum cardinality related to an object propertyLSP-OP-Exact-EN LSP for object property exact cardinality related to an object propertyLSP-SYM-EN LSP for symmetryLSP-U-EN LSP for unionLSP-INTER-EN LSP for intersectionLSP-COMPL-EN LSP for complement1 to N correspondencesLSP-SC-PW-EN LSP for subclass or simple part-whole relationLSP-OP-DP-PW-EN LSP for object property or datatype property or simple part-whole relationLSP-PW-CONS-COM-CE-EN LSP for simple part-whole relation or constituency or componency or collection-entity",
        "publication_date": "2021-09-08",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "1-s2.0-S0020025521009324-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0020025521009324-main.pdf"
    },
    {
        "title": "Challenges in the Implementation of Privacy Enhancing Semantic Technologies (PESTs) Supporting GDPR",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dapreco/daprecokb",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-030-89811-3_20.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This knowledge base currently contains 271 obligations, 76 permissions, and 619 constitutive rules and is available online8."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-89811-3_20",
        "abstract": "Abstract. The EU General Data Protection Regulation (GDPR)imposes different requirements for data controllers collecting personaldata to protect individuals’ privacy. This fact triggered many studies andprojects to investigate Privacy Enhancing Technologies (PETs) for thefulfillment of the compliance requirements. In this paper, after reviewingsome of the current challenges and gaps in GDPR compliance, we arguethe use of Semantic Technologies in PETs in the form of an IntelligentCompliance Agent (ICA) to support data controllers in carrying outa Data Protection Impact Assessment (DPIA). Models and ontologiesrepresenting entities involved in the DPIA process can help data con-trollers determine the risk of their processing activities. Additionally, aninference engine, equipped with a knowledge base of DPIA-related obli-gations, can effectively assist data controllers in taking specific actionswhen a legal fact is triggered based on met conditions.Keywords: Compliance · Semantic web · Rule-based reasoning · DataProtection Impact Assessment · Privacy Enhancing Technologies1 IntroductionThe General Data Protection Regulation (GDPR)1 went into force on May 25,2018, with the primary aim of protecting personal data of all European Union cit-izens, wherever their data is targeted by different organisations. GDPR imposesvarious obligations on the companies; data controllers. Despite the fact thatthese obligations cover a broad range of requirements, the GDPR’s key princi-ples can be summarized as follows: lawfulness, fairness and transparency, purposeand storage limitation, data minimization, accuracy, accountability, and finally,integrity and confidentiality (security). Many principles of GDPR are similarto those in the previous regulations, such as the Data Protection Act 1998 (the1998 Act) [3]. However, there are a few main changes and novelties. For instance,the principles of individual rights (GDPR, Chapter III), international transferof personal data (GDPR, Chapter V), accountability, and Data Protection by1 https://eur-lex.europa.eu/eli/reg/2016/679/oj.c© Springer Nature Switzerland AG 2021V. Rodŕıguez-Doncel et al. (Eds.): AICOL-XI 2018/AICOL-XII 2020/XAILA 2020, LNAI 13048, pp. 283–297, 2021.https://doi.org/10.1007/978-3-030-89811-3_20http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-89811-3_20&domain=pdfhttp://orcid.org/0000-0002-4773-7961https://eur-lex.europa.eu/eli/reg/2016/679/ojhttps://doi.org/10.1007/978-3-030-89811-3_20284 R. SanieiDesign and Default have not been set out before. Also, for the first time, theGDPR imposes a legal duty on controllers to perform a Data Protection ImpactAssessment (DPIA).In the event of a breach, severe financial penalties are imposed to ensurethat the cost of compliance will be less than the cost of the violation. However,this is not the only explanation why businesses should comply with GDPR.After several erupted data privacy scandals, in the center of them the 2018Cambridge Analytica data breach, which affected personal data of more than87 million Facebook users, the general public is becoming more aware of thevast data-driven economies and more concerned about the protection of theirpersonal data collected by major tech companies. As a result, privacy maturityin companies is becoming a way of building trust with customers. Findings from",
        "publication_date": "2021-01-01",
        "authors": "Rana Saniei",
        "file_name": "978-3-030-89811-3_20.pdf",
        "file_path": "./PDFs/978-3-030-89811-3_20.pdf"
    },
    {
        "title": "TempCourt: evaluation of temporal taggers on a new corpus of court decisions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/HeidelTime/heideltime",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/CorpusofCourtDecisions.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The Python code of USFD2 is available online45, but it must be noted that it is developed for the evaluation of specific datasets, so it must be slightly modified for custom use."
                    }
                ]
            }
        ],
        "doi": "10.1017/s0269888919000195",
        "abstract": "AbstractThe extraction and processing of temporal expressions in textual documents has been extensively studiedin several domains, however for the legal domain it remains an open challenge. This is possibly due tothe scarcity of corpora in the domain and the particularities found in legal documents that are highlightedin this paper. Considering the pivotal role played by temporal information when it comes to analyzinglegal cases, this paper presents TempCourt, a corpus of 30 legal documents from the European Courtof Human Rights, the European Court of Justice and the United States Supreme Court with manuallyannotated temporal expressions. The corpus contains two different temporal annotation sets that adhere tothe TimeML standard, the first one capturing all temporal expressions and the second dedicated to temporalexpressions that are relevant for the case under judgment (thus excluding dates of previous court decisions).The proposed gold standards are subsequently used to compare ten state-of-the-art cross-domain temporaltaggers, and to identify not only the limitations of cross-domain temporal taggers but also limitations ofthe TimeML standard when applied to legal documents. Finally, the paper identifies the need for dedicatedresources and the adaptation of existing tools, and specific annotation guidelines that can be adapted todifferent types of legal documents.Keywords: legal corpus, temporal annotation, case law, legal NLP, evaluation1 IntroductionLegal information systems are indispensable tools for many legal practitioners. An emerging area ofresearch is the use of text analytics to derive structured data from legal text (e.g. norms, opinions,recommendations or court decisions). In this context, one of the most relevant activities is the automaticextraction and processing of events and temporal expressions with a view to creating timelines.In this context, a temporal expression (TE) is a word or sequence of words making reference to a timeinstant (e.g. ‘seven o’clock’) or a time interval (e.g. ‘from seven to ten’). Temporal expressions frameevents or happenings implicitly or explicitly mentioned in the document. Temporal relations bind TEs toevents and determine the relative position of some events with respect to other events (through relationssuch as ‘after’ or ‘before’).The example below is a text excerpt from a court decision of the European Court of Human Rightsdescribing the facts of the Aras v. Turkey case (no. 21824/07, 20 July 2017). The text contains three TEs1The two first authors equally contributed to this work.2 NAVAS-LORO ET AL.(in bold below), two of them being in an absolute form (e.g. 11 December 2002) and one in a relative form(same day).”On 11 December 2002 the applicant’s statement was taken by the public prosecutor and, onthe same day, the judge at Istanbul State Security Court ordered her detention on remand.On 7 December 2002 the applicant was arrested on suspicion of membership of a terroristorganisation.”This temporal information is related to three events, namely, the public prosecutor taking the statement,the judge ordering a detention, and the applicant being arrested. Each of the events is related to the otherentities, either named (Istanbul State Security Court) or not (the applicant). Although the two absolute datesin the text above appear in the same format, this is not always the case and very often different formatsare used even within the same document. Although our exemplary legal case can be used to motivate aninvestigation into both temporal and event extraction (e.g. (39, 46)), in this paper we focus specifically ontemporal expressions.Temporal taggers operate on texts like the one above, performing different tasks, namely TE identifica-tion, normalization, and classification. Identification (also called detection or extraction) is a task whichinvolves finding TEs and their start and end position in the text. Normalization (or anchoring) is a taskthat interprets TEs to obtain specific instants and intervals represented in a standard format. This taskresolves relative TEs (as ‘the same day’) from context information, localizes time formats (i.e. mm/dd/yyvs dd/mm/yy), considers timezones and enables the reformatting of the TEs into a standard format (e.g.ISO 86012). In contrast, a classification task is used to determine which kind of TEs have been found. For",
        "publication_date": "2019-01-01",
        "authors": "María Navas-Loro, Erwin Filtz, Victor Rodrı́guez-Doncel, Axel Polleres, Sabrina Kirrane",
        "file_name": "CorpusofCourtDecisions.pdf",
        "file_path": "./PDFs/CorpusofCourtDecisions.pdf"
    },
    {
        "title": "Smart Waste Collection System with Low Consumption LoRaWAN Nodes and Route Optimization",
        "implementation_urls": [],
        "doi": "10.3390/s18051465",
        "abstract": "Abstract: New solutions for managing waste have emerged due to the rise of Smart Cities and theInternet of Things. These solutions can also be applied in rural environments, but they require thedeployment of a low cost and low consumption sensor network which can be used by differentapplications. Wireless technologies such as LoRa and low consumption microcontrollers, such as theSAM L21 family make the implementation and deployment of this kind of sensor network possible.This paper introduces a waste monitoring and management platform used in rural environments.A prototype of a low consumption wireless node is developed to obtain measurements of theweight, filling volume and temperature of a waste container. This monitoring allows the progressivefilling data of every town container to be gathered and analysed as well as creating alerts in caseof incidence. The platform features a module for optimising waste collection routes. This moduledynamically generates routes from data obtained through the deployed nodes to save energy, time andconsequently, costs. It also features a mobile application for the collection fleet which guides everydriver through the best route—previously calculated for each journey. This paper presents a casestudy performed in the region of Salamanca to evaluate the efficiency and the viability of the system’simplementation. Data used for this case study come from open data sources, the report of the Castillay León waste management plan and data from public tender procedures in the region of Salamanca.The results of the case study show a developed node with a great lifetime of operation, a largecoverage with small deployment of antennas in the region, and a route optimization system whichuses weight and volume measured by the node, and provides savings in cost, time and workforcecompared to a static collection route approach.Keywords: smart waste collection; low powered sensors; wireless sensor networks; data analytics;decision-making; LoRaWAN; open data; VRP; CVRP1. IntroductionNowadays, more and more cities are implementing new systems based on the Internet of Things(IoT) to obtain new data about the city, offer new services and optimize the energetic efficiency.These cities use a Smart Cities model [1] which aims to achieve more sustainable cities and makecities better places to live in. Applications developed for Smart Cities include applications for citizensecurity and control of people flow in cities [2], vehicle parking [3], getting information about accessibleplaces [4], managing energy of houses [5] and public lighting [6] together with the use of smart grids,water management, waste management, health services, logistics, and a long list of other domains.Sensors 2018, 18, 1465; doi:10.3390/s18051465 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-0493-4471https://orcid.org/0000-0002-0020-827Xhttps://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://www.mdpi.com/1424-8220/18/5/1465?type=check_update&version=1http://dx.doi.org/10.3390/s18051465http://www.mdpi.com/journal/sensorsEa sensors (mort,ArticleSmart Waste Collection System with LowConsumption LoORaWAN Nodes andRoute OptimizationAlvaro Lozano '*, Javier Caridad 1“, Juan Francisco De Paz 1,Gabriel Villarrubia Gonzalez 1“ and Javier Bajo ”1 Faculty of Science, University of Salamanca, Plaza de la Merced s/n, 37002 Salamanca, Spain;jch@usal.es (J.C.); fcofds@usal.es (J.F.D.P.); gvg@usal.es (G.V.G.)Department of Artificial Intelligence, Polytechnic University of Madrid, Campus Montegancedo s/n,",
        "publication_date": "2018-05-08",
        "authors": "Álvaro Lozano Murciego, Javier Caridad, Juan F. De Paz, Gabriel Villarrubia González, Javier Bajo",
        "file_name": "no_doi_20250624163006.pdf",
        "file_path": "./PDFs/no_doi_20250624163006.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/5/1465/pdf"
    },
    {
        "title": "The RML Ontology: A Community-Driven Modular Redesign After a Decade of Experience in Mapping Heterogeneous Data to RDF",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OP-TED/ePO",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-031-47243-5_9.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "We publish the complete ontol-ogy at http://w3id.org/rml/, and a summary of all modules with links to all their related resources (i.e."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_9",
        "abstract": "Abstract. The Relational to RDF Mapping Language (R2RML)became a W3C Recommendation a decade ago. Despite its wide adop-tion, its potential applicability beyond relational databases was swiftlyexplored. As a result, several extensions and new mapping languages wereproposed to tackle the limitations that surfaced as R2RML was appliedin real-world use cases. Over the years, one of these languages, the RDFMapping Language (RML), has gathered a large community of contribu-tors, users, and compliant tools. So far, there has been no well-defined setof features for the mapping language, nor was there a consensus-markingontology. Consequently, it has become challenging for non-experts tofully comprehend and utilize the full range of the language’s capabilities.After three years of work, the W3C Community Group on KnowledgeGraph Construction proposes a new specification for RML. This paperpresents the new modular RML ontology and the accompanying SHACLshapes that complement the specification. We discuss the motivationsand challenges that emerged when extending R2RML, the methodologywe followed to design the new ontology while ensuring its backward com-patibility with R2RML, and the novel features which increase its expres-siveness. The new ontology consolidates the potential of RML, empowerspractitioners to define mapping rules for constructing RDF graphs thatwere previously unattainable, and allows developers to implement sys-tems in adherence with [R2]RML.c© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14266, pp. 152–175, 2023.https://doi.org/10.1007/978-3-031-47243-5_9http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47243-5_9&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-7195-9935http://orcid.org/0000-0002-3029-6469http://orcid.org/0000-0003-0248-0987http://orcid.org/0000-0003-4734-3847http://orcid.org/0000-0003-1702-8707http://orcid.org/0000-0001-9064-0463http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0003-2138-7972https://doi.org/10.1007/978-3-031-47243-5_9The RML Ontology: A Community-Driven Modular Redesign 153Resource type: Ontology/License: CC BY 4.0 InternationalDOI: 10.5281/zenodo.7918478/URL: http://w3id.org/rml/portal/Keywords: Declarative Language · R2RML · RML · KnowledgeGraph1 IntroductionIn 2012, the Relational to RDF Mapping Language (R2RML) [37] was releasedas a W3C Recommendation. The R2RML ontology [8] provides a vocabularyto describe how an RDF graph should be generated from data in a relationaldatabase (RDB). Although R2RML gained wide adoption, its potential applica-bility beyond RDBs quickly appeared as a salient need [49,63,76,87].Targeting the generation of RDF from heterogeneous data sources other thanRDBs, several extensions [49,76,87] preserving R2RML’s core structure wereproposed. As R2RML and the growing number of extensions were applied in a",
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, Dylan Van Assche, Julián Arenas-Guerrero, Ben De Meester, Christophe Debruyne, Samaneh Jozashoori, Pano Maria, Franck Michel, David Chaves-Fraga, Anastasia Dimou",
        "file_name": "10!1007%978-3-031-47243-5_9.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-47243-5_9.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-031-47243-5_9.pdf"
    },
    {
        "title": "FOOPS!: An Ontology Pitfall Scanner for the FAIR principles.",
        "implementation_urls": [],
        "abstract": "Abstract. This paper presents FOOPS!, a web service designed to as-sess the compliance of vocabularies or ontologies against the FAIR princi-ples. FOOPS! performs a total of 24 different checks from the four FAIRdimensions, reflecting the best practices and latest community discus-sions to adapt FAIR to semantic artefacts. The web service not onlydetect best practices according to each principle, but also offers an ex-planation of why a particular principle fails, and helpful suggestions toovercome common issues.Keywords: Ontology development · FAIR principles · FAIR semanticsPaper type: Demo (available at https://w3id.org/foops)1 IntroductionThe Findable, Accessible, Interoperable, Reusable (FAIR) data principles [6]have become increasingly relevant in the context of research data managementand reproducibility; being a main subject of discussion and adoption in commu-nity initiatives such as the Research Data Alliance, FORCE 11 and the EuropeanOpen Science Cloud. As a result, the FAIR principles have been adapted to otherresearch artifacts, such as software,1 and semantic resources such as ontologies.2In order to help researchers adopt best practices around FAIR, the scien-tific community has developed self-assessment tools and validators that helpresearchers assess the FAIRness of their resources. These are typically targeted? Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).?? The authors would like to thank Raúl Alcazar and Jacobo Mata for their help.This work has been supported by the Madrid Government under the MultiannualAgreement with Universidad Politécnica de Madrid in the line Support for R&Dprojects for Beatriz Galindo researchers, the HORIZON2020 project OntoCommons:Ontology-driven data documentation for Industry Commons (H2020-958371) and byKnowledgeSpaces: Técnicas y herramientas para la gestión de grafos de conocimien-tos para dar soporte a espacios de datos (PID2020-118274RB-I00).1 https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg2 https://www.fairsfair.eu/fair-semantics-interoperability-and-services-0Daniel Garijo, Oscar Corcho, and Maŕıa Poveda-Villalóntowards research data, such as AmIFAIR [7],3 F-UJI,4 or fair-checker,5 withsome recent additions for research software (e.g., howfairis6). However, there isno FAIR validator specifically targeted towards ontologies.In this demo we present FOOPS!, an ontology pitfall scanner for the FAIRprinciples. FOOPS! works for both OWL and SKOS vocabularies, and distin-guishes itself from existing services such as Vapour7 (focused on the quality ofthe content negotiation of resources) and OOPS! [3] (focused on common pitfallson the ontology itself); to provide a comprehensive overview of how a vocabularycomplies with current FAIR best practices for ontologies [4, 2].2 FOOPS! featuresFOOPS! is a web service and application that takes as input an OWL ontologyor SKOS thesauri and runs 24 different checks distributed across the FAIR di-mensions. These checks are based on the best practices and recommendations in[1], [4], [2], and can be summarized as follows:– Findable (9 checks): the service assesses whether the ontology URI is persis-tent, resolvable, has a resolvable version IRI, and whether that IRI is uniquefor that version. FOOPS! will also assess if minimum descriptive metadatais included (e.g., title, description, etc.) and whether the ontology prefix and",
        "publication_date": "2021-01-01",
        "authors": "Daniel Garijo, Óscar Corcho, María Poveda‐Villalón",
        "file_name": "no_doi_20250624163029.pdf",
        "file_path": "./PDFs/no_doi_20250624163029.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper321.pdf"
    },
    {
        "title": "Uncovering hidden therapeutic indications through drug repurposing with graph neural networks and heterogeneous data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/gnn4dr/DRKG",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S0933365723002014-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/gnn4dr/DRKG."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.artmed.2023.102687",
        "abstract": "cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html. [42] Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In: Advances in neural information processing systems. Curran Associates, Inc.; 2013. Accessed: May 09, 2022. [Online]. Available: https://papers.nips.cc/paper/2013/hash/9aa42b31882ec 039965f3c4923ce901b-Abstract.html. [43] Trouillon T, Welbl J, Riedel S, Gaussier E, Bouchard G. Complex embeddings for simple link prediction. In: Proceedings of the 33rd international conference on machine learning. PMLR; Jun. 2016. p. 2071–80. Accessed: May 09, 2022. [Online]. Available: https://proceedings.mlr.press/v48/trouillon16.html. [44] Biewald L. Experiment tracking with weights and biases [Online]. Available: htt ps://www.wandb.com/; 2020. [45] Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv 2013. https://doi.org/10.48550/arXiv.1301.3781. Sep. 06. [46] Qiu J, Dong Y, Ma H, Li J, Wang K, Tang J. Network embedding as matrix factorization: unifying DeepWalk, LINE, PTE, and node2vec. In: Proceedings of the eleventh ACM international conference on web search and data mining. Marina Del Rey CA USA: ACM; Feb. 2018. p. 459–67. https://doi.org/10.1145/ 3159652.3159706. [47] Ahmed NK, et al. Learning role-based graph embeddings. arXiv 2018. https://doi. org/10.48550/arXiv.1802.02896. Jul. 02. [48] Rozemberczki B, Kiss O, Sarkar R. Karate Club: an API oriented open-source Python framework for unsupervised learning on graphs. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM ’20. New York, NY, USA: Association for Computing Machinery; 2020. p. 3125–32. https://doi.org/10.1145/3340531.3412757. [49] Spackman KA. Signal detection theory: valuable tools for evaluating inductive learning. In: Segre AM, editor. Proceedings of the sixth international workshop on machine learning. San Francisco (CA): Morgan Kaufmann; 1989. p. 160–3. https:// doi.org/10.1016/B978-1-55860-036-2.50047-3. [50] Davis J, Goadrich M. The relationship between precision-recall and ROC curves. presented at the Proceedings of the 23rd International Conference on Machine Learning. ACM; Jun. 2006. https://doi.org/10.1145/1143844.1143874. [51] Kim J-H. Estimating classification error rate: repeated cross-validation, repeated hold-out and bootstrap. Comput Stat Data Anal Sep. 2009;53(11):3735–45. https://doi.org/10.1016/j.csda.2009.04.009. [52] Schocken DD, Arrieta MI, Leaverton PE, Ross EA. Prevalence and mortality rate of congestive heart failure in the United States. J Am Coll Cardiol Aug. 1992;20(2): 301–6. https://doi.org/10.1016/0735-1097(92)90094-4. [53] Rengo F, et al. Congestive heart failure in the elderly. Arch Gerontol Geriatr Nov. 1996;23(3):201–23. https://doi.org/10.1016/S0167-4943(96)00734-0. [54] Keating GM, Goa KL. Nesiritide. Drugs Jan. 2003;63(1):47–70. https://doi.org/ 10.2165/00003495-200363010-00004. [55] Colucci WS. Nesiritide for the treatment of decompensated heart failure. J Card Fail Mar. 2001;7(1):92–100. https://doi.org/10.1054/jcaf.2001.22999. [56] Mahle WT, Cuadrado AR, Kirshbom PM, Kanter KR, Simsic JM. Nesiritide in infants and children with congestive heart failure. Pediatr Crit Care Med J Soc Crit Care Med World Fed Pediatr Intensive Crit Care Soc Sep. 2005;6(5):543–6. https://doi. org/10.1097/01.pcc.0000164634.58297.9a. [57] Malm-Erjefält M, et al. Metabolism and excretion of the once-daily human ",
        "publication_date": "2023-10-21",
        "authors": "Adrián Ayuso-Muñoz, Lucía Prieto Santamaría, Esther Ugarte-Carro, Emilio Serrano, Alejandro Rodríguez‐González",
        "file_name": "1-s2.0-S0933365723002014-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0933365723002014-main.pdf"
    },
    {
        "title": "Mapping the Web Ontology Language to the OpenAPI Specification",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KnowledgeCaptureAndDiscovery/OBA",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/mapping_owl_to_oas_2020.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The implementation release is available at the OBA’s GitHub repository.7 Our implementation allows generating API definitions for ontologies of dif-ferent sizes with a reasonable overhead."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-65847-2_11",
        "abstract": "Abstract. Many organizations maintain knowledge graphs that are or-ganized according to ontologies. However, these ontologies are imple-mented in languages (e.g. OWL) that are difficult to understand by userswho are not familiar with knowledge representation techniques. In par-ticular, this affects web developers who want to develop ontology-basedapplications but may find challenging accessing ontology-based data inknowledge graphs through SPARQL queries. To address this problem, wepropose an accessible layer for ontology-based knowledge graphs throughREST APIs. We define a mapping specification between the Web On-tology Language (OWL) and the OpenAPI Specification (OAS) to pro-vide ontology-based API definitions in a language well-known to webdevelopers. Our mapping specification identifies key similarities betweenOWL and OAS and provides implementation guidelines and examples.We also developed a reference mapping implementation that automati-cally transforms OWL ontologies into OpenAPI specifications in a matterof seconds.Keywords: OWL · OpenAPI · REST API · ontologies1 IntroductionMany public and private organizations have adopted a knowledge-driven ap-proach to make publicly available their knowledge graphs. Ontologies [10] playa crucial role in this approach, as they describe the knowledge about a domainin an agreed and unambiguous manner; and they allow organizing data, ease itsreusability, and facilitate its interoperability. Ontologies are usually formalizedin the Web Ontology Language (OWL) [6], a W3C recommendation to representthe semantics of a domain in a machine-readable way. However, OWL has a steeplearning curve due its inherent complexity [12], and newcomers may get confusedwith the meaning of constraints, axioms or the Open World Assumption.This problem has become evident in the case of developers who have an in-terest in taking advantage of the data available in existing knowledge graphsbut are not used to Semantic Web technologies. Instead, developers are famil-iar with REST APIs as a resource-access way which hides details about the2 P. Espinoza-Arias et al.implementation of operations for resource management or how such resourceshave been described according to the data models. To describe APIs, severalInterface Description Languages have been defined to document their domain,functional and non-functional aspects. The OpenAPI Specification3 (OAS) is abroadly adopted de facto standard for describing REST APIs in a programminglanguage-agnostic interface. OAS allows both humans and computers to under-stand and interact with a remote service. Due to its wide adoption, OAS has abig community behind, wich has provided tools to allow developers to generateAPI documentation, server generation, mockup design, etc.In this paper we describe additional work in the direction of making ontology-based data available though REST APIs. We define a mapping specificationbetween OWL and OAS to facilitate the work of those who are interested inusing data represented by semantic technologies and have to face the challeng-ing task of developing ontology-based applications. Our mapping also aims toenhance adherence to the FAIR data principles [13] by facilitating: Findability,as it provides a template for finding types of available resources in knowledgegraphs; Accessibility because it allows translating the ontology to an interfacethat developers are used to; Interoperability because the mapping matches two",
        "publication_date": "2020-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "mapping_owl_to_oas_2020.pdf",
        "file_path": "./PDFs/mapping_owl_to_oas_2020.pdf"
    },
    {
        "title": "T2WML: Table To Wikidata Mapping Language",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/t2wml",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3360901!3364448.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The formal specification of the full language is available in https://github.com/usc-isi-i2/t2wml."
                    }
                ]
            }
        ],
        "doi": "10.1145/3360901.3364448",
        "abstract": "ABSTRACTThe web contains millions of useful spreadsheets and CSV files, butthese files are difficult to use in applications because they use awide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes iteasy to map and link arbitrary spreadsheets and CSV files to theWikidata data model. The output of T2WML consists of Wikidatastatements that can be loaded in the public Wikidata knowledgebase or in a Wikidata clone repository, creating an augmentedWikidata knowledge graph that application developers can queryusing SPARQL.1CCS CONCEPTS• Information systems→Extraction, transformation and load-ing.KEYWORDSKnowledge Graphs; RDF; Entity Linking; WikidataACM Reference Format:Pedro Szekely, Daniel Garijo, Divij Bhatia, JiashengWu, Yixiang Yao and JayPujara. 2019. T2WML: Table ToWikidata Mapping Language. In Proceedingsof the 10th International Conference on Knowledge Capture (K-CAP ’19),November 19–21, 2019, Marina Del Rey, CA, USA. ACM, New York, NY, USA,4 pages. https://doi.org/10.1145/3360901.33644481 INTRODUCTIONThe web contains millions of useful spreadsheets and CSV files,including data from many government and international organi-zations. Most institutions offer their data in web sites where userscan download the data in Excel and CSV formats. The downloadeddata is seldom directly usable because, unlike databases (which useone column per variable), spreadsheets often arrange the data indifferent layouts.Fig. 1 illustrates the problem using data downloaded from theUnited Nations web site2 about homicide rates in different countries.We truncated and colored the files for ease of presentation. Thecells with the homicide numbers are highlighted in green, the cells1This material is based upon work supported by United States Air Force under ContractNo. FA8650-17-C-7715.2https://dataunodc.un.org/crime/intentional-homicide-victimsPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA© 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-7008-0/19/11. . . $15.00https://doi.org/10.1145/3360901.3364448that provide contextual information for the value are highlightedin blue, and header cells are highlighted in dark blue. Fig. 1a shows",
        "publication_date": "2019-09-23",
        "authors": "Pedro Szekely, Daniel Garijo, Divij Bhatia, Jiasheng Wu, Yixiang Yao, Jay Pujara",
        "file_name": "10!1145%3360901!3364448.pdf",
        "file_path": "./PDFs/10!1145%3360901!3364448.pdf"
    },
    {
        "title": "Crossing the chasm between ontology engineering and application development: A survey",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KNowledgeOnWebScale/walder",
                "type": "git",
                "paper_frequency": 47,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S1570826821000305-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Verborgh, Walder, 2020, URL: https://github.com/KNowledgeOnWebScale/walder."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2021.100655",
        "abstract": "ing query to search in titles, abstracts and keywords of relatedarticles: (TITLE-ABS-KEY ((ontology OR OWL OR ‘‘linked data’’OR ‘‘semantic data’’ OR ‘‘knowledge graph’’) AND (API OR ‘‘webAPI’’) AND (tool OR technology OR method OR methodology ORprocess))).To avoid systematic bias [33], we included non-scientific liter-ature describing relevant work in this area such as related W3CRecommendations and Technical Specifications, and existing toolsdeveloped by the Semantic Web community:• Linked Data Platform (LDP) [35]• Linked Data API specification (LDA) [36]• Solid [37]• Pubby [38]• Puelia [39]• ELDA [40]• Linked Data Templates [41]We also contacted experts and researchers working in the areaand asked them whether they knew of any additional efforts,including unpublished results or ongoing work. As a result wecollected the following additional efforts:• Linked Open Data Inspector (LODI) [42]• AtomGraph Processor [43]• RESTful-API for RDF data (R4R) [44]• Restful API Manager Over SPARQL Endpoints (RAMOSE) [45]• OWL2OAS Converter [46]• Ontology Based APIs Framework (OBA) [47]• Community Solid Server [48]• Walder [49]• LDflex [50]33.1.2. Exclusion and inclusion criteriaThe standardized exclusion (EC) and inclusion (IC) criteria forscientific literature review was defined as follows:• EC1: articles not written in English.• EC2: articles not describing a methodology/method/processfor API generation from ontologies/Linked Data/KnowledgeGraphs.• EC3: the full-text of articles does not give details about themethodology/method/process.• EC4: articles with an extended version that presents moredetails about the same methodology/method/process.• EC5: articles referring to semantic annotation of APIs.• EC6: articles which reuse a methodology/method/processbut do not make any changes to it.• EC7: duplicated articles (when retrieved from the database).• EC8: articles describing programming APIs for handling RDF.• IC1: articles including open source code or free-access demo(if a software tool is presented in the article).The exclusion and inclusion criteria for selection of non-scientific literature, unpublished, or ongoing work were:",
        "publication_date": "2021-06-24",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "1-s2.0-S1570826821000305-main.pdf",
        "file_path": "./PDFs/1-s2.0-S1570826821000305-main.pdf"
    },
    {
        "title": "ALSPAC Grant Acknowledgements",
        "implementation_urls": [],
        "file_name": "no_doi_20250624163100.pdf",
        "file_path": "./PDFs/no_doi_20250624163100.pdf",
        "pdf_link": "http://www.bristol.ac.uk/alspac/external/documents/grant-acknowledgements.pdf"
    },
    {
        "title": "Best practices for publishing, retrieving, and using spatial data on the web",
        "implementation_urls": [],
        "doi": "10.3233/sw-180305",
        "abstract": "Abstract. Data owners are creating an ever richer set of information resources online, and these are being used for more and moreapplications. Spatial data on the Web is becoming ubiquitous and voluminous with the rapid growth of location-based services,spatial technologies, dynamic location-based data and services published by different organizations. However, the heterogeneityand the peculiarities of spatial data, such as the use of different coordinate reference systems, make it difficult for data users,Web applications, and services to discover, interpret and use the information in the large and distributed system that is the Web.To make spatial data more effectively available, this paper summarizes the work of the joint W3C/OGC Working Group onSpatial Data on the Web that identifies 14 best practices for publishing spatial data on the Web. The paper extends that work bypresenting the identified challenges and rationale for selection of the recommended best practices, framed by the set of principlesthat guided the selection. It describes best practices that are employed to enable publishing, discovery and retrieving (querying)spatial data on the Web, and identifies some areas where a best practice has not yet emerged.Keywords: Geographic information systems, spatial data, Web technologies, World Wide Web, W3C, Open GeospatialConsortium, OGC1570-0844/19/$35.00 © 2019 – IOS Press and the authors. All rights reservedhttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-180305&domain=pdf&date_stamp=2018-08-1096 L. van den Brink et al. / Best Practices for Publishing, Retrieving, and Using Spatial Data on the Web1. IntroductionSpatial data is important. Firstly, because it has be-come ubiquitous with the explosive growth in position-ing technologies attached to mobile vehicles, portabledevices, and autonomous systems. Secondly, becauseit is fundamentally useful for countless convenientconsumer services like transport planning, or for solv-ing the biggest global challenges like climate changeadaptation [68]. Historically, sourcing, managing andusing high-quality spatial data has largely been thepreserve of military, government and scientific enter-prises. These groups have long recognized the im-portance and value that can be obtained by sharingtheir own specialized data with others to achieve cross-theme interoperability, increased usability and betterspatial awareness, but they have struggled to achievethe cross-community uptake they would like. Spa-tial Data Infrastructures (SDIs) [52], which commonlyemploy the mature representation and access standardsof the Open Geospatial Consortium (OGC), are nowwell developed, but have become a part of the “deepWeb” that is hidden for most Web search engines andhuman information-seekers. Even geospatial expertsstill do not know where to start looking for what theyneed or how to use it when they find it. The integra-tion of spatial data from different sources offers pos-sibilities to infer and gain new information; however,spatial data on the Web is published in various struc-tures, formats and with different granularities. Thismakes publishing, discovering, retrieving, and inter-preting the spatial data on the Web a challenging task.By contrast, the linked data Web, as a platform of prin-ciples, tools, and standards championed by the WorldWide Web Consortium (W3C) enables data discover-ability and usability that is readily visible in, for exam-",
        "publication_date": "2018-08-10",
        "authors": "L. van den Brink, Payam Barnaghi, Jeremy Tandy, Ghislain Auguste Atemezing, Rob Atkinson, Byron Cochrane, Yasmin Fathy, Raúl García Castro, Armin Haller, Andreas Harth, Krzysztof Janowicz, Şefki Kolozali, Bart van Leeuwen, Maxime Lefrançois, Joshua Lieberman, Andrea Perego, Danh Le-Phuoc, Bill Roberts, Kerry Taylor, Raphaël Troncy",
        "file_name": "10!3233%sw-180305.pdf",
        "file_path": "./PDFs/10!3233%sw-180305.pdf"
    },
    {
        "title": "Distributing Text Mining tasks with <i>librAIry</i>",
        "implementation_urls": [],
        "doi": "10.1145/3103010.3121040",
        "abstract": "ABSTRACTWe present librAIry, a novel architecture to store, process and an-alyze large collections of textual resources, integrating existingalgorithms and tools into a common, distributed, high-performancework�ow. Available text mining techniques can be incorporated asindependent plug&play modules working in a collaborative mannerinto the framework. In the absence of a pre-de�ned �ow, librAIryleverages on the aggregation of operations executed by di�erentcomponents in response to an emergent chain of events. Extensiveuse of Linked Data (LD) and Representational State Transfer (REST)principles are made to provide individually addressable resourcesfrom textual documents. We have described the architecture designand its implementation and tested its e�ectiveness in real-worldscenarios such as collections of research papers, patents or ICT aids,with the objective of providing solutions for decision makers andexperts in those domains. Major advantages of the framework andlessons-learned from these experiments are reported.CCS CONCEPTS•Applied computing→Documentmanagement and text pro-cessing; •Computer systems organization → Architectures;KEYWORDSlarge-scale text analysis; NLP; scholarly data; text mining; dataintegration1 INTRODUCTIONGiven the huge amount of textual data about any domain that isdaily being produced or captured in any imaginable domain, itbecomes crucial to provide mechanisms for programmatically pro-cessing this raw data so we can make sense out of it: discarding allthe noisy, non-relevant information and keeping only the data thatcan bring value for the involved agents (general consumers, experts,companies, investors…). While some speci�c tools already allowfor advanced sense-making operations, others opt for composing a�is work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R,�nanced by the Spanish Ministry MINECO and co-�nanced by FEDER.Author’s addresses: C. Badenes-Olmedo and J.L. Redondo-Garcı́a and O. Corcho ,Ontology Engineering Group, Universidad Politécnica de Madrid.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro�t or commercial advantage and that copies bear this notice and the full citationon the �rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi�ed. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci�c permission and/or afee. Request permissions from permissions@acm.org.DocEng ’17, September 04–07, 2017, Valle�a, Malta.© 2017 ACM. 978-1-4503-4689-4/17/09. . .$15.00DOI: h�ps://doi.org/10.1145/3103010.3121040solution where di�erent analysis techniques are integrated under auniform data schema. However, this integration involves signi�-cant e�orts on reconciling data sources, coordinating processing",
        "publication_date": "2017-08-31",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!1145%3103010!3121040.pdf",
        "file_path": "./PDFs/10!1145%3103010!3121040.pdf"
    },
    {
        "title": "Building the Legal Knowledge Graph for Smart Compliance Services in Multilingual Europe.",
        "implementation_urls": [],
        "abstract": "Abstract.  This position paper describes the vision, objectives and methodology of the LYNX project. The aim of Lynx is to create services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law and standards. Keywords. Compliance, Legal Knowledge Graph, regtech, regulatory compliance, semantic web Introduction The term compliance is widely used to refer “to the conformance to a set of laws, regulations, policies, or best practices” [1]. Every company performing almost any activity has some concern with compliance-related problems.  These problems are a bigger concern for small and medium-sized enterprises (SMEs), which cannot afford expensive consultancy services and they are a bigger hurdle for companies trying to sell abroad, as they usually lack the knowledge on the applicable conditions in the target country. According to the European Commission1, only 7% of European SMEs sell across borders, but those who do, exhibit 7% job growth and 26% innovate in their offering, greater numbers than the 1% and 8% of SMEs that do not go outside their local markets.  The European Commission is aware of these legal and language barriers and is trying to build a single market with less entry barriers. The LYNX project is a European research project funded as an H2020 Innovation Action covering the topic ICT-14: Big Data PPP: cross-sectorial and cross-lingual data integration and experimentation. The project is expected to last three years, starting in December 2017. This position paper briefly presents this research project. 1 http://europa.eu/rapid/press-release_IP-10-895_en.htm Proceedings of the 1st Workshop on Technologies for Regulatory Compliance151. The LYNX project Having identified these compliance problem, and having assumed that technology can help lowering these legal and language barriers, the main objective of Lynx can be stated as “to create an ecosystem of cloud services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law, standards and other aspects”. Other specific objectives follow:  to provide a platform that helps companies solving questions and cases related to compliance in different sectors and jurisdictions.  to help European SMEs to reduce costs and effort in organising and monitoring legislation, regulations, and sectorial good practices.  to provide citizens a better access to legal and regulatory information from multiple jurisdictions and to facilitate their in the legislative processes and in the writing process of standards.  to draw a legal knowledge graph across different jurisdictions, comprising legislation, case law, doctrine, standards, norms, and other documents.  to deliver the necessary domain-neutral common services (such as document annotation, interlinking, etc.) as building blocks to orchestrate aggregated business-oriented services.  to cover some particular business case necessities, validating the project’s developed ecosystem.  The referred legal knowledge graph is a collection of structured data and ",
        "publication_date": "2017-01-01",
        "authors": "Elena Montiel-Ponsoda, Victor Rodrı́guez-Doncel, Jorge Gracia",
        "file_name": "no_doi_20250624163134.pdf",
        "file_path": "./PDFs/no_doi_20250624163134.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2049/02paper.pdf"
    },
    {
        "title": "On the Efficiency of Decentralized File Storage for Personal Information Management Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/testingIPFS",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%iscc50000!2020!9219623.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/miker83z/testingIPFS"
                    }
                ]
            }
        ],
        "doi": "10.1109/iscc50000.2020.9219623",
        "arxiv": "2007.03505",
        "abstract": "Abstract—This paper presents an architecture, based on Dis-tributed Ledger Technologies (DLTs) and Decentralized File Stor-age (DFS) systems, to support the use of Personal InformationManagement Systems (PIMS). DLT and DFS are used to managedata sensed by mobile users equipped with devices with sensingcapability. DLTs guarantee the immutability, traceability andverifiability of references to personal data, that are stored inDFS. In fact, the inclusion of data digests in the DLT makes itpossible to obtain an unalterable reference and a tamper-prooflog, while remaining compliant with the regulations on personaldata, i.e. GDPR. We provide an experimental evaluation on thefeasibility of the use of DFS. Three different scenarios have beenstudied: i) a proprietary IPFS approach with a dedicated nodeinterfacing with the data producers, ii) a public IPFS service andiii) Sia Skynet. Results show that through proper configurationof the system infrastructure, it is viable to build a decentralizedPersonal Data Storage (PDS).Index Terms—Personal Information Management System, Dis-tributed Ledger Technologies, Decentralized File Storage, Sensingas a Service.I. INTRODUCTIONThe advent of social media and Web 2.0 favoured a processthat broke the boundaries between authorship and readership:users produce the data that is consumed by other users.This has increased the privacy threats of applications thatare shaped by user-generated content, as it often consists ofhighly personal data. In general, the economics of personalinformation is helped by the more pervasive nature of to-day’s digital world. This information enables organizations toprovide personalized or more useful services in digital andphysical spaces, but it could also have potentially harmfulconsequences for the privacy and autonomy of users andsociety at large. Current platform-centered data managementtechniques threaten the control that individuals exercise overtheir personal information and give to few companies thepower to necessarily rely on them to explore, filter and obtain∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieITN EJD grant agreement No 814177 Law, Science and Technology JointDoctorate - Rights of Internet of Everythingdata of interest. Not mentioning the fact that some of thesecentral entities can operate without any transparency on theuse of users’ data.An individual digital counterpart can be depicted not onlyby using his own personal information, but also that of hissocial links (e.g. friends, family, colleagues) . Thus, it becomeseasier to understand users activity choice and lifestyle patterns[1] and to make more intrusive recommendations using thisdata [2], [3]. Lack of privacy control, for instance, leads anindividual being thrown into a “filter bubble” that can affect",
        "publication_date": "2020-07-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%iscc50000!2020!9219623.pdf",
        "file_path": "./PDFs/10!1109%iscc50000!2020!9219623.pdf"
    },
    {
        "title": "Multi-object tracking in traffic environments: A systematic literature review",
        "implementation_urls": [],
        "doi": "10.1016/j.neucom.2022.04.087",
        "publication_date": "2022-04-21",
        "authors": "Diego M. Jiménez-Bravo, Álvaro Lozano Murciego, André Sales Mendes, Héctor Sánchez San Blas, Javier Bajo",
        "file_name": "1-s2.0-S0925231222004672-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0925231222004672-main.pdf"
    },
    {
        "title": "TEC: Transparent Emissions Calculation Toolkit",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-47243-5_5",
        "abstract": "Abstract. This paper presents FOOPS!, a web service designed to as-sess the compliance of vocabularies or ontologies against the FAIR princi-ples. FOOPS! performs a total of 24 different checks from the four FAIRdimensions, reflecting the best practices and latest community discus-sions to adapt FAIR to semantic artefacts. The web service not onlydetect best practices according to each principle, but also offers an ex-planation of why a particular principle fails, and helpful suggestions toovercome common issues.Keywords: Ontology development · FAIR principles · FAIR semanticsPaper type: Demo (available at https://w3id.org/foops)1 IntroductionThe Findable, Accessible, Interoperable, Reusable (FAIR) data principles [6]have become increasingly relevant in the context of research data managementand reproducibility; being a main subject of discussion and adoption in commu-nity initiatives such as the Research Data Alliance, FORCE 11 and the EuropeanOpen Science Cloud. As a result, the FAIR principles have been adapted to otherresearch artifacts, such as software,1 and semantic resources such as ontologies.2In order to help researchers adopt best practices around FAIR, the scien-tific community has developed self-assessment tools and validators that helpresearchers assess the FAIRness of their resources. These are typically targeted? Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).?? The authors would like to thank Raúl Alcazar and Jacobo Mata for their help.This work has been supported by the Madrid Government under the MultiannualAgreement with Universidad Politécnica de Madrid in the line Support for R&Dprojects for Beatriz Galindo researchers, the HORIZON2020 project OntoCommons:Ontology-driven data documentation for Industry Commons (H2020-958371) and byKnowledgeSpaces: Técnicas y herramientas para la gestión de grafos de conocimien-tos para dar soporte a espacios de datos (PID2020-118274RB-I00).1 https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg2 https://www.fairsfair.eu/fair-semantics-interoperability-and-services-0Daniel Garijo, Oscar Corcho, and Maŕıa Poveda-Villalóntowards research data, such as AmIFAIR [7],3 F-UJI,4 or fair-checker,5 withsome recent additions for research software (e.g., howfairis6). However, there isno FAIR validator specifically targeted towards ontologies.In this demo we present FOOPS!, an ontology pitfall scanner for the FAIRprinciples. FOOPS! works for both OWL and SKOS vocabularies, and distin-guishes itself from existing services such as Vapour7 (focused on the quality ofthe content negotiation of resources) and OOPS! [3] (focused on common pitfallson the ontology itself); to provide a comprehensive overview of how a vocabularycomplies with current FAIR best practices for ontologies [4, 2].2 FOOPS! featuresFOOPS! is a web service and application that takes as input an OWL ontologyor SKOS thesauri and runs 24 different checks distributed across the FAIR di-mensions. These checks are based on the best practices and recommendations in[1], [4], [2], and can be summarized as follows:– Findable (9 checks): the service assesses whether the ontology URI is persis-tent, resolvable, has a resolvable version IRI, and whether that IRI is uniquefor that version. FOOPS! will also assess if minimum descriptive metadatais included (e.g., title, description, etc.) and whether the ontology prefix and",
        "publication_date": "2023-01-01",
        "authors": "Milan Marković, Daniel Garijo, Stefano Germano, Iman Naja",
        "file_name": "10!1007%978-3-031-47243-5_5.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-47243-5_5.pdf"
    },
    {
        "title": "Combining heterogeneous data sources for spatio-temporal mobility demand forecasting",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iipr/mobility-demand",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S156625352200166X-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Complete results, datasets and code The complete set of graphs, the code to produce the datasets, and the code to reproduce the experiments are available at https://github.com/iipr/mobility-demand."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.inffus.2022.09.028",
        "publication_date": "2022-10-07",
        "authors": "Ignacio-Iker Prado-Rujas, Emilio Serrano, Antonio García-Dopico, M. Luisa Córdoba, Marı́a S. Pérez",
        "file_name": "1-s2.0-S156625352200166X-main.pdf",
        "file_path": "./PDFs/1-s2.0-S156625352200166X-main.pdf"
    },
    {
        "title": "Challenges for FAIR Digital Object Assessment",
        "implementation_urls": [],
        "doi": "10.3897/rio.8.e95943",
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "publication_date": "2022-10-12",
        "authors": "Esteban González, Daniel Garijo, Óscar Corcho",
        "file_name": "10!3897%rio!8!e95943.pdf",
        "file_path": "./PDFs/10!3897%rio!8!e95943.pdf"
    },
    {
        "title": "Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [],
        "doi": "10.5220/0010945300003116",
        "abstract": "Abstract: The advent of Distributed Ledger Technologies (DLTs) has paved the way for a new paradigm of traceabilityin all information systems areas. In the context of decision-making processes, however, DLTs are generallyused only to trace the end results. In this work we argue that a reasoning system can be put in place formaking these decisions, in order to enhance auditability, transparency, and finally to provide explainability.We propose the Intelligent Human-input-based Blockchain Oracle (IHiBO), a cross-chain oracle that enablesthe execution and traceability of formal argumentation and negotiation processes, involving the intervention ofhuman experts. We take as reference the decision-making processes of fund managements, as trust is of crucialimportance in such “trust services”. The architecture and implementation of IHiBO are based on leveragingtwo-layer DLTs, smart contracts, argumentation and negotiation in a multi-agent setup. Finally, we providesome experimental results that support our discussion, namely that in the use-case we have considered ourmethodology can increase trust from principals to trusted services.1 INTRODUCTIONIn situations where trust plays a significant role, thedecision-making process might be considered as thepinnacle of the engagement between parties. In thecase of funds management, for instance, investorschoose managers based not only on forecasts of futureperformance but also on factors such as trust and reli-ability (Kostovetsky, 2016). Indeed, in these so called“trust services” the fund managers are in the positionof a fiduciary acting on behalf of the principal, sub-ject to the overall duty to act in the best interest ofthe client, i.e. the principal. Fund managers primar-ily research and determine the best stocks, bonds, orother securities to fit the strategy of the fund, then buyand sell them. The decisions taken by managers af-fect the principals directly, thus the legislator can anddoes declare the principal’s right to check the fidu-ciary’s relevant activities in order to give some weightto this duty by its intended controlability. However,this might not be so straightforward, as these activi-aThese authors contributed equally.b https://orcid.org/0000-0002-7200-6001c https://orcid.org/0000-0002-4159-4269d https://orcid.org/0000-0002-2488-2293e https://orcid.org/0000-0001-7784-6176ties, e.g. securities transactions, are increasingly exe-cuted as a collaborative process that involves not onlya single fund manager but also other managers, ana-lysts, and external entities that maintain business re-lationships. The beliefs and assumptions of this di-verse group of participants can be influenced by a va-riety of different background knowledge and in turnshape the decision that leads to the execution of afund activity. The fund management decision processis characterized by uncertain and changing informa-tion, dynamic opportunities, multiple goals and strate-gic considerations, interdependence among projects,and multiple decision-makers and locations (Cooperet al., 1997). This necessitates a collaborative pro-",
        "publication_date": "2022-01-01",
        "authors": "Liuwen Yu, Mirko Zichichi, Réka Markovich, Amro Najjar",
        "file_name": "10!5220%0010945300003116.pdf",
        "file_path": "./PDFs/10!5220%0010945300003116.pdf"
    },
    {
        "title": "A Framework Based on Distributed Ledger Technologies for Data Management and Services in Intelligent Transportation Systems",
        "implementation_urls": [],
        "doi": "10.1109/access.2020.2998012",
        "publication_date": "2020-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%access!2020!2998012.pdf",
        "file_path": "./PDFs/10!1109%access!2020!2998012.pdf"
    },
    {
        "title": "On the Decentralization of Health Systems for Data Availability: a DLT-based Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ccnc51644.2023.10059701",
        "abstract": "Abstract—Mobile devices entered people’s lives by leaps andbounds, offering various applications relying on private third-party entities to manage their users’ data. Centralized control ofpersonal health data endangers the privacy of the users directlyinvolved. In the future, there will likely be a trend towarddecentralizing the health data collection, relieving central entitiesof this task. This comes with several challenges in a decentralizedenvironment, such as avoiding a single point of failure to guaran-tee data availability. The following work proposes an architecturebased on Distributed Ledger Technology to allow users to decideon their data while ensuring availability by employing socialnetworks. We will outline the mechanisms behind data storageand the implications of using smart contracts in the architecture.In concluding the work, we show the developed architecture andresults deriving from its assessment, highlighting possible usecases applied to the specific health data management context.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed Storage, Social NetworksI. INTRODUCTIONPersonal digital technologies are constantly evolving and arethe primary source of information generation. They broughta fundamental transformation in people’s lives, but the datagenerated usually ends up in private use for reasons relatedto the privacy of an individual. The shift to a decentralizedparadigm now seems immediate, not only to protect theindividual but, more importantly, to enable new technologiesrevolving around data management. Storing data in centralizeddata silos makes it inaccessible to the public and discon-nected from other data [1], [2]. This mechanism hampersinnovation above all. Based on this, interest in decentralizingdata management is proliferating and with great promise toenable these conditions. The healthcare sector can benefitsignificantly from decentralizing information from centralizedsystems. This is because there are so many new implications ofdoing this, ranging from contributing personally to advancingnew medical studies in a disintermediate way and gettingnew solutions on your own directly from devices that becomeincredibly efficient and intelligent [3], [4]. Indeed, this pathis not easy, there are considerable barriers to be addressed interms of security and privacy, but it is the most suitable visionto enable the digital health field.This work has received funding from Regione Marche with DDPF n. 1189,the EU’s Horizon 2020 research and innovation programme under the MSCAITN grant agreement No 814177 LAST-JD-RIoE and from the University ofUrbino through the “Bit4Food” research project.Assuming we had such a decentralized approach available,we could enable a new way of exploiting data in the healthcarespace by storing our data ourselves. However, new issueswould be introduced, such as the data availability problem.The availability of a piece of data indicates its ability to be",
        "publication_date": "2023-01-08",
        "authors": "Gioele Bigini, Mirko Zichichi, Emanuele Lattanzi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "On_the_Decentralization_of_Health_Systems_for_Data_Availability_a_DLT-based_Architecture.pdf",
        "file_path": "./PDFs/On_the_Decentralization_of_Health_Systems_for_Data_Availability_a_DLT-based_Architecture.pdf"
    },
    {
        "title": "CREATING AND IMPROVING EDUCATIONAL MATERIALS: AN APPROACH BASED ON CROWDSOURCING",
        "implementation_urls": [],
        "doi": "10.21125/iceri.2018.1854",
        "abstract": "ABSTRACT Data are becoming the cornerstone of many businesses and entire systems infrastructure.Intelligent Transportation Systems (ITS) are no different. The ability of intelligent vehicles and devices toacquire and share environmental measurements in the form of data is leading to the creation of smart servicesfor the benefit of individuals. In this paper, we present a system architecture to promote the developmentof ITS using distributed ledgers and related technologies. Thanks to these, it becomes possible to create,store and share data generated by users through the sensors on their devices or vehicles, while on themove. We propose an architecture based on Distributed Ledger Technologies (DLTs) to offer features suchas immutability, traceability and verifiability of data. IOTA, a promising DLT for IoT, is used togetherwith Decentralized File Storages (DFSes) to store and certify data (and their related metadata) comingfrom vehicles or by the users’ devices themselves (smartphones). Ethereum is then exploited as the smartcontract platform that coordinates the data sharing through access control mechanisms. Privacy guaranteesare provided by the usage of distributed key management systems and Zero Knowledge Proof. We provideexperimental results of a testbed based on real traces, in order to understand if DLT and DFS technologiesare ready to support complex services, such as those that pertain to ITS. Results clearly show that, whilethe viability of the proposal cannot be rejected, further work is needed on the responsiveness of DLTinfrastructures.INDEX TERMS Intelligent transportation systems, distributed ledger technologies, blockchain, smartcontracts, decentralized file storage, sensing as a service.I. INTRODUCTIONThe future of Intelligent Transportation Systems (ITS) will bebased on the ability of vehicles to sense, store and exchangebig data. Vehicles will be more and more equipped withsensors that track information about the vehicle internals,as well as information about the surrounding environment androad conditions. Moreover, ubiquitous connectivity allowsindividuals to post crowdsensed information through theirsmartphones and mobile devices. This makes them becomeThe associate editor coordinating the review of this manuscript andapproving it for publication was Yanli Xu .an active part of ITS. Such crowd-sensed information isessential for building sophisticated smart services that aimat improving traffic management, transportation efficiencyand safety, raising awareness about the environment, and thusimproving the liveability and health status of the communityof a given territory. We thus envisage that vehicles and theirusers will be able to record data, store them in some databoxes, and communicate with other vehicles or users as well.There are numerous examples of Vehicle-to-Vehicle (V2V)and Vehicle-to-Infrastructure (V2I) based applications, suchas notification services [1], [2], as well as standards forcommunication messages, e.g., ETSI Cooperative Awareness100384 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020https://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0002-3690-6651https://orcid.org/0000-0001-8218-7195M. Zichichi et al.: Framework Based on DLTs for Data Management and Services in ITSMessages (CAM) [3]. In these contexts, one of the mainissues is the unreliability of the exchanged information. Thisproblem is typically due to the physical errors of the sensors,",
        "publication_date": "2018-11-01",
        "authors": "Mari Carmen Suárez-Figueroa, Edna Ruckhaus, Óscar Corcho, Martín Molina, Emilio Serrano, Damiano Zanardini",
        "file_name": "10!21125%iceri!2018!1854.pdf",
        "file_path": "./PDFs/10!21125%iceri!2018!1854.pdf"
    },
    {
        "title": "A scalable, secure, and semantically interoperable client for cloud-enabled Demand Response",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/DeltaCimApp",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S0167739X22003648-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All payloads exchanged and used in the ifferent experiments are available in the CIM repository in the xperimentation folder.21 Fig."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.future.2022.11.004",
        "publication_date": "2022-11-09",
        "authors": "Andrea Cimmino, Juan Cano-Benito, Alba Fernández-Izquierdo, Christos Patsonakis, Apostolos C. Tsolakis, Raúl García‐Castro, Dimosthenis Ioannidis, Dimitrios Tzovaras",
        "file_name": "1-s2.0-S0167739X22003648-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0167739X22003648-main.pdf"
    },
    {
        "title": "Towards Decentralized Complex Queries over Distributed Ledgers: a Data Marketplace Use-case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%icccn52240!2021!9522165.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [6] J."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522165",
        "arxiv": "2104.13819",
        "abstract": "Abstract—Distributed Ledger Technologies (DLT) and Decen-tralized File Storages (DFS) are becoming increasingly used tocreate common, decentralized and trustless infrastructures whereparticipants interact and collaborate in Peer-to-Peer interactions.A prominent use case is represented by decentralized datamarketplaces, where users are consumers and providers at thesame time, and trustless interactions are required. However, datain DLTs and DFS are usually unstructured and there are noefficient mechanisms to query a certain type of data for thesearch in the market. In this paper, we propose the use of aDistributed Hash Table (DHT) as a layer on top of DLTs where,once the data are acquired and stored in the ledger, these can besearched through multiple keyword based queries, thanks to thelookup functionalities offered by the DHT. The DHT network is ahypercube overlay structure, organized for an efficient processingof multiple keyword-based queries. We provide the architectureof such solution for a decentralized data marketplace and ananalysis based on a simulation that proves the viability of theproposed approach.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Data Marketplace, Key-word SearchI. INTRODUCTIONThe transformation brought about by digital technologieshas data at its core and has had a significant impact oneconomies and societies around the world. The ability to easilyget hold of data has the potential to create a data market wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is not ofdubious or false origin is often a challenge.In order to tackle this issue, Distributed Ledger Technolo-gies (DLT) and the realm of decentralized technologies (e.g.Decentralized File Storages (DFS)), that are emerging aroundthem, come to the rescue [1], [2]. By creating a common,decentralized and trustless infrastructure, i.e. a decentralizeddata marketplace, it will be possible for data consumers andproviders to interact and collaborate in Peer-to-Peer interac-tions [3], [4]. DLTs enable peers to engage in financial transac-tions without establishing a trust relationship. Benefits oftencited of DLTs, indeed, include enabling secure transactionsThis work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.between untrusted parties through consensus mechanisms,high availability, and the ability to automate and enforceprocesses through smart contracts [5].With the management of market interactions based on theuse of DLT and decentralized technologies, what remains is",
        "publication_date": "2021-07-01",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%icccn52240!2021!9522165.pdf",
        "file_path": "./PDFs/10!1109%icccn52240!2021!9522165.pdf"
    },
    {
        "title": "WhenTheFact: Extracting Events from European Legal Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia220470",
        "abstract": "Abstract. This paper presents WhenTheFact, a tool that identifies relevant eventsfrom European judgments. It is able to extract the structure of the document, aswell as when the event happened and who carried it out. WhenTheFact builds thena timeline that allows the user to navigate through the annotations in the document.Keywords. event extraction, visualization, NLP, legal domain, timeline generation1. IntroductionEvents and their logical sequence are key to understanding legal decisions, being thestoryline of pivotal importance. We therefore assume that a judgment can be describedas a series of time-marked happenings (events) instead of focusing on the other entities(things), and to this aim we must be able to extract these events in an automatic fashion.Before undertaking the event extraction task itself, discourse extraction is required;since legal decisions are long and complex, where the event is detected within the docu-ment is crucial regarding its relevance. Once the relevant parts of the document are de-termined, the next step involves training a system using documents annotated manuallywith relevant events, as well as the semantic resources available. Finally, the system isable to annotate different documents, allowing to visualize the relevant events in it. Ad-ditionally, in the online demo2, a timeline with these relevant events is generated, easingnavigation through the document.The paper is organized as follows. Section 2 explores previous related work in lit-erature. Section 3 introduces the system created to extract relevant events from legal de-cisions, explaining its different stages: document structure extraction, training strategiesand extraction itself. Section 4 presents the evaluation of the system. Finally, Section 5summarizes the main contributions and the future research lines to explore.2. Related workBeside generic efforts in event extraction such as the carried out by temporal taggersfollowing TimeML [1,2] or related tasks such as frame-semantic parsing [3,4], semantic1Corresponding Author: Marı́a Navas-Loro, Ontology Engineering Group, Universidad Politécnica deMadrid, Spain; E-mail: mnavas@fi.upm.es. This work was funded partially by the project Knowledge Spa-ces:  Tecnicas y herramientas  para la gestion de grafos de conocimientos para dar soporte a  espacios de datos  (Grant  PID2020-118274RB-I00,  funded  by  MCIN/AEI/10.13039/501100011033)  and  by  H2020 MSCA PROTECT (813497).2https://whenthefact.oeg.fi.upm.es/´´Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220470219https://whenthefact.oeg.fi.upm.es/role labeling (SRL) [5,6] or open information extraction3, some proposals have beenmade specifically in the legal domain. These works often involve ad hoc definitions ofevents, ignoring general event annotation schemes.In the context of legal information retrieval, events can be considered as temporallybounded objects that have entities important as participants that played a significant rolein a case. To this aim, Lagos et al. [7] propose an NLP semi automatic approach toenable the use of entity related information corresponding to the relations among the keyplayers of a case, extracted in the form of events. They are interested in the topic, theroles, the location and the time, and consider different types of events. On the other hand,",
        "publication_date": "2022-12-05",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%faia220470.pdf",
        "file_path": "./PDFs/10!3233%faia220470.pdf"
    },
    {
        "title": "FAIROs: Towards FAIR Assessment in Research Objects",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/FAIR-Research-Object",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-031-16802-4_6.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "FAIROs is open source and available on GitHub8."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-16802-4_6",
        "abstract": "Abstract. The FAIR principles have become a popular means to guideresearchers when publishing their research outputs (i.e., data, software,etc.) in a Findable, Accessible, Interoperable and Reusable manner. Inorder to ease compliance with FAIR, different frameworks have beendeveloped by the scientific community, offering guidance and sugges-tions to researchers. However, scientific outputs are rarely publishedin isolation. Research Objects have been proposed as a framework tocapture the relationships and context of all constituents of an inves-tigation. In this paper we present FAIROs, a framework for assessingthe compliance of a Research Object (and its constituents) against theFAIR principles. FAIROs reuses existing FAIR validators for individualresources and proposes i) two scoring methods for assessing the fairnessof Research Objects, ii) an initial implementation of the scoring meth-ods in the FAIROs framework, and iii) an explanation-based approachdesigned to visualize the obtained scores. We validate FAIROs against165 Research Objects, and discuss the advantages and limitations of dif-ferent scoring systems.Keywords: FAIR assessment · Research object · Aggregation methods1 IntroductionThe Findable, Accessible, Interoperable and Reusable principles (FAIR) intro-duce a set of best practices to share data, make data more reusable and sup-port the reproducibility of results in research [22]. FAIR addresses using persis-tent identifiers for resources, rich metadata to favour discovery, explicit licensesto understand usage terms and using well established vocabularies to facilitateinteroperability.Although FAIR was originally proposed for datasets, additional initiativesare appearing to apply these principles to other research outputs, such as soft-ware [15,16], ontologies [19], virtual research environments1 or digital objects [4]among others.1 https://rd-alliance.org/group/fair-virtual-research-environments-wg/case-statement/fair-virtual-research-environments-vres.c© Springer Nature Switzerland AG 2022G. Silvello et al. (Eds.): TPDL 2022, LNCS 13541, pp. 68–80, 2022.https://doi.org/10.1007/978-3-031-16802-4_6http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-16802-4_6&domain=pdfhttp://orcid.org/0000-0003-4112-6825http://orcid.org/0000-0003-0454-7145https://rd-alliance.org/group/fair-virtual-research-environments-wg/case-statement/fair-virtual-research-environments-vreshttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-statement/fair-virtual-research-environments-vreshttps://doi.org/10.1007/978-3-031-16802-4_6FAIROs: Towards FAIR Assessment in Research Objects 69Since research outputs are rarely produced in isolation, the scientific com-munity has proposed Research Objects [2,20] to capture the context around ascientific investigation. Research Objects also provide the means to pack all theresources within some research, easing its understandability and facilitating itsdissemination. However, assessing the compliance of a Research Object againstthe FAIR principles (i.e., their FAIRness) is challenging, as Research Objectsaggregate multiple resources which may be prompt to individual assessment.In this work we describe FAIROs, a framework for assessing the FAIRness ofa research investigation, modeled as Research Object. Our contributions include",
        "publication_date": "2022-01-01",
        "authors": "Esteban González, Alejandro Benítez, Daniel Garijo",
        "file_name": "978-3-031-16802-4_6.pdf",
        "file_path": "./PDFs/978-3-031-16802-4_6.pdf"
    },
    {
        "title": "Link maintenance for integrity in linked open data evolution: Literature survey and open challenges",
        "implementation_urls": [],
        "doi": "10.3233/sw-200398",
        "abstract": "Abstract. RDF data has been extensively deployed describing various types of resources in a structured way. Links betweendata elements described by RDF models stand for the core of Semantic Web. The rising amount of structured data publishedin public RDF repositories, also known as Linked Open Data, elucidates the success of the global and unified dataset proposedby the vision of the Semantic Web. Nowadays, semi-automatic algorithms build connections among these datasets by exploringa variety of similarity computation methods. Interconnected open data demands automatic methods and tools to maintain theirconsistency over time. The update of linked data is considered an key process due to the evolutionary characteristic of thesestructured datasets. However, data changing operations might influence well-formed links, which turns difficult to maintain theconsistencies of connections over time. In this paper, we propose a thorough survey that provides a systematic review of thestate-of-the-art in link maintenance in linked open data evolution scenario. We conduct a detailed analysis of the literature forcharacterising and understanding methods and algorithms responsible for detecting, fixing and updating links between structureddata. Our investigation provides a categorisation of existing approaches as well as describes and discusses existing studies. Theresults reveal an absence of comprehensive solutions suited to fully detect, warn and automatically maintain the consistency oflinked data over time.Keywords: link integrity, link maintenance, RDF evolution link integrity, link maintenance, RDF evolution1. IntroductionIn recent years, a large number of knowledge basesinterconnected on the Web have emerged describingvarious types of resources in a structured way. In par-ticular, Linked Data (LD) refers to machine-readabledata connecting datasets across the Web [1], by explor-ing Semantic Web technologies such as Resource De-scription Framework (RDF)1 and computational on-1https://www.w3.org/TR/WD-rdf-syntax-971002/tologies [2]. RDF refers to a graph-oriented data modelsuited to represent metadata about Web resources.Links between RDF descriptions are at the heart ofthe Web of Data. The growing number of structureddata published as RDF repositories in the Web con-firms the real potentiality of the global data space pro-posed by the Semantic Web vision. Indeed, connec-tions between data elements described via RDF mod-els are in the center of the Semantic Web [3]. The in-terconnection of RDF statements, via explicit links,plays a central role in this scenario to assure data link-age. The links allow previously isolated bases to be ex-plored in combination.mailto:andregregino@gmail.commailto:julio.k.r.matsoui@gmail.commailto:jreis@ic.unicamp.brmailto:rodrigo.bonacin@cti.gov.brmailto:amorshed@swin.edu.aumailto:tsellis@swin.edu.auRDF statements defining resources from the real-world are subject to change when the domain up-dates. This evolution comes from emerging numberof contributions by governments, private institutions,wiki databases (such as DBPedia)2 to create a big andwell-formed Linked Open Data (LOD), available foranyone to consume and contribute. In this scenario,RDF triples can be added or removed to keep the",
        "publication_date": "2020-09-29",
        "authors": "André Gomes Regino, Júlio Cesar dos Reis, Rodrigo Bonacin, Ahsan Morshed, Timos Sellis",
        "file_name": "swj2255.pdf",
        "file_path": "./PDFs/swj2255.pdf"
    },
    {
        "title": "Unveiling the diversity of spatial data infrastructures in Latin America: evidence from an exploratory inquiry",
        "implementation_urls": [],
        "doi": "10.1080/15230406.2020.1772113",
        "abstract": "ABSTRACTGeospatial information has transformed into a primary consumer good in our society. Worldwide, governmental organizations have contributed to this reality through Spatial Data Infrastructures (SDIs), promoting economic development, stimulating better governance, and fostering environmental sustainability. These initiatives have been settled according to three stages of development. Recently, the community is witnessing an emerging fourth stage in the SDI evolution, because of mainstream Information Technologies (IT), and where different IT trends (such as Big Data, cloud computing, or Semantic Web) are already being integrated as part of SDIs worldwide. Nevertheless, their implementation states change among countries according to the initial context and the Digital Divide. Therefore, it is essential to understand the adoption process characteristics, and especially in Latin America, where no previous evidence about the state of geospatial IT trend adoption is available. This paper contributes to an exploratory inquiry for unveiling the diversity of the fourth SDI generation in the Latin American SDI community. For that, we used an open- regional web-based survey, which was divided into four blocks (trend adoption, standardization, monitoring, and future vision and limitations) and performed three times (2014, 2017, 2018/2019). The results depicted the initial presence of IT trends with a heterogeneity scenario. In this way, some initiatives are exploring the fourth generation, while others remain at different stages between the first and third generations. This work pointed out the need for a regional observatory to allow designing policies and action plans associated with the progress of the fourth SDI generation in Latin America.ARTICLE HISTORY Received 27 October 2019  Accepted 18 May 2020 KEYWORDS Spatial Data Infrastructure; IT trends; exploratory inquiry; fourth generation SDI; Latin AmericaIntroductionPeople have relied on maps and geographic information for survival, economic development, and recreation for millennia (Kelmelis, 2003). Recently, geospatial information has transformed into a primary consumer good in our society. People make spatio-temporal decisions every day, for instance, by choosing the best route to the mall or to an appointment given the traffic patterns at a particular time of the day (Meeks & Dasgupta, 2004). This new reality has been possible due to the Digital Earth explosion (Craglia et al., 2012; Goodchild et al., 2012; Li et al., 2014), the fast application of Information and Communication Technologies (ICTs) and the increase of public access and demand to geospatial information and cartography (Roose & Kull, 2012).Governmental organizations around the world have contributed to this scenario through Spatial Data Infrastructures (SDIs), investing millions of dollars in their implementations (Coleman et al., 2016). The SDIs are defined by a set of policies, technologies and institutional arrangements leveraging the provision and use of standardized spatial data and services (Bishop ",
        "publication_date": "2020-07-10",
        "authors": "Luis M. Vilches‐Blázquez, Daniela Ballari",
        "file_name": "anexploratoryinquiry-1.pdf",
        "file_path": "./PDFs/anexploratoryinquiry-1.pdf"
    },
    {
        "title": "Fostering trust with transparency in the data economy era",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/ppop",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/3565011.3569061.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The templates are available at https://github.com/besteves4/ppop/tree/main/templates."
                    }
                ]
            }
        ],
        "doi": "10.1145/3565011.3569061",
        "abstract": "ABSTRACTWhy is it hard for online users to trust service providers when itcomes to their personal data? While users might give away theirdata when using their services, this does not mean that they nec-essarily trust these companies. Building trust in online services isparticularly relevant as digital economy policy strategies, such asthe EU Data Strategy, deposit a considerable amount of faith in thebenefits of a data-driven society. To achieve this goal, transparencyshould be considered a necessary feature, on which trust can bebuilt. According to scholarly literature, the more information pro-vided to data subjects, the less power asymmetry, caused by a lackof knowledge, between them and data controllers will exist. In thisrespect, transparency around data processing has been, and stillis, conveyed through privacy notices. But these are far from beingused as helpful tools to navigate complex data-intensive environ-ments. Technical developments, such as Solid personal datastores,provide a fertile ground for the negotiation of privacy terms be-tween the involved parties. But to do so, it is necessary to haveclear and transparent processing conditions. However, while cer-tain specifications have been developed to accommodate for therepresentation of privacy terms, there is still a lack of developedsolutions to address this problem. With this in mind, we proposethe usage of the Privacy Paradigm ODRL Profile (PPOP), whichextends ODRL and DPV to specify data processing requirementsfor personal datastores envisaged as key core elements of the dataeconomy. To demonstrate the usage of PPOP, a set of policy exam-ples will be provided, as well as a prototype implementation of agenerator of machine and human-readable PPOP policies.CCS CONCEPTS• Information systems → World Wide Web; Ontologies; • Secu-rity and privacy→Human and societal aspects of securityand privacy; Access control; Social aspects of security and privacy;Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.DE ’22, December 9, 2022, Roma, Italy© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9923-4/22/12. . . $15.00https://doi.org/10.1145/3565011.3569061Privacy protections; • Applied computing→ Law; • Social andprofessional topics → Centralization / decentralization; Cen-tralization / decentralization; Privacy policies.KEYWORDStrust, transparency, data economy, data protection, ethics, knowl-edge engineering, personal information management systemsACM Reference Format:",
        "publication_date": "2022-12-01",
        "authors": "Beatriz Esteves, Haleh Asgarinia, Andrés Chomczyk Penedo, Blessing Mutiro, David Lewis",
        "file_name": "3565011.3569061.pdf",
        "file_path": "./PDFs/3565011.3569061.pdf"
    },
    {
        "title": "Automatic Topic Label Generation using Conversational Models",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.8018043",
                "type": "zenodo",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1145/3587259.3627574",
        "abstract": "AbstractProbabilistic topic modelling is an unsupervised machine learning technique that, given a set ofdocuments, is capable of scanning, detecting patterns of words and phrases, and automaticallygrouping words that best characterize a topic. Many times, however, we are interested in know-ing what relates these documents beyond the most characteristic patterns or sets of words inthe set. Consequently, the generation of topic labels appeared, which sought to generate a labelthat would characterize the set of documents in a more interpretable way than having a groupof words that we, a priori, do not know the relationship they have with each other. Currently,new ways of generating these topic labels that are easily understandable automatically are stillbeing investigated.At the same time, Neural Language Models based on Neural Networks with conversationalpurpose have recently emerged, which are trained to understand and generate dialogues be-tween humans and machines. These models possess capabilities beyond the ability to engage inconversation, such as ChatGPT, which has demonstrated the ability to autonomously composeemails or write about a specific topic, for example.Conversational models present an apparent potential to not only have recreational applica-tions, but can also be useful for other tasks, as stated in Sallam’s publication \"ChatGPT Utilityin Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspec-tives and Valid Concerns\" [1], where the author analysed 60 publications that talked about thebenefits of using ChatGPT in different tasks, such as efficient analysis of datasets or code gen-eration for health care research. Given this fact, the purpose of this Final Master’s Project is tostudy the capacity that conversational models may have to automatically and unsupervisedlygenerate tags for probabilistic topics given a set of keywords representative of the topic, follow-ing a methodology which we will refer as Conversational Probabilistic Topic Labelling (CPTL).We also compare the performance of these conversational models with the performance of atask-specific language model trained to generate topic labels.iiiContents1 Introduction 12 Related work 52.1 Probabilistic topic modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52.2 Topic label generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1 Supervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1.1 Term lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92.2.1.2 Term hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . 102.2.1.3 External knowledge sources . . . . . . . . . . . . . . . . . . . . . 102.2.2 Unsupervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112.3 Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 Approach 153.1 Topic Labelling system based on Conversational Models . . . . . . . . . . . . . . 154 Evaluation 194.1 Modules selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.2 Question templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.1.3 Topic words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.1.4 QA models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.2 Modules evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274.2.1 Top words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2 Language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . 28",
        "publication_date": "2023-11-28",
        "authors": "Virginia Ramón-Ferrer, Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": "10!1145%3587259!3627574.pdf",
        "file_path": "./PDFs/10!1145%3587259!3627574.pdf"
    },
    {
        "title": "“Who Should I Trust with My Data?” Ethical and Legal Challenges for Innovation in New Decentralized Data Management Technologies",
        "implementation_urls": [],
        "doi": "10.3390/info14070351",
        "abstract": "Abstract: News about personal data breaches or data abusive practices, such as Cambridge Analytica,has questioned the trustworthiness of certain actors in the control of personal data. Innovations inthe field of personal information management systems to address this issue have regained traction inrecent years, also coinciding with the emergence of new decentralized technologies. However, onlywith ethically and legally responsible developments will the mistakes of the past be avoided. Thiscontribution explores how current data management schemes are insufficient to adequately safeguarddata subjects, and in particular, it focuses on making these data flows transparent to provide anadequate level of accountability. To showcase this, and with the goal of enhancing transparency tofoster trust, this paper investigates solutions for standardizing machine-readable policies to expresspersonal data processing activities and their application to decentralized personal data stores as anexample of ethical, legal, and technical responsible innovation in this field.Keywords: data governance; digital age; transparency; personal data management; identity management1. IntroductionData-driven innovations are expected to deliver further economic and societal develop-ment [1]. Through the analysis, sharing, and (re-)use of data, business models and govern-ments’ processes have been transformed to benefit from those practices [2]. The emergenceof a data-driven society is being fostered by policy actions from different governments on aworldwide scale. The European Union (EU) is no exception to this, as the European Com-mission has put on its agenda the development of “A Europe fit for the Digital Age”. The Eu-ropean Commission’s strategy and related policy documents can be located at the followinglink: https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_en(accessed on 26 May 2023). Regardless of whether it is a Big Tech company based in theUnited States (US), a large data broker in the EU, or a Chinese government-controlledentity, current data practices have been questioned by different societal sectors, from in-dividuals to nongovernmental organizations (NGOs) or from academics to governments.Trust in many digital services has been compromised [3], which has left individuals askingthemselves “who should I trust with my data”.In response to this trust crisis, technology has been looked upon to provide answers.Applied to the field of (personal) data, self-sovereign identity models [4] — as improve-ments over existing Personal Information Management Systems (PIMS) — have been putunder the spotlight due to their potential, but they are also taken with “a grain of salt”, asInformation 2023, 14, 351. https://doi.org/10.3390/info14070351 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14070351https://doi.org/10.3390/info14070351https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-2344-4061https://orcid.org/0000-0002-6820-999Xhttps://orcid.org/0000-0003-0259-7560https://orcid.org/0000-0002-3503-4644https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_enhttps://doi.org/10.3390/info14070351https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14070351?type=check_update&version=1Information 2023, 14, 351 2 of 17they are not free from shortcomings [5]. Through them, users would be in direct control oftheir information and decide when, how, and who can access such information. Certain",
        "publication_date": "2023-06-21",
        "authors": "Haleh Asgarinia, Andrés Chomczyk Penedo, Beatriz Esteves, David Lewis",
        "file_name": "no_doi_20250624163301.pdf",
        "file_path": "./PDFs/no_doi_20250624163301.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/14/7/351/pdf?version=1687339409"
    },
    {
        "title": "Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature",
        "implementation_urls": [
            {
                "identifier": "https://github.com/drugs4covid/knowledge-acquisition",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1016%j!jbi!2023!104382.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of our pre-processing pipeline is available online.14 5."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.jbi.2023.104382",
        "abstract": "to facilitate the use of the KG by end user. This interface abstractsthe complexity of formal query languages to simplify the retrieval ofinformation from the generated KG.https://drugs4covid.oeg.fi.upm.es/rdfhttps://drugs4covid.oeg.fi.upm.es/sparqlhttps://drugs4covid.oeg.fi.upm.es/services/bio-qaJournal of Biomedical Informatics 142 (2023) 104382C. Badenes-Olmedo and O. CorchoFig. 1. Workflow proposed for the creation of KGs from scientific publications.The following sections detail each of the steps in the context ofthe Drugs4COVID initiative: how have they been addressed, whatchallenges have arisen and what are our recommendations to build aknowledge graph from biomedical literature on coronavirus.3. Harvesting research literature on coronavirus (step 1)In this first stage, the initial objective is to identify the most appro-priate sources of information and validate the availability of their data.Once they have been discovered, the data is downloaded and furtherdownloads are planned, depending on the periodicity of the changes.We performed a systematic literature review to identify the datasetsused when creating coronavirus knowledge graphs [13]. The digitalrepositories that were explored were Pubmed, BioRxiv, MedRxiv andScopus. Preprint databases were also considered to explore works pend-ing of publication. The searches were conducted by searching terms intitle or abstract. Specifically, the inclusion criteria can be summarizedas follows:• IC1: Text in English or Spanish.• IC2: The words ‘‘covid’’/’’coronavirus’’/’’SARS-Cov’’ (regardlessof upper or lower case) appears in either title or abstract.• IC3: Text published between 2019 and 2022.• IC4: The words ‘‘Knowledge Graph’’ (regardless of upper or lowercase, or abbreviations) appear in either title or abstract.• IC5: Text in the biomedical/computational biology/computerscience fields.Metadata of the documents returned in the search, such as title, doi,authors, venue, etc, was downloaded as csv files from the repositories.A total of 218 documents were retrieved. Then, they were scannedfor duplicates by looking at duplicates either in the title or in theDOI through an R script. 13 duplicated articles were removed so 205documents were left for further studying. Manual filtering was thencarried out by removing duplicates and by a manual reviewing of thetitles and abstracts in order to discard articles out of the scope of thisreview. The availability of the datasets mentioned in the articles wasalso checked. By following these criteria, from the 205 documents, only14 texts were selected and subject of a narrower reading. Althoughselected, the texts could still be out of the scope of this review dueto low information provided about the dataset used.The sources for input covid publication data were CORD-19 cor-pus (50%), Pubmed (14%), LitCovid (14%), Targeting2019-nCoV (byGHDDI) (7%), University of Luxembourg Covid corpus (7%) and Eu-ropePMC (7%). Moreover, additional unstructured data sources that donot contain covid-19 publications were also considered, these include",
        "publication_date": "2023-05-06",
        "authors": "Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": "10!1016%j!jbi!2023!104382.pdf",
        "file_path": "./PDFs/10!1016%j!jbi!2023!104382.pdf"
    },
    {
        "title": "Morph-Skyline: Virtual Ontology-Based Data Access for Skyline Queries",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/morph-skyline",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Morph-Skyline_Virtual_Ontology-Based_Data_Access_for_Skyline_Queries.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All the resources used are online5,6."
                    }
                ]
            }
        ],
        "doi": "10.1109/wiiat50758.2020.00043",
        "abstract": "Abstract—Skyline queries are being used in decision-makingapplications to help stakeholders find the set of data thatsatisfies certain criteria, whose weight may not be assignedbeforehand. Given the wide availability of heterogeneous datasetsthat are being published following Open Data initiatives, com-bining skyline queries with query processing approaches suchas Ontology-Based Data Access (OBDA), may help stakeholdersto improve their decisions exploiting and integrating multipleand heterogeneous data sources. In this paper, we address theproblem of evaluating SPARQL skyline queries over an OBDAapproach. Our approach implements two different techniques:rewriting skyline queries into SPARQL 1.0 and then translatingto SQL, or translating them directly into queries that can beevaluated by the relational database. Our experimental resultssuggest that the execution time can be reduced by up two ordersof magnitude in comparison to current approaches scaling up tolarger datasets while identifying precisely the skyline set.Index Terms—Skyline, OBDA, Query translation, R2RMLI. INTRODUCTIONIn databases, a skyline is defined as a set of tuples whichstand out among the others because they are of specialinterest for a specific type of problem [1]. For instance, ina database related to retail units in shopping centers, wemay be interested in understanding which are the retail unitswhere more people pass by and/or that have larger amountsof shopping transactions per day and/or while being amongthe smallest ones. From a more formal point of view, givena dominance relationship in a multidimensional dataset, askyline is defined as the set of points that are not dominatedby any other, where a point is considered to dominate anotherone if it is as good or better in all dimensions and better in atleast one dimension [2]. These queries are relevant in manymulti-criteria decision making applications [1]. They can helpstakeholders to make better decisions when multiple criteriaover data are expressed (e.g., footfall, shopping transactions).More specifically, they are really relevant when stakeholderscan not predefine a score function based on these criteria, andall of them are equally important.Skyline queries have been widely studied in relationaldatabases (RDB) extending SQL with the skyline operator [2].Exploiting the benefits of having a well designed RDB, includ-ing typical constraints over its schema, different algorithms [1]are proposed to efficiently obtain a high-quality skyline setfrom an input query. In the context of the knowledge graphs,extensions of the SPARQL query language to incorporateuser qualitative preferences have been also studied [3]–[5].Qualitative preferences allow arbitrary comparisons betweenthe values in tuples while the skyline entails combinationsof totally ordered comparisons between these values. Theseapproaches do not provide a native operator over RDF but they",
        "publication_date": "2020-12-01",
        "authors": "Marlene Goncalves, David Chaves-Fraga, Óscar Corcho",
        "file_name": "Morph-Skyline_Virtual_Ontology-Based_Data_Access_for_Skyline_Queries.pdf",
        "file_path": "./PDFs/Morph-Skyline_Virtual_Ontology-Based_Data_Access_for_Skyline_Queries.pdf"
    },
    {
        "title": "Phenotypes of non-alcoholic fatty liver disease (NAFLD) and all-cause mortality: unsupervised machine learning analysis of NHANES III",
        "implementation_urls": [],
        "doi": "10.1136/bmjopen-2022-067203",
        "abstract": "ABSTRACTObjectives  Non-alcoholic fatty liver disease (NAFLD) is a non-communicable disease with a rising prevalence worldwide and with large burden for patients and health systems. To date, the presence of unique phenotypes in patients with NAFLD has not been studied, and their identification could inform precision medicine and public health with pragmatic implications in personalised management and care for patients with NAFLD.Design  Cross-sectional and prospective (up to 31 December 2019) analysis of National Health and Nutrition Examination Survey III (1988–1994).Primary and secondary outcomes measures  NAFLD diagnosis was based on liver ultrasound. The following predictors informed an unsupervised machine learning algorithm (k-means): body mass index, waist circumference, systolic blood pressure (SBP), plasma glucose, total cholesterol, triglycerides, liver enzymes alanine aminotransferase, aspartate aminotransferase and gamma glutamyl transferase. We summarised (means) and compared the predictors across clusters. We used Cox proportional hazard models to quantify the all-cause mortality risk associated with each cluster.Results  1652 patients with NAFLD (mean age 47.2 years and 51.5% women) were grouped into 3 clusters: anthro-SBP-glucose (6.36%; highest levels of anthropometrics, SBP and glucose), lipid-liver (10.35%; highest levels of lipid and liver enzymes) and average (83.29%; predictors at average levels). Compared with the average phenotype, the anthro-SBP-glucose phenotype had higher all-cause mortality risk (aHR=2.88; 95% CI: 2.26 to 3.67); the lipid-liver phenotype was not associated with higher all-cause mortality risk (aHR=1.11; 95% CI: 0.86 to 1.42).Conclusions  There is heterogeneity in patients with NAFLD, whom can be divided into three phenotypes with different mortality risk. These phenotypes could guide specific interventions and management plans, thus advancing precision medicine and public health for patients with NAFLD.INTRODUCTIONThe epidemiology of non-communicable diseases (NCDs) is largely driven by cardiometabolic risk factors and diseases, namely dyslipidaemias, type 2 diabetes mellitus (T2DM), hypertension and cardio-vascular diseases. Nonetheless, there are other NCDs rapidly growing along with, and as a consequence of,1 2 the afore-mentioned cardiometabolic conditions. Non-alcoholic fatty liver disease (NAFLD) is an outstanding ",
        "publication_date": "2022-11-01",
        "authors": "Rodrigo M. Carrillo‐Larco, Wilmer Cristobal Guzman‐Vilca, Manuel Castillo‐Cara, Claudia Alvizuri-Gómez, Saleh A. Alqahtani, Vanessa Garcia‐Larsen",
        "file_name": "no_doi_20250624163330.pdf",
        "file_path": "./PDFs/no_doi_20250624163330.pdf",
        "pdf_link": "https://bmjopen.bmj.com/content/bmjopen/12/11/e067203.full.pdf"
    },
    {
        "title": "Bilingual Dataset for Information Retrieval and Question Answering over the Spanish Workers Statute",
        "implementation_urls": [
            {
                "identifier": "https://github.com/NLP2RDF/ontologies",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/097.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The dataset with the rest of the documents used in this paper is published online1 having followed the FAIR principles (findable, accessible, interoperable, reusable)."
                    }
                ]
            }
        ],
        "doi": "10.5281/zenodo.4256718",
        "abstract": "Abstract—A bilingual dataset of questions and an-swers over a key document in Spanish labor law legisla-tion, the Workers Statute, is presented. The documentcontains 150 questions and their respective answersin the form of one part number from the 130 partsin which the Workers Statute is divided (articles andother provisions), and with the most relevant excerptof information for the answer. A simple system toretrieve the answers using standard technologies is alsodescribed, providing baseline numbers for accuracy inthis task. This dataset may be of interest for researchersin the area of Q&A over legal documents. Both thequestion and answers are available in English andSpanish languages.Index Terms—Dataset, Q&A, Information Retrieval,Labor Law, Knowledge GraphI. IntroductionInformation Retrieval and Question Answering havebecome core tasks in the so called knowledge-basedsociety we live in. Search engines are our best allieswhen searching for information, regardless of topic orlevel of expertise. As described by Manning [5], Infor-mation Retrieval (IR) used to be an activity that onlya few people engaged in such as reference librarians,paralegals, and similar professional searchers. Now theworld has changed, and hundreds of millions of peopleengage in information retrieval when they use a websearch engine.In the legal domain, Information Retrieval and Ques-tion Answering take a substantial part in the daily workof lawyers when performing domain-specific searchesThis work has been supported by the Lynx project, which hasreceived funding from the European Union’s Horizon 2020 researchand innovation programme under grant agreement No 780602.due to the large amounts of text-based informationgenerated in this area. Such a specific area reliesheavily on the use of the appropriate terms and therelations between them as established in the searchqueries.However, research improvements could not be possi-ble if there are not evaluations to measure the perfor-mance of the new techniques, algorithms and systems.Particularly in the legal domain, there are a lot ofscenarios in which particular use cases have to beachieved. Moreover, legal information is subject toconstant changes and updates, and, in the Europeancontext, dependant of each jurisdiction.In this paper, a new dataset for Information Retrievaland Question Answering has been created in collabo-ration with domain experts from the Spanish law-firm",
        "publication_date": "2020-11-07",
        "authors": "Pablo Calleja Ibáñez, Patricia Martín-Chozas, Elena Montiel-Ponsoda, Víctor Rodríguez-Doncel, Elsa Gómez, Pascual Boil",
        "file_name": "097.pdf",
        "file_path": "./PDFs/097.pdf"
    },
    {
        "title": "Packaging research artefacts with RO-Crate",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ResearchObject/ro-crate",
                "type": "git",
                "paper_frequency": 7,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%ds-210053.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "See https://github.com/ResearchObject/ro-crate/issues/82."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5146227",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.3233/ds-210053",
        "arxiv": "2108.06503",
        "abstract": "Abstract. An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets,software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to processby automated systems. In this paper we introduce RO-Crate, an open, community-driven, and lightweight approach to packagingresearch artefacts along with their metadata in a machine readable manner. RO-Crate is based on Schema.org annotations inJSON-LD, aiming to establish best practices to formally describe metadata in an accessible and practical way for their use in awide variety of situations.An RO-Crate is a structured archive of all the items that contributed to a research outcome, including their identifiers,provenance, relations and annotations. As a general purpose packaging approach for data and their metadata, RO-Crate is usedacross multiple areas, including bioinformatics, digital humanities and regulatory sciences. By applying “just enough” LinkedData standards, RO-Crate simplifies the process of making research outputs FAIR while also enhancing research reproducibility.An RO-Crate for this article1 is archived at https://doi.org/10.5281/zenodo.5146227.Keywords: Data publishing, data packaging, FAIR, Linked Data, metadata, reproducibility, research object1. IntroductionThe move towards Open Science has increased the need and demand for the publication of artefactsof the research process [104]. This is particularly apparent in domains that rely on computational ex-periments; for example, the publication of software, datasets and records of the dependencies that suchexperiments rely on [113].It is often argued that the publication of these assets, and specifically software [80], workflows [55] anddata, should follow the FAIR principles [123]; namely, that they are Findable, Accessible, Interoperableand Reusable. These principles are agnostic to the implementation strategy needed to comply with them.Hence, there has been an increasing amount of work in the development of platforms and specificationsthat aim to fulfil these goals [91].Important examples include data publication with rich metadata (e.g. Zenodo [40]), domain-specificdata deposition (e.g. PDB [16]) and following practices for reproducible research software [101] (e.g.use of containers). While these platforms are useful, experience has shown that it is important to putgreater emphasis on the interconnection of the multiple artefacts that make up the research process [71].The notion of Research Objects [12] (RO) was introduced to address this connectivity, providingsemantically rich aggregations of (potentially distributed) resources with a layer of structure over aresearch study; this is then to be delivered in a machine-readable format.A Research Object combines the ability to bundle multiple types of artefacts together, such as spread-sheets, code, examples, and figures. The RO is augmented with annotations and relationships that de-scribe the artefacts’ context (e.g. a CSV being used by a script, a figure being a result of a workflow).*Corresponding author. E-mail: soiland-reyes@manchester.ac.uk.1https://w3id.org/ro/doi/10.5281/zenodo.5146227https://orcid.org/0000-0002-4763-3943https://orcid.org/0000-0002-1112-1292https://doi.org/10.5281/zenodo.5146227mailto:soiland-reyes@manchester.ac.ukhttps://w3id.org/ro/doi/10.5281/zenodo.5146227S. Soiland-Reyes et al. / Packaging research artefacts with RO-Crate 99This notion of ROs provides a compelling vision as an approach for implementing FAIR data. How-ever, existing Research Object implementations require a large technology stack [14], are typically tai-lored to a particular platform and are also not easily usable by end-users.To address this gap, a new community came together [23] to develop RO-Crate – an approach topackage and aggregate research artefacts with their metadata and relationships. The aim of this paperis to introduce RO-Crate and assess it as a strategy for making multiple types of research artefacts FAIR.Specifically, the contributions of this paper are as follows:1. An introduction to RO-Crate, its purpose and context;2. A guide to the RO-Crate community and tooling;3. Examples of RO-Crate usage, demonstrating its value as connective tissue for different artefacts",
        "publication_date": "2022-01-04",
        "authors": "Stian Soiland‐Reyes, Peter Sefton, Mercè Crosas, Leyla Jael Castro, Frederik Coppens, José M. Fernández, Daniel Garijo, Björn Grüning, M. Rosa, Simone Leo, Eoghan Ó Carragáin, Marc Portier, A. Trisovic, RO-Crate Community, Paul Groth, Carole Goble",
        "file_name": "10!3233%ds-210053.pdf",
        "file_path": "./PDFs/10!3233%ds-210053.pdf"
    },
    {
        "title": "Automatic Easy-to-Read Translation of Morphological Structures in Spanish Texts",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.7990932",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.26342/2023-71-15",
        "abstract": "Abstract: The Easy-to-Read (E2R) Methodology was created to improve the dailylife of people with cognitive disabilities. This methodology aims to present clear andeasily understood documents by means of providing a set of guidelines related to thewriting and layout of texts. Some of these guidelines focus on morphological aspectsthat may cause difficulties in reading comprehension. Examples of those guidelinesare: (a) to avoid the use of adverbs ending in -mente (-ly in English), and (b) to avoidthe use of superlative forms. Currently, the E2R guidelines are applied manually tocreate easy-to-read text materials. To help in such a manual process, our researchline is focused on applying the E2R Methodology in an automatic fashion to improvecognitive accessibility. Specifically, in this paper we present (a) the inclusive designapproach for the development of E2R translation methods for avoiding both adverbsending in -mente and superlative forms, and (b) the initial rule-based methods foradapting those linguistic structures into an Easy-to-Read form.Keywords: Easy-to-Read, Cognitive Accessibility, Automatic Translation.Resumen: La Metodoloǵıa de Lectura Fácil (LF) tiene como objetivo presentardocumentos claros y de fácil comprensión para personas con discapacidad cognitiva.Para ello, incluye una serie de pautas relacionadas con la redacción y disposiciónde textos. Algunas de estas pautas se centran en aspectos morfológicos que puedencausar dificultades en la comprensión lectora, como (a) evitar el uso de adverbiosen -mente y (b) evitar el uso de formas superlativas. Las pautas de LF se aplicanmanualmente, por lo que, para ayudar en este proceso, nuestra ĺınea de investigaciónse centra en aplicar la Metodoloǵıa de LF de forma automática con el fin de mejorarla accesibilidad cognitiva en textos en español. Aśı, en este trabajo presentamos(a) un enfoque de diseño inclusivo en el desarrollo de métodos de adaptación de LFpara evitar adverbios en -mente y superlativos, y (b) dos métodos iniciales basadosen reglas para adaptar dichas estructuras lingǘısticas a versiones en lectura fácil.Palabras clave: Lectura fácil, Accesibilidad cognitiva, Traducción automática.1 IntroductionThe Easy-to-Read (E2R) Methodology (In-clusion Europe, 2009; AENOR, 2018; No-mura et al., 2010) was created to improvethe daily life of people with cognitive dis-abilities and different sectors of the popula-tion who present some difficulties related toreading comprehension processes. The goalof this methodology is to present clear andeasily understood by providing a collectionof guidelines on both the content and the de-sign and layout of written materials, as, forinstance, to use short and simple sentences,to avoid the use of long words, or to divideideas into paragraphs. At the present time,the E2R methodology is applied in a man-ual fashion in the processes of (a) adaptingexisting documents and (b) producing newmaterials. The manual adaptation of doc-uments is an iterative process and involvesProcesamiento del Lenguaje Natural, Revista nº 71, septiembre de 2023, pp. 191-203 recibido 31-03-2023 revisado 14-05-2023 aceptado 17-05-2023ISSN 1135-5948 DOI 10.26342/2023-71-15 ©2023 Sociedad Española para el Procesamiento del Lenguaje Naturalthree key activities: E2R analysis, E2R adap-",
        "publication_date": "2023-09-01",
        "authors": "Mari Carmen Suarez-Figueroa, Isam Diab, Alvaro Gonzalez Sanz, Jesica Rivero-Espinosa",
        "file_name": "PLN_71_15.pdf",
        "file_path": "./PDFs/PLN_71_15.pdf"
    },
    {
        "title": "Astrea: Automatic Generation of SHACL Shapes from Ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Astrea",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-030-49461-2_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Furthermore, the code of the tool is also available in GitHub6 under the Apache 2.0 licence7."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.3571009",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-49461-2_29",
        "abstract": "Abstract. Knowledge Graphs (KGs) that publish RDF data modelledusing ontologies in a wide range of domains have populated the Web. TheSHACL language is a W3C recommendation that has been endowed toencode a set of either value or model data restrictions that aim at validat-ing KG data, ensuring data quality. Developing shapes is a complex andtime consuming task that is not feasible to achieve manually. This arti-cle presents two resources that aim at generating automatically SHACLshapes for a set of ontologies: (1) Astrea-KG, a KG that publishes a setof mappings that encode the equivalent conceptual restrictions amongontology constraint patterns and SHACL constraint patterns, and (2)Astrea, a tool that automatically generates SHACL shapes from a setof ontologies by executing the mappings from the Astrea-KG. These tworesources are openly available at Zenodo, GitHub, and a web application.In contrast to other proposals, these resources cover a large number ofSHACL restrictions producing both value and model data restrictions,whereas other proposals consider only a limited number of restrictionsor focus only on value or model restrictions.Keywords: SHACL shapes · RDF validation · OntologyResource type: Dataset & SoftwareAstrea-KG: http://astrea.helio.linkeddata.es/Astrea-KG DOI: https://doi.org/10.5281/zenodo.3571009Astrea application: http://astrea.linkeddata.es/1 IntroductionKnowledge Graphs (KGs) are becoming pervasive on the Web [5]. Since 2014there is a growing number of KGs from different domains that publish a quitelarge amount of data using RDF and modelled with ontologies [19]. As a result,in the last decade a considerable effort has been put in developing ontologiesfor specific domains [21]. Due to the growth of these public available KGs, theW3C has promoted a recommendation called SHACL (Shapes Constraint Lan-guage) to validate the RDF graphs [2]. In the last years KGs validation by meansof SHACL shapes has gained momentum and has become a relevant researchtopic [14].c© Springer Nature Switzerland AG 2020A. Harth et al. (Eds.): ESWC 2020, LNCS 12123, pp. 497–513, 2020.https://doi.org/10.1007/978-3-030-49461-2_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-49461-2_29&domain=pdfhttp://orcid.org/0000-0002-1823-4484http://orcid.org/0000-0003-2011-3654http://orcid.org/0000-0002-0421-452Xhttp://astrea.helio.linkeddata.es/https://doi.org/10.5281/zenodo.3571009http://astrea.linkeddata.es/https://doi.org/10.1007/978-3-030-49461-2_29498 A. Cimmino et al.A shape defines a set of restrictions that data from a KG must fulfil. There aretwo kinds of restrictions [15], those that refer to the data model, e.g., cardinality,and those that apply to the data values, e.g., string patterns. Due to this reasondeveloping shapes has become the cornerstone solution to validate KG data.Nevertheless, developing data shapes is a complex task due to the potentialsize of the data and all the available restrictions that require a deep domain",
        "publication_date": "2020-01-01",
        "authors": "Andrea Cimmino, Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "10!1007%978-3-030-49461-2_29.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-49461-2_29.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-49461-2_29.pdf"
    },
    {
        "title": "Event Extraction and Semantic Representation from Spanish Workers’ Statute Using Large Language Models",
        "implementation_urls": [],
        "doi": "10.3233/faia230983",
        "abstract": "Abstract. This work uses Large Language Models to process an important piece ofSpanish legislation: the Workers’ Statute. The proposed method extracts the rele-vant events in its articles using a GPT-3.5 model and represents the entities involvedin the events and the relationships between them as RDF triples. The experimentscarried out to select a high-performance strategy include both zero- and few-shotlearning tests. Finally, this work proposes a strategy to uplift the extracted legalrelations into a legal knowledge graph.Keywords. Spanish Workers’ Statute, Large Language Models, Knowledge Graph,Legal Domain, Event Extraction1. IntroductionThe legal domain is a complex and dynamic field that involves interpreting and applyinglaws and regulations. Legal data (court cases, legislations, contracts, etc.) is becominga valuable source to push forward intelligent legal tools [1]. This work proposes an ap-proach using event extraction and semantic graph modeling to bring these systems closerto the public. The event extraction task is being tackled in the state-of-the-art using deeplearning models. However, it presents numerous challenges, especially for Spanish texts,including ambiguity, polysemy, and domain-specific terminology [2].The recent development of Large Language Models (LLMs) [3, 4] has proven tobe an excellent approach to mitigate these problems and an important tool to deal withlimited data [5, 6] through natural language instructions, called prompts.This research aims to improve the performance of the event extraction task withinthe legal domain and to link the information into a semantic graph representation. Thedata to be used will be the Spanish Workers’ Statute, given its importance for legislatorsand the general public, and the availability of an annotated corpus of 133 sentences fromthe Statute gathered by Revenko and Martı́n-Chozas [7]. The low amount of tagged datawill be tackled using the GPT-3.5 model, as it has been proven the high performanceon Natural Language Processing (NLP) tasks like event extraction [3, 5]. Finally, theextracted events will be represented in a knowledge graph.Legal Knowledge and Information SystemsG. Sileno et al. (Eds.)© 2023 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA2309833292. Related WorkThis work defines an event as a textual region likely to compact relevant legal informationencapsulated by the articles on the law. The most common event structure is formed by anevent mention, an event trigger, an event argument, and an argument role. The argumentrole is the relationship between an argument and the event it participates in. The basicargument roles are subject, object, and complement. To classify the legal relations, manyworks [7, 8] use the Hohfeldian classes Right, Duty, No-Right, and Privilege [9].State-of-the-art event extraction relies nowadays on deep learning models like graphneural networks (GAN) [10] and attention mechanisms [11]. However, these models relyon huge amounts of labeled data to improve the model’s performance and are mainly usedfor English corpora. This research uses only 133 annotated sentences from the SpanishWorkers’ Statute [7], which are insufficient to achieve high-performance models.To tackle this issue, common approaches use data augmentation techniques [5],transfer learning [7], and active learning [12]. In recent years, language models (LM)have also been used for this and other NLP tasks [6, 3]. In 2021, the work [13] presented",
        "publication_date": "2023-12-07",
        "authors": "Gabriela Argüelles Terrón, Patricia Martín Chozas, Victor Rodrı́guez-Doncel",
        "file_name": "10!3233%faia230983.pdf",
        "file_path": "./PDFs/10!3233%faia230983.pdf"
    },
    {
        "title": "Inspect4py",
        "implementation_urls": [
            {
                "identifier": "https://github.com/apache/airflow",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/Inspect4py_A_Knowledge_Extraction_Framework_for_Python_Code_Repositories.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "8Benchmark is available at: https://doi.org/10.5281/zenodo.5907936 9https://paperswithcode.com/10https://github.com/apache/airflow 11https://github.com/astropy/astropy Software type Precision Recall F1-score Package 1 0.916 0.956 Library 0.93 1 0.9637 Service 1 1 1 Script 0.967 0.967 0.967 Table 1: Results for software type classification."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5907936",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1145/3524842.3528497",
        "abstract": "ABSTRACTThis work presents inspect4py, a static code analysis frameworkdesigned to automatically extract the main features, metadata anddocumentation of Python code repositories. Given an input folderwith code, inspect4py uses abstract syntax trees and state of theart tools to find all functions, classes, tests, documentation, callgraphs, module dependencies and control flows within all code filesin that repository. Using these findings, inspect4py infers differ-ent ways of invoking a software component. We have evaluatedour framework on 95 annotated repositories, obtaining promisingresults for software type classification (over 95% F1-score). Withinspect4py, we aim to ease the understandability and adoption ofsoftware repositories by other researchers and developers.Code: https://github.com/SoftwareUnderstanding/inspect4pyDOI: https://doi.org/10.5281/zenodo.5907936License: Open (BSD3-Clause)CCS CONCEPTS• General and reference→ Surveys and overviews; • Appliedcomputing→ Document capture; Document analysis.KEYWORDSCode mining, software code, software classification, software docu-mentation, code understandingACM Reference Format:Rosa Filgueira and Daniel Garijo. 2022. Inspect4py: A Knowledge ExtractionFramework for PythonCode Repositories. In 19th International Conference onMining Software Repositories (MSR ’22), May 23–24, 2022, Pittsburgh, PA, USA.ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3524842.35284971 INTRODUCTIONAn increasing number of research results depend on software, inareas ranging from High-Energy Physics [15] to ComputationalBiology [12]. Research software is used to clean and analyze data,simulate real systems or visualize scientific results [3].In the last years, research software has become a subject ofinterest for the scientific community for two main reasons. First,software itself has become a research topic, with multiple researchPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00https://doi.org/10.1145/3524842.3528497challenges such as using efficient code representations [14] forfunction similarity [7] or generating documentation from code [4].Second, the importance of software for Science has promoted theworldwide adoption of the Findable, Accessible, Interoperable and",
        "publication_date": "2022-05-23",
        "authors": "Rosa Filgueira, Daniel Garijo",
        "file_name": "Inspect4py_A_Knowledge_Extraction_Framework_for_Python_Code_Repositories.pdf",
        "file_path": "./PDFs/Inspect4py_A_Knowledge_Extraction_Framework_for_Python_Code_Repositories.pdf"
    },
    {
        "title": "Human-Friendly RDF Graph Construction: Which One Do You Chose?",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/yatter",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "https://github.com/herminiogg/ShExML",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-031-34444-2_19.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "ShExML Java library (See footnote 7), YARRRML-parser (See footnote 21) and Yatter22 are all available as GitHub repositories."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-34444-2_19",
        "abstract": "Abstract. Knowledge Graphs (KGs) are a powerful mechanism tostructure and organize data on the Web. RDF KGs are usually con-structed by declaring a set of mapping rules, specified according to thegrammar of a mapping language (e.g., RML), that relates the input datasources to a domain vocabulary. However, the verbosity and (manual)definition of these rules affect their global adoption. Several user-friendlyserializations for different mapping languages were proposed to facilitateusers with the definition of such rules, e.g., YARRRML, SMS2, XRM,or ShExML. Still, most of them do not cover all features of the mappinglanguages for RDF graph construction (e.g., constructing RDF-star), orthey lack tooling support. In this paper, (i) we present a set of updatesover the YARRRML serialisation to empower it with the latest necessi-ties for constructing RDF graphs; (ii) we implement these new features ina new open-source translator, Yatter, currently used in different real-usecases and international projects; and (iii) we qualitatively compare ourproposal against similar state-of-the-art serialisations, and their asso-ciated translators over a set of conformance test cases. Our proposaladvances the declarative construction of RDF graphs and supports usersin choosing an appropriate serialisation and translator for their use cases.Keywords: Knowledge Graphs · Mapping Languages · YARRRML1 IntroductionKnowledge graphs have proven to be a powerful technology for integrating andaccessing myriads of data available on the Web. Using mapping languages guar-antees sustainable construction of RDF graphs based on a set of declarativemapping rules [18], specified according to a mapping language’s grammar (e.g.,R2RML [8] and RML [10]), which relates data sources to a domain vocabulary.Several mapping languages were proposed to construct RDF graphs [18].W3C recommends the Relational to RDF Mapping Language (R2RML) [8])to construct RDF from Relational Databases. R2RML is a custom mappinglanguage based on the RDF syntax. Multiple works extend R2RML [18] (e.g.,c© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023I. Garrigós et al. (Eds.): ICWE 2023, LNCS 13893, pp. 262–277, 2023.https://doi.org/10.1007/978-3-031-34444-2_19http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-34444-2_19&domain=pdfhttps://doi.org/10.1007/978-3-031-34444-2_19Human-Friendly RDF Graph Construction: Which One Do You Chose? 263RML [10]) enabling its use for heterogeneous data sources. Despite efforts to con-ceptualize and describe these mapping languages, their manual creation process,verbosity, and complexity lead to the appearance of user-friendly serializations.Human-friendly serialisations emerged to ease the definition of mapping rules.YARRRML [13] leverages YAML to offer a user-friendly representation to definemapping rules, while ShExML [11] extends the syntax of the ShEx constraintlanguage [15]. XRM [21] provides an abstract syntax that simulates program-ming languages and SMS2 [17], proposed by Stardog1, is loosely based on theSPARQL query language and extends the features of R2RML to create virtualRDF graphs. Each serialisation is accompanied by a system that translates theirrules into mapping languages, such as RML or R2RML (henceforth abbreviatedas [R2]RML). However, these serialisations and translators were not comparedwith each other in terms of serialisations’ features and system’s characteristics,even though it would help to decide which serialisation fits each use case.",
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, David Chaves-Fraga, Ioannis Dasoulas, Anastasia Dimou",
        "file_name": "978-3-031-34444-2_19.pdf",
        "file_path": "./PDFs/978-3-031-34444-2_19.pdf"
    },
    {
        "title": "TINTO: Converting Tidy Data into image for classification with 2-Dimensional Convolutional Neural Networks",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ElsevierSoftwareX/SOFTX-D-23-00032",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S2352711023000870-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Supplementary data Supplementary material related to this article can be found online at https://doi.org/10.1016/j.softx.2023.101391."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.softx.2023.101391",
        "publication_date": "2023-05-01",
        "authors": "Manuel Castillo‐Cara, Reewos Talla-Chumpitaz, Raúl García‐Castro, Luis Orozco‐Barbosa",
        "file_name": "1-s2.0-S2352711023000870-main.pdf",
        "file_path": "./PDFs/1-s2.0-S2352711023000870-main.pdf"
    },
    {
        "title": "Comparison of Knowledge Graph Representations for Consumer Scenarios",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk-browser",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1007%978-3-031-47240-4_15.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are accessible online for the participants.3,4 2 https://github.com/usc-isi-i2/kgtk-browser/."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.7443836",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47240-4_15",
        "abstract": "Abstract. Knowledge graphs have been widely adopted across organi-zations and research domains, fueling applications that span interactivebrowsing to large-scale analysis and data science. One design decisionin knowledge graph deployment is choosing a representation that opti-mally supports the application’s consumers. Currently, however, there isno consensus on which representations best support each consumer sce-nario. In this work, we analyze the fitness of popular knowledge graphrepresentations for three consumer scenarios: knowledge exploration, sys-tematic querying, and graph completion. We compare the accessibilityfor knowledge exploration through a user study with dedicated brows-ing interfaces and query endpoints. We assess systematic querying withSPARQL in terms of time and query complexity on both synthetic andreal-world datasets. We measure the impact of various representations onthe popular graph completion task by training graph embedding modelsper representation. We experiment with four representations: StandardReification, N-Ary Relationships, Wikidata qualifiers, and RDF-star. Wefind that Qualifiers and RDF-star are better suited to support use casesof knowledge exploration and systematic querying, while Standard Reifi-cation models perform most consistently for embedding model inferencetasks but may become cumbersome for users. With this study, we aimto provide novel insights into the relevance of the representation choiceand its impact on common knowledge graph consumption scenarios.Keywords: Knowledge Graphs · Knowledge Representation · UserStudy · Graph Completion1 IntroductionThe growth of the knowledge graph (KG) user base has triggered the emergenceof new representational requirements. While RDF is the traditional and standardmodel for KG representation, alternative models such as property graphs [25], theWikidata model [34], and RDF-star [12] have also become recently popular. Thec© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14265, pp. 271–289, 2023.https://doi.org/10.1007/978-3-031-47240-4 15http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47240-4_15&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-6606-9735http://orcid.org/0000-0002-1735-0686http://orcid.org/0000-0001-6921-1744http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-031-47240-4_15272 A. Iglesias-Molina et al.promise of these alternative and complementary representation models is thatthey can provide more flexibility to address certain use cases, such as statementannotation, for which RDF-based representations are not straightforward [17].While the plurality of knowledge representation (KR) models provides the meansto address a wider range of possibilities in consumer scenarios, there is currentlyno consensus nor sufficient empirical evidence on which representations are mostsuitable for different KG consumer tasks [16].Previous studies comparing knowledge representations have focused primar-ily on query performance [2,6,14,26,28] and graph interoperability [3,4]. Forthis scenario, the representations need to ensure efficiency to minimize perfor-",
        "publication_date": "2023-01-01",
        "authors": "Ana Iglesias-Molina, Kian Ahrabian, Filip Ilievski, Jay Pujara, Óscar Corcho",
        "file_name": "10!1007%978-3-031-47240-4_15.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-47240-4_15.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-031-47240-4_15.pdf"
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activi-ties when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even leadto unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor tech-niques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extend-ing those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluationson both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document similarity, information search and retrieval, clustering, topic models, hashing1. IntroductionHuge amounts of documents are publicly avail-able on the Web offering the possibility of extractingknowledge from them (e.g. scientific papers in digitaljournals). Document similarity comparisons in manyinformation retrieval (IR) and natural language pro-cessing (NLP) areas are too costly to be performed insuch huge collections of data and require more effi-*Corresponding author. E-mail: cbadenes@fi.upm.es.cient approaches than having to calculate all pairwisesimilarities.In this paper we address the problem of program-matically generating annotations for each of the itemsinside big collections of textual documents, in away that is computationally affordable and enables asemantic-aware exploration of the knowledge inside itthat state-of-the-art methods relying on topic modelsare not able to materialize.Most text mining algorithms represent documents ina common feature space that abstracts the specific se-This article is published online with Open Access and distributed under the terms of the Creative Commons Attribution License (CC BY 4.0).1570-0844/20/$35.00 © 2020 – IOS Press and the authors.mailto:cbadenes@fi.upm.eshttps://orcid.org/0000-0002-2753-9917mailto:jluisred@amazon.comhttps://orcid.org/0000-0002-7413-447Xmailto:ocorcho@fi.upm.eshttps://orcid.org/0000-0002-9260-0753mailto:cbadenes@fi.upm.eshttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-200373&domain=pdf&date_stamp=2020-04-30736 C. Badenes-Olmedo et al. / Topic-based hashing algorithmquence of words used in each document and, with ap-propriate representations, facilitate the analysis of re-lationships between documents even when written us-ing different vocabularies. Although a sparse word orn-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. Latent",
        "publication_date": "2020-05-01",
        "authors": "Carlos Badenes-Olmedo, José Luis Redondo-García, Óscar Corcho",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": "./PDFs/10!3233%sw-200373.pdf"
    },
    {
        "title": "TimeLex: A Suite of Tools for Processing Temporal Information in Legal Texts",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mnavasloro/LawORDate",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/978-3-030-89811-3_18.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Rodríguez-Doncel LawORdate is currently available both as a webapp [9] and as a GitHub repository [10], and finds and replaces misleading legal references in the texts, storing the original references."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-89811-3_18",
        "abstract": "Abstract. In this paper we present a suite of tools named TimeLex, that includesdifferent systems able to process temporal information from legal texts. The firsttool, called lawORdate, helps preprocessing legal references in texts in Spanishthat can bemisleadingwhen trying to find dates in texts. The second one,Añotador,is a temporal tagger (this is, a tool that finds temporal expressions, such as datesor durations) that identifies temporal expressions in texts and provides a standardvalue for each of them. Finally, a third tool, calledWhenTheFact, extracts relevantevents from judgments, allowing a full processing of the temporal dimension ofthis kind of texts, and being a first step towards the complete temporal informationprocessing in the legal domain.Keywords: Temporal expressions · Events · Timeline generation · Legal texts1 IntroductionTemporal information is a very important dimension in documents. Being able to extractitwould enable higher level functionalities, such as event-based summarization or search,pattern detection in cases, and timeline generation, thatwould facilitate the understandingof legal documents, usually difficult to comprehend by layman users, as well as enhanceother NLP tasks over legal documents. Nevertheless, not a lot of research has been donein the legal domain in the field of temporal information.TimeLex [1] is a suite of tools that aims to cover this gap in the domain, providingapproaches to several parts of the temporal information extraction task. In this paper webriefly present the different contributions we have created in order to process this kindof texts from the temporal perspective.The remaining of this paper is structured as follows. Section 2 presents relatedwork in previous literature. Section 3 introduces lawORdate, a preprocessing tool thatdeals with legal references in order to facilitate latter temporal tagging task. Section 4presents Añotador1, a temporal tagger designed to find and normalize temporal expres-sions in legal texts. Section 5 shows a first approach for event extraction, introducing the1 Añotador is a pun: “Año” means “Year” in Spanish, while “Anotador” is the person or tool thatperforms the task of annotation. Añotador is a merge of the two concepts, and would thereforecan be understood as “What annotates years”.© Springer Nature Switzerland AG 2021V. Rodríguez-Doncel et al. (Eds.): AICOL-XI 2018/AICOL-XII 2020/XAILA 2020, LNAI 13048, pp. 260–266, 2021.https://doi.org/10.1007/978-3-030-89811-3_18http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-89811-3_18&domain=pdfhttp://orcid.org/0000-0003-1011-5023http://orcid.org/0000-0003-1076-2511https://doi.org/10.1007/978-3-030-89811-3_18TimeLex 261tool WhenTheFact, which is able to generate a timeline of events from the informationextracted from a document from the European Court of Human Rights2. Finally, Sect. 6presents the conclusions and details the next steps in this research, targeting the semanticrepresentation of the temporal annotations for further applications.2 Related WorkMost effort related to temporal information in the legal domain has been done in relationto normative texts. This is the case of CronoLex [2], that aims to help lawyers byrepresenting the legal norms in Spanish storing information about their life cycle, amongothers. Also in this direction, Akoma Ntoso [3] allows to represent several types of legaltext in a standard way, including temporal information in the metadata.Regarding the processing of temporal expressions and events, Schilder [4] analyzedthe different types of legal documents with regard to temporal information, and divided",
        "publication_date": "2021-01-01",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": "978-3-030-89811-3_18.pdf",
        "file_path": "./PDFs/978-3-030-89811-3_18.pdf"
    },
    {
        "title": "Mapeathor: Simplifying the Specification of Declarative Rules for Knowledge Graph Construction.",
        "implementation_urls": [],
        "abstract": "Abstract. In recent years we have observed an increasing interest bythe scientific community, from social sciences to biomedicine, in the gen-eration and publication of RDF-based knowledge graphs. One possibilityfor creating knowledge graphs consists in using declarative mappings to-gether with their associated parsers. These mappings describe the rela-tionship between the source data and a reference ontology. However, thelearning curve to create these mapping files is steep, hindering its use bya wider community. In this paper we present a user-friendly mapping-language-independent tool, Mapeathor, to declare transformation rulesbased on spreadsheets and translate them into two different mappinglanguages with the purpose of easing the mappings creation process.Keywords: Knowledge Graph · Declarative mapping · Spreadsheet1 IntroductionIn the last few decades, we have seen a significant increase in the publicationof data in a machine understandable manner following Linked Data principles1(e.g., DBpedia2, Wikidata3). Knowledge Graph construction requires integratingdifferent data sources in a structured way, usually following the schema of anontology or group of ontologies. This facilitates the posterior task of mining theknowledge graph with several applications, such as searching recommendationsand learning implicit data patterns.Knowledge graphs can be built in diverse ways. One option is creating ad-hocscripts to transform data, which requires the user to repeat the process of scriptCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).1 https://5stardata.info/en/2 https://wiki.dbpedia.org/3 https://www.wikidata.org/A. Iglesias-Molina et al.writing in every specific use case. Another option is using tools like OpenRe-fine4 to perform data transformation through the creation of an RDF skeleton,which includes proprietary transformation rules and a functionality for knowl-edge graph construction. Lastly, there is an option to keep the transformationrules in specific files that can be later processed by engines that either transformthe data to RDF or create a virtual knowledge graph that can be queried with-out transforming the source data. These rules can be written in a wide variety oflanguages (e.g., R2RML [2], RML [3]) that cover different user’s needs (e.g., thesource data format or the engine that will be used). Although the use of thesemapping files is more flexible and independent, since they can be processed bya wide variety of engines, their creation is still not easy for new users. Expertsare usually needed to carry out these tasks, hindering the use of semantic webtechnologies across the scientific community. That is why it is necessary to lowerthe learning curve and improve mapping reuse and reproducibility.Since mapping languages started to be used by the community, there havebeen multiple approaches for the development of editors to ease their specifica-tion. Most of them enable editing through graphical visualization [4, 6], othersprovide a writing environment (e.g. the Protégé extension OntopPro). Theseeditors are language-oriented, they help to create one kind of mapping, not tak-ing into account the wide variety of mapping languages that currently exist.Moreover, when managing a considerable amount of mapping rules, a graphicalapproach may not be easily handled.",
        "publication_date": "2020-01-01",
        "authors": "Ana Iglesias-Molina, Luis Pozo-Gilo, Daniel Doña, Edna Ruckhaus, David Chaves-Fraga, Óscar Corcho",
        "file_name": "no_doi_20250624163430.pdf",
        "file_path": "./PDFs/no_doi_20250624163430.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2721/paper488.pdf"
    },
    {
        "title": "A comprehensive quality model for Linked Data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170267",
        "abstract": "classes and properties in daQ are defined as abstract and, therefore, they are not directly used. Instead, the intended use of this ontology implies the creation of specific classes and properties defined as subclasses and sub-properties of those defined in daQ. This means that, unlike in Q M O and E V A L , where the elements such as measures and characteristics are defined as in­stances, when using daQ these elements are mainly de­fined as classes. In the context of the conceptual model, daQ is equivalent to Q M O , with the difference of the usage of different terminology. The Data Quality Vocabulary ̂ ( D Q V ) is an ontol­ogy for representing the quality of dataseis that is be­ing developed by the W 3 C . Similarly as daQ, and un­like Q M O and E V A L , D Q V is an ontology specifically developed having in mind dataseis. Currently, D Q V provides classes and properties for capturing informa­tion about quality categories, dimensions and metrics of a dataset, as well as about quality certificates, stan­dards and provenance related to a dataset. However, at this point in time D Q V is still under development and changes to the current design can be expected in the future. In the context of the conceptual model, D Q V tends to provide the means for capturing both the de­tails about quality (i.e., characteristics and measures) and about quality values (results of evaluation). Fur­thermore, although D Q V is specifically designed for dataseis, it does not provide the means to describe some specific aspects of Linked Data. With respect to the state of the art described in this section, this paper contributes with the extension of ex­isting ontologies in order to enable capturing informa­tion related to Linked Data which is not covered by the existing ontologies, as well as with bringing existing ontologies under unique umbrella by connecting their semantically related concepts. 3. Quality model for Linked Data This section describes a quality model for Linked Data and how it was defined using the bottom-up methodology proposed by Radulovic et al. [11]. The starting point for the definition of the quality model was the state of the art in Linked Data quality assess-'http://purl.org/net/EvaluatlonResult# http://www.w3 . org/IR/vocab-dgv/ http://purl.org/net/EvaluatlonResult%23http://www.w3ment and specification, and in particular the work done by Zaveri et al. [16]. Since the quality model presented in this section describes a classification of quality mea­sures (i.e., base measures, derived measures and indi­cators), w e have decided to adopt the terminology as ",
        "publication_date": "2017-01-31",
        "authors": "Filip Radulović, Nandana Mihindukulasooriya, Raúl García‐Castro, Asunción Gómez‐Pérez",
        "file_name": "10!3233%sw-170267.pdf",
        "file_path": "./PDFs/10!3233%sw-170267.pdf"
    },
    {
        "title": "A study of the quality of Wikidata",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/wd-quality",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S1570826821000536-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "We make our code5 [8] and materials6 available to facilitate urther work on analyzing quality of Wikidata statements."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2021.100679",
        "abstract": "insights about these violations, and abstracting them into findingsand recommendations.21Recently, Wikidata has started moving beyond individual prop-erty constraints, representing a higher-level notion of quality inthe form of shapes that are meant to provide norms of well-formedness for sub-graphs describing concepts of interest [7],e.g., human.22 These shapes are collected as Schemas.23 Eachschema defines the desired sub-graph topology describing a givenconcept, using ShEx shape expressions [7]. Schemas are definedthrough consensus among specific communities (e.g., molecularbiology, software engineering, etc.) interested in standardizingconcepts relevant to them.24 We have not addressed the anal-ysis of Wikidata at this level of abstraction; but the approachdescribed in this work can be naturally extended in this direction.A similar observation can be made about prior work that encodesWikidata constraints based on the multi-attributed relationalstructures (MARS) [25], a formal data model for generalizedproperty graphs devised by Marx et al. [26].Recognizing the complexity of the class and type hierarchy inWikidata, the authors of YAGO4 hand-crafted a new, principledtype hierarchy for Wikidata, specifying constraints in SHACL25and OWL [27]; and running scripts to synthesize YAGO by in-gesting the data from Wikidata and processing the SHACL ex-pressions. YAGO4 defines constraints on domain and range, dis-jointness, functionality, and cardinality. The authors report thatenforcing these constraints leads to a removal of 132M state-ments from Wikidata, i.e., 28% of all facts. The constraints definedby YAGO4 overlap partially with the constraints in Wikidata stud-ied in this paper. Subsequent work should compare the findingsfrom validating constraints in YAGO4 and Wikidata, and it shouldgeneralize the in-depth analysis done in this paper to other KGslike YAGO4.Rashid et al. [28] investigated the evolution of 10 classes fromDBpedia over 11 of its releases, measuring aspects of: persis-tence, consistency, and completeness. This effort resembles our18 https://www.wikidata.org/wiki/Wikidata:Primary_sources_tool#References.19 https://www.wikidata.org/wiki/Wikidata:ORES.20 https://www.wikidata.org/wiki/Help:Property_constraints_portal.21 For more information about data quality tools integrated in Wikidata, werefer the reader to: https://www.wikidata.org/wiki/Wikidata:WikiProject_Data_Quality and https://docs.google.com/presentation/d/1rwjqzPaHTsXNNqDc2Op1-qSbcFyaFwOSnkEkStp5L3E/edit.22 https://www.wikidata.org/wiki/EntitySchema:E10.23 https://www.wikidata.org/wiki/Wikidata:WikiProject_Schemas.24 https://www.wikidata.org/wiki/Wikidata:Database_reports/EntitySchema_directory.25 https://www.w3.org/TR/shacl/.https://wikidata-game.toolforge.org/distributed/#https://www.wikidata.org/wiki/Wikidata:Primary_sources_tool#Referenceshttps://www.wikidata.org/wiki/Wikidata:ORES",
        "publication_date": "2021-12-05",
        "authors": "Kartik Shenoy, Filip Ilievski, Daniel Garijo, Daniel Schwabe, Pedro Szekely",
        "file_name": "1-s2.0-S1570826821000536-main.pdf",
        "file_path": "./PDFs/1-s2.0-S1570826821000536-main.pdf"
    },
    {
        "title": "An Abstract Framework for Non-Cooperative Multi-Agent Planning",
        "implementation_urls": [],
        "doi": "10.3390/app9235180",
        "publication_date": "2019-11-29",
        "authors": "Jaume Jordán, Javier Bajo, Vicente Botti, Vicente Julián",
        "file_name": "no_doi_20250624163437.pdf",
        "file_path": "./PDFs/no_doi_20250624163437.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/9/23/5180/pdf?version=1575008421"
    },
    {
        "title": "Simulation of Hybrid Edge Computing Architectures",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt52167.2021.9576121",
        "arxiv": "2108.12592",
        "abstract": "Abstract—Dealing with a growing amount of data is a crucialchallenge for the future of information and communicationtechnologies. More and more devices are expected to transfer datathrough the Internet, therefore new solutions have to be designedin order to guarantee low latency and efficient traffic manage-ment. In this paper, we propose a solution that combines theedge computing paradigm with a decentralized communicationapproach based on Peer-to-Peer (P2P). According to the proposedscheme, participants to the system are employed to relay messagesof other devices, so as to reach a destination (usually a serverat the edge of the network) even in absence of an Internetconnection. This approach can be useful in dynamic and crowdedenvironments, allowing the system to outsource part of the trafficmanagement from the Cloud servers to end-devices. To evaluateour proposal, we carry out some experiments with the help ofLUNES, an open source discrete events simulator specificallydesigned for distributed environments. In our simulations, wetested several system configurations in order to understand theimpact of the algorithms involved in the data dissemination andsome possible network arrangements.Index Terms—simulation, edge computing, peer-to-peer, com-munication, performance evaluationI. INTRODUCTIONWe are living in an era in which digital services areconstantly transformed and revised. All the tools that peopleuse are now digital, producing some kind of data that is notnecessarily stored in a local storage, but that often needs to beuploaded to distributed systems, through some communicationmeans. With the rise of the Internet of Things (IoT), anincreasingly higher number of devices is expected to join theInternet in a near future, interacting with some form of Cloudor decentralized platforms [1]. In order to manage the growingamount of traffic different novel technological solutions arebeing proposed. Among them, 5G stands out, which is capableof offering Internet access to a significantly higher number ofmobile devices with an improved efficiency [2].In this context, smart cities and smart shires [3] are supposedto emerge, with the employment of hybrid physical-digital andintelligent infrastructures that use data-driven technologies toadapt to changes in the physical environment [4]. However,the growth of data exchanges between devices needs to bemanaged not only from a network infrastructure point of view,but also from the perspective of Cloud platforms, in order toavoid an overload of requests to the servers and the resultingincreased latencies or service unavailability [5].Edge computing thus emerges as a paradigm for improvingthe efficiency of the content delivery, by decentralizing themanagement of the system and bringing computation and datastorage in locations geographically closer to the users. Withan edge computing approach, most of the activities usually",
        "publication_date": "2021-09-27",
        "authors": "Luca Serena, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti",
        "file_name": "10!1109%ds-rt52167!2021!9576121.pdf",
        "file_path": "./PDFs/10!1109%ds-rt52167!2021!9576121.pdf"
    },
    {
        "title": "Accessibility and Personalization in OpenCourseWare : An Inclusive Development Approach",
        "implementation_urls": [],
        "doi": "10.1109/icalt49669.2020.00091",
        "abstract": "Abstract—OpenCourseWare (OCW) has become a desirablesource for sharing free educational resources which means therewill always be users with differing needs. It is therefore the re-sponsibility of OCW platform developers to consider accessibilityas one of their prioritized requirements to ensure ease of use forall, including those with disabilities. However, the main challengewhen creating an accessible platform is the ability to address allthe different types of barriers that might affect those with awide range of physical, sensory and cognitive impairments. Thisarticle discusses accessibility and personalization strategies andtheir realisation in the SlideWiki platform, in order to facilitatethe development of accessible OCW. Previously, accessibility wasseen as a complementary feature that can be tackled in theimplementation phase. However, a meaningful integration ofaccessibility features requires thoughtful consideration during allproject phases with active involvement of related stakeholders.The evaluation results and lessons learned from the SlideWiki de-velopment process have the potential to assist in the developmentof other systems that aim for an inclusive approach.I. INTRODUCTIONOpenCourseWare (OCW) platforms have been widely usedfor sharing Open Educational Resources (OER) but thereis little information about their accessibility for users withdisabilities [1]. Accessibility is one of the main objectives thathas to be considered when developing an OCW platform suchas SlideWiki (https://slidewiki.org), that should be inclusiveand easy to use by a wide range of users.Accessibility and Design for All refer to the creation ofproducts, environments, programs and services that can beused by all people, to the greatest extent possible, without theneed for adaptation or specialized design [2]. A number ofaccessibility standards and guidelines are available to directthe development of accessible systems such as W3C WebContent Accessibility Guidelines (WCAG) 2.0 [3] or the W3CCognitive and Learning Disabilities Accessibility Task Force(Cognitive A11Y TF) [4] and Easy-to-read [5] guidelines.Some guidelines have been specifically designed for the de-velopment of accessible e-learning systems, for example, IMSAfA [6]. However, there is no guarantee that aligning toaccessibility standards and guidelines will result in a webservice that is usable and provides content that is easy toreach. Nevertheless, research has shown that a developmentmethodology that follows an inclusive approach i.e. involvinga range of potential users, including those with disabilities, atthe beginning of a project can provide a deeper insight intothe requirements to ensure ease of use [7].The uniqueness of the SlideWiki platform is the mannerin which it provides online multilingual courses that offerauthors the chance to create slides, organized in modularhierarchies with embedded multimodal and dynamic content",
        "publication_date": "2020-07-01",
        "authors": "Mirette Elias, Edna Ruckhaus, E.A. Draffan, Abi James, Mari Carmen Suárez-Figueroa, Steffen Lohmann, Abderrahmane Khiat, Sören Auer",
        "file_name": "Accessibility_and_Personalization_in_OpenCourseWare__An_Inclusive_Development_Approach.pdf",
        "file_path": "./PDFs/Accessibility_and_Personalization_in_OpenCourseWare__An_Inclusive_Development_Approach.pdf"
    },
    {
        "title": "PROFILES &amp; DATA",
        "implementation_urls": [],
        "doi": "10.1145/3184558.3192316",
        "publication_date": "2018-01-01",
        "authors": "Laura Koesten, Elena Demidova, Vadim Savenkov, John G. Breslin, Óscar Corcho, Stefan Dietze, Elena Simperl",
        "file_name": "10!1145%3184558!3192316.pdf",
        "file_path": "./PDFs/10!1145%3184558!3192316.pdf"
    },
    {
        "title": "Simulation of the Internet Computer Protocol: the Next Generation Multi-Blockchain Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt55542.2022.9932122",
        "abstract": "Abstract—The Internet Computer Protocol is a new generationblockchain that aims to provide better security and scalabilitythan the traditional blockchain solutions. In this paper, this inno-vative distributed computing architecture is introduced, modeledand then simulated by means of an agent-based simulation. Theresult is a digital twin of the current Internet Computer, tobe exploited to drive future design and development optimiza-tions, investigate its performance, and evaluate the resilience ofthis distributed system to some security attacks. Preliminaryperformance measurements on the digital twin and simulationscalability results are collected and discussed. The study alsoconfirms that agent-based simulation is a prominent simulationstrategy to develop digital twins of complex distributed systems.Index Terms—Simulation, Performance Evaluation, InternetComputer Protocol, Blockchain, Agent-based Simulation.I. INTRODUCTIONBlockchains and more in general Distributed Ledger tech-nologies (DLTs) are successful examples of distributed sys-tems with a relevant impact on both economy and computerscience. As always happens with any technology, DLTs havesome relevant drawbacks but also some interesting advantagesthat can be used for building a new generation of distributedsystems. To name a few: decentralization, data immutability,transparency, no third parties involved in transactions are fewexamples of properties that can be useful in the design andimplementation of some specific systems aimed to provide newservices to the final users or to improve the security aspectsof supply-chains (both for physical and virtual goods, such asthe software).After a first generation of blockchains, corresponding tothe initial success of Bitcoin, many improved blockchainplatforms have been designed both to support new cryptocur-rencies and for more generic purposes (e.g., Ethereum),succeeding in significantly improving some known problems.Limited scalability, the amount of time required to validatetransactions recorded in the blockchain, decentralized man-agement of blockchain updates and its controlling organizationare all known problems of the first proposed blockchains.The research on these topics is really active and evenmore advanced blockchain solutions are currently investigated,proposed and partially deployed. For example, the InternetComputer Protocol (ICP) architecture1 is based on a networkof networks aimed at combining the resources of severalcomputers and distributing computation. This happens bymeans of a protocol that supports the reading, replication,modification, and procurement of decentralized applications.One of the goals of the ICP is to coordinate a distributedsystem that is composed of many independently-operateddata centers. The obtained system is then able to provide ageneral-purpose abstract platform that is largely transparent",
        "publication_date": "2022-09-26",
        "authors": "Luca Serena, AoXuan Li, Mirko Zichichi, Gabriele D’Angelo, Stefano Ferretti, Su-Kit Tang",
        "file_name": "10!1109%ds-rt55542!2022!9932122.pdf",
        "file_path": "./PDFs/10!1109%ds-rt55542!2022!9932122.pdf"
    },
    {
        "title": "WDPlus: Leveraging Wikidata to Link and Extend Tabular Data (short paper).",
        "implementation_urls": [],
        "abstract": "ABSTRACTScientific observations and other open data are usually made avail-able online in a tabular manner as CSVs and spreadsheets. However,users of these data face three main challenges when attempting touse these products: finding which datasets are related to a topic ofinterest; determining which existing information can be used toextend a given dataset; and how to share their integrated datasetresults with the rest of the community. In this paper we presentWDPlus, a framework designed to address these challenges byleveraging Wikidata. WDPlus allows searching for heterogeneousdatasets, facilitates completing tabular data usingWikidata and pro-poses a mechanism to extend Wikidata in a decentralized manner.KEYWORDSKnowledge Graphs, Entity Linking, Wikidata, RDF1 INTRODUCTIONToday, data about any domain can be found on the web in datarepositories, web APIs and millions of spreadsheets and CSV files.These data comes in a myriad of formats, layouts, terminology andcleanliness that make them difficult to integrate together.Users of these data face three main challenges. The first one isfinding datasets related to a feature or topic of interest. For example,climate scientists often look for years of observational data fromauthoritative sources when estimating the climate of a region. Thesecond challenge is how to complete a given dataset with existingknowledge: machine learning applications are data hungry andrequire asmany data points and features as possible to improve theirpredictions, which often requires integrating data from differentsources. The final challenge is sharing integrated results: onceseveral datasets have been merged together, how to make themavailable to the rest of the community?Knowledge graphs have become the preferred technology to ad-dress these challenges. Large organizations, including search engineproviders, shopping giants and finance institutions are investing inlarge knowledge graphs to integrate and retrieve heterogeneousdata. However, data integration pipelines are usually created man-ually, require significant expertise, and are seldom available to thegeneral public. Similarly, linking to existing datasets in the theLinked Open Data Cloud1usually requires the expertise of a knowl-edge engineer to properly identify the appropriate target instancesto link to in other datasets.1https://lod-cloud.net/Copyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).Recent initiatives such as Data.world,2Google data search [2]and DataCommons",
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Pedro Szekely",
        "file_name": "no_doi_20250624163533.pdf",
        "file_path": "./PDFs/no_doi_20250624163533.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short4.pdf"
    },
    {
        "title": "429646_1_En_6_Chapter 127..128",
        "implementation_urls": [],
        "file_name": "no_doi_20250624163535.pdf",
        "file_path": "./PDFs/no_doi_20250624163535.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_6.pdf"
    },
    {
        "title": "A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2021.115731",
        "publication_date": "2021-08-10",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique",
        "file_name": "1-s2.0-S0957417421011118-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0957417421011118-main.pdf"
    },
    {
        "title": "Using a Legal Knowledge Graph for Multilingual Compliance Services in Labor Law, Contract Management, and Geothermal Energy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_12",
        "abstract": "Abstract This chapter provides insights about the work done and the resultsachieved by the Horizon 2020-funded Innovation Action “Lynx—Building theLegal Knowledge Graph for Smart Compliance Services in Multilingual Europe.”The main objective of Lynx is to create an ecosystem of multilingual, smart cloudservices to manage compliance based on a Legal Knowledge Graph (LKG), whichintegrates and links heterogeneous compliance data sources including legislation,case law, regulations, standards, and private contracts. The chapter provides a shortintroduction in regards to the market needs, gives an overview of the Lynx servicesavailable on the Lynx Services Platform (LySP), and provides valuable insightsinto three real-world compliance solutions developed on top of LySP togetherwith Lynx’s industry partners, namely, (1) Labor Law (Cuatrecasas, Spain), (2)Contract Management (Cybly, Austria), and (3) Geothermal Energy (DNV.GL, theNetherlands).Keywords Compliance · Legal knowledge graph · Multilingualism · NLP ·Labor law · Contract management · Geothermal energyM. Kaltenboeck (�)Semantic Web Company, Vienna, Austriae-mail: martin.kaltenboeck@semantic-web.comP. BoilCuatrecasas, Barcelona, SpainP. VerhoevenDNV, Utrecht, The NetherlandsC. SagederCybly, Salzburg, AustriaE. Montiel-Ponsoda · P. Calleja-IbáñezUniversidad Politécnica de Madrid, Madrid, Spain© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_12253http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_12&domain=pdfmailto:martin.kaltenboeck@semantic-web.comhttps://doi.org/10.1007/978-3-030-78307-5_12254 M. Kaltenboeck et al.Insights into the industry solutions include the concrete business cases, the problemstatements and requirements, the relevant data identified and used, and the LySPAI services combined to realize powerful multilingual compliance solutions inthe respective fields. The chapter closes with findings and learnings from theimplementation phase and a future outlook for further developments, specificallyfor the three vertical solutions and LySP.The chapter relates to the technical priorities of Data Management and DataAnalytics of the European Big Data Value Strategic Research and InnovationAgenda [1]. It addresses all challenges of the horizontal concern Data Managementand some of the challenges of the horizontal concern Data Analytics of the BDVTechnical Reference Model. It addresses the vertical concerns: (a) Big Data Typesand Semantics (with a focus on Text data, including Natural Language Processingdata and Graph data, Network/Web data and Metadata) as well as (b) Standards(standardization of Big Data technology areas to facilitate data integration, sharing,and interoperability). The chapter relates to the Reasoning and Decision Makingcross-sectorial technology enablers of the AI, Data and Robotics Strategic Research,",
        "publication_date": "2022-01-01",
        "authors": "Martin Kaltenboeck, Pascual Boil, Pieter Verhoeven, Christian Sageder, Elena Montiel-Ponsoda, Pablo Calleja-Ibáñez",
        "file_name": "10!1007%978-3-030-78307-5_12.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-78307-5_12.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/978-3-030-78307-5_12.pdf"
    },
    {
        "title": "LawORDate: a Service for Distinguishing Legal References from Temporal Expressions.",
        "implementation_urls": [],
        "abstract": "Abstract. References to documents in the legal domain usually followpatterns containing temporal information in different forms (e.g. ’Di-rective 2001/29’). These references mislead algorithms detecting puretemporal references, and false positives occur in named entity recogni-tion algorithms searching dates or intervals. This paper presents meth-ods and techniques to identify these references, applied to two differentdomains. The first domain is that of news, where the temporal infor-mation plays a crucial role for their understanding and automaticallybuilding timelines can be hampered by the errors induced from these le-gal references. The second domain is that dataset descriptions. Datasetdescriptions sometimes contain temporal information, not only in theirdedicated metadata fields (e.g. dataset creation) but also within the textof their description. LawORDate, the system presented in this paper, isa web service able to detect legal references with temporal informationin Spanish texts. The service identifies these references, avoiding theirannotation by temporal taggers and enabling a further step of linkingthe references to the original sources and building co-reference graphs.Keywords: legal references, temporal expressions, news, dataset de-scription1 IntroductionTemporal expressions detection, mainly focused on news, is a emerging fieldgaining more and more importance in NLP. Efforts such as the NewsReaderproject1 and the TempEval [1, 2] initiatives in SemEval, along with subsequentmore specific temporal tasks [3, 4] show the interest in processing the temporaldimension on all kind of texts. Usually processing of temporal expressions isdone regarding the concrete type of text being faced, both depending on its field(such as news, clinical domain or historical texts) or extension (free texts orlength-limited tweets). Due to this specialization, systems do not usually reactwell when they find expressions from other fields, such as is the case of legalreferences in news or dataset description.1 http://www.newsreader-project.eu/results/data/wikinews/Proceedings of the 1st Workshop on Technologies for Regulatory Compliance25The boom of open data portals also present this kind of mixed information.Thousands of datasets become publicly available everyday, sometimes presentingjust basic scarce metadata such as title and description. Being able to extractadditional information and new search parameters from them, such as named en-tities or temporal references, would facilitate managing them, along with linkingthem resources or queries.To this end, a system2 was built to extract temporal coverage from bothnews and related datasets in Spanish, some of them in the legal domain, and beable to link them based in the temporal dimension. This system calls an existingtemporal tagger, HeidelTime [5], able to detect temporal expressions in texts inSpanish and tag them following the TIMEX3 annotation standard. Nevertheless,this tagger happened to tag as temporal expressions references to Spanish lawsand legal documents that led to false positives, such as shown in the exampleexposed in Fig. 1, extracted from a real article3. The result of the tagging byHeidelTime can be found in Fig. 2.Estas actividades están reguladas por Real Decreto 1341/2007, de 11 de octubresobre la gestión de la calidad de las aguas de baño, incorporando al derecho español",
        "publication_date": "2017-01-01",
        "authors": "María Navas-Loro",
        "file_name": "no_doi_20250624163553.pdf",
        "file_path": "./PDFs/no_doi_20250624163553.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2049/04paper.pdf"
    },
    {
        "title": "Challenges in the Digital Representation of Privacy Terms",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_22",
        "abstract": "Abstract. This paper aims to describe a research project focused on thedigital representation of information related to the privacy and data pro-tection domain. Currently, privacy policies are used by data controllersas a tool to achieve compliance with data protection regulations suchas the EU GDPR, instead of being a privacy instrument at the disposalof both controllers and data subjects. On the other hand, data subjectslack the tools to effectively establish preferences when it comes to theprocessing and disclosure of their personal data, as well as to easily exer-cise their rights. In this regard, this paper discusses the challenges ofthe implementation of a service based on decentralised Web technologiesand Semantic Web standards and specifications to facilitate the com-munication between data subjects and data controllers in the light ofthe GDPR. The main challenges that this service intends to addressare linked to the exercising of GDPR-related rights and obligations, thenegotiation of privacy terms and the governance of access to personaldata stores. A case study in the healthcare and genomics domain willbe explored to experiment with the developed tools. Early-stage resultsrelated to the implementation of semantic policies for the representationof GDPR rights and obligations are presented.Keywords: Personal data · Privacy · Data protection · GDPR ·Semantic web · Policy languages · Intelligent agents1 IntroductionWith the wide spread of technologies in every aspect of our day to day life, theamount of data available has reached a critical level and the legal and ethicalimplications of its exploration has been under debate for quite a few years. Inparticular, the increasing amount of personal data, that is being produced byusers of Web services and applications, has raised greater concerns for privacy,which has led to increased legislative actions. Adding to this situation, the vastThis work has been supported by the European Union’s Horizon 2020 research andinnovation programme under the Marie Sk�lodowska-Curie grant agreement No 813497(PROTECT).c© Springer Nature Switzerland AG 2021V. Rodŕıguez-Doncel et al. (Eds.): AICOL-XI 2018/AICOL-XII 2020/XAILA 2020, LNAI 13048, pp. 313–327, 2021.https://doi.org/10.1007/978-3-030-89811-3_22http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-89811-3_22&domain=pdfhttp://orcid.org/0000-0003-0259-7560https://doi.org/10.1007/978-3-030-89811-3_22314 B. Estevesmajority of personal data is centralised in large data silos and only within thereach of a few, such as government institutions, banks or large telecommunica-tions or technological companies. Thus far, users have been willing to exchangepart of their privacy for personalised services based on the collection of theirpersonal data, however due to the recent cases of personal data breaches, theyare looking for alternative methods to regain control over their own data andensure its protection.In this context, the European Union launched a new regulation focused onthe ‘protection of natural persons with regard to the processing of personal dataand on the free movement of such data’, the so-called General Data ProtectionRegulation (GDPR) [36]. Thus, when the GDPR entered into force on the May25th, 2018, companies had to adapt their personal data driven businesses, as well",
        "publication_date": "2021-01-01",
        "authors": "Beatriz Esteves",
        "file_name": "978-3-030-89811-3_22.pdf",
        "file_path": "./PDFs/978-3-030-89811-3_22.pdf"
    },
    {
        "title": "Dimensionality-reducing classifiers for Spanish winter maintenance of roadways",
        "implementation_urls": [],
        "doi": "10.1109/ciot57267.2023.10084897",
        "abstract": "Abstract—Public administrations try to maintain and guaran-tee safety as one of their priorities. Road networks are one of theenvironments where most incidents usually happen; therefore,it is very important to maintain the safety of drivers andpassengers. This safety must be maintained even in adversecircumstances, such as winter weather conditions. Thus, chemicaltreatments are incorporated to be applied on roads when certainconditions are met. Determining which treatment to apply canbe automated through the use of sensors and machine learningtechniques. To this end, the article proposes a classifier modelcapable of predicting the most appropriate chemical treatmentbased on the environmental properties of the road and itssurroundings. The achieved solution offers good results in termsof evaluation metrics.Index Terms—machine learning, PCA techniques, road main-tenance, road monitoringI. INTRODUCTIONA large part of the society of developed countries livesin a globalized society, and in the future, it is expected thatthis globalization will be present in all corners of the Earth.This globalization is produced thanks to the communicationbetween countries; such communication can be either virtualor physical. On the one hand, when we refer to virtualcommunication we are referring to all the technologies thathave emerged in the last decades related to the field oftelecommunications and information technology; on the otherhand, physical communication refers to the communicationthat takes place by the land, sea and/or air through the differentexisting means of transportation.Spanish Agencia Estatal de InvestigaciónThe physical communication of users is one of the mostused forms of communication daily; people are constantly onthe move to go to work, to carry out recreational activities, tocarry out duties, etc. Therefore, it is essential to maintain thesafety of communication routes and vehicles. Aligned withthis idea is the document of the European Parliament, EURoad Safety Policy Framework 2021-2023 [1]; where roadsafety is specified as one of the objectives to be fulfilled. Tomeet this objective, it is vital to keep roads in good conditionby implementing maintenance tasks and protocols for roadsdeteriorated by use or misuse, as well as road maintenancefor adverse weather conditions. This last aspect, according toMalin et al. [2] is one of the main causes of road accidents;hence, it is essential to pay special attention to the maintenanceof roads in adverse weather conditions.To facilitate this task, the technological advances currentlyavailable can greatly facilitate this process and even automateit. These technologies are advances in computer science andMachine Learning (ML) or Artificial Intelligence (AI) modelsas well as telecommunications and sensing technologies. Con-",
        "publication_date": "2023-03-20",
        "authors": "Diego M. Jiménez-Bravo, Javier Bajo, E. Dopazo, Juan F. De Paz, Valderi Reis Quietinho Leithardt",
        "file_name": "Dimensionality-reducing_classifiers_for_Spanish_winter_maintenance_of_roadways.pdf",
        "file_path": "./PDFs/Dimensionality-reducing_classifiers_for_Spanish_winter_maintenance_of_roadways.pdf"
    },
    {
        "title": "Morph-CSV: Virtual Knowledge Graph Access for Tabular Data.",
        "implementation_urls": [],
        "abstract": "Abstract. Virtual knowledge graph access has traditionally focused onproviding ontology-based access to relational databases (RDB) propos-ing SPARQL-to-SQL query translation techniques and optimizations.With the advent of mapping languages or annotations such as RML orCSVW, these techniques have been applied over tabular data by con-sidering each source as a single table that can be loaded into an RDB.However, such techniques do not take into account those characteristicsthat are normally present in real-world CSV files (e.g., normalization,constraints, joins). In this paper we present Morph-CSV, a frameworkfor enhancing virtual knowledge graph access over a set of CSV files byusing a combination of CSVW annotations and RML mappings withFnO transformation functions. Exploiting these inputs, the frameworkcreates an enriched RDB representation of the CSV files together withthe corresponding R2RML mappings, enabling the use of existing querytranslation (SPARQL-to-SQL) techniques and tools.Keywords: Knowledge Graphs · CSV · RML · CSVW1 IntroductionSemi-structured data formats, and particularly spreadsheets in the form of CSVor Excel files, are one of the most widely-used formats to publish data on theWeb. There are several reasons why tabular formats are so popular for datapublication. First, they are easy to generate by data providers. In many cases,they are even used as one of the main ways to manage data inside organiza-tions. Second, they are easy to consume with common office tools (e.g., Excel,LibreOffice) and there are advanced tools that can be used to process them (e.g.,OpenRefine, Tableau). However, more advanced consumers (e.g., application de-velopers, knowledge workers) often have to face some relevant challenges whenconsuming tabular data: there is no standard way to query data in them as itcan be done with other types of data formats, such as RDB, JSON or XML; dataCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).Chaves-Fraga et al.are difficult to integrate since data constraints and relationships across differentfiles are not explicit; data are often difficult to understand since column namesare generally heterogeneous.Some of these challenges may be dealt following a Semantic Web approach.Virtual knowledge graphs (VKG) provide a unified view and common access toa set of data sources based on ontologies and mappings, translating SPARQLqueries into queries that are supported by the underlying source. Although cur-rent proposals [7] provide support for querying this kind of formats, they treateach source as if it was a single not-normalized RDB table with no keys orintegrity constraints, important elements that are used by SPARQL-to-SQL en-gines for efficient querying. Several languages have been proposed to specifyannotations to deal with the heterogeneity of tabular datasets such as CSVW[8] metadata and RML+FnO [5] mapping rules, but engines or systems have totake them into account in their VKG access pipeline.In this demo we present Morph-CSV, an open source engine1 that extendsthe typical VKG workflow to enhance performance and query completeness overtabular datasets. Our approach exploits the information from CSVW anno-tations and RML+FnO mappings so as to obtain details on the underlyingschema, required transformation functions, missing information, etc., pushing",
        "publication_date": "2020-01-01",
        "authors": "David Chaves-Fraga, Luis Pozo-Gilo, Jhon Toledo, Edna Ruckhaus, Óscar Corcho",
        "file_name": "no_doi_20250624163556.pdf",
        "file_path": "./PDFs/no_doi_20250624163556.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2721/paper478.pdf"
    },
    {
        "title": "Interoperability of Heterogeneous Systems of Systems: Review of Challenges, Emerging Requirements and Options",
        "implementation_urls": [],
        "doi": "10.1145/3555776.3577692",
        "abstract": "AbstractInteroperability stands as a critical hurdle in developing and overseeing distributed and collaborative systems. Thus, it becomes imperative to gain a deep comprehen-sion of the primary obstacles hindering interoperability and the essential criteria that systems must satisfy to achieve it. In light of this objective, in the initial phase of this research, we conducted a survey questionnaire involving stakeholders and practitioners engaged in distributed and collaborative systems. This effort resulted in the identification of eight essential interoperability requirements, along with their corresponding challenges. Then, the second part of our study encompassed a critical review of the literature to assess the effectiveness of prevailing conceptual approaches and associated technologies in addressing the identified requirements. This analysis led to the identification of a set of components that promise to deliver the desired interoperability by addressing the requirements identified earlier. These elements subsequently form the foundation for the third part of our study, a refer-ence architecture for interoperability-fostering frameworks that is proposed in this paper. The results of our research can significantly impact the software engineering of interoperable systems by introducing their fundamental requirements and the best practices to address them, but also by identifying the key elements of a framework facilitating interoperability in Systems of Systems.Keywords  Interoperability · System of Systems · Interoperability requirements · Interoperability challenges · Interoperability architecture · Distributed collaborative systems1  IntroductionThe term System of Systems (SoS) refers to a collaborative and interactive ecosys-tem, typically characterized as an environment which is open at the top (i.e., there is no pre-defined top-level application, and new applications can be created at any Extended author information available on the last page of the articlehttp://crossmark.crossref.org/dialog/?doi=10.1007/s11227-023-05774-3&domain=pdf\t M. Sadeghi et al.1 3time), open at the bottom (i.e., system primitives are defined functionally rather than concretely), and continually evolving (i.e., functions are stable enough to be useful, but they are understood to be subject to modification) [1, 10, 55]. The concept of SoS can be applied to a broad range of application domains including telecommu-nication, Internet of Things, cloud computing, enterprise, e-commerce, healthcare and transportation systems [63]. One of the main obstacles to the creation of such large, distributed and collaborative systems is the lack of interoperability, which can severely hamper the seamless collaboration and interaction of the SoS’s heteroge-neous constituent systems [63]. Interoperability enables the interaction, coopera-tion and sharing among systems; it facilitates the distribution of technology and resources, avoids vendor lock-in, and promotes the establishment of an open, fair market. This makes interoperability key to the enterprise survival, rather than just a technological preference [16].Nevertheless, despite a large body of research on interoperability, there is a short-age of practical solutions. One reason might be that the current state of the art in the study and analysis of interoperability has been heavily academically oriented, rely-ing on scholarly resources and focusing especially on understanding the theoretical and conceptual aspects of interoperability. To address this shortcoming, in this work, we aim to identify and categorize the core interoperability requirements directly from the practitioners’ point of view. Indeed, understanding the core interoperabil-",
        "publication_date": "2023-03-27",
        "authors": "Mersedeh Sadeghi, Alessio Carenini, Óscar Corcho, Matteo Rossi, Riccardo Santoro, Andreas Vogelsang",
        "file_name": "s11227-023-05774-3.pdf",
        "file_path": "./PDFs/s11227-023-05774-3.pdf"
    },
    {
        "title": "Integrating WordNet and Wiktionary with lemon",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-28249-2_3",
        "abstract": "abstractFormprefRefaltRefhiddenRefWordPhrasePartlemFig. 1 The core lemon modelThe core classes of the lemon model can be seen in Figure 1. The core classes arethe ones that form the main path between the Ontology and the lexical realisationrepresented in the Lexical Entry class. A Lexical Entry may also have multiple Lex-ical Forms representing morphological variants, each of which is associated with aWritten Representation. The Lexical Sense class provides a principled link betweenan ontology concept and its lexical realization. Since ‘concepts’ or world objects,as defined in ontologies, and ‘lexical entries’, as defined in lexicons, can rarely besaid to truly overlap, the Lexical Sense class provides the adequate restrictions (us-age, context, register, etc.) that make a certain lexical entry appropriate for naminga certain concept in the specific context of the ontology being lexicalised.The design principles of this model make it ideal for interchanging lexica onthe Web. Since lemon builds on the RDF data model, URIs are used to name anddereference linguistic annotations, and links can be easily created between lexiconsusing RDF triples. Moreover, the model is modular in the sense that, according tothe final application needs, certain modules can be used or not. This also allows fornew modules to be created if this is required by a certain application. In this sense,the lemon model can be said to be suited for the publication and linking of lexicalresources on the Web.4 John McCrae and Elena Montiel-Ponsoda and Philipp CimianoMultilingualism is also foreseen in lemon, as several lexica in different languagescan be associated to one and the same ontology. Moreover, translation relations canbe established at the Lexical Sense class, even allowing for conceptualization mis-matches between languages to be represented, if needed. In fact, a specific modulefor representing translations has been proposed for lemon (Montiel-Ponsoda et al.(2011)). The main idea of this module is to provide metadata about translations(such as provenance, confidence level, etc.), as well as to capture different types oftranslations (descriptive translations vs. culturally equivalent translations).4 Methods4.1 WordNetThe transformation of WordNet into lemon has been already described before (Mc-Crae et al. (2011)). This conversion was performed automatically based on the man-ual alignment of the WordNet vocabulary to the lemon vocabulary. Hereby, synsetsin WordNet were essentially converted into ontology concepts, words into lemonlexical entries, and senses into lemon lexical senses, respectively. The major changewas the modelling of forms as RDF resources, in contrast to treating them as prop-erties. A disadvantage of using ad-hoc formats when publishing lexical resources asLinked Data is the fact that schema changes might be required when the schema ofthe underlying resources changes. For example, when using an ad-hoc conversion toRDF schema, the conversion of WordNet 3.0 and WordNet 2.0 would yield differentschemas as form variants are specified in WordNet 3.0 in extra files. Having a prin-cipled and uniform format such as lemon would overcome this issue of changing",
        "publication_date": "2012-01-01",
        "authors": "John P. McCrae, Elena Montiel-Ponsoda, Philipp Cimiano",
        "file_name": "Integrating_WordNet_and_Wiktionary_with_lemon.pdf",
        "file_path": "./PDFs/Integrating_WordNet_and_Wiktionary_with_lemon.pdf"
    },
    {
        "title": "Now, Later, Never: A Study of Urgency in Mobile Push-Notifications",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-20436-4_4",
        "abstract": "Abstract. Push-notifications, by design, attempt to grab the attentionof subscribers and impart new or valuable information in a particular con-text. These nudges are commonly initiated by marketing teams and sub-sequent delivery interruptions tend to conflict with subscriber prioritiesand activities. In this work, we present a definition of urgency applied tonotifications. We describe its value in an ontology for push-notificationannotation and also evaluate a variety of classification models taskedwith distinguishing urgency levels in notification text. The best modelachieved an F1-score of 0.89. The proposed models have the potential tobenefit subscribers by helping them better prioritize incoming notifica-tions and also aid marketers in creating time-relevant campaigns.Keywords: Push-notification · Urgency · Semantic web · Multi-labeltext classification · Marketing1 IntroductionPush-notifications were first used as a mechanism for alerting email users theyhad received a new message [12], with the intention of saving users’ time andeffort repeatedly checking for new emails. Almost 20 years later, non-urgent noti-fications are still pushed and delivered at the discretion of apps and marketingteams, with little regard for subscribers. The situation today is much more dif-ficult to manage as notifications are spawned from sources beyond the originaldesktop email client to include mobile devices and other IoT devices.Research in the area of intelligent Notification Management has exploredthe relationship between notifications and user attendance in order to help sub-scribers prioritize their time with respect to incoming nudges. These methodsfor improving notification delivery depend on clear notification features whichexpress the intent and value of a notification in a given moment. Few notifica-tion features exist to explicitly express the time-sensitivity or the period of timefor which a notification remains relevant which could be used by the subscriberto better prioritize notification attendance and could also be leveraged by thepublishing app to update or remove notifications which have expired and mayc© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022P. Delir Haghighi et al. (Eds.): MoMM 2022, LNCS 13634, pp. 38–44, 2022.https://doi.org/10.1007/978-3-031-20436-4_4http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-20436-4_4&domain=pdfhttp://orcid.org/0000-0003-0259-7560http://orcid.org/0000-0002-5363-0706http://orcid.org/0000-0001-8077-4011http://orcid.org/0000-0002-9054-9747http://orcid.org/0000-0003-1076-2511https://doi.org/10.1007/978-3-031-20436-4_4Now, Later, Never: A Study of Urgency in Mobile Push-Notifications 39contain inaccuracies or misleading information due to the time lapse betweendelivery and action. In this work we define a novel feature which expresses theurgency and period of relevancy for mobile notifications and evaluate a rangeof classification models on their ability to predict the urgency of a notificationusing minimal input features.2 BackgroundIn their work examining the prioritization of emails by subscribers, Cox et al. [5]discuss the importance of urgency as a feature indicative of email response. Theauthors also highlight how urgency is but one contributing factor for deciding",
        "publication_date": "2022-01-01",
        "authors": "Beatriz Esteves, Kieran Fraser, Shridhar Kulkarni, Owen Conlan, Victor Rodrı́guez-Doncel",
        "file_name": "978-3-031-20436-4_4.pdf",
        "file_path": "./PDFs/978-3-031-20436-4_4.pdf"
    },
    {
        "title": "Balancing coverage and specificity for semantic labelling of subject columns",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/tada-entity",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S095070512101159X-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The results of the different datasets and settings (more than 100 diagrams) are available online [59] and on GitHub.26 5.1."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.knosys.2021.108092",
        "abstract": "Page attributes (e.g., Page title, URL, . . . ), Text (abstracts belongsto the classes), Majority + Frequency + Page attributes + Text.22We report the results in Table 2 (see Fig. 5).We see that the best precision of our approach is 0.93, which isthe same precision as the T2K Extended approach. The precisionof T2K (Page attributes) is very high (0.97) but covers only a smallset as it has a low recall (0.37). The recall of our approach is 0.98,which is higher than the T2K Extended approach (0.91). The F1score of our approach is 0.95, which is higher than all the T2Kapproaches.21 It is different specificity function than the one we use in our approach.22 For more details on the baselines refer to the original paper by Ritzeet al. [44].https://github.com/oeg-upm/tada-entity/blob/master/docs/evaluation.mdA. Alobaid and O. Corcho Knowledge-Based Systems 240 (2022) 108092tabCCpto4twaOcwFig. 5. F1 scores of the semantic labelling of the T2Dv2 dataset.Fig. 6. F1 scores of the semantic labelling of the SemTab dataset Round 1.Looking closer to the wrong annotations, they are similaro the ones reported in T2Dv1. For example, there is a tablebout ‘‘Cricketer’’, which was annotated as ‘‘OfficeHolder’’. This isecause entity linking is matched to more Politicians instead ofricketers as they are more common in the knowledge graph thanricketers and also due to common names. We observe that thisroblem is the most common one in entity linking approacheshat are based on labels only. This also agrees with the confusionf annotating entities reported by Quercini et al. [30]..2.3. SemTabThis gold standard is noisier in comparison to the previouswo. We found 11 invalid file names in the gold standard, whiche were not able to correct. We also found some suboptimalnnotations. The same observation has also been reported byliveira and dÁquin [54]. For example, a column that containsountry names is annotated by our tool as dbo:Country while thegold standard has it as dbo:PopulatedPlace, which is not optimal.We also found a similar case about a column in the gold stan-",
        "publication_date": "2022-01-13",
        "authors": "Ahmad Alobaid, Óscar Corcho",
        "file_name": "1-s2.0-S095070512101159X-main.pdf",
        "file_path": "./PDFs/1-s2.0-S095070512101159X-main.pdf"
    },
    {
        "title": "MuHeQA: Zero-shot question answering over multiple and heterogeneous knowledge bases",
        "implementation_urls": [
            {
                "identifier": "https://github.com/librairy/MuHeQA",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-233379.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This evaluation set is available as part of our additional material.12 4.2."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-233379",
        "abstract": "Abstract. There are two main limitations in most of the existing Knowledge Graph Question Answering (KGQA) algorithms.First, the approaches depend heavily on the structure and cannot be easily adapted to other KGs. Second, the availability andamount of additional domain-specific data in structured or unstructured formats has also proven to be critical in many of thesesystems. Such dependencies limit the applicability of KGQA systems and make their adoption difficult. A novel algorithm isproposed, MuHeQA, that alleviates both limitations by retrieving the answer from textual content automatically generated fromKGs instead of queries over them. This new approach (1) works on one or several KGs simultaneously, (2) does not requiretraining data what makes it is domain-independent, (3) enables the combination of knowledge graphs with unstructured infor-mation sources to build the answer, and (4) reduces the dependency on the underlying schema since it does not navigate throughstructured content but only reads property values. MuHeQA extracts answers from textual summaries created by combininginformation related to the question from multiple knowledge bases, be them structured or not. Experiments over Wikidata andDBpedia show that our approach achieves comparable performance to other approaches in single-fact questions while beingdomain and KG independent. Results raise important questions for future work about how the textual content that can be createdfrom knowledge graphs enables answer extraction.Keywords: Question answering, natural language processing, knowledge graphs1. IntroductionKnowledge graphs are now being applied in multiple domains. Knowledge Graph Question Answering (KGQA)has emerged as a way to provide an intuitive mechanism for non-expert users to query knowledge graphs. KGQAsystems do not require specific technical knowledge (e.g., knowledge of SPARQL or Cypher), providing answers innatural language for questions that are also expressed in natural language.One of the main challenges in the design of KGQA systems is semantic parsing [14]. In this step, natural languagequeries (NLQs) are translated into a specific query language (e.g. SPARQL1 for RDF-based KGs, or Cypher2 for*Corresponding author. E-mail: carlos.badenes@upm.es.1https://www.w3.org/TR/rdf-sparql-query2https://opencypher.org1570-0844 © 2024 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:carlos.badenes@upm.eshttps://orcid.org/0000-0002-2753-9917mailto:oscar.corcho@upm.eshttps://orcid.org/0000-0002-9260-0753mailto:carlos.badenes@upm.eshttps://www.w3.org/TR/rdf-sparql-queryhttps://opencypher.orghttps://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-233379&domain=pdf&date_stamp=2023-06-071548 C. Badenes-Olmedo and O. Corcho / MuHeQAproperty graphs). KGQA systems typically use templates with placeholders for relations and entities that mustbe identified in the original NLQ. Once the template is filled in, the generated query (in SPARQL, Cypher, etc.)is evaluated in the system that stores the KG (e.g. Triple store, property graph DB) and the results will providethe answers to the question. Thus, KGQA systems can be reduced to entity and relationship search processes thatpopulate predefined query templates that are commonly identified using supervised machine learning techniques(e.g., supervised classifiers) [23]. This approach is heavily dependent on the data schema to explore the entityrelationships and on the availability of training data to create supervised classification models.Mitigating the dependency of KGQA systems on the underlying graph structure and eliminating the need fortraining sets is crucial to create cross-cutting solutions more efficiently and with less cost. The first research questionaddressed in this work is: “How to extract the answer from a knowledge graph without translating the naturallanguage question into a formal query language?”. Moreover, the dependence on the underlying data schema makesit also difficult for existing KGQA systems to combine knowledge from several graphs to extract a single answer.They typically translate the question into specific queries for each supported KG and obtain multiple answers, ratherthan combining the knowledge from each KG before drawing a single answer. The combined use of more than one",
        "publication_date": "2023-06-09",
        "authors": "Carlos Badenes-Olmedo, Óscar Corcho",
        "file_name": "10!3233%sw-233379.pdf",
        "file_path": "./PDFs/10!3233%sw-233379.pdf"
    },
    {
        "title": "esT5s: A Spanish Model for Text Summarization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/t5-spanish-news-summarization",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%ssw220020.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "For the sake of reproducibility, the source code is available 3."
                    }
                ]
            }
        ],
        "doi": "10.3233/ssw220020",
        "abstract": "Abstract. Deep Learning models based on the Transformer architecture have rev-olutionized the state of the art of NLP tasks. As English is the language in whichmost significant advances are made, languages like Spanish require specific train-ing, but this training has a computational cost so high that only big corporationswith servers and GPUs are capable of generating them. This work has exploredhow to create a model for the Spanish language from a big multilingual model.Specifically, a model aimed at creating text summarization, a very common taskin NLP. The results, concerning the quality of the summarization (ROUGE score),point out that these small models, for a specific language, achieve similar resultsthan much bigger models, with a reasonable training in terms of time required andcomputational power, and are significantly faster at inference.Keywords. Deep learning, T5, Spanish, Text summarization1. IntroductionSummarization is a Natural Language Processing task that consists of condensing themost relevant information from a document. This task can be divided into two categories:extractive summarization and abstractive summarization. Extractive summarization con-sists of identifying and copying the most relevant and useful information pieces (typicallysentences) from the original content. In contrast, abstractive summarization requires adeeper understanding of the language to summarize the most relevant content, paraphras-ing the original sentences, combining and using synonyms or new words, without losinginformation and preserving cohesion and coherence [1]. Thus, abstractive summarizationis a difficult task in natural language processing.Currently, the state-of-the-art of language models based on transformers [2] havereached a high level of language comprehension. However, all research is mainly focusedon the English language and then applied to other languages. Even important languagessuch as Spanish, which is the second language spoken in the world, has an enormous1Corresponding Author: Mariano Rico; E-mail:mariano.rico@upm.es.The authors gratefully acknowledge the computer resources at Artemisa, funded by the European UnionERDF and Comunitat Valenciana as well as the technical support provided by the Instituto de Fı́sicaCorpuscular, IFIC (CSIC-UV). Also we acknowledge the Universidad Politécnica de Madrid for providingcomputing resources on Magerit Supercomputer. This work was funded partially by the project KnowledgeSpaces (PID2020-118274RB-I00), funded by MCIN/AEI/ 10.13039/501100011033; and project HCommonK(RTC2019-007134-7, funded by MCIN/AEI/ 10.13039/501100011033).Towards a Knowledge-Aware AIA. Dimou et al. (Eds.)© 2022 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution License 4.0 (CC BY 4.0).doi:10.3233/SSW220020184gap in their language models compared to English [3]. For example, the T5 model [4]is one of the best language models, which exploits the features of text-to-text transferlearning and it is usually used for the text summarization task, it is only trained forEnglish language, and there is no Spanish version yet.Despite this lack of models for non-English languages, there are multilingual mod-els (also including English). This is the case of the multilingual T5 (mT5) [5] which istrained in 101 languages, including English and Spanish among them. These multilin-gual approaches outperform monolingual models because similar languages have posi-tive transfer between them [6]. However, the effort required to create (and also execute)these multilingual models is very high. Our approach takes advantage of these multi-",
        "publication_date": "2022-09-06",
        "authors": "Adrian Vogel-Fernandez, Pablo Calleja, Mariano Rico",
        "file_name": "10!3233%ssw220020.pdf",
        "file_path": "./PDFs/10!3233%ssw220020.pdf"
    },
    {
        "title": "Personal eBanking Solutions based on Semantic Web Services",
        "implementation_urls": [],
        "doi": "10.1007/978-3-540-37017-8_13",
        "abstract": "Abstract. We describe how Semantic Web Service technology can be used for the provisión of personal e-banking online services. We describe two deployed applications: an overdraft notification service and a mortgage comparison ser-vice. The former accesses the bank accounts of a user as well as utility goods Web sites where invoicing information is stored and estimates whether the user will be in an overdraft situation in the near future, alerting him/her by e-mail or SMS. The latter accesses the mortgage information provided by the heteroge-neous Web sites of different banks and allows users to compare them according to different types of criteria. The chapter not only focuses on the technological decisions followed to implement and deploy these services, but also on the added valué of applying Semantic Web Services for them. 1 Introduction and motivation The Internet has revolutionized our lives in áreas such as communication (very low cost, large scope), online business (transactions), and access to content (millions of resources, irrespective of location and language). Also in our \"financial lives\", the Internet has provoked significant changes: instead of going to a physical bank branch, we can go to the bank's web site and make many different types of transactions. There are important differences in the use of Internet banking between different countries [2], with a complex set of factors that influence adoption, such as access technology and infrastructure related factors, sector-specific Internet banking factors, and other socioeconomic factors. Most financial institutions allow their clients to access their accounts and consult their account information via Internet. Moreover, most of these institutions allow their clients to make transactions via Internet. In both cases, several options are made available for clients, ranging from regular bank ser­vices to associated services, such as information about grants, e-commerce services, mobility, shopping or on-line payments services, and so on. In general, the main processes covered in banking operations can be classified into three categories: o Inter-banking processes. They refer to the exchange of documents or account entries (cheques, receipts, international and national transfers). For example, when a Bank pays a cheque from another bank, a movement of funds is produced from the second Bank (the payer) to the first (the payee). This movement of funds is always accompanied by information on the operation (e.g., the cheque number) and, in some cases, the document itself. o Bank-provider processes. This refers to basic supplies common to any industrial sector (paper, IT equipment, software, office furniture, etc.), with the sole excep-tion of those information providers that are specific to banking (defaulter regis-try, real estate appraisal entities, etc.). o Bank-client processes. This refers to product sale processes and service usage processes through the different channels made available by the bank. Here, we must include as well the internal operations of the Bank, since they make the re-lationship with the client possible. From another perspective, the banking business can be also divided according to the following categories: products, services and channels. o Products. They are contractual operations that involve the deposit of money (accounts, mortgages, deposits, investment and pension funds, etc.) or money loans (credit, loans, mortgages, guarantees, etc.). o Services. They are operations that involve the entry or exit of funds from a bank­ing product (i.e.: credit cards, cheques, promissory notes, receipts, transfers). In a broad sense this category would include every kind of service that is offered by ",
        "publication_date": "2007-01-01",
        "authors": "Óscar Corcho, Silvestre Losada, Richard Benjamins, José Luis Bas, Sergio Bellido",
        "file_name": "CL04_OpenArchive.pdf",
        "file_path": "./PDFs/CL04_OpenArchive.pdf"
    },
    {
        "title": "Architectural Patterns for the Semantic Grid",
        "implementation_urls": [],
        "doi": "10.1007/978-0-387-37831-2_8",
        "publication_date": "2007-02-11",
        "authors": "Ioannis Kotsiopoulos, Paolo Missier, Pinar Alper, Óscar Corcho, Sean Bechhofer, Carole Goble",
        "file_name": "KotsiopoulosEtAl2006_CoreGrid.pdf",
        "file_path": "./PDFs/KotsiopoulosEtAl2006_CoreGrid.pdf "
    },
    {
        "title": "Blockchain-Based Data Management for Smart Transportation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-07535-3_20",
        "abstract": "Abstract Smart services for Intelligent Transportation Systems (ITS) are currentlydeployed over centralized system solutions. Conversely, the use of decentralizedsystems to support these applications enables the distribution of data, only to thoseentities that have the authorization to access them, while at the same time guaran-teeing data sovereignty to the data creators. This approach not only allows sharinginformation without the intervention of a “trusted” data silo, but promotes data veri-fiability and accountability. We discuss a possible framework based on decentralizedsystems, with a focus on four requirements, namely data integrity, confidentiality,access control and persistence. We also describe a prototype implementation andrelated performance results, showing the viability of the chosen approach.1 IntroductionIn the last decade, Intelligent Transportation Systems (ITS) have emerged as a wayto efficiently improve mobility, travel security and increase the options for travellers.As defined in the European Union directive 2010/40/EU [12], ITS are advancedapplications for the provision of innovative transport and traffic management ser-vices, with the ultimate purpose of aiding individuals within the infrastructure tomake safe and timely decisions. The general idea is usually that of devising a sortMirko ZichichiOntology Engineering Group, Universidad Politécnica de Madrid, Spaine-mail: mirko.zichichi@upm.esStefano FerrettiDepartment of Pure and Applied Sciences, University of Urbino “Carlo Bo\", Italye-mail: stefano.ferretti@uniurb.itGabriele D’AngeloDepartment of Computer Science and Engineering, University of Bologna, Italye-mail: g.dangelo@unibo.it12 Zichichi et al.of data management middleware to build advanced applications for the provisionof innovative transport and traffic management services, with the aim of enablingusers “to be better informed and make safer, more coordinated and ‘smarter’ useof transport networks” [12]. Vehicles and transportation infrastructures are becom-ing increasingly “smarter”, which means that they are equipped with sensors thattrack and process a huge amount of different types of information, e.g. data sensedby the interior of the vehicle, the surrounding environment, road conditions, etc.This enables the creation of applications “without embodying intelligence as such”,which brings out the real essence of an infrastructure of this kind. The interactionprocesses between two individuals, or an individual and a vehicle, or an individualand the infrastructure, within the ITS, should include the least possible presence ofa human intermediary. All of this constitute a network of user-owned and infrastruc-ture devices that is usually referred as VANET (Vehicular Ad-hoc NETwork) [31].In this vision, the intelligence shifts from that of a human third-party to that of anartificial intelligence that has been optimized for this use case. This artificial inter-vention leads to the creation of “innovative services relating to different modes oftransport and traffic management” [12], that take advantage of faster processing andbetter performances. When there are no human intermediaries, indeed, traditionalprocesses become faster to execute.In addition, the growth of smartphones and Internet-of-Things devices enablesindividuals’ ubiquitous connectivity and the ability to collect environmental and per-sonal information or crowd-sensed data [42]. Thus, users become an active part of",
        "publication_date": "2022-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1007%978-3-031-07535-3_20.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-07535-3_20.pdf"
    },
    {
        "title": "Towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.8018593",
                "type": "zenodo",
                "paper_frequency": 8,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-42280-5_12",
        "abstract": "Abstract. The Easy-to-Read (E2R) Methodology was created toimprove the daily life of people with cognitive disabilities. This method-ology aims to present clear and easily understood documents. The E2RMethodology includes, among others, a set of guidelines related to thewriting of texts. Some of these guidelines focus on morphological featuresthat may cause difficulties in reading comprehension. Examples of thoseguidelines are: (a) to avoid the use of adverbs ending in -mente (-ly inEnglish), and (b) to avoid the use of superlative forms. Both linguisticstructures are quite long, which is also related to another E2R guideline(“The use of long words should be avoided”). Currently, E2R guidelinesare applied manually to create easy-to-read text materials. To help insuch a manual process, our research line is focused on applying the E2RMethodology in Spanish texts in a (semi)-automatic fashion. Specifically,in this paper we present (a) the inclusive design approach for the develop-ment of E2R adaptation methods for avoiding adverbs ending in -menteand superlative forms, (b) the initial methods for adapting those mor-phological features to an E2R version, and (c) a preliminary user-basedevaluation of the implementation of those methods.Keywords: Easy-to-Read Methodology · Cognitive Accessibility ·Artificial Intelligence1 IntroductionPeople with cognitive disabilities present some difficulties related to reading com-prehension processes. Hence, a methodology called Easy-to-Read (E2R) [1,13,17]was created with the goal of presenting clear and easily understood content. Thismethodology provides a collection of guidelines concerning both the content oftexts and their design and layout, such as to use short and simple sentences, toavoid the use of long words, to divide ideas into paragraphs, or to use imagesthat complement the content of the text.c© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023J. Abdelnour Nocera et al. (Eds.): INTERACT 2023, LNCS 14142, pp. 176–198, 2023.https://doi.org/10.1007/978-3-031-42280-5_12http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-42280-5_12&domain=pdfhttp://orcid.org/0000-0003-3807-5019http://orcid.org/0000-0002-3967-0672https://doi.org/10.1007/978-3-031-42280-5_12Automatic Easy-to-Read Adaptation of Morphological Features in Spanish 177Currently, the E2R methodology is applied in a manual fashion. This adap-tation process is iterative and involves three key activities: analysis, adaptation,and validation [1]. This manual process is labour-intensive and costly, and itwould benefit from having a technological support. In this context, our researchline is focused on applying different Artificial Intelligence (AI) methods andtechniques1 to (semi)-automatically perform the analysis and the adaptationof documents to obtain easy-to-read versions of original documents written inSpanish. Specifically, this paper is focused on two of the E2R guidelines thataffect the writing of texts [1]: (a) to avoid the use of adverbs ending in -mente(-ly in English) and (b) to avoid the use of the superlative form of adjectives andadverbs. These two guidelines are considered useful for the daily work of E2Rexperts by one in every four experts as reported in [27].In linguistic terms, these two structures are the result of the so-called pro-cess of word formation. Among other word classes, this process results on derived",
        "publication_date": "2023-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Isam Diab, Álvaro González, Jesica Rivero-Espinosa",
        "file_name": "978-3-031-42280-5_12.pdf",
        "file_path": "./PDFs/978-3-031-42280-5_12.pdf"
    },
    {
        "title": "Complex queries over decentralised systems for geodata retrieval",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1049%ntw2!12037.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "DATA AVAILABILITY STATEMENT The data that support the findings of this study are openly available in Zenodo at https://doi.org/10.5281/zenodo."
                    }
                ]
            }
        ],
        "doi": "10.1049/ntw2.12037",
        "abstract": "AbstractDecentralised systems have been proved to be quite effective to allow for trusted andaccountable data sharing, without the need to resort to a centralised party that collects allthe information. While complete decentralisation provides important advantages in termsof data sovereignty, absence of bottlenecks and reliability, it also adds some issues con-cerned with efficient data lookup and the possibility to implement complex querieswithout reintroducing centralised components. In this paper, we describe a system thatcopes with these issues, thanks to a multi‐layer lookup scheme based on Distributed HashTables that allows for multiple keyword‐based searches. The service of peer nodesparticipating in this discovery service is controlled and rewarded for their contribution.Moreover, the governance of this process is completely automated through the use ofsmart contracts, thus building a Decentralised Autonomous Organization (DAO). Finally,we present a use case where road hazards are collected in order to test the goodness ofour system for geodata retrieval. Then, we show results from a performance evaluationthat confirm the viability of the proposal.1 | INTRODUCTIONThe digitalisation process, which has been ongoing over thelast decades, has seen data management and data deliverybecome crucial issues. The transformation brought about bydigital technologies has data at its core and it had a significantimpact on economies and societies around the world. Theability to easily get hold of data has the potential to createseveral new services based on data and new markets wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is notfrom a dubious (or false) origin is often a challenge. In order tocope with the increasingly higher number of content that isdemanded through the Web, multiple solutions for efficient useof the Internet have been designed. In particular, thanks to thedecentralisation of content storage and delivery, it is possible toavoid the single point of failure, while reducing the workload atdata centres and allowing a distribution of data that remains‘closer’ to the original data producer. Decentralisation alsofosters the creation of open systems, where participants canfreely join the system and then contribute to its functioning.Recently, Distributed Ledger Technologies (DLTs) and arealm of decentralised systems, for example, Decentralised FileStorages (DFS), have emerged as Peer‐to‐Peer (P2P) technol-ogies capable of offering interesting features related to datavalidation and trustfulness [2, 3]. DLTs have gained popularitywith the advent of cryptocurrencies, which allow users to tradecrypto‐assets without any central entity being involved,ensuring transparency and data integrity. By creating a com-mon, decentralised and trustless infrastructure, it will bepossible for data consumers and providers to interact andcollaborate in P2P interactions [4, 5]. Benefits often cited ofDLTs, indeed, include the enabling of secure transactions be-tween untrusted parties through consensus mechanisms, highavailability, and the ability to automate and enforce processesthrough smart contracts [6]. Besides the financial use case,",
        "publication_date": "2022-03-26",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1049%ntw2!12037.pdf",
        "file_path": "./PDFs/10!1049%ntw2!12037.pdf"
    },
    {
        "title": "MOVO: a dApp for DLT-based Smart Mobility",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/movoApp",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%icccn52240!2021!9522257.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of Movo is available on GitHub: https://github.com/miker83z/movoApp."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522257",
        "arxiv": "2104.13813",
        "abstract": "Abstract—Plenty of research on smart mobility is currentlydevoted to the inclusion of novel decentralized software archi-tectures to these systems, due to the inherent advantages interms of transparency, traceability, trustworthiness. MOVO is adecentralized application (dApp) for smart mobility. It includes:(i) a module for collecting data from vehicles and smartphonessensors; (ii) a component for interacting with Distributed LedgerTechnologies (DLT) and Decentralized File Storages (DFS), forstoring and validating sensor data; (iii) a module for \"offline\"interaction between devices. The dApp consists of an Androidapplication intended for use inside a vehicle, which helps theuser/driver collect contextually generated data (e.g. a driver’sstress level, an electric vehicle’s battery level), which can then beshared through the use of DLT (i.e., IOTA DLT and Ethereumsmart contracts) and DFS (i.e., IPFS). The third module consistsof an implementation of a communication channel that, via Wi-FiDirect, allows two devices to exchange data and payment informa-tion with respect to DLT (i.e. cryptocurrency and token) assets.In this paper, we describe the main software components andprovide an experimental evaluation that confirms the viability ofthe MOVO dApp in real mobility scenarios.Index Terms—Smart Mobility, Distributed Ledger Technolo-gies, Decentralized Application, VANETI. INTRODUCTIONIn the last decade smart mobility has emerged as way toefficiently improve mobility, travel security and increase theoptions for travellers. The general idea is usually that ofdevising a sort of middleware to build advanced applicationsfor the provision of innovative transport and traffic manage-ment services, with the aim of enabling users “to be betterinformed and make safer, more coordinated and ‘smarter’ useof transport networks” [1].Vehicles and infrastructures are becoming increasingly“smarter”, which means that they are equipped with sensorsthat track a huge amount of different types of information,e.g. data sensed by the interior of the vehicle, the surroundingenvironment, road conditions, etc. In addition, the growthof smartphones and Internet-of-Things (IoT) devices enablesindividuals’ ubiquitous connectivity and the ability to collectpersonal information or crowdsensing data. All of this consti-tute a network of devices that is usually referred as VANET(Vehicular Ad-hoc NETwork) [2], [3].This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.In this context, a middleware platform can be designedto share and reuse data, services and computation, simpli-fying the development of new services and the integration",
        "publication_date": "2021-07-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%icccn52240!2021!9522257.pdf",
        "file_path": "./PDFs/10!1109%icccn52240!2021!9522257.pdf"
    },
    {
        "title": "Scheduling Ontology Engineering Projects Using gOntt",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_14",
        "abstract": "Abstract In order to manage properly ontology development projects in complexsettings and to apply correctly the NeOn Methodology, it is crucial to haveknowledge of the entire ontology development life cycle before starting the devel-opment projects. The ontology project plan and scheduling helps the ontologydevelopment team to have this knowledge and to monitor the project execution.To facilitate the planning and scheduling of ontology development projects, theNeOn Toolkit plugin called gOntt has been developed. gOntt is a tool that supportsthe scheduling of ontology network development projects and helps to executethem. In addition, prescriptive methodological guidelines for scheduling ontologydevelopment projects using gOntt are provided.14.1 IntroductionOne of the crucial aspects within engineering processes is the issue of planning andscheduling development projects. These two terms are often thought of as synony-mous; however, they are not. While planning1 is the act of drawing up a plan, that is,a series of steps to be carried out to achieve an objective, scheduling2 is defined asthe activity of placing planned events along a timeline. Scheduling clearly dependson planning, and both are crucial in any project.Bearing in mind that ontologies are part of software products and that sometimesontologies are considered a kind of software, experiences and practices in softwareM.C. Suárez-Figueroa (*) • A. Gómez-Pérez • O. Muñoz-Garcı́aOntology Engineering Group, Facultad de Informática, Universidad Politécnica de Madrid,Campus de Montegancedo sn., 28660 Boadilla del Monte, Madrid, Spaine-mail: mcsuarez@fi.upm.es; asun@fi.upm.es; omunozgarcia@gmail.com1 http://www.wordnet-online.com/planning.shtml2 http://www.wordnet-online.com/scheduling.shtmlM.C. Suárez-Figueroa et al. (eds.), Ontology Engineering in a Networked World,DOI 10.1007/978-3-642-24794-1_14, # Springer-Verlag Berlin Heidelberg 2012303engineering can be adopted and adjusted in the ontology engineering community.For this reason and with the aim of achieving in ontology engineering a similardegree of maturity to that of the software engineering field, we take as basissoftware engineering works to provide ontology engineers with help in planningand scheduling ontology development projects.In software engineering, every development project has a life cycle (Taylor2008), which is produced by instantiating a particular life cycle model. Lifecycle models can be seen as abstractions of the phases or stages through whicha product passes along its life. Examples of life cycle models are waterfall (Royce1970), incremental (Sommerville 2007), iterative (Pfleeger 2001), evolutionaryprototyping (Davis et al. 1988), and rapid throwaway prototyping (Davis et al.1988).To properly manage software development projects, it is crucial to have knowl-edge of the entire software development life cycle (Stellman and Greene 2005). Inthis regard, software engineers always plan and schedule every development projectbefore starting it. The project plan defines the tasks to be carried out and establishesthe human resources to perform the project work. To estimate the effort required toperform each task, techniques such as (Stellman and Greene 2005) WidebandDelphi, PROBE, and COCOMO II can be used.The project schedule is a calendar that links the tasks to be performed withthe resources to support their performance. One of the most common forms ofrepresenting schedules is to use a Gantt chart (Gantt 1974); and the most popular",
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez, Óscar Muñoz-García",
        "file_name": "Scheduling_Ontology_Engineering_Projects_Using_gOntt.pdf",
        "file_path": "./PDFs/Scheduling_Ontology_Engineering_Projects_Using_gOntt.pdf"
    },
    {
        "title": "Data Quality Barriers for Transparency in Public Procurement",
        "implementation_urls": [],
        "doi": "10.3390/info13020099",
        "publication_date": "2022-02-20",
        "authors": "Ahmet Soylu, Óscar Corcho, Brian Elvesæter, Carlos Badenes-Olmedo, Francisco Yedro Martínez, Matej Kovacic, Matej Posinković, Mitja Medvešček, Ian Makgill, Chris Taggart, Elena Simperl, Till Christopher Lech, Dumitru Roman",
        "file_name": "no_doi_20250624163718.pdf",
        "file_path": "./PDFs/no_doi_20250624163718.pdf",
        "pdf_link": "https://www.mdpi.com/2078-2489/13/2/99/pdf?version=1645523351"
    },
    {
        "title": "Toward the Ontological Modeling of Smart Contracts: A Solidity Use Case",
        "implementation_urls": [],
        "doi": "10.1109/access.2021.3115577",
        "abstract": "Abstract: Governments need to be accountable and transparent for their public spending decisionsin order to prevent losses through fraud and corruption as well as to build healthy and sustainableeconomies. Open data act as a major instrument in this respect by enabling public administrations,service providers, data journalists, transparency activists, and regular citizens to identify fraud oruncompetitive markets through connecting related, heterogeneous, and originally unconnected datasources. To this end, in this article, we present our experience in the case of Slovenia, where wesuccessfully applied a number of anomaly detection techniques over a set of open disparate data setsintegrated into a Knowledge Graph, including procurement, company, and spending data, through alinked data-based platform called TheyBuyForYou. We then report a set of guidelines for publishinghigh quality procurement data for better procurement analytics, since our experience has shown usthat there are significant shortcomings in the quality of data being published. This article contributesto enhanced policy making by guiding public administrations at local, regional, and national levels onhow to improve the way they publish and use procurement-related data; developing technologies andsolutions that buyers in the public and private sectors can use and adapt to become more transparent,make markets more competitive, and reduce waste and fraud; and providing a Knowledge Graph,which is a data resource that is designed to facilitate integration across multiple data silos by showinghow it adds context and domain knowledge to machine-learning-based procurement analytics.Keywords: public procurement; fraud and corruption; data integration; knowledge graph; linkedopen data; anomaly detection1. IntroductionPublic procurement is a business impacting the lives of millions. Globally governmentsspend trillions of dollars a year on public contracts for goods, services, and works (https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/ (ac-cessed on 19 February 2022)); for example, public authorities in the European Union(EU) spend around 14% of GDP every year (https://ec.europa.eu/growth/single-market/public-procurement_en (accessed on 19 February 2022)). A market of such a size hassubstantial challenges, such as delivering quality services with greatly reduced bud-gets; preventing losses through fraud and corruption; and building healthy and sus-tainable economies. Even a small percentage of cost increase can easily have a massiveInformation 2022, 13, 99. https://doi.org/10.3390/info13020099 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13020099https://doi.org/10.3390/info13020099https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0002-2753-9917https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enhttps://doi.org/10.3390/info13020099https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13020099?type=check_update&version=20101001010Pred formationArticle",
        "publication_date": "2021-01-01",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": "10!1109%access!2021!3115577.pdf",
        "file_path": "./PDFs/10!1109%access!2021!3115577.pdf"
    },
    {
        "title": "Developing accessibility multimedia services",
        "implementation_urls": [],
        "doi": "10.1145/3389189.3397973",
        "abstract": "ABSTRACT People with various degrees of disabilities (e.g., visually or hearing impaired) often find it difficult to access mainstream products and services and thus they are excluded from enjoying audio-visual services on an equal basis as people without disabilities. These people feel marginalized in today’s digital society and they are unable to reach their maximum potential personally, professionally and socially. EasyTV aims to address these issues by providing numerous services that not only ease the access of people with disabilities to multimedia services, but also enhance interaction with the media through a multi-language approach that adapts content based on user’s preferences. In this work, the main EasyTV services are described in detail. Furthermore, an evaluation of the main services by end users is presented and discussed in order to highlight the importance of offering accessibility services to people with disabilities.  CCS CONCEPTS • Accessibility systems and tools • Accessibility technologies   • Interactive systems and tools KEYWORDS Accessibility, Interaction, Sign language, Personalization, Artificial Intelligence, Semantics 1 INTRODUCTION Even in the poorest countries and societies, media has great importance because it provides the channels, networks, formats, and languages through which much of life takes place and finds meaning. As a result, it is of real concern that people with disabilities may not have access to either the media or the information provided through specific channels (TV, movies, newspapers, magazines, the Internet, etc.). Ensuring a barrier-free life for people with disabilities and enabling access to the media and content for all is crucial for a full democratic participation.  Accessibility prevents or removes barriers to the use of mainstream products and services. It allows the perception, operation and understanding of these products and services by people with disabilities, on an equal basis with others. The inclusion of people with disabilities is a key objective in any program of social and economic measures to create a real Information Society for all, where each citizen has the same opportunities to develop to its maximum potential his/her personal, professional and social aptitudes. Achieving an inclusive digital society, in which people with disabilities have access to online services and information on an equal footing to people without disabilities is of paramount importance. Studies have shown that blind and visually impaired persons watch TV as much as normally-sighted persons as they consider TV watching to be an important family activity, while their desire is to improve the television experience [1]. On the other hand, the aged people link TV watching with their occupational well-being [2], highlighting the significance of ",
        "publication_date": "2020-06-29",
        "authors": "Dimitrios Konstantinidis, Kosmas Dimitropoulos, Kiriakos Stefanidis, Thanassis Kalvourtzis, Salim Gannoum, Nikolaos Kaklanis, Konstantinos Votis, Petros Daras, Sara Rovira-Esteva, Pilar Orero, Silvia Uribe, Francisco Moreno, A. Llorente, Pablo Calleja, María Poveda‐Villalón, Pasquale Andriani, Giuseppe Vitolo, Giuseppa Caruso, Nicolamaria Manes, Fabrizio Giacomelli, Jordi Fabregat, Francesc Mas, Jordi Mata, Stavros Skourtis, Chrysostomos Bourlis, Giuliano Frittelli, Emilio Ferreiro Lago, Federico Álvarez",
        "file_name": "3389189.3397973.pdf",
        "file_path": "./PDFs/3389189.3397973.pdf"
    },
    {
        "title": "Towards a Linked Democracy Model",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_4",
        "abstract": "Abstract In this chapter we lay out the properties of participatory ecosystems aslinked democracy ecosystems. The goal is to provide a conceptual roadmap thathelps us to ground the theoretical foundations for a meso-level, institutional theoryof democracy. The identification of the basic properties of a linked democracyeco-system draws from different empirical examples that, to some extent, exhibitsome of these properties. We then correlate these properties with Ostrom’s designprinciples for the management of common-pool resources (as generalised to groupscooperating and coordinating to achieve shared goals) to open up the question ofhow linked democracy ecosystems can be governed.Keywords Linked democracy � Common-Pool resources4.1 IntroductionIn previous chapters we have suggested that our model of linked democracy can berepresented as a three-layered, overlapping structure of Linked Open Data (LOD),linked platforms, and linked ecosystems. A linked democracy model represents thedistributed interplay between people, digital technologies, and data (see Fig. 4.1).We have also provided examples of digital platforms and ecosystems that exhibit acertain degree of connectedness by tapping on LOD, on open data, or on crowd-sourced data produced elsewhere.Breaking silos down is a common, distinctive feature of the examples we havereviewed. But are there any other properties than we can distill from these exam-ples? Moreover, is it possible to turn those properties into design principles thathelp to orchestrate a linked democracy model? Design principles should guide theimplementation of a linked democracy model; they should also capture the insti-tutional arrangements needed to produce aligned decision making in a givendomain, either local or global. As we have seen with the Icelandic or Mexico Cityexamples, a lack of institutional endorsement of carefully designed participatory© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_475http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_4outcomes can bring crowdsourced constitutional processes to a deadlock. Linkeddemocracy is about finding ways out of locked democracy.We are fully aware that generalizing specific design principles for the efficientfunctioning of a linked democracy would require an exhaustive, large-scale surveyof case studies. We have examined some illustrative examples in the previouschapters, but this falls short of providing a comprehensive panorama. Therefore, inthis chapter we will first identify some distinctive properties of a linked democracymodel based on our previous examples. Second, we will map these properties ontothe well-established set of design principles that Elinor Ostrom identified asenabling effective management of ‘common-pool resources’ (CPR) groups (Ostrom1990, 90–102). Recently, David Wilson et al. reviewed Ostrom’s principles from anevolutionary perspective to argue that they ‘have a wider range of application thanCPR groups and are relevant to nearly any situation where people must cooperateand coordinate to achieve shared goals’ (Wilson et al. 2013, 522). We considerlinked democracy ecosystems to be one of those situations involving cooperation—in performing a wide range of tasks—and coordination—of large groups of indi-",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "no_doi_20250624163805.pdf",
        "file_path": "./PDFs/no_doi_20250624163805.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_4.pdf"
    },
    {
        "title": "An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science",
        "implementation_urls": [
            {
                "identifier": "https://github.com/actionprojecteu/dmptool",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1108%dta-10-2020-0253.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A technical description can be found at https://zenodo.org/record/3885566 and the software is available with an open source license at https://github.com/actionprojecteu/dmptool and https://github.com/actionprojecteu/dmptool-generator."
                    }
                ]
            }
        ],
        "doi": "10.1108/dta-10-2020-0253",
        "abstract": "Abstract. Purpose: Citizen Science – public participation in scientific projects – is becoming a global practice engaging volunteer participants, often non-scientists, with scientific research. Citizen Science is facing major challenges, such as quality and consistency, to reap open the full potential of its outputs and outcomes, including data, software, and results. In this context, the principles put forth by Data Science and Open Science domains are essential for alleviating these challenges, which have been addressed at length in these domains. The purpose of this study is to explore the extent to which Citizen Science initiatives capitalise on Data Science and Open Science principles. Approach: We analysed 48 Citizen Science projects related to pollution and its effects. We compared each project against a set of Data Science and Open Science indicators, exploring how each project defines, collects, analyses and exploits data to present results and contribute to knowledge. Roman, D., Reeves, N., Gonzalez, E., Celino, I., Abd El Kader, S., Turk, P., Soylu, A., Corcho, O., Cedazo, R., Re Calegari, G., Scandolari, D. and Simperl, E. (2021), \"An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science\",Data Technologies and Applications, Vol. ahead-of-print No. ahead-of-print. https://doi.org/10.1108/DTA-10-2020-0253This author accepted manuscript is deposited under a Creative Commons Attribution Non-commercial 4.0 International (CC BY-NC) licence. This means that anyone may distribute, adapt, and build upon the work for non-commercial purposes, subject to full attribution. If you wish to use this manuscript for commercial purposes, please contact permissions@emerald.com.https://creativecommons.org/licenses/by-nc/4.0/2   Findings: The results indicate several shortcomings with respect to commonly accepted Data Science principles, including lack of a clear definition of research problems and limited description of data management and analysis processes, and Open Science principles, including lack of the necessary contextual information for reusing project outcomes.  Originality: In the light of this analysis, we provide a set of guidelines and recommendations for better adoption of Data Science and Open Science principles in Citizen Science projects, and introduce a software tool to support this adoption, with a focus on preparation of data management plans in Citizen Science projects.  Keywords: Citizen Science, Data Science, Open Science, pollution projects, Data Management Plan, software 1.  Introduction Citizen Science (CS) describes the active engagement of volunteer participants within scientific research. Projects vary greatly in terms of the role that volunteers play and the degree of agency that they have, from more passive models where volunteers install software and sensors to more collaborative models where volunteers actively define problem spaces and research topics (Haklay, 2013). Nevertheless, CS commonly entails the gathering of data by volunteers for later dissemination and publication (Haklay, 2013; Pocock et al., 2017).  CS data are of significant value not only in the projects in which they are gathered, but for subsequent analysis and re-use (Wang et al., 2015). Volunteer-contributed data may complement and offer the opportunity to contextualise and expand upon existing monitoring efforts by professional organisations, without significant added cost (Hadj-Hammou et al., 2017). In some contexts, the vast majority of available data are from CS sources (Groom et al., 2017; Poisson et al., 2020). This is equally true of software, with CS projects developing a wide variety of software ",
        "publication_date": "2021-05-04",
        "authors": "Dumitru Roman, Neal Reeves, Esteban González, Irene Celino, Shady Abd El Kader, Philip Turk, Ahmet Soylu, Óscar Corcho, R. Cedazo, Gloria Re Calegari, Damiano Scandolari, Elena Simperl",
        "file_name": "10!1108%dta-10-2020-0253.pdf",
        "file_path": "./PDFs/10!1108%dta-10-2020-0253.pdf"
    },
    {
        "title": "Increasing the Intensity over Time of an Electric-Assist Bike Based on the User and Route: The Bike Becomes the Gym",
        "implementation_urls": [],
        "doi": "10.3390/s18010220",
        "abstract": "Abstract: Nowadays, many citizens have busy days that make finding time for physical activitydifficult. Thus, it is important to provide citizens with tools that allow them to introduce physicalactivity into their lives as part of the day’s routine. This article proposes an app for an electricpedal-assist-system (PAS) bicycle that increases the pedaling intensity so the bicyclist can achievehigher and higher levels of physical activity. The app includes personalized assist levels that havebeen adapted to the user’s strength/ability and a profile of the route, segmented according to itsslopes. Additionally, a social component motivates interaction and competition between users basedon a scoring system that shows the level of their performances. To test the training module, acase study in three different European countries lasted four months and included nine people whotraveled 551 routes. The electric PAS bicycle with the app that increases intensity of physical activityshows promise for increasing levels of physical activity as a regular part of the day.Keywords: personalized assistance level; coaching; physical activity; electric bicycles1. IntroductionAdvances in the field of technology and developments in common transport systems have greatlyreduced people’s physical activity [1]. In developed countries, the majority of people travel to andfrom work using motor transport systems, e.g., statistics from 2011 show that, in England and Wales,85% of the population used motorized transport as their usual commute mode [2]. Nowadays, citizensspend much more time at sedentary activities, such as working in front of the computer. In 2012, datafrom 66 high and low income countries show that the percentage of adults who spent four or morehours sitting each day was 41.5% [1]. Due to these changes in our lifestyle, the risk of suffering healthproblems as a result of physical activity is increasingly high [3]. Experts from diverse entities, suchas WHO (World Health Organization), recommend an average of 150 min of exercise per week or30 min daily [4]. Physical activity provides a well-known set of health benefits [5]. Exercise has beenproven to reduce the risk of suffering from high blood pressure, stroke and others [4]. It increasescardiorespiratory and muscular fitness, bone health or increased functional health. Moreover, it canhelp prevent depression [4]. In 2010, the Global Health Observatory (GHO) estimated that the dailyphysical activity of more than 20% of adults is insufficient [6]. The low exercise, combined with thedaily ingestion of fat and calorie rich foods, is leading our society to an obesity epidemic [7].Nowadays, people who wish to make exercise a part of their daily routine usually go to gymsor sign up for different sports. This commitment implies an economic cost of registration and sportsequipment, travel to sports centers, as well as the necessary free time to carry out the activity and aSensors 2018, 18, 220; doi:10.3390/s18010220 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743https://orcid.org/0000-0002-2829-1829http://dx.doi.org/10.3390/s18010220http://www.mdpi.com/journal/sensorsSensors 2018, 18, 220 2 of 21willingness to attend regularly. Performing a team sport (e.g., basketball, football or volleyball) alsorequires adequate sports facilities, a group of people to carry out the activity and a skill in that sport.For these reasons, many people do not perform physical activity regularly [8,9]. As a result, the dailyuse of a bicycle in routine trips is an important alternative to the gym or other sports [10].One of the most widespread ways of fostering active transport among users is by promoting theuse of bicycles in the city [11,12]. Biking will not only help people to get fit but will also reduce trafficcongestion, environmental contamination, climate change and energetic sustainability [13,14]. Theupgrading of infrastructures for cyclists helps provide a positive experience and as a result increases theuse of bicycles in the city [15,16]. In recent years, cities have been promoting the use of bicycles through",
        "publication_date": "2018-01-14",
        "authors": "Daniel H. de la Iglesia, Juan F. De Paz, Gabriel Villarrubia González, Alberto López Barriuso, Javier Bajo, Juan M. Corchado",
        "file_name": "no_doi_20250624163814.pdf",
        "file_path": "./PDFs/no_doi_20250624163814.pdf",
        "pdf_link": "https://www.mdpi.com/1424-8220/18/1/220/pdf?version=1515922049"
    },
    {
        "title": "SPARQL2Flink: Evaluation of SPARQL Queries on Apache Flink",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oscarceballos/sparql2flink",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624163836.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/oscarceballos/sparql2flink (accessed on 24 March 2020)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app11157033",
        "abstract": "Abstract: Existing SPARQL query engines and triple stores are continuously improved to handlemore massive datasets. Several approaches have been developed in this context proposing the storageand querying of RDF data in a distributed fashion, mainly using the MapReduce ProgrammingModel and Hadoop-based ecosystems. New trends in Big Data technologies have also emerged (e.g.,Apache Spark, Apache Flink); they use distributed in-memory processing and promise to deliverhigher data processing performance. In this paper, we present a formal interpretation of some PACTtransformations implemented in the Apache Flink DataSet API. We use this formalization to providea mapping to translate a SPARQL query to a Flink program. The mapping was implemented in aprototype used to determine the correctness and performance of the solution. The source code of theproject is available in Github under the MIT license.Keywords: massive static RDF data; SPARQL; PACT Programming Model; Apache Flink1. IntroductionThe amount and size of datasets represented in the Resource Description Framework(RDF) [1] language are increasing; this leads to challenging the limits of existing triple storesand SPARQL query evaluation technologies, requiring more efficient query evaluationtechniques. Several proposals have been documented in state of the art use of Big Datatechnologies for storing and querying RDF data [2–6]. Some of these proposals havefocused on executing SPARQL queries on the MapReduce Programming Model [7] and itsimplementation, Hadoop [8]. However, more recent Big Data technologies have emerged(e.g., Apache Spark [9], Apache Flink [10], Google DataFlow [11]). They use distributedin-memory processing and promise to deliver higher data processing performance thantraditional MapReduce platforms [12]. These technologies are widely used in researchprojects and all kinds of companies (e.g., Google, Twitter, and Netflix, or even by smallstart-ups).To analyze whether or not we can use these technologies to provide query evaluationover large RDF datasets, we will work with Apache Flink, an open-source platform fordistributed stream and batch data processing. One of the essential components of theFlink framework is the Flink optimizer called Nephele [13]. Nephele is based on the Paral-lelization Contracts (PACTs) Programming Model [14] which is in turn a generalization ofthe well-known MapReduce Programming Model. The output of the Flink optimizer is acompiled and optimized PACT program which is a Directed Acyclic Graphs (DAG)-baseddataflow program. At a high level, Flink programs are regular programs written in Java,Appl. Sci. 2021, 11, 7033. https://doi.org/10.3390/app11157033 https://www.mdpi.com/journal/applscihttps://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0001-5214-4127https://orcid.org/0000-0002-6488-8649https://orcid.org/0000-0002-2858-0276https://orcid.org/0000-0002-0236-4284https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/app11157033https://doi.org/10.3390/app11157033https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/app11157033https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app11157033?type=check_update&version=2ririrg appliedA sciences",
        "publication_date": "2021-07-30",
        "authors": "Oscar Ceballos, Carlos Alberto Ramírez Restrepo, María Constanza Pabón, Andrés M. Castillo, Óscar Corcho",
        "file_name": "no_doi_20250624163836.pdf",
        "file_path": "./PDFs/no_doi_20250624163836.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/11/15/7033/pdf?version=1628044316"
    },
    {
        "title": "Assigning Creative Commons Licenses to Research Metadata: Issues and Cases",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_16",
        "arxiv": "1609.05700",
        "abstract": "Abstract. This paper discusses the problem of lack of clear licensing and transparency of usage terms and conditions for research metadata. Making research data connected, discoverable and reusable are the key enablers of the new data revolution in research. We discuss how the lack of transparency hinders discovery of research data and make it disconnected from the publication and other trusted research outcomes. In addition, we discuss the application of Creative Commons licenses for research metadata, and provide some examples of the applicability of this approach to internationally known data infrastructures. Keywords. Semantic Web, research metadata, licensing, discoverability, data infrastructure, Creative Commons, open data  Introduction The emerging paradigm of open science relies on increased discovery, access, and sharing of trusted and open research data. New data infrastructures, policies, principles, and standards already provide the bases for data-driven research. For example, the FAIR Guiding Principles for scientific data management and stewardship [23] describe the four principles—findability, accessibility, interoperability, and reusability—that should inform how research data are produced, curated, shared, and stored. The same principles are applicable to metadata records, since they describe datasets and related research information (e.g. publications, grants, and contributors) that are essential for data discovery and management. Research metadata are an essential component of the open science ecosystem and, as stated in [17], “for a molecule of research metadata to                                                              1 Corresponding Author: marta.pobletbalcell@rmit.edu.au move effectively between systems, the contextual information around it - the things that are linked to, must also be openly and persistently available”.  Yet, finding relevant, trusted, and reusable datasets remains a challenge for many researchers and their organisations. New discovery services address this issue by drawing on open public information, but the lack of transparency about legal licenses and terms of use for metadata records compromises their reuse. If licenses and terms of use are absent or ambiguous, discovery services lack basic information on how metadata records can be used, to what extent they can be transformed or augmented, or whether they can be utilised as part of commercial applications. Ultimately, legal uncertainty hinders investment and innovation in this domain. The rest of this paper is organised as follows: Section 1 presents the most widely adopted research metadata protocols and practices; Section 2 provides some global figures about the types of licenses used for research metadata; Section 3 identifies the main stakeholders; Section 4 reviews the most common choices for metadata licenses and discusses both advantages and disadvantages of such choices; Section 5 offers six compact case studies from different research data services. Finally, the conclusion raises some questions to guide future work.  1. Research metadata protocols and practices   A number of instruments covering the management of research metadata are currently available. For example, the Open Archives Initiative (OAI) developed the Protocol for Metadata Harvesting OAI-PMH to facilitate interoperability between repositories and metadata service providers [14]. OAI-PMH enables harvesting the metadata of open access repositories such as PubMed, Arxiv, HAL, the Wikipedia [5], or the World Bank’s Open Knowledge Repository (OKR).  The Dublin Core Metadata Initiative (DCMI) promotes interoperability and reusability in metadata design and best practices by developing semantic standards and recommendations, model-based specifications, and syntax guidelines, such as the ",
        "publication_date": "2018-01-01",
        "authors": "Marta Poblet, Amir Aryani, Paolo Manghi, Kathryn Unsworth, Jingbo Wang, Brigitte Hausstein, Sünje Dallmeier-Tiessen, Claus-Peter Klas, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "10!1007%978-3-030-00178-0_16.pdf",
        "file_path": "./PDFs/10!1007%978-3-030-00178-0_16.pdf"
    },
    {
        "title": "Development Experience of a Context-Aware System for Smart Irrigation Using CASO and IRRIG Ontologies",
        "implementation_urls": [],
        "doi": "10.3390/app10051803",
        "abstract": "Abstract: The rapid development of information and communication technologies and wirelesssensor networks has transformed agriculture practices. New tools and methods are used to supportfarmers in their activities. This paper presents a context-aware system that automates irrigationdecisions based on sensor measurements. Automatic irrigation overcomes the water shortageproblem, and automatic sensor measurements reduce the observational work of farmers. This paperfocuses on a method for developing context-aware systems using ontologies. Ontologies are usedto solve heterogeneity issues in sensor measurements. Their main goal is to propose a shared dataschema that precisely describes measurements to ease their interpretations. These descriptions arereusable by any machine and understandable by humans. The context-aware system also containsa decision support system based on a rules inference engine. We propose two new ontologies:The Context-Aware System Ontology addresses the development of the context-aware systemin general. The Irrigation ontology automates a manual irrigation method named IRRINOV®.These ontologies reuse well-known ontologies such as the Semantic Sensor Network (SSN) and SmartAppliance REFerence (SAREF). The decision support system uses a set of rules with ontologies toinfer daily irrigation decisions for farmers. This project uses real experimental data to evaluate theimplementation of the decision support system.Keywords: agriculture; smart irrigation; context-aware system; ontology; rules1. IntroductionIn the agricultural domain, farmers need to observe natural phenomena to engage in appropriateactivities on their fields. For example, in traditional irrigation, farmers go to their fields to examineAppl. Sci. 2020, 10, 1803; doi:10.3390/app10051803 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3517-0945https://orcid.org/0000-0002-3076-5499https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-3459-8568https://orcid.org/0000-0002-7011-4535http://dx.doi.org/10.3390/app10051803http://www.mdpi.com/journal/applsciAppl. Sci. 2020, 10, 1803 2 of 41the crop development stage and measure the soil moisture provided by probes in the soil. Then, theyuse practical experience or follow an irrigation method to estimate manually the water needs of theircrops. Based on their estimations, the farmers decide whether to irrigate the fields. This conventionalapproach has two significant drawbacks. First, irrigation requires daily observations, often made byfarmers. Manual observations are influenced by other factors such as the weather or the situation offarmers. For example, a daily observation could be skipped if the farmer is sick. Second, the resourceshortage problem demands that farmers use water sparingly.Context-aware systems (CASs) can overcome the above-mentioned situations. A CAS uses awireless sensor network (WSN) to monitor environmental phenomena and uses those measurementsfor further processes. In a CAS, context refers to “any information that can characterize the situation ofan entity. An entity could be a person, a place or an object that is considered relevant to the interactionbetween a user and an application, including the user and the application themselves” [1]. A CAShas two contexts: a low-level context and a high-level context [2]. The low-level context containsquantitative data. The high-level context contains qualitative data that synthesize a situation and easethe decision. For example, the statement “soil moisture is 160 centibar (cbar)” presents a low-levelcontext; however, the statement “soil is dry” presents a high-level context.The CAS has some peculiar characteristics. First, sensors in the system are heterogeneous. Eachtype of phenomenon demands a different type of sensor. For example, in agriculture, pluviometersmeasure rain quantity, and tensiometers measure soil moisture. Thus, the CAS should address",
        "publication_date": "2020-03-05",
        "authors": "Quang-Duy Nguyen, Catherine Roussey, María Poveda‐Villalón, Christophe de Vaulx, Jean-Pierre Chanet",
        "file_name": "no_doi_20250624163908.pdf",
        "file_path": "./PDFs/no_doi_20250624163908.pdf",
        "pdf_link": "https://www.mdpi.com/2076-3417/10/5/1803/pdf?version=1583938924"
    },
    {
        "title": "Applying the LOT Methodology to a Public Bus Transport Ontology aligned with Transmodel: Challenges and Results",
        "implementation_urls": [
            {
                "identifier": "https://github.com/CiudadesAbiertas/vocab-transporte-autobus",
                "type": "git",
                "paper_frequency": 12,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-210451.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Its corresponding GitHub repository with all the intermediate and final artefacts is available at https://github.com/CiudadesAbiertas/vocab-transporte-autobus including a Readme in English.6 The scope of this ontology on public bus transportation is focused on the representation of static information related to lines, routes, stops and timetables, and real time information on expected arrival times to bus stops."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210451",
        "abstract": "Abstract. We present an ontology that describes the domain of Public Transport by bus, which is common in cities around theworld. This ontology is aligned to Transmodel, a reference model which is available as a UML specification and which was devel-oped to foster interoperability of data about transport systems across Europe. The alignment with this non-ontological resourcerequired the adaptation of the Linked Open Terms (LOT) methodology, which has been used by our team as the methodologicalframework for the development of many ontologies used for the publication of open city data. The ontology is structured intothree main modules: (1) agencies, operators and the lines that they manage, (2) lines, routes, stops and journey patterns, and (3)planned vehicle journeys with their timetables and service calendars. Besides reusing Transmodel concepts, the ontology alsoreuses common ontology design patterns from GeoSPARQL and the SOSA ontology. As part of the LOT data-driven valida-tion stage, RDF data has been generated taking as input the GTFS feeds (General Transit Feed Specification) provided by theMadrid public bus transport provider (EMT). Mapping rules from structured data sources to RDF were developed using the RDFMapping Language (RML) to generate RDF data, and queries corresponding to competency questions were tested.Keywords: Ontology, Transmodel, public bus, Open Cities, RDF1. IntroductionOpen data initiatives across public administrations worldwide date back to more than a decade ago. In the specificcase of Spanish cities, the most relevant landmarks are associated to the first transposition of the EU Public SectorInformation directive in 2007,1 the publication of the UNE 178301:2015 technical norm on Open Data for SmartCities,2 and the development of the open data guide by the Spanish Federation of Municipalities and Provinces(FEMP) in 2017 [8] and the catalogue of high-value open datasets for cities in 2019 [9].*Corresponding author. E-mail: eruckhaus@fi.upm.es.1https://eur-lex.europa.eu/eli/dir/2003/98, https://www.boe.es/eli/es/l/2007/11/16/37/con.2https://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N00543181570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:eruckhaus@fi.upm.esmailto:adolfo.anton.bravo@upm.esmailto:ocorcho@fi.upm.esmailto:mario.scrocca@cefriel.commailto:eruckhaus@fi.upm.eshttps://eur-lex.europa.eu/eli/dir/2003/98https://www.boe.es/eli/es/l/2007/11/16/37/conhttps://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N0054318https://creativecommons.org/licenses/by/4.0/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-210451&domain=pdf&date_stamp=2021-11-22640 E. Ruckhaus et al. / Applying the LOT Methodology to a Public Bus Transport Ontology aligned with TransmodelDomains that have been addressed in these initiatives include public sector, demography, environment, economy,commerce, transport and treasury, among others. As part of the initiatives and projects that have led the advance-ment of open data among cities in Spain we can cite the Ciudades Abiertas3 project, a public-private collaborativeproject led by four Spanish municipalities (Zaragoza, Madrid, Santiago de Compostela and A Coruña) with thegeneral aim to facilitate the implementation of common Open Government policies that are reusable by many othermunicipalities inside and outside Spain.Among the project actions on open data, several (12) ontologies are being developed using the Linked OpenTerms (LOT) methodology [5,17]. These ontologies allow publishing Open Data homogeneously across cities,using common CSU structures, as well as following Linked Data principles [29]. They are being added to thosethat had been already developed in the context of the Spanish network of Open Data for Smart Cities,4 and theycorrespond to a subset of the catalogue of datasets included in the aforementioned FEMP open data guide [9]. Allof the ontologies are publicly available and versioned in GitHub,5 with the corresponding repositories including usecases and user stories, requirements, the ontology implementation in OWL, the ontology HTML documentation inSpanish and English, and example data and queries.In the area of transport, three ontologies have been developed so far under the umbrella of these initiatives,focused on the representation of open data about Public Bicycles, Motor Vehicle Traffic and Public Bus Transport.",
        "publication_date": "2021-11-23",
        "authors": "Edna Ruckhaus, Adolfo Antón Bravo, Mario Scrocca, Óscar Corcho",
        "file_name": "10!3233%sw-210451.pdf",
        "file_path": "./PDFs/10!3233%sw-210451.pdf"
    },
    {
        "title": "Incentivized Data Mules Based on State-Channels",
        "implementation_urls": [],
        "doi": "10.1109/icbc54727.2022.9805515",
        "abstract": "Abstract—Many services that are taken for granted in smartcities are not even remotely available in dislocated areas, i.e.“smart territories”. With the aim to offer a practical and secureway to transport data in such constrained scenarios, we focus onthe problem of incentivizing to Data Mules, i.e. devices dedicatedto enable communication even in the absence of the Internet. Wecombine decentralized technologies and State-Channels to verifythe correct behavior of participants in an offline scenario.Index Terms—Data Mules, State Channels, Smart TerritoriesI. INTRODUCTION AND BACKGROUNDEvents linked to the COVID-19 pandemic have shown thatmore and more people decides to move towards countrysidesand rural areas, i.e. to the smart territories [1]. For some un-derprivileged smart territories cases it is not possible to imple-ment (costly) smart city services due to not being supported bya wide area network connectivity, and certain networking solu-tions might result to be too costly (e.g. satellite connections).We argue that what is needed is a set of novel opportunisticsolutions, which allows us to share and reuse data, services,computation and bandwidth [1]. Data Mules (acronym forMobile Ubiquitous LAN Extensions), for instance, are sometypes of devices that, even in the absence of Internet, areable to collect data from sensors and to exploit their ownmobility to carry the information to destination by using awireless short-range communication medium, and effectivelycreate a data communication link [2]. In this paper we presentInDaMul, a decentralized application that combines the useof Distributed Ledger Technologies (DLTs) and DecentralizedFile Storages (DFS), mostly for verifying the correct behaviorof all the participants and to incentivize them in Data Mule-based communications. DLTs are cryptographically guaranteedto be tamper-proof and unforgeable, enabling the creation ofa “trusted” mechanisms. On top of that, Smart Contracts areinstructions stored in the ledger and automatically triggeredonce a required condition is met. In this work, we will refer tothe Ethereum Smart Contracts implementation and to secondlayer cryptocurrencies, i.e. ERC-20 tokens [3].We make use of a layer-two protocol that can also beexecuted without the need of constantly being connected to theInternet and where the communication conducted only amongnodes in the physical vicinity suffices, i.e. State Channels [3].A state channel is opened between a sender and a receiver,through some tokens deposit in a Smart Contract, and thenThis work has received funding from the EU H2020 MSCA ITN EuropeanJoint Doctorate grant agreement No 814177 LAST-JD - RIoE and from theUniversity of Urbino through the “Bit4Food” research project.both actors can interact to adjust the channel token balancethrough off-chain, i.e. outside of the ledger, messages fromtime to time. When more channels are opened involving thesame actors, then this consists of an establishment of state",
        "publication_date": "2022-05-02",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele DrAngelo",
        "file_name": "Incentivized_Data_Mules_Based_on_State-Channels.pdf",
        "file_path": "./PDFs/Incentivized_Data_Mules_Based_on_State-Channels.pdf"
    },
    {
        "title": "Language Resources as Linked Data for the Legal Domain",
        "implementation_urls": [],
        "doi": "10.3233/faia190019",
        "abstract": "Abstract. This Chapter describes a four-stage methodology to generate LinguisticLinked Data for the legal domain: identification, creation, transformation (to RDF)and linking. The goal of this process is to enhance the presence of legal languageresources in the Linguistic Linked Open Data cloud. Since this Chapter is framedwithin the H2020 LYNX project, aimed at creating a Legal Knowledge Graph, aparallel objective is to employ the resources generated as a linguistic foundation toannotate, classify and translate the legal resources represented in this graph.Keywords. legal language resources, terminology extraction, linked data, legalknowledge graph1. IntroductionOriginally, language resources have been considered as works that collect any type oflinguistic information. For the purposes of this work, we define language resources aspieces of data containing linguistic information in machine readable forms. There areseveral types of language resources depending on their format and the type of infor-mation represented: glossaries and terminologies (specialised terms), lexical databases(linguistic knowledge for computers), dictionaries (general terms), thesauri (hierarchi-cal controlled vocabularies), etc. Many general dictionaries are available online, such asMerriam Webster1 and Oxford Dictionary2; other terminological resources containingspecialised knowledge can also be found on the Internet, such as TermSciences3 andUNterm4. However, language resources for the legal domain are not that present in theWeb, since they tend to be owned by legal publishers, thus, not accessible and some-times published in obsolete and proprietary formats. Moreover, legal jargon is intricateand the meaning of terms varies as the legal framework changes. Updating non-machinereadable legal glossaries is a time-consuming and difficult task to accomplish. On theother hand, a good understanding of legal terminology is essential to comprehend legaldocumentation, which also tends to be outdated.To soften the mentioned hindrances regarding legal terminology and legal documen-tation, the LYNX project aims at creating a Legal Knowledge Graph (LKG), that is in-1https://www.merriam-webster.com/.2https://www.oxforddictionaries.com/.3http://www.termsciences.fr/termsciences/?lang=en.4https://unterm.un.org/UNTERM/portal/welcome.aKnowledge of the Law in the Big Data AgeG. Peruginelli and S. Faro (Eds.)© 2019 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA190019170terlinking public and private legal resources, metadata, standards and general open datafrom the legal domain. The idea is to offer access to updated multilingual and multi-jurisdictional legal information. For that purpose, a steady open-access legal languagefoundation is required.Linked Data [1] is a particularly convenient form to create such a language cloud,since it is intended to publish interlinked machine-readable data in open-source and non-proprietary formats. In fact, the Linguistic Linked Open Data (LLOD)5 cloud gatherslanguage resources published according to the Linked Data Principles [2], following theRDF set of W3C specifications. Again, within this cloud, legal knowledge is underrep-resented. The objective of this contribution is to create the Linguistic Legal Linked Open",
        "publication_date": "2019-01-01",
        "authors": "Martín-Chozas Patricia, M. Silva Elena, Rodríguez-Doncel Víctor",
        "file_name": "78616.pdf",
        "file_path": "./PDFs/78616.pdf"
    },
    {
        "title": "An estimation of heavy-duty vehicle fleet CO2 emissions based on sampled data",
        "implementation_urls": [],
        "doi": "10.1016/j.trd.2021.102784",
        "publication_date": "2021-03-25",
        "authors": "Nikiforos Zacharof, Georgios Fontaras, Biagio Ciuffo, Alessandro Tansini, Ignacio-Iker Prado-Rujas",
        "file_name": "1-s2.0-S1361920921000882-main.pdf",
        "file_path": "./PDFs/1-s2.0-S1361920921000882-main.pdf"
    },
    {
        "title": "Towards Automated Hypothesis Testing in Neuroscience",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-33752-0_18",
        "abstract": "Abstract. Scientific data generation in the world is continuous. How-ever, scientific studies once published do not take advantage of new data.In order to leverage this incoming flow of data, we present Neuro-DISK,an end-to-end framework to continuously process neuroscience data andupdate the assessment of a given hypothesis as new data become avail-able. Our scope is within the ENIGMA consortium, a large internationalcollaboration for neuro-imaging and genetics whose goal is to under-stand brain structure and function. Neuro-DISK includes an ontologyand framework to organize datasets, cohorts, researchers, tools, work-ing groups and organizations participating in multi-site studies, such asthose of ENIGMA, and an automated discovery framework to continu-ously test hypotheses through the execution of scientific workflows. Weillustrate the usefulness of our approach with an implemented example.Keywords: Hypothesis evaluation · Scientific workflow · Ontology ·Automated discovery · Neuroscience1 IntroductionScientific discoveries are based on hypothesis testing and rigorous data analysis.Such analyses are often time consuming and include steps that are difficult tointerpret from scientific publications, and therefore, hard to systemically repro-duce. Often, the designed hypothesis is tested only once against the acquired datasample and later archived. Interestingly, in empirical sciences such as the biolog-ical sciences, it is not uncommon for a hypothesis to yield contradictory resultswhen evaluated on different data samples. In our data-driven world, data thatD. Garijo and S. Fakhraei—Co-first author.c© Springer Nature Switzerland AG 2019V. Gadepally et al. (Eds.): DMAH 2019/Poly 2019, LNCS 11721, pp. 249–257, 2019.https://doi.org/10.1007/978-3-030-33752-0_18http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-33752-0_18&domain=pdfhttps://doi.org/10.1007/978-3-030-33752-0_18250 D. Garijo et al.may be potentially relevant for testing a hypothesis is being continuously gen-erated but is often not studied to its full potential for hypothesis re-evaluationin combination with other related data. The lack of an integrated system toconstantly monitor the hypothesis of interest and update the underlying anal-ysis when new data become available, is one of the challenges for automatichypothesis re-evaluation. Having a framework that can keep such hypothesesalive requires systematically capturing the knowledge about the data and ana-lytics involved in the hypothesis testing, which is often heterogeneous and com-partmentalized.In this paper, we propose a solution to address the above challenges in theneurosciences based on our previous work for Automated DIscovery of ScientificKnowledge (DISK) [1]. We have extended DISK to explore brain-aging relatedhypothesis and data by generalizing the ability for the system to connect toexternal knowledge bases, including projects available within the EnhancingNeuro Imaging Genetics through Meta-Analysis (ENIGMA)1 consortium [2],a neuroscience collaboration where projects span many contributors from dif-ferent institutions around the world. In our proposed solution we address chal-lenges of data, analytics, and hypothesis complexity. The data shared throughimaging initiatives such as the ENIGMA consortium includes multiple levels ofheterogeneity, and are regularly expanding in volume. The analytics related to",
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Shobeir Fakhraei, Varun Ratnakar, Qifan Yang, Hanna Endrias, Yibo Ma, Regina Wang, Michael M. Bornstein, Joanna K. Bright, Yolanda Gil, Neda Jahanshad",
        "file_name": "978-3-030-33752-0_18.pdf",
        "file_path": "./PDFs/978-3-030-33752-0_18.pdf"
    },
    {
        "title": "Towards the Assessment of Easy-to-Read Guidelines Using Artificial Intelligence Techniques",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-58796-3_10",
        "abstract": "Abstract. The Easy-to-Read (E2R) Methodology was created to improve thedaily life of people with cognitive disabilities, who have difficulties in readingcomprehension. The main goal of the E2R Methodology is to present clear andeasily understood documents. This methodology includes a set of guidelines andrecommendations that affect the writing of texts, the supporting images, thedesign and layout of documents, and the final editing format. Such guidelinesare used in the manual processes of (a) adapting existing documents and(b) producing new materials. The process of adapting existing documents iscyclic and implies three activities: analysis, transformation, and validation. Allthese activities are human resource consuming, due to the need of involvingpeople with cognitive disabilities as well as E2R experts. In order to alleviatesuch processes, we are currently investigating the development of methods,based on Artificial Intelligence (AI) techniques, to perform the analysis andtransformation of documents in a (semi)-automatic fashion. In this paper wepresent our AI-based method for assessing a particular document with respect tothe E2R guidelines as well as an initial implementation of such a method; ourresearch on the transformation of documents is out of the scope of this paper.We carried out a comparative evaluation of the results obtained by our initialimplementation against the results of the document analysis performed bypeople with cognitive disabilities.Keywords: E2R methodology � Cognitive accessibility � Artificial intelligence1 IntroductionPeople with cognitive disabilities have some problems related to reading comprehen-sion, communication, and ability to respond to routine situations as well as to chal-lenging scenarios. These obstacles become a daily barrier in the understanding,interaction, and use of products and services in different environments. To overcome© Springer Nature Switzerland AG 2020K. Miesenberger et al. (Eds.): ICCHP 2020, LNCS 12376, pp. 74–82, 2020.https://doi.org/10.1007/978-3-030-58796-3_10https://orcid.org/0000-0003-3807-5019http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-58796-3_10&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-58796-3_10&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-58796-3_10&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-58796-3_10the aforementioned barriers and to improve the daily life of people with cognitivedisabilities, a methodology called Easy-to-Read (E2R) [1, 2] was created. Thismethodology aims to present clear and easy to understand contents to different sectorsof the population that include people with disabilities and people with limited languageor reading proficiency, among others. The E2R Methodology provides guidelines forcontent, language, illustrations, as well as graphic layout in documents. The final aim isto have materials that are compliant with the E2R guidelines and recommendations,such as to use simple and short phrases, to avoid technicalities, abbreviations, acro-nyms, among others, to have an adequate typography, to include pictures that reinforcethe message and clarify the content, and to select an editing format that is easy to useand convenient for the expected use of the material.When a particular material needs to be adapted to the E2R Methodology, three keyactivities are performed as part of a cyclic process: (1) analysis of which E2R guide-lines are fulfilled in the material, this can be seen as a kind of assessment activity;(2) transformation of the material by E2R experts based on the unsatisfied guidelinesdiscovered during the previous assessment; and (3) validation of the transformed",
        "publication_date": "2020-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Edna Ruckhaus, Jorge López-Guerrero, Maria Isabel Nogueira Cano, A. S. J. Cervera",
        "file_name": "978-3-030-58796-3_10.pdf",
        "file_path": "./PDFs/978-3-030-58796-3_10.pdf"
    },
    {
        "title": "PSM-Flow: Probabilistic Subgraph Mining for Discovering Reusable Fragments in Workflows",
        "implementation_urls": [],
        "doi": "10.1109/wi.2018.00-93",
        "abstract": "Abstract—Scientific workflows define computational processesneeded for carrying out scientific experiments. Existing workflowrepositories contain hundreds of scientific workflows, wherescientists can find materials and knowledge to facilitate workflowdesign for running related experiments. Identifying reusablefragments in growing workflow repositories has become in-creasingly important. In this paper, we present PSM-Flow, aprobabilistic subgraph mining algorithm designed to discovercommonly occurring fragments in a workflow corpus using amodified version of the Latent Dirichlet Allocation algorithm. Theproposed model encodes the geodesic distance between workflowsteps into the model for implicitly modeling fragments. PSM-Flow captures variations of frequent fragments while maintainingits space complexity bounded polynomially, as it requires nocandidate generation. We applied PSM-Flow to three real-worldscientific workflow datasets containing more than 750 workflowsfor neuroimaging analysis. Our results show that PSM-Flowoutperforms three state of the art frequent subgraph miningtechniques. We also discuss other potential future improvementsof the proposed method.I. INTRODUCTIONScientific workflows describe computational experimentswhich typically involve computational steps, along with thedatasets used and generated by those steps. Scientific work-flows are created in workflow systems that manage theirexecution with the required computational resources [13]. Rep-resenting workflows explicitly improves the reproducibility ofscientific experiments [12].Scientific workflow repositories contain collections ofrecorded scientific workflows. [23] Users may explore andreuse workflows created by others to facilitate the developmentof their computational experiments. While one can directlyreuse an existing workflow, only a portion or fragment of aworkflow is often reused. In addition, identifying commonlyused fragments of workflows facilitates overviewing and ex-ploring the contents of a workflow repository. [11]In [10], the authors formulated reusable workflow fragmentidentification as a frequent subgraph mining problem and ap-plied frequent subgraph mining algorithms (FSM) to detect thesubgraphs with high support count (number of occurrences)as candidate fragments. However, frequent subgraph miningtechniques present several limitations. First, these techniquestypically involve a candidate fragment generation process anda subgraph isomorphism test. Both present a time complex-ity (combinatorial exploration of candidate fragments) and aspace complexity (a large number of candidate subgraphs aregenerated in memory) that are exponential in the worst case.Second, frequent fragments may appear in different workflowswith small variations (e.g., with changes in node labels, orwith an additional node). Conventional frequent subgraph",
        "publication_date": "2018-12-01",
        "authors": "Chin Wang Cheong, Daniel Garijo, Cheung Kwok Wai, Yolanda Gil",
        "file_name": "psmFlow.pdf",
        "file_path": "./PDFs/psmFlow.pdf"
    },
    {
        "title": "Taxi dispatching strategies with compensations",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2019.01.001",
        "arxiv": "2401.11553",
        "abstract": "Abstract Urban mobility efficiency is of utmost importance in big cities. Taxi vehicles are key elements in daily traffic activity. The advance of ICT and geo-positioning systems has given rise to new opportunities for improving the efficiency of taxi fleets in terms of waiting times of passengers, cost and time for drivers, traffic density, CO2 emissions, etc., by using more informed, intelligent dispatching. Still, the explicit spatial and temporal components, as well as the scale and, in particular, the dynamicity of the problem of pairing passengers and taxis in big towns, render traditional approaches for solving standard assignment problem useless for this purpose, and call for intelligent approximation strategies based on domain-specific heuristics. Furthermore, taxi drivers are often autonomous actors and may not agree to participate in assignments that, though globally efficient, may not be sufficently beneficial for them individually. This paper presents a new heuristic algorithm for taxi assignment to customers that considers taxi reassignments if this may lead to globally better solutions. In addition, as such new assignments may reduce the expected revenues of individual drivers, we propose an economic compensation scheme to make individually rational drivers agree to proposed modifications in their assigned clients. We carried out a set of experiments, where several commonly used assignment strategies are compared to three different instantiations of our heuristic algorithm. The results indicate that our proposal has the potential to reduce customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point of view.  Keywords: Coordination, dynamic fleet management, dynamic optimization, multi-agent systems, open systems, taxi assignment.  1. Introduction Urban mobility is one of the main concerns that public managers face in big cities nowadays. Traffic congestions generate a high quantity of CO2 emissions and cause extra time spent by travelers. One of the main actors involved in the daily traffic activity in urban areas are taxi fleets. They consist of several thousands of vehicles in big cities (e.g. about 15,000 taxis in Madrid, Spain). They are usually affiliated to mediator services, which coordinate service calls and taxi dispatching. Lately, new mobility systems that benefit from the advances in information and communication technologies have emerged, such as Uber1, Lyft2 or Liftago3 among others. Two of the main goals of a taxi fleet are (i) to reduce the response time (e.g., the time between a customer call and the moment a taxi arrives at the customer’s location) and (ii) reduce costs of empty movements (e.g., movements taxis have to make in order to get to the location of customers). The provision of efficient methods for taxi assignment to customers is a challenge that can contribute to reducing distances of empty trips with the resulting decrease of traffic flow, pollution, time and so on. Typically, taxi fleet coordination companies apply the first-come first-serve strategy to assign taxis to customers. Once the taxi accepts the passenger, the dispatching is irreversible. This method is known to be inefficient (Egbelu & Tanchoco, 1984).  1 http://www.uber.com 2 http://www.lyft.com 3 http://www.liftago.com https://doi.org/10.1016/j.eswa.2019.01.001 2 The aforementioned case falls into a specific class of assignment problems which is characterized by a dynamic demand in time and space. To efficiently solve such problems, dynamic algorithms are required instead of classical assignment optimization methods. For this purpose, techniques from the field of intelligent systems are promising, because they allow for developing heuristics-based algorithms that intelligently prune the search space, so as to reduce the computational complexity and to support a sufficient degree of scalability. Furthermore, taxi drivers are usually autonomous actors, i.e. they can freely choose whether to accept or to reject a recommendation proposed by the mediator service, which puts additional ",
        "publication_date": "2019-01-03",
        "authors": "Holger Billhardt, Alberto Fernández, Sascha Ossowskí, Javier Palanca, Javier Bajo",
        "file_name": "10!1016%j!eswa!2019!01!001.pdf",
        "file_path": "./PDFs/10!1016%j!eswa!2019!01!001.pdf"
    },
    {
        "title": "Improving the Results of Citizen Science Projects Through Reputation Systems: The Case of Wolf’s Number Experiment",
        "implementation_urls": [],
        "doi": "10.1109/access.2020.3030006",
        "publication_date": "2020-01-01",
        "authors": "R. Cedazo, Esteban González, M. Serra‐Ricart, Alberto Brunete",
        "file_name": "10!1109%access!2020!3030006.pdf",
        "file_path": "./PDFs/10!1109%access!2020!3030006.pdf"
    },
    {
        "title": "A review of mobile sensing systems, applications, and opportunities",
        "implementation_urls": [],
        "doi": "10.1007/s10115-019-01346-1",
        "abstract": "AbstractMobile phones, vehicles, appliances, and other types of devices have sensors in the last few years. On the good side, this makes the world increasingly interconnected every day. However, this interconnection generates Big Data that cannot be processed using tradi-tional tools because of its volume, variety, and speed. This paper contributes with a review of mobile sensing systems, including their applications, shortcomings, and opportuni-ties. A taxonomy covering the different systems revised is proposed. Moreover, the main characteristics of mobile sensing architectures are explained and research-related works are studied into the context of these characteristics. Multi-agent systems (MASs) are con-sidered as a perfect match to create large-scale, multi-device, and multi-purpose mobile sensing systems with the potential of obtaining information from heterogeneous devices, open sources, and social networks. Finally, the paper also contributes with the overview of a MAS architecture that aims to leverage these features while the studied dimensions observed in the reviewed literature are covered.Keywords  Mobile sensing · Multi-agent systems · Human agent societies ·  Social computing *\t Emilio Serrano \t emilioserra@fi.upm.es\t Francisco Laport‑López \t francisco.laport@udc.es\t Javier Bajo \t jbajo@fi.upm.es\t Andrew T. Campbell \t andrew.t.campbell@dartmouth.edu1\t Group of Electronic Technology and Communications, Department of Computer Engineering, University of A Coruña, A Coruña, Spain2\t Ontology Engineering Group, Artificial Intelligence Department, Universidad Politécnica de Madrid, Madrid, Spain3\t Dartmouth College, Hanover, NH, USAhttp://orcid.org/0000-0002-6543-8236http://orcid.org/0000-0001-7587-0703http://orcid.org/0000-0003-0824-4133http://crossmark.crossref.org/dialog/?doi=10.1007/s10115-019-01346-1&domain=pdf146\t F. Laport‑López et al.1 31  IntroductionDuring the last years, there has been a notable increase in population in the big cities, and workers move their homes and families to the new major cities where they can find more labor options, leaving rural life and areas. This population migration added with the great technological advance achieved in the last decades has generated a suitable environment for the creation and establishment of intelligent services in these overpopulated urban cent-ers. These services will use new technologies, capable of reacting to their environment, with the aim of improving the quality of life of citizens, helping to move toward sustaina-bility and supporting the economic development of the city. The new cities overpopulation coupled with the ubiquity of mobile phones and the ever-increasing appearance of intel-ligent devices in our daily lives such as smartwatches will provide a huge amount of data which can feed powerful tools for our society after an adequate processing and analysis. These tools have the potential of changing from everyday life of the citizens to important factors of the economy.The big challenge that appears with these new conditions for a sensing system is mainly ",
        "publication_date": "2019-03-08",
        "authors": "Francisco Laport, Emilio Serrano, Javier Bajo, Andrew T. Campbell",
        "file_name": "s10115-019-01346-1.pdf",
        "file_path": "./PDFs/s10115-019-01346-1.pdf"
    },
    {
        "title": "Traffic Optimization Through Waiting Prediction and Evolutive Algorithms",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.12.001",
        "abstract": "ABSTRACT Collective intelligence projects based on citizen participation are gaining momentum in today’ssociety. Citizen science applies crowdsourcing techniques to produce reliable data, quickly and easily. Theseprojects allow getting new knowledge and help professional scientists to come to real conclusions. This paperproposes that the use of reputation systems improves the results obtained in citizen science projects. To provethis hypothesis, a reputation system is applied to a real experiment and the results are analyzed. The goal ofthe experiment is to calculate the real-time solar activity, known as the Wolf number, using the infrastructureand user community of the GLORIA project (a set of professional robotic telescopes running since 2013).The sample size of the study are 196 end-users and 2,108 executions of the experiment. The key findingspresented in the paper are: 1) the online experiment with volunteers correctly reproduces the traditionalmethod of the year 1848 performed by astronomers or advanced amateurs, 2) the model is contrasted andvalidated with the values published by the official organization, and 3) the reputation system reduces theerror in calculations by more than half, discarding the contributions of the users with lowest karma.INDEX TERMS Citizen science, collective intelligence, crowdsourcing, reputation system, solar activity.I. INTRODUCTIONIn 1848, the Swiss astronomer Rudolf Wolf introduced amethod for registering solar activity by counting the numberof visible sunspots, known as the Wolf number, also knownworldwide as the International Sunspot Index. This method,updated in 2014 by several authors [1], has been used in thesolar activity records for the last 400 years [2] and continuesto be a rigorous method used in much research today [3], [4].Since 1981, the Solar Influences Data Analysis Center(SIDC) is the world data center for the Wolf number. Its mis-sion, as is indicated in its webpage,1 is to advance knowledgeabout the Sun and its influence on the solar system, throughresearch and observations.The associate editor coordinating the review of this manuscript andapproving it for publication was Feng Xia .1Website of the Solar Influences Data Analysis Center (SIDC):www.sidc.beThe SIDC produces the longest running time-series ofsolar activity, under the SILSO (Sunspot Index and Long-termSolar Observations) project.2 The data are freely availablefrom their website, as shown in Fig. 1, where is representedthe daily sunspot number (yellow), monthly mean sunspotnumber (blue), smoothed monthly sunspot number (red) forthe last 13 years and 12-month ahead predictions. It servesas a reference input to multiple applications in a wide rangeof scientific disciplines, such as studies of the solar cyclemechanism and of the solar forcing on the Earth’s climate.An important community is subscribed to its services, amongthem more than 500 for sunspot products; individuals (e.g.wider public, radio amateurs, space scientists, meteorologistsand paleo-climatologists); and institutions (e.g. public orga-nizations like the International Astronomical Union (IAU),2SILSO, World Data Center - Sunspot Number and Long-term SolarObservations, Royal Observatory of Belgium, on-line Sunspot Number cat-alogue: http://www.sidc.be/SILSO/, 2020.186026 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020https://orcid.org/0000-0002-4361-4331",
        "publication_date": "2023-01-01",
        "authors": "Francisco García, Helena Hernández, Marı́a N. Moreno Garcı́a, Juan F. De Paz, Vivian F. López, Javier Bajo",
        "file_name": "10!9781%ijimai!2023!12!001.pdf",
        "file_path": "./PDFs/10!9781%ijimai!2023!12!001.pdf"
    },
    {
        "title": "Developing Ontologies within Decentralised Settings",
        "implementation_urls": [],
        "doi": "10.1007/978-1-4419-5908-9_4",
        "abstract": "Abstract This chapter addresses two research questions: \"How should a well-engineered methodology facilítate the development of ontologies within com-munities of practice?\" and \"What methodology should be used?\" If ontologies are to be developed by communities then the ontology development life eyele should be better understood within this context. This chapter presents the Melting Point (MP), a proposed new methodology for developing ontologies within decentralised settings. It describes how MP was developed by taking best practices from other methodologies, provides details on recommended steps and recommended pro-cesses, and compares MP with alternatives. The methodology presented here is the product of direct first-hand experience and observation of biological communities of practice in which some of the authors have been involved. The Melting Point is a methodology engineered for decentralised communities of practice for which the designers of technology and the users may be the same group. As such, MP pro­vides a potential foundation for the establishment of standard practices for ontology engineering. 4.1 Introduction The maturity of a particular scientific discipline can be defined by its progress through three main stages. First, innovation followed by the subsequent dissemi-nation of the resulting knowledge or artefact. Second, the formation of communities or collaborations, that utilise or build upon the innovations. Third, the proposal and agreement upon standards for protocols to achieve the unified and consistent pro-gression of innovation and knowledge [1]. The discipline of ontology engineering can be thought of as progressing through the second stage of scientific maturity, moving from ontologies developed by a single authoritative expert to harvesting the collective intelligence of an application domain [2^1]. This trend is also reflected in the availability of software supporting the engagement of several domain experts,communities, representing knowledge and developing ontologies [2, 5]. Therefore,ontology engineering is on the cusp of the third stage of scientific maturity, requiringthe development of common working practices or standard methodologies.Knowledge engineering (KE) is a field that involves integrating knowledgewithin computer systems [6] or the building, maintaining and development ofknowledge-based systems [7]. Therefore, some of the methods proposed within thefield of KE are applicable when building ontologies [8]. However, the experiencesfrom KE have not always been applied when developing ontologies. In general KEmethodologies focus primarily on the use of the ontology by a software system asopposed to the development of the ontology [9].Within the domain of ontology engineering several methodologies have been pro-posed and applied [10–17]. The majority of the proposed methodologies have beenengineered for centralised settings. However, none of these have gained widespreadacceptance, predominant use or have been proven to be applicable for multipleapplication domains or development environments [18]. To date the communityhas not been widely involved or considered within ontology engineering method-ologies. This situation has encouraged debate amongst those within the ontologycommunity as to which methodology or combination of methodologies are mostapplicable [18, 9].The language choice for encoding an ontology is still an open debate acrossthe ontology building communities. This situation can be illustrated by the useof both the OBO format and the OWL within the bio-ontology community [19].Conforming to or accepting a single formalism for ontology encoding would bringconsistency and standardisation to the engineering methodology, such as tool sup-",
        "publication_date": "2010-01-01",
        "authors": "Alexander García, Kieran O’Neill, Leyla García, Phillip Lord, Robert Stevens, Óscar Corcho, F. Gibson",
        "file_name": "10!1007%978-1-4419-5908-9_4.pdf",
        "file_path": "./PDFs/10!1007%978-1-4419-5908-9_4.pdf"
    },
    {
        "title": "Mind the gap: The AURORAL ecosystem for the digital transformation of smart communities and rural areas",
        "implementation_urls": [],
        "doi": "10.1016/j.techsoc.2023.102304",
        "publication_date": "2023-07-08",
        "authors": "Oihane Gómez–Carmona, David Buján, Diego Casado–Mansilla, Diego López–de–Ipiña, Juan Cano-Benito, Andrea Cimmino, María Poveda‐Villalón, Raúl García‐Castro, Jorge Almela-Miralles, Dimitris Apostolidis, Anastasios Drosou, Dimitrios Tzovaras, Martin Wagner, María Guadalupe-Rodriguez, Diego Salinas, David Esteller, Martí Riera-Rovira, Arnau González, Jaime Clavijo-Ágreda, Alberto Díez-Frias, María del Carmen Bocanegra-Yáñez, Rui Pedro-Henriques, Elsa Ferreira-Nunes, Marian Lux, Nikol Bujalkova",
        "file_name": "1-s2.0-S0160791X23001094-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0160791X23001094-main.pdf"
    },
    {
        "title": "Governing Decentralized Complex Queries Through a DAO",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3462203!3475910.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Author: Mirko Zichichi; Luca Serena; Stefano Ferretti; Gabriele D'Angelo Publisher: ACM The final published version is available online at: http://dx.doi.org/10.1145/3462203.3475910 Rights/License: © 2021 Association for Computing Machinery."
                    }
                ]
            }
        ],
        "doi": "10.1145/3462203.3475910",
        "arxiv": "2107.06790",
        "abstract": "ACM must be honored. Abstracting with credit is permitted. To copy otherwise,to republish, to post on servers, or to redistribute to lists, requires prior specificpermission and/or  a fee. Request permissions from permissions@acm.org orPublicationsDept., ACM, Inc., fax +1 (212) 869-0481.https://www.acm.org/publications/policies/copyright-policyThis item was downloaded from arXiv (https://arxiv.org).When citing, please refer to the published version.arXiv:2107.06790v2  [cs.DC]  10 Sep 2021Governing Decentralized ComplexQueries Through a DAOMirko ZichichiOntology Engineering GroupUniversidad Politécnica de Madrid, Spainmirko.zichichi@upm.esLuca SerenaCIRI ICTUniversity of Bologna, Italyluca.serena2@unibo.itStefano FerrettiDepartment of Pure and Applied SciencesUniversity of Urbino “Carlo Bo\", Italystefano.ferretti@uniurb.itGabriele D’AngeloDepartment of Computer Science and EngineeringUniversity of Bologna, Italyg.dangelo@unibo.itABSTRACTRecently, a new generation of P2P systems capable of addressingdata integrity and authenticity has emerged for the developmentof new applications for a \"more\" decentralized Internet, i.e., Dis-tributed Ledger Technologies (DLT) and Decentralized File Systems",
        "publication_date": "2021-08-19",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1145%3462203!3475910.pdf",
        "file_path": "./PDFs/10!1145%3462203!3475910.pdf"
    },
    {
        "title": "Bringing Federated Semantic Queries to the GIS-Based Scenario",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/no_doi_20250624164112.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A detailed explanation of connecting our approach and QGIS is available on https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS (accessed on 23 November 2021)."
                    }
                ]
            }
        ],
        "doi": "10.3390/ijgi11020086",
        "abstract": "Abstract: Geospatial data is increasingly being made available on the Web as knowledge graphsusing Linked Data principles. This entails adopting the best practices for publishing, retrieving, andusing data, providing relevant initiatives that play a prominent role in the Web of Data. Despitethe appropriate progress related to the amount of geospatial data available, knowledge graphsstill face significant limitations in the GIScience community since their use, consumption, andexploitation are scarce, especially considering that just a few developments retrieve and consumegeospatial knowledge graphs from within GIS. To overcome these limitations and address somecritical challenges of GIScience, standards and specific best practices for publishing, retrieving, andusing geospatial data on the Web have appeared. Nevertheless, there are few developments andexperiences that support the possibility of expressing queries across diverse knowledge graphs toretrieve and process geospatial data from different and distributed sources. In this scenario, wepresent an approach to request, retrieve, and consume (geospatial) knowledge graphs available atdiverse and distributed platforms, prototypically implemented on Apache Marmotta, supportingSPARQL 1.1 and GeoSPARQL standards. Moreover, our approach enables the consumption ofgeospatial knowledge graphs through a lightweight web application or QGIS. The potential of thiswork is shown with two examples that use GeoSPARQL-based knowledge graphs.Keywords: GeoSPARQL; SPARQL; federated query; knowledge graph; geospatial data1. IntroductionGeospatial data is increasingly being made available on the Web [1] in the form ofknowledge graphs in the Semantic Web, often using Linked Data principles [2]. To achievethese knowledge graphs, (geospatial) resources need to be identified using HTTP URIs,indexed by search engines, and connected, or linked, to other resources [3]. Therefore, thisentails adopting the best practices for publishing, retrieving, and using data on the Web [2,3].These best practices are being embraced by a growing number of data providers, leading tobuilding a global data space containing billions of assertions—the Web of Data [3].The transformation and publication of geospatial data as knowledge graphs were pioneeredby initiatives such as GeoNames (http://www.geonames.org/ontology/documentation.html(accessed on 23 November 2021)), OpenStreetMap [4], and Ordnance Survey [5]. After theseinitiatives, many geospatial datasets have been published in the Web of Data (http://lod-cloud.net/ (accessed on 23 November 2021)). This has entailed geospatial data playing a pre-eminentrole in the Web of Data cloud, operating as central nexuses that interconnect events, people, andobjects [6] and offering an ever-growing semantic representation of the geospatial informationwealth [7].Despite the relevant progress related to the amount of data available, geospatial knowl-edge graphs still face significant limitations in the GIScience community since their use,consumption, and exploitation are really scarce in this community, especially consideringISPRS Int. J. Geo-Inf. 2022, 11, 86. https://doi.org/10.3390/ijgi11020086 https://www.mdpi.com/journal/ijgihttps://doi.org/10.3390/ijgi11020086https://doi.org/10.3390/ijgi11020086https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-5799-469Xhttp://www.geonames.org/ontology/documentation.htmlhttp://lod-cloud.nethttp://lod-cloud.nethttps://doi.org/10.3390/ijgi11020086https://www.mdpi.com/journal/ijgi",
        "publication_date": "2022-01-25",
        "authors": "Oswaldo Páez, Luis M. Vilches‐Blázquez",
        "file_name": "no_doi_20250624164112.pdf",
        "file_path": "./PDFs/no_doi_20250624164112.pdf",
        "pdf_link": "https://www.mdpi.com/2220-9964/11/2/86/pdf?version=1643204664"
    },
    {
        "title": "Annotador: a temporal tagger for Spanish",
        "implementation_urls": [],
        "doi": "10.3233/jifs-179865",
        "abstract": "Abstract. Temporal information is crucial in knowledge ex-traction. Being able to locate events in a timeline is necessaryto understand the narrative behind every text. To this aim,several temporal taggers have been proposed in literature –nevertheless, not all languages received the same attention.Most taggers work only for English texts, and not many havebeen developed for other languages. Also the scarcity of an-notated corpora in other languages notably hinders the task.In this paper we present a new rule-based tagger called An-notador (Añotador in Spanish) able to process texts both inSpanish and English. Furthermore, a new corpus with morethan 300 short texts containing common temporal expres-sions, called the HourGlass corpus, has been built in order totest it and to facilitate the development of new resources andtools. Professionals from different domains intervened in thegathering of the text, making it heterogeneous and easy touse thanks to the tags added to each entry. Finally, we ana-lyzed main challenges in the time expression extraction task.Keywords: Time Expression, Temporal Tagger, Spanish lan-guage, NLP1. IntroductionA temporal tagger is a system that extracts tempo-ral expressions from texts and recognizes their mean-ing. Time expressions (also known as temporal expres-sions or TEs) are “constructions referring to points orintervals on the timeline” [1], and in general they canbe understood as anything that answers the questions‘when’ or ‘how long’ but does not involve an event(e.g. “2 May 2019” or “one hour”). Temporal taggersmust first identify the time expressions (identification),and then resolve (normalization) their meaning, ob-taining a fixed date from expressions such as “tomor-row”. Table 1 shows some examples of normalization.Table 1Examples of normalization for several expressions using as refer-ence date (anchor date) December 20, 2019 (Friday).# Spanish Expression English Normalized Value1 mañana tomorrow 2019-12-212 el mes que viene the next month 2020-013 el pasado lunes last Monday 2019-12-16This paper presents Annotador (from the SpanishAñotador1), a temporal tagger for Spanish and English–although we will focus in Spanish in the scope of thispaper. Annotador was conceived for the automatic cre-ation of timelines from legal documents, but it is ofgeneral purpose2. Annotador is part of the suite of ser-vices offered by the H2020 **ANONYMIZED FORREVIEWERS** project for both English and Spanishtexts.Temporal tagging is a well recognized NLP task,",
        "publication_date": "2020-08-31",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": "913.pdf",
        "file_path": "./PDFs/913.pdf"
    },
    {
        "title": "Modelling a smart environment for nonintrusive analysis of attention in the workplace",
        "implementation_urls": [],
        "doi": "10.1111/exsy.12275",
        "abstract": "AbstractNowadays, the world is getting increasingly competitive and the quality and the amount of thework presented are one of the decisive factors when choosing an employee. It is no longernecessary to only perform but, to achieve a product with quality, on time, at the lowest possiblecost and with the minimum resources. For this reason, the employee must have a high score ofattention when performing a task, and the factors that influence attention negatively must bereduced. This is true in many different domains, from the workplace to the classroom. In thispaper, we present a nonintrusive smart environment for monitoring people's attention whenworking in teams. The presented system provides real time information about each individualand information about the team. It can be very useful for team managers to identify potentiallydistracting events or individuals because when the attention of an individual is not at its bestwhen performing the proposed task, her/his performance will be negatively affected, withconsequences for the individual and for the organization.KEYWORDSattention behaviour, distributed computing, nonintrusive, smart environment1 | INTRODUCTIONThe rapid progress of wireless communication and sensing technologies enabled the development of smart learning environments, which are ableto detect the environmental context and quantifying the attention of a worker in his/her workplace. For this reason, making intelligent learningsystems has been the objective of many researchers in the field of computer science.In the field of computer science, a smart environment is a digitally augmented physical world where sensor‐enabled and networked deviceswork continuously and collaboratively to make the lives of the inhabitants more comfortable. Indeed, significant advances in smart devices, wirelessmobile communications, sensor networks, pervasive computing, machine learning, robotics, middleware and agent technologies, and humancomputer interfaces have made the dream of smart environments a reality. In this concept, the word “smart” means the ability to autonomouslyacquire and apply knowledge, and the word “environment” means our surroundings (Cook & Das, 2005).With this technological evolution, job offers have changed, bringing along many significant and broad changes. Some of the most notoriousones can be pointed out by the emergence of indicators such as attentiveness which, in extreme cases, can compromise the life and well‐beingof the workers. In more moderate cases it will impair attention, general cognitive skills, and productivity. In addition to these factors, many of thesejobs are the so‐called desk jobs, in which people frequently sit for more than 8 hr (Liao & Drury, 2000).Until now, the level of attention of a worker has been evaluated through his/her productivity: The more one produces, the better his/herattention at work. Although the true nature of this relationship is yet to be thoroughly studied (properly contextualized in each work domain), thereare other issues that need to be addressed. First, the worst aspect about this approach is that it only points out a potential decrease of attentionafter a productivity loss. This means that the “damage” is already done and that it is most likely too late for the worker to cope with whatevercaused the attention loss. An approach that could point out, in advance, upcoming breaks in attention (e.g., through the observation of behaviouralpatterns) could allow active/preventive interventions rather than reactive ones (Carneiro, Novais, Andrade, Zeleznikow, & Neves, 2013).Attention is a very complex process through which an individual continuously analyses a spectrum of stimuli and, in a sufficiently short amountof time, choses one to focus on (Estes, 2014). In most of us, which can only focus on a reduced group of stimuli at a time, this implies ignoring otherperceivable stimuli and information.© 2018 John Wiley & Sons, Ltdwileyonlinelibrary.com/journal/exsy 1 of 13http://orcid.org/0000-0002-8313-7023mailto:d.alves@alumnos.upm.eshttps://doi.org/10.1111/exsy.12275https://doi.org/10.1111/exsy.12275http://wileyonlinelibrary.com/journal/exsyhttp://crossmark.crossref.org/dialog/?doi=10.1111%2Fexsy.12275&domain=pdf&date_stamp=2018-06-052 of 13 DURÃES ET AL. 14680394, 2018, 5, Downloaded from https://onlinelibrary.wiley.com",
        "publication_date": "2018-06-05",
        "authors": "Dalila Durães, Davide Carneiro, Javier Bajo, Paulo Nováis",
        "file_name": "intheworkplace.pdf",
        "file_path": "./PDFs/intheworkplace.pdf"
    },
    {
        "title": "Agent-based tool to reduce the maintenance cost of energy distribution networks",
        "implementation_urls": [],
        "doi": "10.1007/s10115-017-1120-7",
        "abstract": "Abstract There has been continuous research in the energy distribution sector because ofits huge impact on modern societies. Nonetheless, aerial high voltage power lines are stillsupported by old transmission towers which involve some serious risks. Those risks may beavoided with periodic and expensive reviews. The main objective of this work is to reducethe number of these periodic reviews so that the maintenance cost of power lines is alsoreduced.More specifically, thework is focused on reducing the number of periodic reviews oftransmission towers to avoid step and touch potentials, which are very dangerous for humans.A virtual organization-based multi-agent system is proposed in conjunction with differentartificial intelligence methods and algorithms. The developed system is able to propose asample of transmission towers from a selected set to be reviewed. The system ensures thatthe whole set will have similar values without needing to review all the transmission towers.As a result of this work, a website application is provided to manage all the review processesand all the transmission towers of Spain. It allows the companies that review the transmissiontowers to initiate a new review process for a whole line or area, while the system indicatesthe transmission towers to review. The system is also able to recommend the best place tolocate a new transmission tower or the best type of structure to use when a new transmissiontower must be used.Keywords Virtual organizations · Transmission towers · Maintenance · Case-basedreasoningB Pablo Chamosochamoso@usal.es1 BISITE Research Group, University of Salamanca, Edificio I+D+i, Calle Espejo 2, 37007Salamanca, Spain2 Departamento de Inteligencia Artificial, Universidad Politécnica deMadrid, CampusMontegancedo,28660 Boadilla del Monte, Madrid, Spain123http://crossmark.crossref.org/dialog/?doi=10.1007/s10115-017-1120-7&domain=pdfhttp://orcid.org/0000-0001-5109-3583660 P. Chamoso et al.1 IntroductionPower line maintenance is a topic that has generated a variety of research lines [1–3]. Inalmost all developed countries, the transmission towers (TTs) that support the aerial powerlines must be reviewed on a regular basis, depending on their characteristics. There aredifferent parameters that have to be measured when reviewing TTs, which are necessary todetermine the ground resistance, as well as the step potential (Kp) and touch potential (Kc).These reviews involve a significantly high cost because they require qualified personnel towork in remote locationswith expensive equipment.However,manyof the valuesmeasured inthe reviews could in fact be predicted; this is because most of the TTs share the same featuresand are located on similar terrain, so they have similar behavior. Therefore, the possibility ofreducing the costs associated with this kind of maintenance is not only attractive, but quitereasonable.As technology has continued to advance, there have been different approaches that attemptto apply innovations both in the review and themaintenance processes, resulting in a commonneed to reduce costs. Indeed, this is precisely the reason for having created the proposedpredictive maintenance system.There are four common maintenance types for TTs: (i) corrective, to solve existingproblems; (ii) preventive, to prevent system failures; (iii) predictive, to predict possible irreg-ularities; (iv) proactive, which is a combination of preventive and predictive maintenances[4]. The present work is focused on the predictive maintenance, where different techniquesare already being used: some authors have used artificial neural networks to model the envi-",
        "publication_date": "2017-10-23",
        "authors": "Pablo Chamoso, Juan F. De Paz, Javier Bajo, Gabriel Villarrubia González",
        "file_name": "s10115-017-1120-7.pdf",
        "file_path": "./PDFs/s10115-017-1120-7.pdf"
    },
    {
        "title": "Potentially inappropriate medications in older adults living with HIV",
        "implementation_urls": [],
        "doi": "10.1111/hiv.12883",
        "abstract": "Antiviral Therapy. Baltimore, MD, May 2018 [Abstract 20].18 Rogero-Blanco E, Lopez-Rodriguez JA, Sanz-Cuesta T, Aza-Pascual-Salcedo M, Bujalance-Zafra MJ, Cura-Gonzalez I.Use of an electronic clinical decision support system inprimary care to assess inappropriate polypharmacy in youngseniors with multimorbidity: observational, descriptive,cross-sectional study. JMIR Med Inform 2020; 8: e14130.19 Loikas D, Wettermark B, von Euler M, Bergman U. Schenck-Gustafsson K. Differences in drug utilisation between menand women: a cross-sectional analysis of all dispensed drugsin Sweden. BMJ Open 2013; 3: e002378.20 Manteuffel M, Williams S, Chen W, Verbrugge RR, PittmanDG, Steinkellner A. Influence of patient sex and gender onmedication use, adherence, and prescribing alignment withguidelines. J Womens Health 2014; 23: 112–119.21 Blanco JR, Morillo R, Abril V et al. Deprescribing of non-antiretroviral therapy in HIV-infected patients. Eur J ClinPharmacol 2019; 76: 305–318.Supporting InformationAdditional supporting information may be found onlinein the Supporting Information section at the end of thearticle.Table S1. Description of the antiretroviral classes pre-scribed to 1292 older people living with HIVTable S2. Description of nonantiretroviral medicationsclassified according to ATC anatomic and therapeuticsubgroup code among 1292 older people living with HIV© 2020 British HIV Association HIV Medicine (2020), 21, 541--546546 B L�OPEZ-CENTENO et al. 14681293, 2020, 8, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/hiv.12883 by Republic of Macedonia Hinari NPL, Wiley Online Library on [24/06/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley O",
        "publication_date": "2020-06-09",
        "authors": "Beatriz López Centeno, Carlos Badenes-Olmedo, Angel Mataix-San-Juan, José M. Bellón, Leire Pérez‐Latorre, JC López, Juana Benedı́, SH Khoo, Catia Marzolini, MJ Calvo‐Alcántara, Juan Berenguer",
        "file_name": "10!1111%hiv!12883.pdf",
        "file_path": "./PDFs/10!1111%hiv!12883.pdf"
    },
    {
        "title": "A Case-Based Reasoning Model Powered by Deep Learning for Radiology Report Recommendation",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2021.08.011",
        "abstract": "AbstractCase-Based Reasoning models are one of the most used reasoning paradigms in expert-knowledge-driven areas. One of the most prominent fields of use of these systems is the medical sector, where explainable models are required. However, these models are considerably reliant on user input and the introduction of relevant curated data. Deep learning approaches offer an analogous solution, where user input is not required. This paper proposes a hybrid Case-Based Reasoning, Deep Learning framework for medical-related applications, focusing on the generation of medical reports. The proposal combines the explainability and user-focused approach of case-based reasoning models with the deep learning techniques performance. Moreover, the framework is fully modular to fit a wide variety of tasks and data, such as real-time sensor captured data, images, or text, to name a few. An implementation of the proposed framework focusing on radiology report generation assistance is provided. This implementation is used to evaluate the proposal, showing that it can provide meaningful and accurate corrections, even when the amount of information available is minimal. Additional tests on the optimization degree of the case base are also performed, evidencing how the proposed framework can optimize this base to achieve optimal performance.* Corresponding author.E-mail addresses: eamador@fi.upm.es (E. Amador-Domínguez), emilioserra@fi.upm.es (E. Serrano), daniel.manrique@upm.es (D. Manrique), jbajo@fi.upm.es (J. Bajo).DOI:  10.9781/ijimai.2021.08.011I.\t IntroductionDeep Learning is currently a fundamental approach in Artificial Intelligence applied to the medical domain. Their applications include image segmentation [1]–[3], 3D image reconstruction [4], [5], and disease diagnosis [6], [7]. While these approaches offer outstanding results, they suffer from a considerable flaw: lack of explainability. This issue is particularly concerning in the medical domain, where it is crucial to understand the inference procedure carried by a model to perform a task. Moreover, deep learning-based approaches require a considerable amount of labelled data to be truly accurate, which may not always be available.Opposite to this approach, the Case-Based Reasoning (CBR) methodology [8], [9] provides computational models closely related to human reasoning. In CBR, the resolution of problems provides knowledge that permits to solve new, similar ones. A CBR model discovers the closest situation to the current one to solve and adapt its solution to fit the present scenario. One of CBR’s essential advantages is that it is easy to follow and understand the inference process they conduct, which has prompted its use in, for example, the medical domain [10], [11]. This paper proposes a hybrid CBR-deep learning model to tackle the problem of radiology report writing assistance. The main efforts in the radiology domain reside within image-related tasks, such as diagnosis or X-ray image segmentation. In this image-dominated field, medical reports play a secondary role, mostly used to support the aforementioned tasks. Thus, high quality labelled textual data in this domain may not always be available, which hinders the use of deep learning techniques.The proposed approach uses a CBR model to work with a few cases that can scale up, assisted by deep learning models to improve its performance. Therefore, it is a blended solution between a knowledge-based system [12], where the knowledge must be elicited, and a deep ",
        "publication_date": "2021-01-01",
        "authors": "Elvira Amador-Domínguez, Emilio Serrano, Daniel Manrique, Javier Bajo",
        "file_name": "10!9781%ijimai!2021!08!011.pdf",
        "file_path": "./PDFs/10!9781%ijimai!2021!08!011.pdf"
    },
    {
        "title": "How to Validate Ontologies with Themis",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-32327-1_11",
        "abstract": "Abstract. Validating ontologies regarding the requirements they needto satisfy is a crucial activity during ontology development in orderto assure, both to domain experts and ontology developers, that theontologies are complete regarding their needs. The aim of this work is topresent Themis, a web-based tool for validating ontologies by means oftest expressions, which represent the desired behaviour expected in anontology if a requirement is satisfied. The purpose of these test expres-sions is to ease the formalization of the requirements into test cases and,therefore, the validation process.Keywords: Ontology testing · Ontology requirements · Ontologydevelopment1 IntroductionThe validation of ontologies, which deals with assessing the correct conceptual-ization that the ontologies need to specify [6], is a crucial activity for assuringthe quality of the ontologies delivered and published on the Web. One of themain concerns in the validation of ontologies is to check whether the ontologyrequirements that need to be covered are satisfied, with the aim of assuring boththe domain experts and the ontology developers that the ontologies they arebuilding or using are complete regarding their needs.The main problem in validating ontologies regarding their requirements isthe ambiguity of such ontological requirements, which sometimes are difficult toformalize into tests and to translate into axioms. There are some testing tools,such as TDDOnto [5] or OntologyTest [4], that execute tests on an ontology.TDD is focused on checking the presence of axioms and added them to theontology if they are absent while OntologyTest supports several types of testsrelated to the instances of an ontology.In this context, the goal within this paper is to present a testing tool to helpontology practitioners and users to validate ontologies regarding their functionalrequirements, i.e., the requirements which define the knowledge the ontology hasto represent [7]. This tool is called Themis1 and can be used independently ofthe ontology development platform without any installation. Themis supports a1 http://themis.linkeddata.es/.c© Springer Nature Switzerland AG 2019P. Hitzler et al. (Eds.): ESWC 2019 Satellite Events, LNCS 11762, pp. 52–57, 2019.https://doi.org/10.1007/978-3-030-32327-1_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-32327-1_11&domain=pdfhttp://themis.linkeddata.es/https://doi.org/10.1007/978-3-030-32327-1_11How to Validate Ontologies with Themis 53collection of test expressions [2] which are extracted from a collection of lexico-syntactic patterns (LSPs) [1] collected by CORAL Corpus [3] in order to ease therequirements formalization into tests, and that represent the expected knowledgethat should be added to an ontology.The remainder of this paper is structured as follows. Section 2 presentsthe main features of Themis while Sect. 3 describes its architecture. Section4 describes the demonstration that will be performed. Finally, Sect. 5 outlinessome conclusions and future steps to improve this testing tool.2 Themis FeaturesThemis supports and implements a set of 15 test expressions which wereextracted from the existing LSPs related to ontological requirements collected",
        "publication_date": "2019-01-01",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "978-3-030-32327-1_11.pdf",
        "file_path": "./PDFs/978-3-030-32327-1_11.pdf"
    },
    {
        "title": "Optimizing Automated Term Extraction for Terminological Saturation Measurement.",
        "implementation_urls": [],
        "abstract": "Abstract. Assessing the completeness of a document collection, within a domain of interest, is a complicated task that requires substantial effort. Even if an auto-mated technique is used, for example, terminology saturation measurement based on automated term extraction, run times grow quite quickly with the size of the input text. In this paper, we address this issue and propose an optimized approach based on partitioning the collection of documents in disjoint constituents and computing the required term candidate ranks (using the c-value method) inde-pendently with subsequent merge of the partial bags of extracted terms. It is proven in the paper that such an approach is formally correct – the total c-values can be represented as the sums of the partial c-values. The approach is also vali-dated experimentally and yields encouraging results in terms of the decrease  of the necessary run time and straightforward parallelization without any loss  in quality.  Keywords: Automated term extraction, terminological saturation, partial  c-value, merged-partial c-value, optimization 1 Introduction Ontology learning from texts is a developing research field that aims to extract domain description theories from text corpora. It is increasingly acknowledged as a plausible alternative to ontology development based on the interviews of domain knowledge stakeholders. One shortcoming of learning an ontology from texts is that the input cor-pus has to be quite big for being representative for the subject domain. Another short-coming is that learning ontologies from text is expensive, in terms of taken time, as it involves the use of several algorithms, in a pipeline [1], that are computationally hard.   https://orcid.org/0000-0001-6157-8111https://orcid.org/0000-0002-5159-254Xmailto:gen.dobr@gmail.commailto:vadim@ermolayev.commailto:dchaves@fi.upm.esmailto:egorfedorencko@gmail.comAutomated term extraction (ATE) is an essential step at the beginning of the pipeline for ontology learning [1, 2], that is known to be bulky in terms of the increase of the run time with the growth of the input text corpus. Therefore, finding a way to reduce: (i) either the size of the processed text; or (ii) the time spent for term extraction; or (iii) both is of importance.  In our prior work [2, 3, 4, 5], we developed the ATE-based approach (OntoElect) that helps circumscribe the minimal possible representative part of a documents collec-tion, which forms the corpus for further ontology learning. This technique is based on measuring terminological saturation in the collection of documents, which is computa-tionally quite expensive in the terms of the run time.  In this paper, we present the approach, based on the partitioning of a document col-lection, which allows substantially reducing ATE run time in the OntoElect processing pipeline.  The remainder of the paper is structured as follows. In Sect. 2, we outline our Onto-Elect approach to detect terminological saturation in document collections describing a subject domain. In Sect. 3, we review the related work in ATE and argue for the choice of the c-value method as the best appropriate for measuring terminological saturation. In Sect. 4, we explain our motives to optimize the c-value method based on partitioning a document collection and present a formal framework for that. Section 5 reports on the setup and results of our experimental evaluation of the proposed optimization approach. Finally, we draw the conclusions and outline our plans for the future work in Sect. 6.  ",
        "publication_date": "2019-01-01",
        "authors": "Victoria Kosa, David Chaves-Fraga, Hennadii Dobrovolskyi, Egor Fedorenko, Vadim Ermolayev",
        "file_name": "no_doi_20250624164214.pdf",
        "file_path": "./PDFs/no_doi_20250624164214.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2387/20190001.pdf"
    },
    {
        "title": "Automating ontology engineering support activities with OnToology",
        "implementation_urls": [
            {
                "identifier": "https://github.com/GeorgFerdinandSchneider/bot",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1016%j!websem!2018!09!003.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Electronic copy available at: https://ssrn.com/abstract=3260516 Pr ep rin t n ot p ee r r ev ie w ed supported by the integration of WIDOCO [12], a standalone application for generating HTML documentation for an individual ontol-ogy."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.09.003",
        "abstract": "AbstractDue to the increasing uptake of semantic technologies, ontologies are now part of a good number of informa-tion systems. As a result, software development teams that have to combine ontology engineering activitieswith software development practices are facing several challenges, since these two areas have evolved, in gen-eral, separately. In this paper we present OnToology, an approach to manage ontology engineering supportactivities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based applicationthat builds on top of Git-based environments and integrates existing semantic web technologies. We havevalidated OnToology against a set of representative requirements for ontology development support activitiesin distributed environments, and report on a survey of the system to assess its usefulness and usability.Keywords: ontology engineering, ontology evaluation, ontology documentation, ontology publication1. IntroductionSince the late 1990s, several ontology engineeringmethodologies have been proposed to transform theart of developing ontologies into an engineering ac-tivity. Methodologies such as METHONTOLOGY[1], On-To-Knowledge [2] and the NeOn Methodol-ogy [3] define clear guidelines, processes, activitiesand life cycles to guide ontology development.Now that ontologies are being increasinglyadopted in information systems, it is clear that on-tology development tasks may also benefit from theapplication of common software engineering prac-tices. Most of the ontology development supportactivities, such as documentation, visualization andevaluation, are usually performed individually, exe-cuting heterogeneous tools that make these activi-ties cumbersome and time consuming. In addition,maintaining and keeping track of the generated re-Email addresses: aalobaid@fi.upm.es (AhmadAlobaid), dgarijo@isi.edu (Daniel Garijo),mpoveda@fi.upm.es (Maŕıa Poveda-Villalón),isantana@fi.upm.es (Idafen Santana-Pérez),albafernandez@fi.upm.es (Alba Fernández-Izquierdo),ocorcho@fi.upm.es (Oscar Corcho)sources for each version of an ontology has becomea challenge for ontology developers.The ontology engineering community has alreadyshown progress towards adapting ontology develop-ment to agile software development methodologies[4, 5, 6]; as well as supporting collaborative ontol-ogy development throughout the use of common-practice software engineering tools [7, 8]. In fact,it is now common among ontology developers touse Git-based environments [9] such as GitHub1(usual in software development) for keeping trackof ontology revisions. However, existing approachespresent either partial solutions; require specializedskills that complicate their adoption (e.g., complexinstallation setup); or produce their outcome usingidiosyncratic formats that are difficult to integrate",
        "publication_date": "2018-10-09",
        "authors": "Ahmad Alobaid, Daniel Garijo, María Poveda‐Villalón, Idafen Santana-Pérez, Alba Fernández-Izquierdo, Óscar Corcho",
        "file_name": "10!1016%j!websem!2018!09!003.pdf",
        "file_path": "./PDFs/10!1016%j!websem!2018!09!003.pdf"
    },
    {
        "title": "Calculating Heavy-Duty Truck Energy and Fuel Consumption Using Correlation Formulas Derived From VECTO Simulations",
        "implementation_urls": [],
        "doi": "10.4271/2019-01-1278",
        "abstract": "Abstract The Vehicle Energy Consumption calculation Tool (VECTO) is used in Europe for calculating standardised energy consumption and CO2 emissions from Heavy-Duty Trucks (HDTs) for certification purposes. The tool requires detailed vehicle technical specifications and a series of component efficiency maps, which are difficult to retrieve for those that are outside of the manufacturing industry. In the context of quantifying HDT CO2 emissions, the Joint Research Centre (JRC) of the European Commission received VECTO simulation data of the 2016 vehicle fleet from the vehicle manufacturers. In previous work, this simulation data has been normalised to compensate for differences and issues in the quality of the input data used to run the simulations. This work, which is a continuation of the previous exercise, focuses on the deeper meaning of the data received to understand the factors contributing to energy and fuel consumption. Fuel efficiency distributions and energy breakdown figures were derived from the data and are presented in this work. Correlation formulas were produced to calculate the energy loss contributions of individual components and resistances (air drag, rolling resistance, axle losses, gearbox losses, etc.) over the Regional Delivery and Long Haul cycles, given a limited number of input parameters such as vehicle characteristics and average component efficiencies. Default values and meaningful ranges of variation of these parameters obtained from the data of the fleet are also reported in this work. The importance of air drag and rolling resistance losses are highlighted since these losses account for about 70% of the energy consumed downstream the engine. Finally, based on the correlation formulas to calculate the individual energy losses, a method is presented that calculates the final energy consumption and CO2 emissions for all the regulated HDTs classes and that does not rely on the use of VECTO. Introduction The CO2 certification for Heavy-Duty Trucks (HDTs) was set to start from January 2019 [1] and the European Commission is focused on determining the current levels of CO2 emissions of these vehicles under different operating conditions [2]. Within the context of regulating emissions, attention is also given on the primary sources of energy losses during operation and the identification of the margins for their improvement [3]. Regulators, manufacturers and other stakeholders are interested in quantifying the reduction potentials and creating realistic scenarios for technology diffusion that would promote CO2 emissions reduction from road transport. In order to investigate the CO2 emissions reduction potential, it was required to define a reliable reference basis for certification and monitoring, which was achieved with the development of the Vehicle Energy Consumption calculation Tool (VECTO) [4–6]. VECTO is a vehicle simulation software that calculates energy consumption (EC), fuel consumption (FC) and CO2 emissions from HDTs. To comply with this task, and with the accuracy requested, VECTO adopts a sophisticated simulation approach that is based on certified ",
        "publication_date": "2019-04-02",
        "authors": "Alessandro Tansini, Georgios Fontaras, Biagio Ciuffo, Federico Millo, Ignacio-Iker Prado-Rujas, Nikiforos Zacharof",
        "file_name": "sae_paper_jrc116080_final_green.pdf",
        "file_path": "./PDFs/sae_paper_jrc116080_final_green.pdf"
    },
    {
        "title": "Encoding of Media Value Chain Processes Through Blockchains and MPEG-21 Smart Contracts for Media",
        "implementation_urls": [],
        "doi": "10.1109/mmul.2023.3303393",
        "abstract": "Abstract—Distributed Ledger Technologies can be used forrights management in the audiovisual production sector, wheredominant business models still need to provide a sustainableway to support the claims of content creators and rights holdersfully. This paper describes the combination of the current setof MPEG-21 multimedia framework standards with DistributedLedger Technologies and smart contracts. Their gathering shapesthe Smart Contracts for Media, a specification that can be used toencode the terms and conditions of a contract for media-relateddelivery and consumption. We provide the implementation of asystem based on the Smart Contract for Media to enable thetwofold process of reducing the complexity of contract termscompliance validation and making stakeholders more aware ofthe media value chain. We also provide the technical details for aVideo-On-Demand Services use case in which the exploitation ofmedia rights is modeled through the MPEG-21 framework andthe Smart Contract for Media. Finally, we perform an evaluationof our smart contracts implementation by analyzing the gas cost.Results suggest the viability of our approach.Index Terms—MPEG, Distributed Ledger Technology, SmartContract, Non Fungible TokenI. INTRODUCTIONMPEG (Moving Picture Experts Group) has developedseveral well-known media encoding international standards foraudio, video, and genomic information. One of its endeavors isthe definition of the Multimedia Framework, known as MPEG-21, which is an open framework for delivering and consumingmultimedia.The newest part of the International Standard is the SmartContract for Media (MPEG-21 SCM), which enables theuse of Distributed Ledger Technologies (DLTs) and smartcontracts to address a set of very well-defined challenges [1].DLTs can easily fit into the MPEG-21 framework to exploittheir advantages. These technologies can act as resonanceboxes for instances created using the MPEG-21 frameworkand directly enforce what has been determined in terms of useof the media [2]. The features DLTs provide can reduce theopacity of complex systems processes: (i) transparency, for theappend-only ledger is auditable by the whole DLT network;(ii) immutability, as data cannot be easily tampered with;(iii) traceability and non-repudiation, because each networkparticipant cryptographically signs each transaction issuedin the immutable ledger and (iv) decentralized execution ofimmutable instructions, i.e., smart contracts.Smart contracts are software procedures that can be runto ensure the proper execution of new types of applicationsdirectly on a DLT. In some implementations, a smart contractcan be considered a specific interpretation and translation ofsome contractual terms. However, the mere fact that a smartcontract is stored on a DLT does not give rise to a legal",
        "publication_date": "2023-08-11",
        "authors": "Mirko Zichichi, Victor Rodrı́guez-Doncel",
        "file_name": "919.pdf",
        "file_path": "./PDFs/919.pdf"
    },
    {
        "title": "Construcción de una red de ontologías sobre eventos meteorológicos a partir de periódicos históricos",
        "implementation_urls": [],
        "doi": "10.1590/1678-9865202032e180077",
        "abstract": "AbstractCurrently, only a few people would deny the value of newspaper content for understanding issues associated with politics, culture, and society. Therefore, the digitalization of the newspapers’ archives allows retrieving outstanding historical and cultural articles. However, there is a large amount of “minor data” hidden within these newspapers. This paper addresses the challenge for accessing and dealing with the resources of National newspaper and periodicals libraries from Colombia, Ecuador, México and Uruguay, which TransInformação, Campinas, 32:e180077, 2020 http://dx.doi.org/10.1590/1678-9865202032e180077L. M. VILCHES-BLÁZQUEZ  et al.2collect newspapers where news about meteorological events from the XIX-XX centuries were posted. A news corpus is developed on these newspapers, which through technical readings and a bibliomining process using different tools allow building an ontology network. This network is composed of different modules (technical, general and news), which are built using different approaches (top-down and bottom-up) and methodologies (Methontology and NeOn), for providing a common and sharing understanding of the historical meteorological events in Latin America. Hence, this work entails an approach to take the newspaper and periodicals libraries to Semantic Web.Keywords: Bibliomining. Meteorology. Ontology. Newspaper.IntroducciónLos periódicos se encuentran entre las fuentes más valiosas para los estudiosos interesados en investigar la opinión pública y su configuración en el transcurso del tiempo. No obstante, entre algunos investigadores hubo cierto escepticismo sobre la utilización de periódicos históricos como fuente de información, debido a su dudosa precisión y naturaleza efímera (Bingham, 2010). Sin embargo, en la actualidad, pocos niegan el valor del contenido del periódico para comprender cuestiones relacionadas con política, cultura y sociedad, así como para conocer cómo los desarrollos e ideas se percibieron y se extendieron por diferentes países (Bingham, 2010; Neudecker; Antonacopoulos, 2016). Por todo ello, los periódicos facilitan un nuevo punto de acceso a la información que se almacena en recursos históricos (Smits, 2014), proporcionando acceso a una amplia gama de fuentes de información.Uno de los desarrollos más útiles para los historiadores modernos es la digitalización de los archivos de periódicos (Bingham, 2010), ya que permite rescatar artículos históricos y culturales relevantes y centrar el esfuerzo de análisis y descripción en ciertas publicaciones. Además, existen infinidad de “datos menores” ocultos en estos periódicos, por lo que se convierte en un desafío acceder a ellos puesto que pueden suministrar valiosa información no recogida en otros documentos.La creciente digitalización proporciona “nuevos” desafíos técnicos, ya que frecuentemente no se aplican tecnologías de reconocimiento de texto sobre estos recursos de información (Neudecker; Antonacopoulos, 2016). Esto obstaculiza la posibilidad de hacerlos más accesibles a otras instituciones y dificulta la aplicación de tecnologías para la extracción de eventos en formas narrativas o reconstrucción y análisis de patrones (Wijfjes, 2017) que aporten mayor claridad al contexto histórico recogido en los periódicos. No obstante, refleja que el acceso a la información ha cambiado y que existen nuevos modelos de producción, distribución y consumo, pues la información digital presenta características distintas a las tradicionales (Rodríguez García, 2016).Hoy resulta común hablar sobre la World Wide Web, Internet, bibliotecas digitales, repositorios digitales, XML, Web 2.0, Web Semántica, entre otros tópicos. Técnicamente, muestran cómo las tecnologías de la información protagonizan el escenario con relación al acceso a la información en la biblioteca (Rodríguez García, 2016). En este contexto de la Web y las bibliotecas digitales, Chowdhury y Chowdhury (2007) aclaran que las ontologías juegan un papel significativo porque sus mecanismos permiten el análisis del significado de los recursos, favoreciendo su desempeño en el proceso de acceso a la información al organizar y reunir la información heterogénea contenida en los recursos digitales.El término ontología ha sido objeto de estudio en diferentes áreas de investigación y en varios dominios ",
        "publication_date": "2020-01-01",
        "authors": "Luis M. Vilches‐Blázquez, Diana Comesaña, Lorena de Jesús Arrieta Moreno",
        "file_name": "10!1590%1678-9865202032e180077.pdf",
        "file_path": "./PDFs/10!1590%1678-9865202032e180077.pdf"
    },
    {
        "title": "Participation of women in doctorate, research, innovation, and management activities at Universidad Politécnica de Madrid: analysis of the decade 2006–2016",
        "implementation_urls": [],
        "doi": "10.1007/s11192-019-03179-9",
        "abstract": "AbstractThis article studies the participation of women in doctorate, lecturing and research, innova-tion, and management activities at Universidad Politécnica de Madrid (UPM), the most important and largest university in Spain devoted to engineering and architecture. The analyses revealed significant differences in the ratio of male (76%) and female (24%) lec-turing and research staff. This unequal ratio conducted to women underrepresentation in other actions such as coordination of international projects, decision-making designations, patenting and software licensing, collaboration with companies, and PhD supervision. PhD enrolment and PhD defence data, disaggregated by gender and by technological area, were also analysed as they are the starting point of the academic career, and showed a wide-spread male prevalence over women (ca. 70% men vs. 30% women). The aim of this paper is to present actual, accurate, objective, and gender-segregated information extracted from UPM databases, to carry out a qualitative study drawing on an opinion survey and a “gap” analysis, and to undertake a critical examination of the historic, political, sociocultural and personal factors affecting gender inequalities in academia. Policy recommendations to improve the situation of women and to achieve gender balance in the disciplines of engi-neering and architecture are also provided.Keywords  Research · Innovation · Doctorate · Management · Women · STEAMMathematics Subject Classification  62-07 · 62-09JEL Classification  I23 · I24 · J24 · J71 · N34 · O30Electronic supplementary material  The online version of this article (https​://doi.org/10.1007/s1119​2-019-03179​-9) contains supplementary material, which is available to authorized users. *\t Asunción Gómez‑Pérez \t vicerrector.investigacion@upm.es1\t Vice‑Rectorate for Research, Innovation, and Doctoral Studies, Universidad Politécnica de Madrid, C/Ramiro de Maeztu 7, 28040 Madrid, Spainhttp://orcid.org/0000-0001-7869-6704http://orcid.org/0000-0001-9689-4798http://orcid.org/0000-0001-9299-1371http://orcid.org/0000-0002-3037-0331http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-019-03179-9&domain=pdfhttps://doi.org/10.1007/s11192-019-03179-9https://doi.org/10.1007/s11192-019-03179-91060\t Scientometrics (2019) 120:1059–10891 3IntroductionThe participation of women in scientific fields has attracted significant attention in the past years. Different reports have analysed the equality between men and women in the EU, studying the situation of women in different scenarios, such as tertiary education and exec-utive jobs. Moreover, the adoption of anti-discriminatory measures, the discussion of equal treatment policies and women’s rights was also well-documented in these papers (Lipinsky 2013; EC 2015, 2017).In the particular case of Spain, former governments have published reports analysing the participation of the women in science and technology (MINECO 2011; Puy 2015). In the same line, the enrolment and the contribution of women in the engineering and archi-tecture areas have been also widely documented (Pérez-Artieda et al. 2014), showing the prevalence of men over women in all the scenarios studied: undergraduates, PhD gradu-ates, workforce, and decision-making positions. Nevertheless the gender gap in the field of engineering and architecture is especially significant and, although extensively literature has described the gender imbalance in science, has not been completely explored.",
        "publication_date": "2019-07-11",
        "authors": "Estela Hernández‐Martín, F. Calle, Juan C. Dueñas, Miguel Holgado, Asunción Gómez‐Pérez",
        "file_name": "10!1007%s11192-019-03179-9.pdf",
        "file_path": "./PDFs/10!1007%s11192-019-03179-9.pdf"
    },
    {
        "title": "An Analysis of Existing Production Frameworks for Statistical and Geographic Information: Synergies, Gaps and Integration",
        "implementation_urls": [],
        "doi": "10.3390/ijgi10060374",
        "abstract": "Abstract: The production of official statistical and geospatial data is often in the hands of highlyspecialized public agencies that have traditionally followed their own paths and established their ownproduction frameworks. In this article, we present the main frameworks of these two areas and focuson the possibility and need to achieve a better integration between them through the interoperabilityof systems, processes, and data. The statistical area is well led and has well-defined frameworks.The geospatial area does not have clear leadership and the large number of standards establish aframework that is not always obvious. On the other hand, the lack of a general and common legalframework is also highlighted. Additionally, three examples are offered: the first is the applicationof the spatial data quality model to the case of statistical data, the second of the application of thestatistical process model to the geospatial case, and the third is the use of linked geospatial andstatistical data. These examples demonstrate the possibility of transferring experiences/advancesfrom one area to another. In this way, we emphasize the conceptual proximity of these two areas,highlighting synergies, gaps, and potential integration.Keywords: geospatial information; statistical data; framework; interoperability1. IntroductionThe production of statistical data is nowadays more and more “geo”, and the produc-tion of geospatial data is more and more “statistical”; therefore, it is logical to envision agreater integration of these two areas. In this way, and according to the United NationsCommittee of Experts on Global Geospatial Information Management [1], the integrationof statistical and geospatial information and the resulting geospatially enabled statistics aresignificant components in meeting the data demands that inform decision-making needsat either the local, national, regional, or global level. Thereby, linking data about people,businesses, or the environment to a geographic location and their integration with othergeospatial information through their location can promote a much better understanding ofeconomic, social, and environmental perspectives.ISPRS Int. J. Geo-Inf. 2021, 10, 374. https://doi.org/10.3390/ijgi10060374 https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-6491-7430https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0002-8415-9531https://orcid.org/0000-0002-2983-4629https://orcid.org/0000-0001-6521-0760https://orcid.org/0000-0002-6373-4410https://doi.org/10.3390/ijgi10060374https://doi.org/10.3390/ijgi10060374https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/ijgi10060374https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/article/10.3390/ijgi10060374?type=check_update&version=1: International Journal ofiSpES Geo-InformationArticleAn Analysis of Existing Production Frameworks for Statisticaland Geographic Information: Synergies, Gaps and IntegrationFrancisco Javier Ariza-Lépez 1*, Antonio Rodriguez-Pascual ”, Francisco J. Lopez-Pellicer >,Luis M. Vilches-BlazquezManuel Antonio Urefia-Camara",
        "publication_date": "2021-06-02",
        "authors": "Francisco Javier Ariza López, Antonio Rodríguez-Pascual, Francisco J. López-Pellicer, Luis M. Vilches‐Blázquez, Agustín Villar-Iglesias, Joan Masó, Efrén Díaz-Díaz, Manuel A. Ureña-Cámara, Alberto González-Yanes",
        "file_name": "no_doi_20250624164241.pdf",
        "file_path": "./PDFs/no_doi_20250624164241.pdf",
        "pdf_link": "https://www.mdpi.com/2220-9964/10/6/374/pdf?version=1622623685"
    },
    {
        "title": "A combination of supervised dimensionality reduction and learning methods to forecast solar radiation",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.6856079",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "DOI",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/s10489-022-04175-y",
        "abstract": "AbstractMachine learning is routinely used to forecast solar radiation from inputs, which are forecasts of meteorological variablesprovided by numerical weather prediction (NWP) models, on a spatially distributed grid. However, the number of featuresresulting from these grids is usually large, especially if several vertical levels are included. Principal Components Analysis(PCA) is one of the simplest and most widely-used methods to extract features and reduce dimensionality in renewableenergy forecasting, although this method has some limitations. First, it performs a global linear analysis, and second it isan unsupervised method. Locality Preserving Projection (LPP) overcomes the locality problem, and recently the LinearOptimal Low-Rank (LOL) method has extended Linear Discriminant Analysis (LDA) to be applicable when the numberof features is larger than the number of samples. Supervised Nonnegative Matrix Factorization (SNMF) also achieves thisgoal extending the Nonnegative Matrix Factorization (NMF) framework to integrate the logistic regression loss function.In this article we try to overcome all these issues together by proposing a Supervised Local Maximum Variance Preserving(SLMVP) method, a supervised non-linear method for feature extraction and dimensionality reduction. PCA, LPP, LOL,SNMF and SLMVP have been compared on Global Horizontal Irradiance (GHI) and Direct Normal Irradiance (DNI)radiation data at two different Iberian locations: Seville and Lisbon. Results show that for both kinds of radiation (GHI andDNI) and the two locations, SLMVP produces smaller MAE errors than PCA, LPP, LOL, and SNMF, around 4.92% betterfor Seville and 3.12% for Lisbon. It has also been shown that, although SLMVP, PCA, and LPP benefit from using a non-linear regression method (Gradient Boosting in this work), this benefit is larger for PCA and LPP because SMLVP is ableto perform non-linear transformations of inputs.Keywords Dimensionality reduction · Hybrid learning · Solar radiation forecast · Data mining1 IntroductionConsiderable efforts have been made in the past decadesto make solar energy a real alternative to the conventionalenergy generation system. There are two main technologies,solar thermal electricity (STE) and solar photovoltaic (PV)energy, and many countries have already reached a notablesolar share in their energy mixes. Moreover, importantgrowth is expected in the near future (International EnergyAgency, 2018).Contrary to conventional generation, solar electricitygeneration is conditioned by weather, and thus it is highly� Esteban Garcı́a-Cuestaesteban.garcia@fi.upm.esExtended author information available on the last page of the article.intermittent. Transient clouds and aerosol intermittencylead to considerable variability in the solar power plantsyield on a wide range of temporal scales, particularlyin minutes to hours time scales. This presents seriousissues regarding solar power plant management and theiryield integration into the electricity grid [1]. Currently, inaddition to expensive storage-based solutions, the use ofsolar radiation forecasts is the only plausible way to mitigatethe intermittency. Therefore, the development of accuratesolar radiation forecasting methods has become an essentialresearch topic [2].Solar forecasting methods can be classified dependingon the forecasting horizon. Nowcasting methods are mostlyrelated to one-hour ahead forecasts, short-term forecastingwith up to 6 hours ahead forecasts and forecasting methodsare aimed at producing days ahead forecasts. The techniquesassociated with these methods are essentially different [3–",
        "publication_date": "2022-10-06",
        "authors": "Esteban García-Cuesta, Ricardo Aler, David Pozo‐Vázquez, Inés M. Galván",
        "file_name": "10!1007%s10489-022-04175-y.pdf",
        "file_path": "./PDFs/10!1007%s10489-022-04175-y.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007/s10489-022-04175-y.pdf"
    },
    {
        "title": "A Visual SHACL Shapes Editor Based On OntoPad.",
        "implementation_urls": [],
        "abstract": "AbstractOn the Semantic Web, vocabularies and ontologies play a fundamental role to express the terminologyand rules of certain domains. New technologies like SHACL provide the possibility to express dataschemata specific to certain data sets, applications, and domains. However, the domain modeling pro-cess is collaborative and when using RDF, it requires technical knowledge. In this paper, we presenta tool to support a two-step-process to model a terminology and a schema with a combined graphicalRDF Schema editor and visual SHACL editor. This tool allows domain experts to create a terminologyand schema without the need for a deep understanding of RDF Schema or SHACL.Demo URL: https://ontopad.aksw.org/1. IntroductionThe W3C has promoted the Shapes Constraint Language (SHACL) [6] as recommendation toconstruct schematic blueprints as shapes of RDF data. These shapes can be used to validatedata, to construct input forms to author new RDF data, and to express a domain model. Inthis way, they provide a pragmatic and flexible way to express how the individual terms ina vocabulary (classes and properties) relate to each other and how the instance data shouldlook like. To model a domain means to understand and express its language and rules. Toformally express this domain model with means of the Semantic Web is a technical process.The overall process of domainmodeling is a collaborative process that requires the involvementof domain experts. Providing a graphical tool that allows to interact with SHACL shapes byusing a visual diagram component would allow to make the RDF layer transparent to its usersand provide a visual language to interact with the data model. To support the collaborativedomain modeling process, visual editors could help to increase the involvement of domainSEMANTiCS 2021, September 6–9, 2021, Amsterdam, NLarndt@informatik.uni-leipzig.de (N. Arndt); valdestilhas@informatik.uni-leipzig.de (A. Valdestilhas);gustavo.publio@informatik.uni-leipzig.de (G. Publio); cimmino@fi.upm.es (A. Cimmino);konrad.hoeffner@uni-leipzig.de (K. Höffner); thomas.riechert@htwk-leipzig.de (T. Riechert)https://aksw.org/NatanaelArndt (N. Arndt); https://aksw.org/AndreValdestilhas (A. Valdestilhas); https://aksw.org/GustavoPublio (G. Publio); https://aksw.org/KonradHoeffner (K. Höffner); https://aksw.org/ThomasRiechert(T. Riechert)0000-0002-8130-8677 (N. Arndt); 0000-0002-0079-2533 (A. Valdestilhas); 0000-0002-3853-3588 (G. Publio);0000-0002-1823-4484 (A. Cimmino); 0000-0001-7358-3217 (K. Höffner); 0000-0003-2053-5347 (T. Riechert)© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)https://ontopad.aksw.org/mailto:arndt@informatik.uni-leipzig.demailto:valdestilhas@informatik.uni-leipzig.demailto:gustavo.publio@informatik.uni-leipzig.demailto:cimmino@fi.upm.esmailto:konrad.hoeffner@uni-leipzig.demailto:thomas.riechert@htwk-leipzig.dehttps://aksw.org/NatanaelArndthttps://aksw.org/AndreValdestilhashttps://aksw.org/GustavoPubliohttps://aksw.org/GustavoPubliohttps://aksw.org/KonradHoeffnerhttps://aksw.org/ThomasRiechert",
        "publication_date": "2021-01-01",
        "authors": "Natanael Arndt, André Valdestilhas, Gustavo Publio, Andrea Cimmino Arriaga, Konrad Höffner, Thomas Riechert",
        "file_name": "no_doi_20250624164305.pdf",
        "file_path": "./PDFs/no_doi_20250624164305.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2941/paper16.pdf"
    },
    {
        "title": "The modular SSN ontology: A joint W3C and OGC standard specifying the semantics of sensors, observations, sampling, and actuation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/w3c/sdw",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!3233%sw-180320.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3The complete ontology file for the example is avail-able at: https://github.com/w3c/sdw/blob/gh-pages/ssn/integrated/examples/house134.ttl."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-180320",
        "abstract": "Abstract. The joint W3C (World Wide Web Consortium) and OGC (Open Geospatial Consortium) Spatial Data on the Web(SDW) Working Group developed a set of ontologies to describe sensors, actuators, samplers as well as their observations, actu-ation, and sampling activities. The ontologies have been published both as a W3C recommendation and as an OGC implementa-tion standard. The set includes a lightweight core module called SOSA (Sensor, Observation, Sampler, and Actuator) available at:http://www.w3.org/ns/sosa/, and a more expressive extension module called SSN (Semantic Sensor Network) available at: http://www.w3.org/ns/ssn/. Together they describe systems of sensors and actuators, observations, the used procedures, the subjectsand their properties being observed or acted upon, samples and the process of sampling, and so forth. The set of ontologies adoptsa modular architecture with SOSA as a self-contained core that is extended by SSN and other modules to add expressivity andbreadth. The SOSA/SSN ontologies are able to support a wide range of applications and use cases, including satellite imagery,large-scale scientific monitoring, industrial and household infrastructures, social sensing, citizen science, observation-driven on-tology engineering, and the Internet of Things. In this paper we give an overview of the ontologies and discuss the rationalebehind key design decisions, reporting on the differences between the new SSN ontology presented here and its predecessor [WebSemantics: Science, Services and Agents on the World Wide Web 17 (2012), 25–32] developed by the W3C Semantic Sensor1570-0844/19/$35.00 © 2019 – IOS Press and the authors. All rights reservedmailto:armin.haller@anu.edu.aumailto:kerry.taylor@anu.edu.aumailto:jano@geog.ucsb.edumailto:simon.cox@csiro.aumailto:maxime.lefrancois@emse.frmailto:danh.lephuoc@tu-berlin.demailto:jlieberman@fas.harvard.edumailto:rgarcia@fi.upm.esmailto:rob@metalinkage.com.aumailto:cstadler@informatik.uni-leipzig.dehttp://www.w3.org/ns/sosa/http://www.w3.org/ns/ssn/http://www.w3.org/ns/ssn/http://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-180320&domain=pdf&date_stamp=2018-08-2910 A. Haller et al. / The modular SSN ontology: A joint W3C and OGC standardNetwork Incubator group (the SSN-XG). We present usage examples and describe alignment modules that foster interoperabilitywith other ontologies.Keywords: Ontology, sensor, actuator, observation, actuation, sampling, linked data, Web of things, internet of things1. IntroductionSensors are a major source of data available on theWeb today. The trend towards making cities, offices,and homes ‘smarter’ by turning them into sensor-richenvironments [30] drives the demand for specificationsthat describe how to model and publish sensor and ac-tuator data as well as how to foster interoperabilityacross platforms on the Web or other data infrastruc-tures.Sensor readings are often provided only as raw nu-meric values, but any searching, reusing, integrating,or interpreting of these data requires more than just theobservation results. Of equal importance for the properinterpretation of these values is contextual informationabout the studied feature of interest, such as a river,the observed property, such as flow velocity, the uti-lized sampling strategy, such as the specific locationsor sampling stations and times at which the velocity",
        "publication_date": "2018-08-31",
        "authors": "Armin Haller, Krzysztof Janowicz, Simón Cox, Maxime Lefrançois, Kerry Taylor, Danh Le-Phuoc, Joshua Lieberman, Raúl García‐Castro, Rob Atkinson, Claus Stadler",
        "file_name": "10!3233%sw-180320.pdf",
        "file_path": "./PDFs/10!3233%sw-180320.pdf"
    },
    {
        "title": "Semantic technologies and interoperability in the built environment",
        "implementation_urls": [],
        "doi": "10.3233/sw-180321",
        "abstract": "Abstract. The built environment consists of plenty of physical assets with which we interact on a daily basis. In order toimprove not only our built environment, but also our interaction with that environment, we would benefit a lot from semanticrepresentations of this environment. This not only includes buildings, but also large infrastructure (bridges, tunnels, waterways,underground systems), and geospatial data. With this special issue, an insight is given into the current state of the art in termsof semantic technologies and interoperability in this built environment. This editorial not only summarizes the content of theSpecial Issue on Semantic Technologies and interoperability in the Built Environment, it also provides a brief overview of thecurrent state of the art in general in terms of standardisation and community efforts.Keywords: Semantics, built environment, building information model, linked data, architecture, construction1. Semantics in the built environmentThe built environment encompasses a variety of ar-tifacts ranging from buildings to infrastructures. Theseartifacts are linked at different scales and in variousways. Namely, relations can be found between ele-ments in a singular building, furniture, street lights,and so forth to elements and concepts covering theentire city, such as infrastructure, traffic, and peopleflows. Apart from this multiplicity of artifacts, the builtenvironment also gathers multiple stakeholders whocollaborate in various ways. Collaboration and inter-action not only happens in the built environment as itexists on a daily basis, but even more so throughout*Corresponding author. E-mail: pipauwel.pauwels@ugent.be.all the design, construction and operation phases tak-ing place within the built environment. This includesspecialists (architects, engineers, and contractors), butalso local administrators, facility managers, and citi-zens.A large part of this environment is governed by theArchitecture, Engineering, and Construction (AEC)industry. An effective collaboration and interoperabil-ity between these different actors throughout the life-cycle of the built environment has always been akey challenge to this industry. Data from stakehold-ers is modeled and published in various languagesand scales, in particular using Building InformationModelling (BIM) tools [2], and data evolves consid-erably over time. Hence, maintaining data consistencythroughout the whole life-cycle of a building, espe-1570-0844/18/$35.00 © 2018 – IOS Press and the authors. All rights reservedmailto:pipauwel.pauwels@ugent.bemailto:mpoveda@fi.upm.esmailto:alvaro.sicilia@salle.url.edumailto:Jerome.Euzenat@inria.frmailto:pipauwel.pauwels@ugent.behttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-180321&domain=pdf&date_stamp=2018-08-24732 P. Pauwels et al. / Semantic technologies and interoperability in the built environmentcially during the design and construction phases, is afundamental challenge to this industry.Beyond the design and construction phases, impor-tant other amounts of data are present as well in the",
        "publication_date": "2018-08-24",
        "authors": "Pieter Pauwels, María Poveda‐Villalón, Álvaro Sicilia, Jérôme Euzenat",
        "file_name": "10!3233%sw-180321.pdf",
        "file_path": "./PDFs/10!3233%sw-180321.pdf"
    },
    {
        "title": "Pody: A Solid-Based Approach to Embody Agents in Web-Based Multi-Agent-Systems",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-48539-8_15",
        "abstract": "Abstract. In this paper we discuss the problem of situatedness foragents perceiving and acting on the Web (namely, “Web agents”). As-suming Web agents are embodied on the World Wide Web, then we mustdefine what is a Web agent’s body. We first provide an abstract defini-tion of a Web agent’s body in terms of what it should comprise. Then wepropose a concrete definition of it relying on Solid, a recent Web technol-ogy for Social Linked Data: we implement a Web agent’s body as a datapod. Consequently, we coin the term pody to refer to the Web entity thatembodies an agent on the Web with Solid. This paper summarises thefindings of a working group from the Dagstuhl Seminar 23081: Agentson the Web (February 19-24, 2023).Keywords: MAS · Semantic Web · Solid· Embodiment · Situatedness1 IntroductionSituatedness and embodiment are key notions in research on intelligent agents.The dominant view is that intelligent, rational behaviour is closely related to theenvironment an agent occupies and is not disembodied [17]. This view emergedin the late ’80s in close relationship with research on intelligent robots [11], whichare naturally situated and embodied in a physical environment. The complexityof virtual environments, such as the Web, now rivals that of physical environ-ments. Furthermore, with the recent standardisation of the Web of Things atthe W3C and the IETF, the Web now extends to the physical world – and thusbecomes a uniform hypermedia fabric that interconnects virtual and physical en-vironments. This evolution unlocks new practical use cases for intelligent agents2 A. Zimmermann et al.on the Web, that need to be situated and embodied in their environment. Thisvision that can be traced back to the early days of the Web 6.In this paper, we discuss howWeb agents can be embodied into the Web, bothat an abstract level and concretely using Web standards and technologies. In anutshell, we envision a Web agent’s body as a collection of Web resources andWeb interfaces that are attached to the identity of the agent. The Web agent’sbody allows the agent to participate in collective work as part of a multi-agentsystem (MAS) on the Web: to perceive and actuate Web resources (includingWeb-enabled devices), to be discovered and perceived by other agents, to par-ticipate in organisations, to communicate with other agents, etc. We illustratethis vision through a concrete example of Web agents embodiement using Solidpods, the core concept and technology from Sir Tim Berners-Lee’s project forSocial Linked Data – an initiative to preserve the decentralised nature of theWeb and to radically decentralise personal data. In particular, this enables toseamlessly address MAS use cases where a strong emphasis on ownership of theagents’ personal data and resources is needed.The paper is organized as follows: We first present in Section 2 the context inwhich our proposal arose. Then we present in Section 3 our vision of how agentsshould be situated and embodied on the Web, independently of the technologiesused. Finally we show in Section 4 how this can be implemented using Solid.In the end, we discuss in Section 5 what other abstractions would be neededto articulate podies with other essential dimensions of Web-based MAS and weconclude in Section 6.2 BackgroundIn this section, we first discuss the notions of situatedness and embodiment inArtificial Intelligence – and, in particular, in MAS engineering (Section 2.1).",
        "publication_date": "2023-01-01",
        "authors": "Antoine Zimmermann, Andrei Ciortea, Catherine Faron Zucker, Eoin O’Neill, María Poveda‐Villalón",
        "file_name": "10!1007%978-3-031-48539-8_15.pdf",
        "file_path": "./PDFs/10!1007%978-3-031-48539-8_15.pdf"
    },
    {
        "title": "InDaMul: Incentivized Data Mules for Opportunistic Networking Through Smart Contracts and Decentralized Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/umbral-rs",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1145%3587696.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Retrieved from https://github.com/miker83z/umbral-rs."
                    }
                ]
            }
        ],
        "doi": "10.1145/3587696",
        "publication_date": "2023-03-18",
        "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1145%3587696.pdf",
        "file_path": "./PDFs/10!1145%3587696.pdf"
    },
    {
        "title": "A Distributed Ledger Based Infrastructure for Smart Transportation System and Social Good",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1109%ccnc46108!2020!9045640.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [4] S."
                    }
                ]
            }
        ],
        "doi": "10.1109/ccnc46108.2020.9045640",
        "arxiv": "1910.03280",
        "abstract": "AbstractThis paper presents a system architecture to promote the development of smart transportation systems. Thanks to the use ofdistributed ledgers and related technologies, it is possible to create, store and share data generated by users through their sensors,while moving. In particular, IOTA and IPFS are used to store and certify data (and their related metadata) coming from sensors orby the users themselves. Ethereum is exploited as the smart contract platform that coordinates the data sharing and provisioning.The necessary privacy guarantees are provided by the usage of Zero Knowledge Proof. We show some results obtained from someuse case scenarios that demonstrate how such technologies can be integrated to build novel smart services and to promote socialgood in user mobility.I. INTRODUCTIONIn the last decades, smart transportation systems have emerged as a way to improve transportation efficiency, travel safety,vehicle security and better choices for drivers and passengers. Today, intelligent vehicles and transportation systems representfundamental technologies, that improve drivers comfort and security. A variety of applications and protocols can be enforcedaltogether to obtain advanced and improved transportation systems. However, to fully exploit their potential and promotethe development of smart mobility applications and services for social good, several novel challenges must be faced, thatrequire substantial changes in transportation system models. The mentioned goals can be accomplished only through the useof procedures, systems and devices that allow data gathering, communication, analysis and distribution among individualsvehicles, infrastructures and services.A reduced presence of (human) intermediaries can lead to the creation of smart services that take advantage of fasterprocessing and better performances to provide the basis for smart moving and peer-to-peer services. Moreover, in the case ofdata sharing, service automation enables users to completely maintain control over the data they produce, making possible anintelligent sharing for social good.In this scenario, another prominent technology that can play a main role is the decentralized management of crowd-sourceddata, i.e. the blockchain [1]. The blockchain, made famous by Bitcoin [2], enabled a new vision for both finance and trustin distributed systems. Since their inception, the growth of blockchain technologies renewed the concepts of contracts anddigital democracy, especially after the introduction of Ethereum [3]. The decentralized computation enabled by the Ethereumblockchain allows to create self-managed structures that do not rely on a central control, thus eliminating the presence ofsingle point of failures [4]. Moreover, this blockchain allows using smart contracts in order to build Decentralized Applications(dApps) and Decentralized Autonomous Organizations (DAOs) that can realize novel important applications for social good [5].Under the technical viewpoint, a blockchain is a specific type of Distributed Ledger Technology (DLT) with the scope tomove trust from a human intermediary, that manages a transaction between two parties, to a protocol that allows the twoparties to transact directly, i.e. without the need of such third party. There are different implementations of DLTs, each onewith its pros and cons. For example, Ethereum [3] provides a distributed virtual machine that is able to process any kind ofcomputation but with constraints in scalability. Conversely, IOTA ledger [6] is thought to provide better scalability but it doesnot support distributed computation. Thus, if one wants to build a sophisticated software architecture, acting as the middlewarefor secure and certified smart transportation system applications, multiple DLTs can be utilized and combined, so as to takethe best of multiple worlds. This is the philosophy we followed in our approach.The aim of this work is to use DLTs to propose an infrastructure for smart transportation systems. Two main features areat the basis on this infrastructure: data sharing and smart services. We claim that the combination of data sharing and smartservices allows creating a framework that promotes social good in user mobility. In particular, data sharing services are definedto let users and sensors to share their data. These services allow defining how the data can be shared but also how (from whoand through which technology) they are acquired. The proposed infrastructure is based on DLTs, in combination with other0 The publisher version of this paper is available at https://doi.org/10.1109/CCNC46108.2020.9045640. This is the pre-peer reviewed version of thearticle: “Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo. A Distributed Ledger Based Infrastructure for Smart Transportation System andSocial Good. Proceedings of the IEEE Consumer Communications and Networking Conference 2020 (CCNC 2020).”.arXiv:1910.",
        "publication_date": "2020-01-01",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo",
        "file_name": "10!1109%ccnc46108!2020!9045640.pdf",
        "file_path": "./PDFs/10!1109%ccnc46108!2020!9045640.pdf"
    },
    {
        "title": "Injecting data into ODRL privacy policies dynamically with RDF mappings",
        "implementation_urls": [
            {
                "identifier": "https://github.com/helio-ecosystem/helio-action-odrl",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/3543873.3587358.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Helio provides an online playground to play and test these mappings; in fact, the demo is available as one of the playground examples4."
                    }
                ]
            }
        ],
        "doi": "10.1145/3543873.3587358",
        "abstract": "ABSTRACT The privacy of the data provided by available sources is one of the major concerns of our era. In order to address this challenge, the W3C has promoted recommendations to allow expressing privacy policies. One of these recommendations is the Open Digital Rights Language (ODRL) vocabulary. Although this standard has wide adoption, it is not suitable in domains such as IoT, Ubiquitous and Mobile Computing, or discovery. The reason behind is the fact that ODRL privacy policies are not able to cope with dynamic informa-tion that may come from external sources of data and, therefore, these policies can not defne privacy restrictions upon data that is not already written in the policy beforehand. In this demo paper, a solution to this challenge is presented. It is shown how ODRL policies can overcome the aforementioned limitation by being com-bined with a mapping language for RDF materialisation. The article shows how ODRL policies are able to consider data coming from an external data source when they are solved, in particular, a weather forecast API that provides temperature values. The demonstration defnes an ODRL policy that grants access to a resource only when the temperature of the API is above a certain value. KEYWORDS Privacy, ODRL, RDF materialisation ACM Reference Format: Juan Cano-Benito, Andrea Cimmino, and Raúl García-Castro. 2023. Injecting data into ODRL privacy policies dynamically with RDF mappings. In Com-panion Proceedings of the ACM Web Conference 2023 (WWW ’23 Companion), April 30–May 04, 2023, Austin, TX, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3543873.3587358 1 INTRODUCTION Nowadays, the privacy associated to sources of data is one of the major concerns of our era. Due to this fact, organisations such as the W3C have presented recommendations to model and defne privacy policies associated with digital assets that encode the conditions under which a requester may or may not perform an action over these assets [5] and researches have expressed their interest to express privacy policies with the semantic web [1]. In this context, W3C established a recommendation called the Open Digital Rights Language (ODRL) [4]. ODRL provides a common vocabulary for Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWW ’23 Companion, April 30–May 04, 2023, Austin, TX, USA © 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9419-2/23/04. https://doi.org/10.1145/3543873.3587358 describing and managing rights in a standardised, interoperable, and machine-readable way, allowing data owners and consumers to express, understand, and manage the conditions and obligations ",
        "publication_date": "2023-04-28",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": "3543873.3587358.pdf",
        "file_path": "./PDFs/3543873.3587358.pdf"
    },
    {
        "title": "Satellite Earth Observation for Essential Climate Variables Supporting Sustainable Development Goals: A Review on Applications",
        "implementation_urls": [],
        "doi": "10.3390/rs15112716",
        "abstract": "Abstract: Essential climate variables (ECVs) have been recognized as crucial information for achievingSustainable Development Goals (SDGs). There is an agreement on 54 ECVs to understand climateevolution, and multiple rely on satellite Earth observation (abbreviated as s-ECVs). Despite the effortsto encourage s-ECV use for SDGs, there is still a need to further integrate them into the indicatorcalculations. Therefore, we conducted a systematic literature review to identify s-ECVs used inSDG monitoring. Results showed the use of 14 s-ECVs, the most frequent being land cover, ozone,precursors for aerosols and ozone, precipitation, land surface temperature, soil moisture, soil carbon,lakes, and leaf area index. They were related to 16 SDGs (mainly SDGs 3, 6, 11, 14, and 15), 33 targets,and 23 indicators. However, only 10 indicators (belonging to SDGs 6, 11, and 15) were calculatedusing s-ECVs. This review raises research opportunities by identifying s-ECVs yet to be used in theindicator calculations. Therefore, indicators supporting SDGs must be updated to use this valuablesource of information which, in turn, allows a worldwide indicator comparison. Additionally, thisreview is relevant for scientists and policymakers for future actions and policies to better integrates-ECVs into the Agenda 2030.Keywords: SDG; sustainable development; satellite; Earth observation; review; essential variables;climate1. IntroductionThe Agenda 2030 for Sustainable Development and its 17 goals (SDGs) are connectedwith the environment, economy, and society dimensions of sustainable development [1].The 17 goals, their 169 associated targets, and 231 indicators are based on the first data-driven policy development framework, following the principle of “If you don’t measureit, you can’t manage it” [2] (p. 2). Despite the recognized importance of measuring theprogress towards the SDGs, two-thirds of the indicators remain unreported, especially inlow-income countries [3]. Moreover, less than 44% of the SDG indicators can be easilymeasured [4]. Therefore, it is a priority to boost the measuring and monitoring of theprogress towards the SDGs. In our work, we focus on two key approaches to pursue thisRemote Sens. 2023, 15, 2716. https://doi.org/10.3390/rs15112716 https://www.mdpi.com/journal/remotesensinghttps://doi.org/10.3390/rs15112716https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.comhttps://orcid.org/0000-0002-6926-4827https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0001-7380-0701https://orcid.org/0000-0001-5145-9223https://orcid.org/0000-0001-6559-2033https://doi.org/10.3390/rs15112716https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.com/article/10.3390/rs15112716?type=check_update&version=1Remote Sens. 2023, 15, 2716 2 of 25aim: (1) essential variables (EVs) that have been defined as an intermediate layer betweenobservations and indicators [5] and (2) satellite Earth observation (sEO) data that gainedparticular attention as worldwide feasible, cost-effective, and analysis-ready data acrossscales in remote, non-accessible, and poorly monitored regions [6].The EVs emerged in various social and environmental scientific communities relatedto specific domains such as climate, biodiversity, agriculture, and society [5,7–9]. Referto [10,11] for detailed EV compendiums. These kinds of variables are “a minimal setof variables that determine the system’s state and developments, [which] are crucial for",
        "publication_date": "2023-05-24",
        "authors": "Daniela Ballari, Luis M. Vilches‐Blázquez, María Lorena Orellana-Samaniego, Francisco Salgado, Ana Ochoa‐Sánchez, Valerie Graw, Nazli Turini, Jörg Bendix",
        "file_name": "no_doi_20250624164421.pdf",
        "file_path": "./PDFs/no_doi_20250624164421.pdf",
        "pdf_link": "https://www.mdpi.com/2072-4292/15/11/2716/pdf?version=1684894645"
    },
    {
        "title": "Conformance testing of ontologies through ontology requirements",
        "implementation_urls": [
            {
                "identifier": "github.com/kierendavies/tddonto2",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S0952197620303079-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3 The last version of TDDOnto is available in the following URL: https:/github.com/kierendavies/tddonto2."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.engappai.2020.104026",
        "abstract": "(TSS). The test cases are combined into an abstract test suite (ATS)using a specific testing language such as TTCN. It is worth mentioningthat the test suite is abstract in the sense that the tests are developedindependently of any implementation.Based on the ATS, a set of executable test cases (ETS) is generated.Such ETSs are then verified against a number of implementationsto test (IUT) for correct operation according to some agreed proce-dures. An implementation extra information for test (IXIT) associatedto the ATS should be produced to help executing protocol conformancetesting. The results of this verification process are documented in aconformance report. Fig. 1 summarises these activities.A. Fernández-Izquierdo and R. García-Castro Engineering Applications of Artificial Intelligence 97 (2021) 104026aFig. 2. Development of ETSI test specifications (Moseley et al., 2003).2.1.2. Conformance testing by the ETSIThe ETSI conformance testing specifications are developed accord-ing to the method described in the ISO/IEC 9646. The specificationprovided by the ETSI (Moseley et al., 2003) is also focused on severalartefacts, previously defined in the ISO/IEC 9646, namely, the Im-plementation Conformance Statement (ICS), the Implementation eXtraInformation for Testing (IXIT), the Testing Purposes (TP), the AbstractTest Suite (ATS), and the Executable Test Suite (ETS). Fig. 2 shows theprocess development of test specifications.Similarly as in the ISO/IEC 9646, the ATS represents the entirecollection of test cases. In the ETSI conformance testing method, eachtest case specifies the detailed coding of the TP written using the stan-dardised test specification language TTCN-3 (Grabowski et al., 2003),which is an updated version of TTCN. The ETS can be implementedfrom the ATS using the TTCN compilers available on testing platforms.All the test specifications developed by ETSI are available onlineand can be searched and downloaded via the ETSI Work Programmeapplication.1 These specifications include a description of the tests in ahuman-readable format as well as the executable tests written followingthe TTCN language.2.1.3. Conformance testing by the W3CThe conformance testing approach presented by the W3C is focusedon the requirements and good practices for including conformancein W3C specifications (Dubost et al., 2005). It emphasises the needof conformance clauses in order to develop successful interoperabilityof implementations, rather than on defining an homogeneous way todefine tests for analysing conformance.The W3C proposes conformance requirements such as the additionof conformance clauses in every specification and the identification ofwhich conformance requirements are mandatory, recommended andoptional. Additionally, it also requires to use a consistent style forconformance requirements and to explain how to distinguish them. Anexample of detailed conformance clauses following the W3C guidelinesis included in the Scalable Vector Graphics 1.1 specification (Dahlströmet al., 2011), which describes all the requirements that should befulfilled.",
        "publication_date": "2020-10-23",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "1-s2.0-S0952197620303079-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0952197620303079-main.pdf"
    },
    {
        "title": "A multi-agent architecture for mobile sensing systems",
        "implementation_urls": [],
        "doi": "10.1007/s12652-019-01608-4",
        "abstract": "AbstractMobile sensing systems based on smartphones, connected vehicles and integrated sensors on new mobile devices have become an important alternative for the development of intelligent services in large urban environments. Massive data col-lection and its real-time analysis are essential for big cities to move towards energy efficiency, sustainable mobility, protec-tion of the environment and economic sustainability. Current research and applications are mainly focused on the use of individual devices and the analysis of information on a single domain (e.g. activity recognition). However, it is still necessary to provide solutions for social problems based on smart mobile devices connected to the city. In this paper, we present an architecture for mobile sensing systems in large cities based on the intelligent agent paradigm and multi-agent systems. The presented platform provides support for multi-purpose machine learning services, implementing expert learning agents in each domain where the system collects data. Furthermore, the main challenges in mobile sensing systems such as scalability in crowded environments, handling of a large amount of data and the increasing appearance of sensing devices are addressed by the architecture due to the agent paradigm and multi-agent systems suit these demands naturally.Keywords  Mobile sensing · Multi-agent systems · Human-agent societies1  IntroductionThe exponential growth during the last years of smart-phones in the society together with the continued inclusion of more and more sensors into everyday devices (e.g. vehi-cles, household appliances, etcetera) allows applications to obtain large amounts of information about users activities and behaviors. This approach, applied in a large urban envi-ronment, will allow the city to provide intelligent systems to its population, which can help to improve their quality of life and social welfare. Citizens smart devices, such as wearables, mobile phones, autonomous cars or electric bicy-cles are continuously gathering data from their surround-ings. The communication and interaction of these devices can create a context-aware system, which based on the collected data through the sensors can provide to the city powerful tools to move towards energy efficiency, sustain-able mobility, protection of the environment and economic sustainability.Given this new scenario, several challenges must be addressed. The main challenge for a sensing system is its scalability, there are not yet any examples of a smart city that supports hundreds of thousands of people, much less millions. So the crucial problem for a mobile sensing archi-tecture that aims to sense a metropolis is how to handle the huge amount of information of a Big Data environment and its dynamism. Moreover, the implementation of mobile sens-ing systems requires the support of different technologies and tools such as middleware, communication protocols, learning and decision algorithms, etcetera.Multi-agent systems (MAS) and the intelligent agent para-digm appears to be one of the best approaches for supporting the diverse, dynamic and heterogeneous nature of a mobile sensing architecture. Its distributed setting and the internal characteristics of each individual agent such as reactivity,  *\t Emilio Serrano \t emilioserra@fi.upm.es\t Francisco Laport ",
        "publication_date": "2019-12-07",
        "authors": "Francisco Laport, Emilio Serrano, Javier Bajo",
        "file_name": "s12652-019-01608-4.pdf",
        "file_path": "./PDFs/s12652-019-01608-4.pdf"
    },
    {
        "title": "An information sharing strategy based on linked data for net zero energy buildings and clusters",
        "implementation_urls": [],
        "doi": "10.1016/j.autcon.2021.103592",
        "publication_date": "2021-02-01",
        "authors": "Yehong Li, Shushan Hu, Cathal Hoare, James O’Donnell, Raúl García‐Castro, Sergio Vega Sánchez, Xiangyang Jiang",
        "file_name": "1-s2.0-S0926580521000431-main.pdf",
        "file_path": "./PDFs/1-s2.0-S0926580521000431-main.pdf"
    },
    {
        "title": "The NeOn Methodology for Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_2",
        "abstract": "Abstract In contrast to other approaches that provide methodological guidance forontology engineering, the NeOn Methodology does not prescribe a rigid workflow,but instead it suggests a variety of pathways for developing ontologies. The ninescenarios proposed in the methodology cover commonly occurring situations, forexample, when available ontologies need to be re-engineered, aligned, modular-ized, localized to support different languages and cultures, and integrated withontology design patterns and non-ontological resources, such as folksonomies orthesauri. In addition, the NeOn Methodology framework provides (a) a glossary ofprocesses and activities involved in the development of ontologies, (b) two onto-logy life cycle models, and (c) a set of methodological guidelines for differentprocesses and activities, which are described (a) functionally, in terms of goals,inputs, outputs, and relevant constraints; (b) procedurally, by means of workflowspecifications; and (c) empirically, through a set of illustrative examples.2.1 IntroductionGiven the large increase in the number of ontologies, which are available online,ontology development is more and more becoming a reuse-centric process (Simperl2009). In particular, the level of reuse may vary significantly, depending on whetherM.C. Suárez-Figueroa (*) • A. Gómez-PérezOntology Engineering Group, Facultad de Informática, Universidad Politécnica de Madrid,Campus de Montegancedo sn., 28660 Boadilla del Monte, Madrid, Spaine-mail: mcsuarez@fi.upm.es; asun@fi.upm.esM. Fernández-LópezEscuela Politécnica Superior, Universidad San Pablo CEU, Urbanización Monteprı́ncipe sn.,28668 Boadilla del Monte, Madrid, Spaine-mail: mfernandez.eps@ceu.esM.C. Suárez-Figueroa et al. (eds.), Ontology Engineering in a Networked World,DOI 10.1007/978-3-642-24794-1_2, # Springer-Verlag Berlin Heidelberg 20129mailto:mcsuarez@fi.upm.esmailto:asun@fi.upm.esmailto:mfernandez.eps@ceu.esit concerns (a) other ontologies, such as DOLCE1, SUMO (Pease et al. 2002), andKowien2; (b) ontology modules (Cuenca-Grau et al. 2007); (c) ontology statementsand ontology design patterns (Gangemi 2007; Presutti and Gangemi 2008); and(d) non-ontological resources (Jimeno-Yepes et al. 2009), such as thesauri, lexicons,DBs, UML diagrams, and classification schemas (e.g., NAICS3 and SOC4).Thus, in this context ontology development can be then characterized as theconstruction of a network of ontologies, where the different resources may bemanaged by different people, possibly in different organizations.Given this new vision of ontology engineering by reuse, it then becomesimportant to provide strong methodological support for the collaborative develop-ment of ontology networks.Methodological frameworks are widely accepted in different mature fields(Fernández-López 1999), like Software Engineering and Knowledge Engineering.Such methodological frameworks cover aspects, such as development process, lifecycle models, as well as the methods, techniques, and tools that can be used tosupport the development process. Accordingly, a mature methodology for develop-ing ontologies should also cover these aspects.This chapter describes the NeOn Methodology for building ontologies andontology networks, a scenario-based methodology that supports different aspects",
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez, Mariano Fernández‐López",
        "file_name": "The_NeOn_Methodology_for_Ontology_Engineering.pdf",
        "file_path": "./PDFs/The_NeOn_Methodology_for_Ontology_Engineering.pdf"
    },
    {
        "title": "DockerPedia: A Knowledge Graph of Software Images and Their Metadata",
        "implementation_urls": [],
        "doi": "10.1142/s0218194022500036",
        "abstract": "1.1 concepts and abstract syntax, W3C recommendation 25(02) (2014) 1–22.[27] I. Santana-Perez, R. F. da Silva, M. Rynge, E. Deelman, M. S. Pérez-Hernández andO. Corcho, Reproducibility of execution environments in computational science usingsemantics and clouds, Future Generation Computer Systems 67 (2017) 354–367.[28] Docker manifest specification v2.2. Accessed from https://docs.docker.com/registry/spec/manifest-v2-2/.[29] Clair GitHub repository. Accessed from https://github.com/quay/clair.[30] Quai main page. Accessed from http://status.quay.io.[31] Conda documentation page. Accessed from https://conda.io/docs.[32] Digital Ocean main page. Accessed from https://www.digitalocean.com/.[33] Common vulnerabilities and exposures database home page. Accessed from https://cve.mitre.org/.[34] CVE reports. Accessed from https://ubuntu.com/security/cve.[35] Debian security bug tracker home page. Accessed from https://security-tracker.debian.org/tracker/.[36] Red Hat Product Security Data Homepage. Accessed from https://access.redhat.com/security/data.[37] Alpine Security Database of Backported fixes. Accessed from https://github.com/alpinelinux/alpine-secdb.[38] M. Osorio and C. Buil-Aranda, Dockerpedia dataset (December 2018), doi: https://doi.org/10.5281/zenodo.1897809.[39] M. Osorio, C. Buil-Aranda, I. Santana-Pérezand D. Garijo, Queries used to illustrate the DockerPedia Knowledge Graph (Jun2021), doi: 10.6084/m9.figshare.14718450.v1.[40] Dash package documentation page. Accessed from http://manpages.ubuntu.com/manpages/xenial/man1/dash.1.html.[41] M. Osorio, dockerpedia/soykb: thesis (December 2018), doi: https://doi.org/10.5281/zenodo.1889356.[42] Dockerpedia package vulnerability visualization page. Accessed from https://dockerpedia.inf.utfsm.cl/visualization.[43] P. Amstutz, M. R. Crusoe, N. Tijanić, B. Chapman, J. Chilton, M. Heuer,A. Kartashov, D. Leehr, H. Ménager, M. Nedeljkovich, M. Scales, S. Soiland-Reyes and L. Stojanovic, Common Workflow Language, v1.0 (7 2016), doi:10.6084/m9.figshare.3115156.v2.[44] D. Garijo, Y. Gil and O. Corcho, Abstract, link, publish, exploit: An end to endframework for workflow sharing, Future Generation Computer Systems 75 (2017).[45] K. Belhajjame, M. Roos, E. Garcia-Cuesta, G. Klyne, J. Zhao, D. De Roure, C. Goble,J. M. Gomez-Perez, K. Hettne and A. Garrido, Why workflows break — understand-ing and combating decay in taverna workflows, in Proceedings of the 2012 IEEE 8thInternational Conference on E-Science (e-Science), E-SCIENCE ’12, (IEEE Com-puter Society, Washington, DC, USA, 2012), pp. 1–9.[46] N. D. Rollins, C. M. Barton, S. Bergin, M. A. Janssen and A. Lee, A computationalmodel library for publishing model documentation and code, Environ. Model. Softw.March 2, 2022 11:13 WSPC/INSTRUCTION FILE output20 Osorio et. al.61 (November 2014) 59–64.[47] F. Chirigati, D. Shasha and J. Freire, Reprozip: Using provenance to support com-putational reproducibility, in Presented as part of the 5th {USENIX} Workshop onthe Theory and Practice of Provenance, 2013.[48] V. Steeves, R. Rampin and F. Chirigati, Using ReproZip for reproducibility and",
        "publication_date": "2022-01-01",
        "authors": "Maximiliano Osorio, Carlos Buil-Aranda, Idafen Santana-Pérez, Daniel Garijo",
        "file_name": "reproducibility_docker.pdf",
        "file_path": "./PDFs/reproducibility_docker.pdf"
    },
    {
        "title": "T2WML: A Cell-Based Language to Map Tables into Wikidata Records.",
        "implementation_urls": [],
        "abstract": "Abstract. The web contains millions of useful spreadsheets and CSVfiles, but these files are difficult to use in applications because they usea wide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes it easyto map and link arbitrary spreadsheets and CSV files to the Wikidatadata model. The output of T2WML consists of Wikidata statementsthat can be loaded in the public Wikidata, or loaded in a Wikidataclone, creating an augmented Wikidata knowledge graph that applicationdevelopers can query using SPARQL.1Keywords: Knowledge Graphs, RDF, Entity Linking, Wikidata1 IntroductionThe web contains millions of useful spreadsheets and CSV files, including datafrom many government and international organizations. Organizations that offerdata often have web sites where users can search, browse and download dataon a large number of topics. Most institutions offer their data in Excel andCSV formats. The downloaded data is seldom directly usable because, unlikedatabases, which use one column per variable, spreadsheets often arrange thedata in different layouts.Fig. 1 illustrates the problem using data about homicide rates in differentcountries, downloaded from the United Nations web site2. We truncated andcolored the files for ease of presentation. The cells with the homicide numbersare highlighted in green, the cells that provide contextual information for thevalue are highlighted in blue, and header cells are highlighted in dark blue.Fig. 1a shows the layout of the data provided in the UN website; Fig. 1b showsa more compact representation using multi-level headers; Fig. 1c shows a layout1 Copyright (c) 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0). This material is basedupon work supported by United States Air Force under Contract No. FA8650-17-C-7715.2 https://dataunodc.un.org/crime/intentional-homicide-victimsFig. 1. Intentional Homicide Data (Excel file downloaded from dataunodc.un.org)that could be used to store the data in a database, and that can be used directlyin tools such as Pandas; Fig. 1d illustrates a common convention for arrangingdata by topic, by creating stacked tables that share common headings. All tablespresent the same homicide data. The interpretation of each value is defined byfour cells (country, year, population and source) that identify the context for avalue. In each table, the context cells are located in different parts of the data.Only in Fig. 1c (Database) the context cells are in the same row as the value; inthe other tables, context cells appear in different rows, in header rows (examplesa and b), or in visually distinct rows within the table (example d).Existing languages for mapping structured data to RDF, including R2RML3,RML [1], Karma [3] and CSV2RDF [2] process tabular data row by row, requiringtabular data to be in database format (Fig. 1c). RML supports non-tabularformats (JSON and XML) and Karma provides folding and unfolding operatorsto rearrange data for row-based processing. None support complex layouts suchas those in examples b or d.T2WML is a mapping language designed to meet three objectives: 1) Identifyand map data and their context qualifiers in arbitrary data layouts found in Exceland CSV files without the need of complex preprocessing steps to transformtables into a canonical \"Database\" representation; 2) Enable users who are not",
        "publication_date": "2019-01-01",
        "authors": "Pedro Szekely, Daniel Garijo, Jay Pujara, Divij Bhatia, Jiasheng Wu",
        "file_name": "no_doi_20250624164450.pdf",
        "file_path": "./PDFs/no_doi_20250624164450.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper12.pdf"
    },
    {
        "title": "Themis: a tool for validating ontologies through requirements",
        "implementation_urls": [],
        "doi": "10.18293/seke2019-117",
        "abstract": "Abstract— The validation of ontologies, whose aim is to checkwhether an ontology matches the conceptualization it is meantto specify, is a key activity for guaranteeing the quality ofontologies. This work is focused on the validation throughrequirements, with the aim of assuring, both the domainexperts and ontology developers, that the ontologies they arebuilding or using are complete regarding their needs. Inspiredby software engineering testing processes, this work proposesa web-based tool called Themis, independent of any ontologydevelopment environment, for validating ontologies by meansof the application of test expressions which, following lexico-syntactic patterns, represent the desired behaviour that willpresent an ontology if a requirement is satisfied.I. INTRODUCTIONIn software engineering it is inconceivable to deliver asoftware product without its pertinent tests which guaranteethat it fulfills all its requirements. Besides, there are severalapproaches integrated into the software development processwhose aim is to test the software. Unit testing [1], whichvalidates that each unit of the software performs as designed,and behaviour-driven development [2], which focuses onthe behaviour the software product is implementing, areexamples of these approaches. Moreover, there are specificsyntaxes, such as Gherkin,1 which generate unambiguousspecifications of software to automate the testing process.However, in ontology engineering there is a lack of clearlydefined testing processes in order to be able to ascertainwhether an ontology satisfies its functional requirements [3],which state the particular knowledge that should be repre-sented. Such ontological requirements used to be writtenin form of competency questions [4] or natural languagesentences. The main issue when performing testing processesin the ontology engineering field is the ambiguity of theontological requirements, which sometimes are difficult toformalize into tests and to translate into axioms. Therefore,inspired by software engineering and its specific syntax forthe definition of tests, we propose Themis,2 a tool whichprovides a set of test expressions based on lexico-syntacticpatterns (LSPs) related to ontological requirements. TheseLSPs allows to relate different types of requirements withthe axioms needed to implement them in an ontology, andsuch implementations are used by Themis to identify whethera requirement is satisfied.DOI reference number: 10.18293/SEKE2019-1711https://docs.cucumber.io/gherkin2http://themis.linkeddata.esThemis can be used by both domain experts and ontologydevelopers to validate ontologies regarding their functionalrequirements. Other type of requirements, such as non-functional ones (e.g., “the ontology URIs must be in En-",
        "publication_date": "2019-07-10",
        "authors": "Alba Fernández-Izquierdo, Raúl García‐Castro",
        "file_name": "10!18293%seke2019-117.pdf",
        "file_path": "./PDFs/10!18293%seke2019-117.pdf"
    },
    {
        "title": "Semantic Web-Enabled Protocol Mediation for the Logistics Domain",
        "implementation_urls": [],
        "doi": "10.4018/978-1-60566-066-0.ch004",
        "abstract": "Abstract Among the problems that arise when trying to make different applications interoperate with each other, protocol mediation is one of the most difficult ones and for which less relevant literature can be found. Protocol mediation is concerned with non-matching message interaction patterns in application interaction. In this paper we describe the design and implementation of a protocol mediation component that has been applied in the interoperation between two heterogeneous logistic provider systems (using two different standards: RosettaNet and EDIFACT), for a specific freight forwarding task. 1 Current Situation Logistics management is a typical business problem where the use of a Service Oriented Architecture is clearly suited. As pointed out in (Evans-Greenwood and Stason, 2006) the current trend in logistics is to divide support between planning applications, which compute production plans overnight, and execution applications, which manage the flow of events in an operational environment. This disconnection forces users to deal with business exceptions (lost shipments, for example), manually resolving the problems by directly updating the execution and planning applications. However, this human-dependency problem can be ameliorated by using Web technology to create a heterogeneous composite application involving all participants in the process, providing a complete Third-Party Logistics solution, and giving users a single unified view into the logistics pipeline. This consolidated logistics solution greatly simplifies the task of identifying and correcting business exceptions (e.g., missing shipments or stock shortages) as they occur. Furthermore, (Evans-Greenwood and Stason, 2006) also talk about the possibility of combining multiple Third-Party Logistics solutions into a single heterogeneous virtual logistics network. With such a virtual network, each shipment is assigned a route dynamically assembled from one or more individual logistics providers, using dynamically created virtual supply chains. Most of these business functions are still manual and offline, but most of them can be automated with the use of Service Oriented Architectures, as will be presented in this chapter. Obviously, the main advantages of using such solutions are the decreases in cost and speed in transactions, which influence in a better quality of the service provided to customers. The main barrier to set up a business relationship with a company in the logistics domain is that it usually requires an initial large investment of time and money. This is ameliorated by the emergence of some industry standards like EDIFACT (EDIFACT), AnsiX12 (AnsiX12) or RosettaNet (RosettaNet), which ease the integration tasks between information systems that comply with them. However, given that these standards have some flexibility in what respects the content and sequencing of the messages that can be exchanged, the integration of systems is still time and effort consuming. Besides, there is sometimes a need to integrate systems that use different standards, what makes the integration task even more time and effort consuming. This is the focus of one of the four case studies developed in the context of the EU project SWWS1 (Semantic-Web enabled Web Services), a demonstrator of business-to-business integration in the logistics domain using Semantic Web Service technology. All the features of this                                                            demonstrator are described in detail in (Preist et al., 2005), including aspects related to the discovery and selection of relevant services, their execution and the mediation between services following different protocols.  In this chapter we will focus on the last aspect (mediation) and more specifically on protocol mediation, which is concerned with the problem of non-matching message interaction patterns. We will describe the design and implementation of the protocol mediation component applied in this case study to show how to make logistic provider systems using two different standards (RosettaNet and EDIFACT) interoperate for a specific freight forwarding task.  The chapter is structured as follows. The rest of this section introduces a motivating example, focusing on the needs for protocol mediation, and gives some background on how the problem of mediation can be characterised in general and on the approaches for mediation proposed in the ",
        "publication_date": "2009-01-01",
        "authors": "Óscar Corcho, Silvestre Losada, Richard Benjamins",
        "file_name": "10!4018%978-1-60566-066-0!ch004.pdf",
        "file_path": "./PDFs/10!4018%978-1-60566-066-0!ch004.pdf"
    },
    {
        "title": "Ontology Requirements Specification",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-24794-1_5",
        "abstract": "Abstract The goal of the ontology requirements specification activity is to statewhy the ontology is being built, what its intended uses are, who the end users are,and which requirements the ontology should fulfill. This chapter presents detailedmethodological guidelines for specifying ontology requirements efficiently. Theseguidelines will help ontology engineers to capture ontology requirements andproduce the ontology requirements specification document (ORSD). The ORSDwill play a key role during the ontology development process because it facilitates,among other activities, (1) the search and reuse of existing knowledge resourceswith the aim of reengineering them into ontologies, (2) the search and reuse ofontological resources (ontologies, ontology modules, ontology statements as wellas ontology design patterns), and (3) the verification of the ontology along theontology development.5.1 IntroductionOne of the key processes in software development is software specification(Sommerville 2007), whose aim is to understand and define what functionalitiesare required from the software product. It has been proved that a detailed softwarerequirements document provides several benefits (IEEE 1993), such as (a) theestablishment of the basis for agreement between customers and suppliers onwhat the software product is supposed to do, (b) the reduction of the developmenteffort, (c) the provision of a basis for estimating costs and schedules, and (d) theoffer of a baseline for validation and verification.M.C. Suárez-Figueroa (*) • A. Gómez-PérezOntology Engineering Group, Facultad de Informática, Universidad Politécnica de Madrid,Campus de Montegancedo sn., 28660 Boadilla del Monte, Madrid, Spaine-mail: mcsuarez@fi.upm.es; asun@fi.upm.esM.C. Suárez-Figueroa et al. (eds.), Ontology Engineering in a Networked World,DOI 10.1007/978-3-642-24794-1_5, # Springer-Verlag Berlin Heidelberg 201293When a software application based on ontologies is being developed, ontologyrequirements should be identified in addition to the application requirements. Ourexperience in building ontology-based applications, in domains as diverse assatellite data processing1, finding funding programs2, fishery stocks3, user context4,and e-employment5, has shown that more critical than capturing softwarerequirements was the efficient and precise identification of the knowledge that theontology should contain. Up to now, application developers already have precisemethodologies (Sommerville 2007; IEEE 1993; Wiegers 2003) that help them todefine application requirements. However, the guidelines included in currentmethodologies for building ontologies are not enough for defining ontologyrequirements.For this reason, this chapter presents detailed methodological guidelines forspecifying ontology requirements as part of the NeOn Methodology (see Chap. 2).Such methodological guidelines are based on the use of the so-called competencyquestions (CQs) (Gr€uninger and Fox 1995) and are inspired by how methodologiesfor building ontologies propose to perform the ontology requirements specificationactivity (Staab et al. 2001; Uschold 1996; Noy and McGuinness 2001). Theseguidelines are also inspired by available practices and previous experiences indifferent national and European funded projects. These methodological guidelineshelp to capture knowledge from users and to produce the ontology requirementsspecification document that will be used by ontology developers to developan ontology that will fulfill the requirements identified.",
        "publication_date": "2011-12-01",
        "authors": "Mari Carmen Suárez-Figueroa, Asunción Gómez‐Pérez",
        "file_name": "Ontology_Requirements_Specification.pdf",
        "file_path": "./PDFs/Ontology_Requirements_Specification.pdf"
    },
    {
        "title": "Spanish corpora for sentiment analysis: a survey",
        "implementation_urls": [],
        "doi": "10.1007/s10579-019-09470-8",
        "abstract": "Abstract Corpora play an important role when training machine learning systemsfor sentiment analysis. However, Spanish is underrepresented in these corpora, asmost primarily include English texts. This paper describes 20 Spanish-language textcorpora—collected to support different tasks related to sentiment analysis, rangingfrom polarity to emotion categorization. We present a brand-new framework for thecharacterization of corpora. This includes a number of features to help analyzeresources at both corpus level and document level. This survey—besides depictingthe overall landscape of corpora in Spanish—supports sentiment analysis practi-tioners with the task of selecting the most suitable resources.Keywords Sentiment analysis � Corpora � Opinion mining � Polarity �Emotion1 IntroductionCorpora are generally understood as large collections of digital texts, where everydocument has been marked as belonging to one or more specific categories or inwhich some documents and/or fragments have been tagged with additionalinformation. Corpora are necessary to train statistical systems which will be usedThis work has been partially funded by a Predoctoral Grant from the I?D?i program of the UniversidadPolitécnica de Madrid and by Project Datos 4.0 (TIN2016-78011-C4-2-R).& Marı́a Navas-Loromnavas@fi.upm.esVı́ctor Rodrı́guez-Doncelvrodriguez@fi.upm.es1 Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid, Spain123Lang Resources & Evaluation (2020) 54:303–340https://doi.org/10.1007/s10579-019-09470-8http://orcid.org/0000-0003-1011-5023http://orcid.org/0000-0003-1076-2511http://crossmark.crossref.org/dialog/?doi=10.1007/s10579-019-09470-8&amp;domain=pdfhttps://doi.org/10.1007/s10579-019-09470-8for sentiment analysis. However, complete well-documented corpora in Spanish arescarce, despite widespread acceptance that having a variety of high-qualityresources is critical to be able to achieve good results.The recent study on ‘Language equality in the digital age’ (Rivera Pastor et al.2017), commissioned by the Directorate-General for Parliamentary ResearchServices of the European Parliament, reported that the Spanish language isunderrepresented on the web and as regards the number of language resourcesavailable.The objective of this paper is to collect and systematically describe the Spanish-language corpora available for sentiment analysis—across different sectors, formatsand even classifications, with categories ranging from basic polarity to complexemotion annotation. This review considers different aspects that could be useful fora number of NLP tasks, and provides a short comment for each of the corpora anddetails the tasks for which they are most suitable. To the best of our knowledge, noanalysis of this kind exists for Spanish-language corpora in the field of sentimentanalysis.The paper is organized as follows: Sect. 2 presents an evaluation framework wehave developed specifically for corpora which could be used for sentiment analysis.The framework makes the systematic study and comparison of corpora easier, andidentifies a number of features that can be of help to characterize them. In Sect. 3,",
        "publication_date": "2019-05-31",
        "authors": "María Navas-Loro, Victor Rodrı́guez-Doncel",
        "file_name": "s10579-019-09470-8.pdf",
        "file_path": "./PDFs/s10579-019-09470-8.pdf"
    },
    {
        "title": "GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/1-s2.0-S1570826820300354-main.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All the resources described in this section are available online.10 3.1."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2020.100596",
        "publication_date": "2020-08-08",
        "authors": "David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, Óscar Corcho",
        "file_name": "1-s2.0-S1570826820300354-main.pdf",
        "file_path": "./PDFs/1-s2.0-S1570826820300354-main.pdf"
    },
    {
        "title": "Introduction to the Special Issue “Artificial Intelligence Knowledge Representation”",
        "implementation_urls": [],
        "doi": "10.3390/systems7030035",
        "publication_date": "2019-07-22",
        "authors": "Paola Di Maio, Mari Carmen Suárez-Figueroa",
        "file_name": "no_doi_20250624164500.pdf",
        "file_path": "./PDFs/no_doi_20250624164500.pdf",
        "pdf_link": "https://www.mdpi.com/2079-8954/7/3/35/pdf?version=1563965423"
    },
    {
        "title": "Machine Learning-based Query Augmentation for SPARQL Endpoints",
        "implementation_urls": [],
        "doi": "10.5220/0006925300570067",
        "abstract": "Abstract:Linked Data repositories have become a popular source of publicly-available data. Users accessingthis data through SPARQL endpoints usually launch several restrictive yet similar consecutivequeries, either to find the information they need through trial-and-error or to query related re-sources. However, instead of executing each individual query separately, query augmentation aimsat modifying the incoming queries to retrieve more data that is potentially relevant to subsequentrequests. In this paper, we propose a novel approach to query augmentation for SPARQL end-points based on machine learning. Our approach separates the structure of the query from itscontents and measures two types of similarity, which are then used to predict the structure andcontents of the augmented query. We test the approach on the real-world query logs of the Spanishand English DBpedia and show that our approach yields high-accuracy prediction. We also showthat, by caching the results of the predicted augmented queries, we can retrieve data relevant toseveral subsequent queries at once, achieving a higher cache hit rate than previous approaches.1 INTRODUCTIONLinked Data repositories have grown to providea wealth of publicly-available data, with somerepositories containing millions of concepts de-scribed by RDF triples (e.g. DBpedia1, FOAF2,GeoNames3). Users access the data in theserepositories through public SPARQL endpointsthat allow them to issue SPARQL queries, thestandard query language for RDF stores. Consec-utive queries received from the same client usuallyexhibit some patterns, such as querying identicalor similar resources than previous queries.Caching query results was first proposed tokeep recently retrieved data in a memory cachefor use with later queries (Dar et al., 1996; Mar-tin et al., 2010; Yang and Wu, 2011). However,caching only works if the exact same data is ac-cessed multiple times. In reality, it is more com-1DBpedia: https://wiki.dbpedia.org/2FOAF: http://www.foaf-project.org/3GeoNames: http://www.geonames.org/mon to have similar consecutive queries that re-trieve related resources from the repository (Boni-fati et al., 2017; Mario et al., 2011). Queryaugmentation takes advantage of this fact, re-trieving data that will potentially be used by fu-ture queries before the queries are received bythe SPARQL endopint. Previous approaches toquery augmentation are divided into two maincategories, (1) techniques based on informationfound in the data source, and (2) techniquesbased on analysis of previous (historic) queries,as discussed in section 2.In this paper, we present an approach to queryaugmentation for SPARQL endpoints based ondetecting recurring patterns in historic querylogs. The novelty of our approach is that we",
        "publication_date": "2018-01-01",
        "authors": "Mariano Rico, Rizkallah Touma, Anna Queralt, Marı́a S. Pérez",
        "file_name": "10!5220%0006925300570067.pdf",
        "file_path": "./PDFs/10!5220%0006925300570067.pdf"
    },
    {
        "title": "Semantic conflation in GIScience: a systematic review",
        "implementation_urls": [],
        "doi": "10.1080/15230406.2021.1952109",
        "abstract": "ABSTRACTManifold providers from a wide range of initiatives (private organizations, volunteered efforts, social media, etc.) offer enormous data amounts with geospatial characteristics. These efforts of many data providers entail multiple data scenarios and imply many viewpoints about the same feature, involving different representations, accuracy, models, vocabularies, etc. Various techniques or processes are employed to deal with these heterogeneity problems related to diverse data sources within the conflation research area. However, semantic conflation has not been addressed widely in the literature, unlike geometrical conflation. Hence, it is unclear what issues semantic conflation tries to solve and what activities, methods, metrics, and techniques have been used in existing GIScience investigations. In this article, we carry out a systematic review of approaches that focus on semantic aspects for geospatial data conflation. Besides, we analyze a wide selection of contributions following different criteria to depict a detailed semantic conflation status in GIScience. Our contributions are: (i) an overview of semantic conflation application domains, (ii) a characterization of semantic issues within these domains, (iii) the recognition of gaps and weaknesses of collected researches, and (iv) several open challenges and opportunities for next steps in this GIScience research area.ARTICLE HISTORY Received 20 January 2021  Accepted 2 July 2021 KEYWORDS Conflation; semantics; systematic review; application domains; activities1. IntroductionCurrently, users have access to enormous amounts of data from multiple providers associated with a wide range of initiatives with different characteristics (private organizations, volunteered efforts, social media, etc.). These have become a relevant stream in collecting geo- related data (Salk et al., 2016) because of Web 2.0, mobile devices, citizen participation, and crowdsourcing, and they contribute to official and governmental organizations’ efforts. This context subsequently leads to more prolific data scenarios, increasing volume, variety, and heterogeneity in spatial and non-spatial data sources (Abbaspour et al., 2018; Guo et al., 2017).Simultaneously, as an additional consequence of this scenario, many data providers’ presence implies several aspects of the same (geospatial) feature (Xavier et al., 2016). Therefore, objects of the same entity may have different representations, differ in accuracy and density, use diverse data models, or be encoded using ad-hoc vocabularies (Abbaspour et al., 2018; Ruiz et al., 2011; Sheeren et al., 2009; Vilches-Blázquez et al., 2014). The term used in this context is conflation. It is understood as “the process of unifying two or more separate datasets, which share certain characteristics, into one integrated all-encompassing result” (Rieke & Pross, 2013). Nevertheless, the identification of similar objects or ",
        "publication_date": "2021-08-24",
        "authors": "Luis M. Vilches‐Blázquez, José Ángel Ramos",
        "file_name": "GIScienceareview.pdf",
        "file_path": "./PDFs/GIScienceareview.pdf"
    },
    {
        "title": "Type Prediction of RDF Knowledge Graphs Using Binary Classifiers with Structural Data",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03056-8_27",
        "abstract": "Abstract. Type information, which is useful for responding manyqueries, plays a key role in Semantic Web. Nevertheless, it is common thattype information of some instances is not present in knowledge graphs.Thus, type prediction of a given instance using background knowledge isan important knowledge graph completion task. To this end, the objec-tive of this paper is to propose a data-driven type prediction approachusing the structural information of the given instance utilising machinelearning techniques. The experiments presented in the paper demon-strate that a binary classifier with structural information as features canbe effectively used for type prediction of RDF knowledge graphs withhigh accuracy. The accuracy of the classifier is related to the diversityof training data as well as the how conceptually similar are the differentclasses in the training and test data. Further, the experiments demon-strate that it is possible to build universal classifiers to a given class,i.e., a model training on one dataset can produce good predictions foranother dataset in cases where training data contains conceptually dif-ferent classes. For example, a model training on the English DBpediacan be used to predict types of the Spanish DBpedia.Keywords: Linked Data · RDF · Knowledge graph · Data quality1 IntroductionIn the Semantic Web vision paper [1], Berners-Lee et al. identify the value of hav-ing access to structured collections of information with their associated explicitsemantics and inference rules that can be used for automated reasoning. Seman-tic Web has come a long way since then and a large number of Semantic Webstandards have been developed and a huge volume of data is available as a partof the Linked (Open) Cloud. Such data is generally expressed using ontologies orvocabularies and is represented using the RDF model. We refer to such datasetsas knowledge graphs (KGs) from here onwards.Naming things using globally identifiable URIs and providing their type infor-mation play an important role in making the data on the Semantic Web discov-erable and reusable. For instance, such information allows a KG to answer thec© Springer Nature Switzerland AG 2018C. Pautasso et al. (Eds.): ICWE 2018, LNCS 11153, pp. 279–287, 2018.https://doi.org/10.1007/978-3-030-03056-8_27http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-03056-8_27&domain=pdfhttps://doi.org/10.1007/978-3-030-03056-8_27280 N. Mihindukulasooriya and M. Ricoquery, “What is La Tomatina?” by looking at its type declaration. Further, suchinformation enables exploratory queries such as “Find all scientists in a givenknowledge graph”. For instance, if a knowledge base contains a type declara-tion e.g. Albert Einstein is a Scientist, this will allow him to be included in theaforementioned query. Nevertheless, type information is not always available forall instances in a KG due to various reasons such as deficiencies in the datageneration or transformation process (e.g., lack of mappings) or deficiencies insource data (e.g., missing type information).In the Linked Open Data cloud, DBpedia [2] stands out as the central hubof Linked Open Data (LOD) because it provides a vast amount of informationand most other datasets in the LOD cloud link to DBpedia. DBpedia data isextracted from Wikipedia and transformed to RDF using the mappings. Whilebeing a useful source of information containing more than 6.6 million entities",
        "publication_date": "2018-01-01",
        "authors": "Nandana Mihindukulasooriya, Mariano Rico",
        "file_name": "978-3-030-03056-8_27.pdf",
        "file_path": "./PDFs/978-3-030-03056-8_27.pdf"
    },
    {
        "title": "The impact of NFT profile pictures within social network communities",
        "implementation_urls": [],
        "doi": "10.1145/3524458.3547230",
        "arxiv": "2206.06443",
        "publication_date": "2022-08-23",
        "authors": "Simone Casale-Brunet, Mirko Zichichi, Lee Hutchinson, Marco Mattavelli, Stefano Ferretti",
        "file_name": "10!1145%3524458!3547230.pdf",
        "file_path": "./PDFs/10!1145%3524458!3547230.pdf"
    },
    {
        "title": "Multilayered Linked Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_3",
        "abstract": "Abstract Although confidence in democracy to tackle societal problems is falling,new civic participation tools are appearing supported by modern ICT technologies.These tools implicitly assume different views on democracy and citizenship whichhave not been fully analysed, but their main fault is their isolated operation innon-communicated silos. We can conceive public knowledge, like in Karl Popper’sWorld 3, as distributed and connected in different layers and by different connec-tors, much as it happens with the information in the web or the data in the linkeddata cloud. The interaction between people, technology and data is still to bedefined before alternative institutions are founded, but the so called linkeddemocracy should rest on different layers of interaction: linked data, linked plat-forms and linked ecosystems; a robust connectivity between democratic institutionsis fundamental in order to enhance the way knowledge circulates and collectivedecisions are made.Keywords Linked democracy � Multilayered linked democracy � Linked data �Linked platforms � Linked ecosystems � World 3 � Institutions3.1 IntroductionContemporary democracies face growing scepticism about their capacity to managecomplex societal problems. Financial crises, inequality and poverty, climate changeand armed conflicts routinely test the resilience of our democratic systems.Researchers are predominantly expressing concern about the developments of thelast decade. Larry Diamond draws from Freedom House data to argue that we are ina ‘mild but protracted democratic recession’ since 2006 (Diamond 2015, 144).Roberto Foa and Yascha Mounk analyse World Values Surveys to conclude thatcitizens in Western democracies have ‘become more cynical about the value of© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_351http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_3democracy as a political system, less hopeful that anything they do might influencepublic policy, and more willing to express support for authoritarian alternatives’(Foa and Mounk 2016, 7). John Boik et al. warn that traditional democratic insti-tutions are failing and that ‘the versions of democracy attempted by newlydemocratizing nations have been even less effective’ (Boik et al. 2015). Globally,voter turnout—a standard proxy to measure citizens’ satisfaction with democraticinstitutions—has been steadily but consistently declining since the 1960s (IDEAInternational 2016).This sceptical outlook coexists with some unprecedented technology trends: by2020, about 1.7 megabytes of new information will be created every second, forevery human being (Forbes 2015); there will be more mobile phone subscriptionsthan people on the planet and more than 6 billion of these devices will be smart-phones (ITU 2015). Digital technologies not only disrupt business models, theynow shape the way we access information, knowledge, and increasingly, the waywe exercise our rights. In doing so, they also transform civic action and enable newforms of citizenship.Political science, media and culture studies, and ICT disciplines have alreadyproduced a vast literature on civic participation online (e.g., see meta-analysis by",
        "publication_date": "2019-01-01",
        "authors": "Marta Poblet, Pompeu Casanovas, Victor Rodrı́guez-Doncel",
        "file_name": "no_doi_20250624164541.pdf",
        "file_path": "./PDFs/no_doi_20250624164541.pdf",
        "pdf_link": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-13363-4_3.pdf"
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "publication_date": "2020-11-24",
        "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D’Angelo, Victor Rodrı́guez-Doncel",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": "./PDFs/10!1109%nca51143!2020!9306721.pdf"
    },
    {
        "title": "Multi-label Text Classification for Public Procurement in Spanish",
        "implementation_urls": [],
        "doi": "10.26342/2022-69-6",
        "abstract": "Abstract: Public procurement accounts for a 14% of the annual budget of thedifferent governments of the European Union. In Europe, contracting processes areclassified using Common Procurement Vocabulary codes (CPVs), a taxonomy de-signed to facilitate statistical reporting, search and the creation of alerts that canbe used by potential bidders. CPVs are commonly assigned manually by publicemployees in charge of contracting processes. However, CPV classification is not atrivial task, as there are more than 9,000 different CPV categories, which are oftenassigned following heterogeneous criteria. In this paper we have created a CPVclassifier that uses as an input the textual description of the contracting process,and assigns CPVs from the 45 top-level CPV categories. We work only with textsin Spanish, although our approach may be easily extended to other languages. Ourresults improve the state of the art (10% F1-score improvement) and are availableonline.Keywords: CPV, Multi-label Classification, Public Procurement, HierarchicalClassification.Resumen: Las licitaciones públicas suponen el 14% del presupuesto anual de laUnión Europea. En Europa, los procesos de contratación se clasifican usando lataxonomı́a Common Procurement Vocabulary (CPVs), diseñada para facilitar lageneración de estad́ısticas, las búsquedas y la creación de alertas que puedan uti-lizar los posibles licitadores. Los códigos CPV suelen ser asignados manualmentepor los empleados públicos encargados del proceso de contratación. Sin embargo, laclasificación de textos de acuerdo con estos códigos no es trivial, pues existen más de9000 CPVs y no siempre se siguen los mismos criterios para su asignación. En esteart́ıculo se propone un clasificador que utiliza como entrada la descripción textualdel proceso de contratación, y produce códigos de entre las 45 categoŕıas de CPVmás generales de la jerarqúıa. Trabajamos sólo con textos en español, aunque nue-stro enfoque puede extenderse fácilmente a otros idiomas. Los resultados obtenidossuperan el estado del arte (10% de mejora en F1), y se encuentran disponibles online.Palabras clave: CPV, Clasificación Multi-etiqueta, Licitaciones Públicas, Clasifi-cación Jerárquica.1 IntroductionPublic authorities in the European Unionspend around 14% of the yearly Gross Do-mestic Product (around 2 trillion euros) pur-chasing services, utilities and supplies.1 Ac-cess to this data is crucial for enabling a sin-gle digital market in Europe, as well as for ac-countability and transparency. Hence manygovernments provide this data in their open1https://ec.europa.eu/growth/single-market/public-procurement_endata portals as well as in data.europa.eu,and a number of platforms have been de-veloped to improve both the efficiency andtransparency in public procurement2 (Soyluet al., 2022).Common Procurement Vocabulary codes(CPVs)3 help classify public procurementprocesses in the European Union across dif-2https://opentender.eu/es/about/",
        "publication_date": "2022-09-01",
        "authors": "Marıa Navas-Loro, Daniel Garijo, Oscar Corcho",
        "file_name": "PLN_69_06.pdf",
        "file_path": "./PDFs/PLN_69_06.pdf"
    },
    {
        "title": "A scoping review on the use, processing and fusion of geographic data in virtual assistants",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/10!1111%tgis!12720.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [5] M."
                    }
                ]
            }
        ],
        "doi": "10.1111/tgis.12720",
        "abstract": "Abstract—This paper presents an architecture of a PersonalInformation Management System, in which individuals can definethe access to their personal data by means of smart contracts.These smart contracts, running on the Ethereum blockchain,implement access control lists and grant immutability, traceabil-ity and verifiability of the references to personal data, which isstored itself in a (possibly distributed) file system. A distributedauthorization mechanism is devised, where trust from multiplenetwork nodes is necessary to grant the access to the data.To this aim, two possible alternatives are described: a SecretSharing scheme and Threshold Proxy Re-Encryption scheme. Theperformance of these alternatives is experimentally comparedin terms of execution time. Threshold Proxy Re-Encryptionappears to be faster in different scenarios, in particular whenincreasing message size, number of nodes and the threshold value,i.e. number of nodes needed to grant the data disclosure.I. INTRODUCTIONThe transformation introduced by digital technologies hashad (and is having) a significant impact on economy and soci-ety. Data is at the heart of this transformation and individualsare the main sources generating more and more of it. There isan urgent need to place (again) individuals at the center and torelieve the absence of technical instruments and standards thatmake the exercise of one’s rights simple and not excessivelyburdensome [1], [2]. The EU’s GDPR 1 helps to promote thisvision and at the same time seeks to pave the way for opendata spaces for the social and economic good 2.Our aim is to seek such a technology by enabling userswith the sovereignty over their data, while guaranteeing itsconfidentiality. In our view, the data owner can define accessby limiting the scope of data utility, delegating these privilegesor giving up ownership completely, without the need to relyon (un)trusted entities to facilitate this task. The developmentof a Personal Information Management System (PIMS) 3 that∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - RIoE.1Council of European Union, Regulation 2016/679 - directive 95/462European Commission, COM(2020) 66, “A European strategy for data”3European Data Protection Supervisors, Opinion 9/2016, “EDPS Opinionon Personal Information Management Systems”fulfils these goals can be based on a distributed softwarearchitecture, where each individual is associated to a digitalspace containing personal data. This space will be used toattend the data access requests coming from data providersand data consumers. Distributed Ledger Technologies (DLT)and Decentralized File Storages (DFS) combination providesa range of features suitable for data management and sharing,such as transparency, immutability and reliability [2], [3].",
        "publication_date": "2021-01-05",
        "authors": "Carlos Granell, Paola Pesántez-Cabrera, Luis M. Vilches‐Blázquez, Rosario Achig, Miguel R. Luaces, Alejandro Cortiñas‐Álvarez, Carolina Chayle, Villie Morocho",
        "file_name": "10!1111%tgis!12720.pdf",
        "file_path": "./PDFs/10!1111%tgis!12720.pdf"
    },
    {
        "title": "Semantic Interoperability for DR Schemes Employing the SGAM Framework",
        "implementation_urls": [],
        "doi": "10.1109/sest48500.2020.9203338",
        "abstract": "Abstract—Demand Response (DR) systems are gaining mo-mentum in the EU energy markets albeit based on fragmentedstandards that, as a result, hinder interoperability. These discrep-ancies necessitate the introduction of a semantically enrichedumbrella framework that will allow DR systems to exchangeand consume data transparently, an issue that is currentlyunaddressed. Furthermore, to support semantically interoperableDR architectures, a multi-layer compliance testing framework isrequired that will examine and quantify the technical, syntacticand semantic properties of individual DR systems. In this work,the aforementioned gaps in the literature are addressed by, first,introducing an OpenADR-based semantic enrichment compo-nent. According to the guidelines of the Smart Grid ArchitectureModel (SGAM) framework, a concrete evaluation procedure ofthis component is presented, which allows for a step-by-stepsyntactic and semantic testing. Following the identification ofthe instruments composing the testbed and the equipment/linksunder test at SGAM’s communication and information layers, theBasic Application Interoperability Profiles (BAIOPs) are definedand their involved steps are described. Experiments demonstratethe validity of the presented methodology, while also evaluatingthe introduced component.Keywords—Semantic Interoperability, Demand Response,Smart Grids, W3C, SGAMI. INTRODUCTIONElectricity grids are undergoing radical transformationsdriven by policies that mandate the smooth, yet constantlyincreased integration of Renewable Energy Sources (RES) andthe full-blown inclusion of end-customers in energy markets.These policies impose a new mode of operation for smartgrids that revolves around decentralised electricity generationwhere consumers will also act as producers (prosumers). Inthis modern operational paradigm, Demand Response (DR)is recognised at a global level [1], [2] as a key enabler ofdecentralised demand-side flexibility management.The fragmentation of standards endowed for building appli-ances, building and/or energy management systems, as wellas, marketplaces, has imposed severe roadblocks in the large-scale deployment of DR services, especially with the expo-nential growth of Internet of Things (IoT) solutions that areavailable in the markets. Relying on energy-related standards,e.g., USEF [3] or SAREF [4], different frameworks havebeen proposed to deliver semantic interoperable architectures,which allow the transparent exchange and consumption ofinformation amongst multiple smart grid layers.Nevertheless, the focus of those frameworks has been allo-cated only at the infrastructure level and not on the informationexchange level. In addition, since numerous standards lacksemantics, as defined by the W3C (e.g., OpenADR [5]), suchendeavours become even more cumbersome. The significance",
        "publication_date": "2020-09-01",
        "authors": "Andrea Cimmino, Nikoleta Andreadou, Alba Fernández-Izquierdo, Christos Patsonakis, Apostolos C. Tsolakis, Alexandre Lucas, Dimosthenis Ioannidis, Evangelos Kotsakis, Dimitrios Tzovaras, Raúl García‐Castro",
        "file_name": "Semantic_Interoperability_for_DR_Schemes_Employing_the_SGAM_Framework.pdf",
        "file_path": "./PDFs/Semantic_Interoperability_for_DR_Schemes_Employing_the_SGAM_Framework.pdf"
    },
    {
        "title": "A SAREF Extension for Semantic Interoperability in the Industry and Manufacturing Domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/idafensp/ar2dtool",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "./PDFs/SAREF4INMA.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "More details related to LOT are available online in its website.12 The following sections present the main definitions and guidelines provided by the methodology for each of the above-mentioned activities."
                    }
                ]
            }
        ],
        "doi": "10.1002/9781119564034.ch25",
        "abstract": "Abstract. The IoT landscape is characterized by a fragmentation of standards, platforms and technologies, often scattered amongdifferent vertical domains. To prevent the market to continue to be fragmented and power-less, a protocol-independent semanticlayer can serve as enabler of interoperability among the various smart devices from different manufacturers that co-exist in aspecific industry domain, but also across different domains. To that end, the SAREF ontology was created in 2015 with theintention to interconnect data, enabling the communication between IoT devices that use different protocols and standards. Anumber of industrial sectors consequently expressed their interest to extend SAREF into their domains in order to fill the gaps ofthe semantics not yet covered by their communication protocols. Therefore, the SAREF4INMA ontology was recently createdto extend SAREF for describing the Smart Industry & Manufacturing domain. SAREF4INMA is based on several standards andIoT initiatives, as well as on real use cases, and includes classes, properties and instances specifically created to cover the industryand manufacturing domain. This work describes the approach followed to develop this ontology, specifies its requirements andalso includes a practical example of how to use it.Keywords: industry 4.0, ontology, standard, SAREF, SAREF4INMA1. IntroductionThis paper presents the resulting model after extending the Smart Applications REFerence ontology (SAREF)for the Industry & Manufacturing domain1 together with the methodology followed and modelling decisions takenduring the development. This paper builds on the success achieved in the past years with SAREF2, which is areference ontology for IoT created in close interaction with the industry [1] during a study requested by the EuropeanCommission in 20153. SAREF is published as an ETSI Technical Specification series that also includes dedicatedextensions to specific domains (TS 103 410, parts 1-6). A proof-of-concept solution based on SAREF in the energydomain and implemented on existing commercial products4 was demonstrated in 2017 [2].The motivation behind SAREF is that the IoT landscape is characterized by a fragmentation of standards, plat-forms and technologies, often scattered among different vertical domains [3, 4] . To prevent the market to continue*Corresponding author. E-mail: mike.deroode@tno.nl.1https://portal.etsi.org/STF/stfs/STFHomePages/STF5342https://ec.europa.eu/digital-single-market/en/blog/new-standard-smart-appliances-smart-home3https://sites.google.com/site/smartappliancesproject4https://ec.europa.eu/digital-single-market/en/news/digitalising-energy-sector-common-language-consumer-centric-world1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:mike.deroode@tno.nlmailto:laura.daniele@tno.nlmailto:albafernandez@fi.upm.esmailto:mpoveda@fi.upm.esmailto:rgarcia@fi.upm.esmailto:mike.deroode@tno.nl2 M.A.W. de Roode et al. / SAREF for Industry and Manufacturing1 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 15",
        "publication_date": "2018-10-26",
        "authors": "Laura Daniele, Matthijs Punter, Christopher Brewster, Raúl García Castro, María Poveda, Alba Fernández",
        "file_name": "SAREF4INMA.pdf",
        "file_path": "./PDFs/SAREF4INMA.pdf"
    },
    {
        "title": "First Attempt to an Easy-to-Read Adaptation of Repetitions in Captions",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-08648-9_48",
        "abstract": "Abstract. Subtitles of audiovisual content produced in the same lan-guage as the oral discourse are called captions. Such type of subtitles is crucial to ensure that audiovisual resources have inclusive and equal access for people with functional diversity. When talking about people with reading comprehension difficulties, captions must be written in easy reading. During the subtitling process, it is important to bear in mind that the oral mode includes some unique characteristics such as the use of punctual reiterations or redundancies. However, excessive repetition in the written mode slows down reading and makes it difficult to under-stand. Currently, repetition is not considered as a problematic aspect in the Easy-to-Read (E2R) Methodology, since this linguistic resource is not frequent in the written mode. Despite this, we believe that some fea-tures of the oral mode, such as repetitions, should be considered within the captioning process. Hence, our current research is focused on discov-ering whether captions with repetitions, coming from the oral mode, can be a problem for people with cognitive disabilities. To achieve such a goal, we performed a user study whose main goal is to find out whether people with reading comprehension difficulties prefer audiovisual cap-tions with or without repetitions. Initial findings indicate that captions without repetitions are the most preferred ones. For this reason, we have also created a method for automatically adapting repetitions in captions following an E2R approach. Keywords: Easy-to-Read methodology · Lexical repetitions · Captions 1 Introduction One way of achieving accessibility in audiovisual resources, such as video con-tent, live streams or video conferencing, is to provide captions or intralinguistic subtitles. According to the W3C Web Accessibility Initiative, captions are a text �c Springer Nature Switzerland AG 2022 K. Miesenberger et al. (Eds.): ICCHP-AAATE 2022, LNCS 13341, pp. 417–424, 2022. https://doi.org/10.1007/978-3-031-08648-9_48 http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-08648-9_48&domain=pdfhttp://orcid.org/0000-0003-3807-5019http://orcid.org/0000-0002-3967-0672https://doi.org/10.1007/978-3-031-08648-9_48mailto:jrivero@fundaciononce.esmailto:a.gsanz@alumnos.upm.esmailto:isam.diab@upm.esmailto:mcsuarez@fi.upm.es418 M. C. Suárez-Figueroa et al. version of the speech and non-speech audio information needed to understand the content1 , mainly by people who are deaf and hard-of-hearing. During the subtitling process a balance between text cohesion and text length is needed. There are two main strategies to accomplish such a balance [12]: (a) the reduction of linguistic redundancy to a minimum and (b) the omission of certain elements which are not essential for understanding the message. Both strategies refer to the so-called “intrasemiotic redundancy” [11], and are related to spoken language features, such as repetitions and false starts. According to [1], any changes performed during the subtitling process must ensure the coherence and final understanding of the subtitling. ",
        "publication_date": "2022-01-01",
        "authors": "Mari Carmen Suárez-Figueroa, Isam Diab, Álvaro González, Jesica Rivero-Espinosa",
        "file_name": "978-3-031-08648-9_48.pdf",
        "file_path": "./PDFs/978-3-031-08648-9_48.pdf"
    },
    {
        "title": "Towards a new generation of ontology based data access",
        "implementation_urls": [],
        "doi": "10.3233/sw-190384",
        "abstract": "Abstract. Ontology Based Data Access (OBDA) refers to a range of techniques, algorithms and systems that can be used todeal with the heterogeneity of data that is common inside many organisations as well as in inter-organisational settings andmore openly on the Web. In OBDA, ontologies are used to provide a global view over multiple local datasets; and mappings arecommonly used to describe the relationships between such global and local schemas. Since its inception, this area has evolvedin several directions. Initially, the focus was on the translation of original sources into a global schema, and its materialisation,including non-OBDA approaches such as the use of Extract Transform Load (ETL) workflows in data warehouses and, morerecently, in data lakes. Then OBDA-based query translation techniques, relying on mappings, were proposed, with the aim ofremoving the need for materialisation, something especially useful for very dynamic data sources. We think that we are nowwitnessing the emergence of a new generation of OBDA approaches. It is driven by the fact that a new set of declarative mappinglanguages, most of which stem from the W3C Recommendation R2RML for Relational Databases (RDB), are being created.In this vision paper, we enumerate the reasons why new mapping languages are being introduced. We discuss why it may berelevant to work on translations among them, so as to benefit from the engines associated to each of them whenever one languageand/or engine is more suitable than another. We discuss the emerging concept of “mapping translation”, the basis for this newgeneration of OBDA, together with some of its desirable properties: information preservation and query result preservation. Weshow several scenarios where mapping translation can be or is being already applied, even though this term has not necessarilybeen used in existing literature.Keywords: OBDA, data translation, query translation, mapping translation1. IntroductionDatabase technologies play a vital role in the devel-opment of information systems for all sorts of organi-sations. So far, relational databases (RDB) are still thedominating type of structure and technology used fordata management inside organisations, although otherformats (e.g. JSON, spreadsheets, XML) and typesof databases (e.g. noSQL, graph databases) have alsoemerged as alternatives for data representation andmanagement in the last decades.*Corresponding author. E-mail: ocorcho@fi.upm.es.In the early days of information system devel-opment, it was natural for organisations to developtheir own data models, which were strongly alignedwith their activities. This led to a large heterogene-ity across organisations, and even across different de-partments inside the same organisation. Such hetero-geneity was especially evident in the case of organ-isational changes, merges, etc. Similarly, data ware-houses were also used in order to align and materialisedata from different sources, normally from the sameorganisation, so as to provide support for analyticalqueries and for the generation of reports. These situ-ations made researchers and professionals start work-1570-0844/20/$35.00 © 2020 – IOS Press and the authors. All rights reservedmailto:ocorcho@fi.upm.esmailto:fpriyatna@fi.upm.esmailto:dchaves@fi.upm.esmailto:ocorcho@fi.upm.eshttp://crossmark.crossref.org/dialog/?doi=10.3233%2FSW-190384&domain=pdf&date_stamp=2019-12-31154 O. Corcho et al. / Towards a new generation of ontology based data accessing on solutions for data integration, where data fromseveral sources needed to be accessible according to a",
        "publication_date": "2019-12-31",
        "authors": "Óscar Corcho, Freddy Priyatna, David Chaves-Fraga",
        "file_name": "10!3233%sw-190384.pdf",
        "file_path": "./PDFs/10!3233%sw-190384.pdf"
    },
    {
        "title": "Towards Blockchain and Semantic Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-36691-9_19",
        "abstract": "Abstract. Blockchain has become a pervasive technology in a widenumber of sectors like industry, research, and academy. In the last decadea large number of tailored-domain problems have been solved thanks tothe blockchain. Due to this reason, researchers expressed their interestin combining the blockchain with other well-known technologies, likeSemantic Web. Unfortunately, as far as we known, in the literature noone has presented the different scenarios in which Semantic Web andblockchain can be combined, and the further benefits for both. In thispaper, we aim a providing an in-depth view of the beneficial symbi-otic relation that these technologies may reach together and report thedifferent scenarios that we have identified in the literature to combineSemantic Web and blockchain.Keywords: Blockchain · Semantic Web · Semantic blockchain1 IntroductionIn the last decade the blockchain technologies have become a pervasive in ourworld [1]. Sectors like finance, security, IoT, or public services have benefited fromthe quantum leap that block chain has brought [2]. The wide range of domainsin which this technology has been used has led researchers to elicit and analysethe problems and challenges related to the use of blockchain technologies [3].One of the interests that researchers have shown lately is to combine theSemantic Web and blockchain technologies [4,4,5]. The reason of this interestrelies on the symbiotic relationship that enhances both technologies, and thepotential that can be reached combining them [6]. As far as we known currentliterature focuses mainly in applications that rely on blockchain and SemanticWeb, with the exception of English et al. who presented the only article thatanalyses the benefits of combining these technologies [7]. The work of Englishet al. provides an overview of what the Semantic Web can do for the blockchainand vice-versa, nevertheless their work focuses on covering a large number oftopics, and not only the benefits, and thus, they lack of an in-depth analysis ofthe benefits and the scenarios in which both technologies are combined.In this paper we aim to extend the work of English et al. [7], by providingan in-depth analysis of the benefits that blockchain may find by relying onSemantic Web and vice-versa. In addition, our goal is to provide an overviewc© Springer Nature Switzerland AG 2019W. Abramowicz and R. Corchuelo (Eds.): BIS 2019 Workshops, LNBIP 373, pp. 220–231, 2019.https://doi.org/10.1007/978-3-030-36691-9_19http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-36691-9_19&domain=pdfhttps://doi.org/10.1007/978-3-030-36691-9_19Towards Blockchain and Semantic Web 221of the different scenarios and approaches to combine blockchain with SemanticData by analysing the advantages and disadvantages of the different scenarios.The rest of this article is organised as follows: Sect. 2 introduces the keyconcepts of the blockchain and the Semantic Web. Thirdly, Sect. 3 presents thedifferent benefits that both technologies may offer to the other. After that, Sect. 4introduces the different scenarios that we have identified to combine blockchainand Semantic Web. Finally, Sect. 5 recaps our conclusions.2 PreliminariesIn this section we aim at introducing the key-concepts of the blockchain andthe Semantic Web, as well as, the main characteristics of both. Our goal is notto provide an in-depth description, instead we aim at describing only the key-",
        "publication_date": "2019-01-01",
        "authors": "Juan Cano-Benito, Andrea Cimmino, Raúl García‐Castro",
        "file_name": "Towards-blockchain-and-semantic-web.pdf",
        "file_path": "./PDFs/Towards-blockchain-and-semantic-web.pdf"
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "publication_date": "2021-05-07",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": "./PDFs/10!3233%ssw51.pdf"
    }
]